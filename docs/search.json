[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Ancient Metagenomics",
    "section": "",
    "text": "Introduction\nAncient metagenomics applies cutting-edge metagenomic methods to the degraded DNA content of archaeological and palaeontological specimens. The rapidly growing field is currently uncovering a wealth of novel information for both human and natural history, from identifying the causes of devastating pandemics such as the Black Death, to revealing how past ecosystems changed in response to long-term climatic and anthropogenic change, to reconstructing the microbiomes of extinct human relatives. However, as the field grows, the techniques, methods, and workflows used to analyse such data are rapidly changing and improving.\nIn this book we will go through the main steps of ancient metagenomic bioinformatic workflows, familiarising students with the command line, demonstrating how to process next-generation-sequencing (NGS) data, and showing how to perform de novo metagenomic assembly. Focusing on host-associated ancient metagenomics, the book consists of a combination of theory and hands-on exercises, allowing readers to become familiar with the types of questions and data researchers work with.\nBy the end of the textbook, readers will have an understanding of how to effectively carry out the major bioinformatic components of an ancient metagenomic project in an open and transparent manner.\n\n\n\n\n\n\nNote\n\n\n\nIf you export the PDF or ePub versions of this book, some sections maybe excluded (such as videos, and embedded slide decks). Always refer to this website in doubt.\n\n\nAll material was originally developed for the SPAAM Summer School: Introduction to Ancient Metagenomics"
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "The creation of this text book was developed through a series of …\n\n\n\n\n\n\n\n\n2022\n\n🇬🇧 James Fellows Yates is an archaeology-trained biomolecular archaeologist and convert to palaeogenomics, and is recently pivoting to bioinformatics. He specialises in ancient metagenomics analysis, generating tools and high-throughput approaches and high-quality pipelines for validating and analysing ancient (oral) microbiomes and palaeogenomic data.\n\n\n2022\n\n🇺🇸 Christina Warinner is Group Leader of Microbiome Sciences at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and Associate Professor of Anthropology at Harvard University. She serves on the Leadership Team of the Max Planck-Harvard Research Center for the Archaeoscience of the Ancient Mediterranean (MHAAM), and is a Professor in the Faculty of Biological Sciences at Friedrich Schiller University in Jena, Germany. Her research focuses on the use of metagenomics and paleoproteomics to better understand past human diet, health, and the evolution of the human microbiome.\n\n\n2022\n\n🇪🇸 Aida Andrades Valtueña is a geneticist interested in pathogen evolution, with particular interest in prehistoric pathogens. She has been exploring new methods to analyse ancient pathogen data to understand their past function and ecology to inform models of pathogen emergence.\n\n\n2022\n\n🇩🇪 Alexander Herbig is a bioinformatician and group leader for Computational Pathogenomics at the Max Planck Institute for Evolutionary Anthropology. His main interest is in studying the evolution of human pathogens and in methods development for pathogen detection and bacterial genomics.\n\n\n2022\n\n🇩🇪 Alex Hübner is a computational biologist, who originally studied biotechnology, before switching to evolutionary biology during his PhD. For his postdoc in the Warinner lab, he focuses on investigating whether new methods in the field of modern metagenomics can be directly applied to ancient DNA data. Here, he is particularly interested in the de novo assembly of ancient metagenomic sequencing data and the subsequent analysis of its results.\n\n\n2022\n\n🇩🇪 Alina Hiss is a PhD student in the Computational Pathogenomics group at the Max Planck Institute for Evolutionary Anthropology. She is interested in the evolution of human pathogens and working on material from the Carpathian basin to gain insights about the presence and spread of pathogens in the region during the Early Medieval period.\n\n\n2022\n\n🇫🇷 Arthur Kocher initially trained as a veterinarian. He then pursued a PhD in the field of disease ecology, during which he studied the impact of biodiversity changes on the transmission of zoonotic diseases using molecular tools such as DNA metabarcoding. During his Post-Docs, he extended his research focus to evolutionary aspects of pathogens, which he currently investigates using ancient genomic data and Bayesian phylogenetics.\n\n\n2022\n\n🇩🇪 Clemens Schmid is a computational archaeologist pursuing a PhD in the group of Stephan Schiffels at the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. He is trained both in archaeology and computer science and currently develops computational methods for the spatiotemporal co-analysis of archaeological and ancient genomic data. He worked in research projects on the European Neolithic, Copper and Bronze age and maintains research software in R, C++ and Haskell.\n\n\n2022\n\n🇺🇸 Irina Velsko is a postdoc in the Microbiome group of the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. She did her PhD work on oral microbiology and immunology of the living, and now works on oral microbiomes of the living and the dead. Her work focuses on the evolution and ecology of dental plaque biofilms, both modern and ancient, and the complex interplay between microbiomes and their hosts.\n\n\n2022\n\n🇫🇷 Maxime Borry is a doctoral researcher in bioinformatics at the Max Planck Institute for Evolutionary Anthropology in Germany. After an undergraduate in life sciences and a master in Ecology, followed by a master in bioinformatics, he is now working on the completion of his PhD, focused on developing new tools and data analysis of ancient metagenomic samples.\n\n\n2022\n\n🇺🇸 Megan Michel is a PhD student jointly affiliated with the Archaeogenetics Department at the Max Planck Institute for Evolutionary Anthropology and the Human Evolutionary Biology Department at Harvard University. Her research focuses on using computational genomic analyses to understand how pathogens have co-evolved with their hosts over the course of human history.\n\n\n2022\n\n🇷🇺 Nikolay Oskolkov is a bioinformatician at Lund University and the bioinformatics platform of SciLifeLab, Sweden. He defended his PhD in theoretical physics in 2007, and switched to life sciences in 2012. His research interests include mathematical statistics and machine learning applied to genetics and genomics, single cell and ancient metagenomics data analysis.\n\n\n2022\n\n🇦🇺 Sebastian Duchene is an Australian Research Council Fellow at the Doherty Institute for Infection and Immunity at the University of Melbourne, Australia. Prior to joining the University of Melbourne he obtained his PhD and conducted postdoctoral work at the University of Sydney. His research is in molecular evolution and epidemiology of infectious pathogens, notably viruses and bacteria, and developing Bayesian phylodynamic methods.\n\n\n2022\n\n🇬🇷 Thiseas Lamnidis is a human population geneticist interested in European population history after the Bronze Age. To gain the required resolution to differentiate between Iron Age European populations, he is developing analytical methods based on the sharing of rare variation between individuals. He has also contributed to pipelines that streamline the processing and analysis of genetic data in a reproducible manner, while also facilitating dissemination of information among interdisciplinary colleagues."
  },
  {
    "objectID": "acknowledgements.html#financial-support",
    "href": "acknowledgements.html#financial-support",
    "title": "Acknowledgements",
    "section": "Financial Support",
    "text": "Financial Support\n\nThe content of this textbook was developed from the SPAAM Summer School: Introduction to Ancient Metagenomics summer school series, sponsored by the Werner Siemens-Stiftung (Grant: Paleobiotechnology, awarded to Pierre Stallforth, Hans-Knöll Institute, and Christina Warinner, Max Planck Institute for Evolutionary Anthropology)"
  },
  {
    "objectID": "acknowledgements.html#institutional-support",
    "href": "acknowledgements.html#institutional-support",
    "title": "Acknowledgements",
    "section": "Institutional Support",
    "text": "Institutional Support"
  },
  {
    "objectID": "acknowledgements.html#infrastructural-support",
    "href": "acknowledgements.html#infrastructural-support",
    "title": "Acknowledgements",
    "section": "Infrastructural Support",
    "text": "Infrastructural Support\n\nThe practical sessions of the summers schools work was supported by the BMBF-funded de.NBI Cloud within the German Network for Bioinformatics Infrastructure (de.NBI) (031A532B, 031A533A, 031A533B, 031A534A, 031A535A, 031A537A, 031A537B, 031A537C, 031A537D, 031A538A). z"
  },
  {
    "objectID": "before-you-start.html#creating-a-conda-environment",
    "href": "before-you-start.html#creating-a-conda-environment",
    "title": "Before you Start",
    "section": "Creating a conda environment",
    "text": "Creating a conda environment\nOnce conda is installed and bioconda configured, at the beginning of each chapter, to create the conda environment from the yml file, you will need to run the following:\n\nDownload the Zenodo tar archive file either the download button and or ‘Copy link address’ the URL and run\ncurl -O &lt;url&gt;\n## or\nwget &lt;url&gt;\nExtract the tar archive with\ntar -xvf &lt;tar_file&gt;.tar.gz\nand change into the directory with\ncd &lt;directory&gt;/\nWithin the resulting directory run the following conda command to install the software into it’s dedicated environment\nconda env create -f &lt;env_file&gt;.yml\n::: {.callout-note} Note: you only have to run the environment creation once. :::\nFollow the instructions as prompted. Once created, you can see a list of environments with\nconda env list\nTo load the relevant environment, you can run\nconda activate &lt;name_of_envonment&gt;.yml\nOnce finished with the chapter, you can deactivate the environment with\nconda deactivate\n\nTo reuse the environment, just run step 4 and 5 as necessary.\n\n\n\n\n\n\nTip\n\n\n\nTo delete a conda software environment, just get the path listed on conda env list and delete the folder with rm -rf &lt;path&gt;."
  },
  {
    "objectID": "before-you-start.html#additional-software-.",
    "href": "before-you-start.html#additional-software-.",
    "title": "Before you Start",
    "section": "Additional Software .",
    "text": "Additional Software .\nFor some chapters you may need the following software/and or data manually installed, which are not available on bioconda:\n\nDe novo assembly\n\nMetaWrap\n\nconda create -n metawrap-env python=2.7\nconda activate metawrap-env\nconda install biopython bwa maxbin2 metabat2 samtools=1.9\ncd ~/bin/\ngit clone https://github.com/bxlab/metaWRAP.git\necho \"export PATH=$PATH:~/bin/metaWRAP/bin\" &gt;&gt; ~/.bashrc\nFunctional Profiling\n\nHUMAnN3 UniRef database (where the functional providing conda environment is already activated - see the Functional Profiling chapter for more details)\nhumann3_databases --download uniref uniref90_ec_filtered_diamond /vol/volume/5c-functional-genomics/humann3_db\n\nPhylogenomics\n\nTempest (v1.5.3)\nIt is also recommended to assign the following bash variable so you can access the tool without the full path\nexport tempest='bash /home/ubuntu/bin/TempEst_v1.5.3/bin/tempest'\nMEGAX (v11.0.11)"
  },
  {
    "objectID": "section-theory.html#lectures",
    "href": "section-theory.html#lectures",
    "title": "Theory",
    "section": "Lectures",
    "text": "Lectures\n\nIntroduction to NGS Sequencing\nIn this chapter, we will introduce how we are able to convert DNA molecules to human readable sequences of A, C, T, and Gs, which we can subsequently can computationally analyse.\nThe field of Ancient DNA was revolutionised by the development of ‘Next Generation Sequencing’ (NGS), which relies on sequencing of millions of short fragments of DNA in parallel. The global leading DNA sequencing company is Illumina, and the technology used by Illumina is also most popular by palaeogeneticists. Therefore we will go through the various technologies behind Illumina next-generation sequencing machines.\nWe will also look at some important differences in the way different models of Illumina sequences work, and how this can influence ancient DNA research. Finally we will cover the structure of ‘FASTQ’ files, the most popular file format for representing the DNA sequence output of NGS sequencing machines.\n\n\nIntroduction to Ancient DNA\nThis chapter introduces you to ancient DNA and the enormous technological changes that have taken place since the field’s origins in 1984. Starting with the quagga and proceeding to microbes, we discuss where ancient microbial DNA can be found in the archaeological record and examine how ancient DNA is defined by its condition, not by a fixed age.\nWe next cover genome basics and take an in-depth look at the way DNA degrades over time. We detail the fundamentals of DNA damage, including the specific chemical processes that lead to DNA fragmentation and C-&gt;T miscoding lesions. We then demystify the DNA damage “smiley plot” and explain the how the plot’s symmetry or asymmetry is related to the specific enzymes used to repair DNA during library construction. We discuss how DNA damage is and is not clock-like, how to interpret and troubleshoot DNA damage plots, and how DNA damage patters can be used to authenticate ancient samples, specific taxa, and even sequences. We cover laboratory strategies for removing or reducing damage for greater accuracy for genotype calling, and we discuss the pros and cons of single-stranded library protocols. We then take a closer look at proofreading and non-proofreading polymerases and note key steps in NGS library preparation during which enzyme selection is critical in ancient DNA studies.\nFinally, we examine the big picture of why DNA damage matters in ancient microbial studies, and its effects on taxonomic identification of sequences, accurate genome mapping, and metagenomic assembly.\n\n\nIntroduction to Metagenomics\nThis chapter introduces you to the basics of metagenomics, with an emphasis on tools and approaches that are used to study ancient metagenomes. We begin by covering the basic terminology used in metagenomics and microbiome research and discuss how the field has changed over time. We examine the species concept for microbes and challenges that arise in classifying microbial species with respect to taxonomy and phylogeny. We then proceed to taxonomic profiling and discuss the pros and cons of different taxonomic profilers.\nAfterwards, we explain how to estimate preservation in ancient metagenomic samples and how to clean up your datasets and remove contaminants. Finally, we discuss strategies for exploring and comparing the ecological diversity in your samples, including different strategies for data normalization, distance calculation, and ordination.\n\n\nIntroduction to Microbial Genomics\nThe field of microbial genomics aims at the reconstruction and comparative analyses of genomes for gaining insights into the genetic foundation and evolution of various functional aspects such as virulence mechanisms in pathogens.\nIncluding data from ancient samples into this comparative assessment allows for studying these evolutionary changes through time. This, for example, provides insights into the emergence of human pathogens and their development in conjunction with human cultural transitions.\nIn this chapter we will look examples for how to utilise data from ancient genomes in comparative studies of human pathogens and today’s practical sessions will highlight methodologies for the reconstruction of microbial genomes.\n\n\nIntroduction to Evolutionary Biology\nPathogen genome data are an invaluable source of information about the evolution and spread of these organisms. This chapter will focus on molecular phylogenetic methods and the insight that they can reveal from improving our understanding of ancient evolution to the epidemiological dynamics of current outbreaks.\nThe first section will introduce phylognenetic trees and a set of core terms and concepts for their interpretation. Next, it will focus on some of the most popular approaches to inferring phylogenetic trees; those based on genetic distance, maximum likelihood, and Bayesian inference. These methods carry important considerations regarding the process that generated the data, computational capability, and data quality, all of which will be discussed here. Finally, we will direct our attention to examples of analyses of ancient and modern pathogens (e.g. Yersinia pestis, Hepatitis B virus, SARS-CoV-2) and critically assess appropriate choice of models and methods."
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#lecture",
    "href": "introduction-to-ngs-sequencing.html#lecture",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.1 Lecture",
    "text": "1.1 Lecture\n\n\nPDF version of the slide lectures can be downloaded from here."
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#readings",
    "href": "introduction-to-ngs-sequencing.html#readings",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.2 Readings",
    "text": "1.2 Readings\n\nReviews\n(Schuster 2008)\n(Shendure and Ji 2008)\n(Slatko, Gardner, and Ausubel 2018)\n(Dijk et al. 2014)\n\n\nSequencing Library Construction\n(Kircher, Sawyer, and Meyer 2012)\n(Meyer and Kircher 2010)\n\n\nErrors and Considerations\n(Ma et al. 2019)\n(Sinha et al. 2017)\n(Valk et al. 2019)"
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "href": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.3 Questions to think about",
    "text": "1.3 Questions to think about\n\nWhy is Illumina sequencing technologies useful for aDNA?\nWhat problems can the 2-colour chemistry technology of NextSeq and NovaSeqs cause in downstream analysis?\nWhy is ‘Index-Hopping’ a problem?\nWhat is good software to evaluate the quality of your sequencing runs?\n\n\n\n\n\nDijk, Erwin L van, Hélène Auger, Yan Jaszczyszyn, and Claude Thermes. 2014. “Ten Years of Next-Generation Sequencing Technology.” Trends in Genetics 30 (9): 418–26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. “Double Indexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina Platform.” Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder, Michael N Edmonson, Yu Liu, et al. 2019. “Analysis of Error Profiles in Deep Next-Generation Sequencing Data.” Genome Biology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. “Illumina Sequencing Library Preparation for Highly Multiplexed Target Capture and Sequencing.” Cold Spring Harbor Protocols 2010 (6): db.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. “Next-Generation Sequencing Transforms Today’s Biology.” Nature Methods 5 (1): 16–18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. “Next-Generation DNA Sequencing.” Nature Biotechnology 26 (10): 1135–45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle Joseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017. “Index Switching Causes ‘Spreading-of-Signal’ Among Multiplexed Samples in Illumina HiSeq 4000 DNA Sequencing.” bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018. “Overview of Next-Generation Sequencing Technologies.” Current Protocols in Molecular Biology / Edited by Frederick M. Ausubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dalén, and Katerina Guschanski. 2019. “Index Hopping on the Illumina HiseqX Platform and Its Consequences for Ancient DNA Studies.” Molecular Ecology Resources, March. https://doi.org/10.1111/1755-0998.13009."
  },
  {
    "objectID": "introduction-to-ancient-dna.html#lecture",
    "href": "introduction-to-ancient-dna.html#lecture",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.1 Lecture",
    "text": "2.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-ancient-dna.html#questions-to-think-about",
    "href": "introduction-to-ancient-dna.html#questions-to-think-about",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.2 Questions to think about",
    "text": "2.2 Questions to think about\n\nWhat is ancient DNA?\nWhere do we find ancient DNA from microbes?\nHow does DNA degrade?\nHow do I interpret a DNA damage plot?\nHow is DNA damage used to authenticate ancient genomes and samples?\nWhat methods are available for managing DNA damage?\nHow does DNA damage matter for my analyses?"
  },
  {
    "objectID": "introduction-to-metagenomics.html#introduction",
    "href": "introduction-to-metagenomics.html#introduction",
    "title": "3  Introduction to Metagenomics",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction"
  },
  {
    "objectID": "introduction-to-metagenomics.html#lecture",
    "href": "introduction-to-metagenomics.html#lecture",
    "title": "3  Introduction to Metagenomics",
    "section": "3.2 Lecture",
    "text": "3.2 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-metagenomics.html#questions-to-think-about",
    "href": "introduction-to-metagenomics.html#questions-to-think-about",
    "title": "3  Introduction to Metagenomics",
    "section": "3.3 Questions to think about",
    "text": "3.3 Questions to think about\n\nWhat is a metagenome? a microbiota? a microbiome?\nWhat is ancient metagenomics?\nWhat challenges do DNA degradation and sample decay pose for ancient metagenomics\nHow do you find out “who’s there” in your samples?\nHow do alignment based and k-mer based taxonomic profilers differ? What are the advantages and disadvantages of each?\nWhy does database selection matter?\nHow do you estimate the preservation and integrity of your ancient metagenome?\nWhat are tools you can use to identify poorly preserved samples and remove contaminant taxa?\nWhat aspects of diversity are important in investigating microbial communities?\nWhich distance metrics are commonly used to compare the beta-diversity of microbial communities and why? What are some advantages and disadvantages to these different approaches?"
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#lecture",
    "href": "introduction-to-microbial-genomics.html#lecture",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.1 Lecture",
    "text": "4.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "href": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.2 Questions to think about",
    "text": "4.2 Questions to think about"
  },
  {
    "objectID": "introduction-to-evolutionary-biology.html",
    "href": "introduction-to-evolutionary-biology.html",
    "title": "5  Introduction to Evolutionary Biology",
    "section": "",
    "text": "5.0.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-the-command-line-bare-bones-bash",
    "href": "section-useful-skills.html#introduction-to-the-command-line-bare-bones-bash",
    "title": "Useful Skills",
    "section": "Introduction to the Command Line (Bare Bones Bash)",
    "text": "Introduction to the Command Line (Bare Bones Bash)\nComputational work in metagenomics often involves connecting to remote servers to run analyses via the use of command line tools. Bash is a programming language that is used as the main command line interface of most UNIX systems, and hence most remote servers a user will encounter. By learning bash, users can work more efficiently and reproducibly on these remote servers.\nIn this chapter we will introduce the basic concepts of bash and the command line. Students will learn how to move around the filesystem and interact with files, how to chain multiple commands together using “pipes”, and how to use loops and regular expressions to simplify the running of repetitive tasks.\nFinally, readers will learn how to create a bash script of their own, that can run a set of commands in sequence. This session requires no prior knowledge of bash or the command line and is meant to serve as an entry-level introduction to basic programming concepts that can be applicable in other programming languages too."
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-r",
    "href": "section-useful-skills.html#introduction-to-r",
    "title": "Useful Skills",
    "section": "Introduction to R",
    "text": "Introduction to R\nR is an interpreted programming language with a particular focus on data manipulation and analysis. It is very well established for scientific computing and supported by an active community developing and maintaining a huge ecosystem of software packages for both general and highly derived applications.\nIn this chapter we will explore how to use R for a simple, standard data science workflow. We will import, clean, and visualise context and summary data for and from our ancient metagenomics analysis workflow. On the way we will learn about the RStudio integrated development environment, dip into the basic logic and syntax of R and finally write some first useful code within the tidyverse framework for tidy, readable and reproducible data analysis.\nThis chapter will be targeted at beginners without much previous experience with R or programming and will kickstart your journey to master this powerful tool."
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-python",
    "href": "section-useful-skills.html#introduction-to-python",
    "title": "Useful Skills",
    "section": "Introduction to Python",
    "text": "Introduction to Python\nWhile R has traditionally been the language of choice for statistical programming for many years, Python has taken away some of the hegemony thanks to its numerous available libraries for machine and deep learning. With its ever increasing collection of libraries for statistics and bioinformatics, Python has now become one the most used language in the bioinformatics community.\nIn this tutorial, mirroring to the R session, we will learn how to use the Python libraries Pandas for importing, cleaning, and manipulating data tables, and producing simple plots with the Python sister library of ggplot2, plotnine.\nWe will also get ourselves familiar with the Jupyter notebook environment, often used by many high performance computing clusters as an interactive scripting interface.\nThis chapter is meant for participants with a basic experience in R/tidyverse, but assumes no prior knowledge of Python/Jupyter."
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-git-and-github",
    "href": "section-useful-skills.html#introduction-to-git-and-github",
    "title": "Useful Skills",
    "section": "Introduction to Git and GitHub",
    "text": "Introduction to Git and GitHub\nAs the size and complexity of metagenomic analyses continues to expand, effectively organizing and tracking changes to scripts, code, and even data, continues to be a critical part of ancient metagenomic analyses. Furthermore, this complexity is leading to ever more collaborative projects, with input from multiple researchers.\nIn this chapter, we will introduce ‘Git’, an extremely popular version control system used in bioinformatics and software development to store, track changes, and collaborate on scripts and code. We will also introduce, GitHub, a cloud-based service for Git repositories for sharing data and code, and where many bioinformatic tools are stored. We will learn how to access and navigate course materials stored on GitHub through the web interface as well as the command line, and we will create our own repositories to store and share the output of upcoming sessions."
  },
  {
    "objectID": "bare-bones-bash.html",
    "href": "bare-bones-bash.html",
    "title": "6  Introduction to the Command Line",
    "section": "",
    "text": "7 Lecture"
  },
  {
    "objectID": "bare-bones-bash.html#session-1",
    "href": "bare-bones-bash.html#session-1",
    "title": "6  Introduction to the Command Line",
    "section": "7.1 Session 1",
    "text": "7.1 Session 1\nFor a full screen version on the presentation and press f on your keyboard.\nIntro to Bash\nPDF version of these slides can be downloaded from here.\nThe teaching material for the FULL BareBonesBash course can be found on the BareBonesBash website"
  },
  {
    "objectID": "bare-bones-bash.html#session-2",
    "href": "bare-bones-bash.html#session-2",
    "title": "6  Introduction to the Command Line",
    "section": "7.2 Session 2",
    "text": "7.2 Session 2\nFor a full screen version click on the presentation and press f on your keyboard.\nIntro to Bash\nPDF version of these slides can be downloaded from here.\nThe teaching material for the FULL BareBonesBash course can be found on the BareBonesBash website"
  },
  {
    "objectID": "introduction-to-r.html#lecture",
    "href": "introduction-to-r.html#lecture",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.1 Lecture",
    "text": "7.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-r.html#the-working-environment",
    "href": "introduction-to-r.html#the-working-environment",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.2 The working environment",
    "text": "7.2 The working environment\n\n7.2.1 R, RStudio and the tidyverse\n\nR is a fully featured programming language, but it excels as an environment for (statistical) data analysis (https://www.r-project.org)\nRStudio is an integrated development environment (IDE) for R (and other languages): (https://www.rstudio.com/products/rstudio)\nThe tidyverse is a collection of R packages with well-designed and consistent interfaces for the main steps of data analysis: loading, transforming and plotting data (https://www.tidyverse.org)\n\nThis introduction works with tidyverse ~v1.3.0\nWe will learn about readr, tibble, ggplot2, dplyr, magrittr and tidyr\nforcats will be briefly mentioned\npurrr and stringr are left out"
  },
  {
    "objectID": "introduction-to-r.html#loading-data-into-tibbles",
    "href": "introduction-to-r.html#loading-data-into-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.3 Loading data into tibbles",
    "text": "7.3 Loading data into tibbles\n\n7.3.1 Reading data with readr\n\nWith R we usually operate on data in our computer’s memory\nThe tidyverse provides the package readr to read data from text files into the memory\nreadr can read from our file system or the internet\nIt provides functions to read data in almost any (text) format:\nreadr::read_csv()   # .csv files\nreadr::read_tsv()   # .tsv files\nreadr::read_delim() # tabular files with an arbitrary separator\nreadr::read_fwf()   # fixed width files\nreadr::read_lines() # read linewise to parse yourself\nreadr automatically detects column types – but you can also define them manually\n\n\n\n7.3.2 How does the interface of read_csv work\n\nWe can learn more about a function with ?. To open a help file: ?readr::read_csv\nreadr::read_csv has many options to specify how to read a text file\nread_csv(\nfile,                      # The path to the file we want to read\ncol_names = TRUE,          # Are there column names?\ncol_types = NULL,          # Which types do the columns have? NULL -&gt; auto\nlocale = default_locale(), # How is information encoded in this file?\nna = c(\"\", \"NA\"),          # Which values mean \"no data\"\ntrim_ws = TRUE,            # Should superfluous white-spaces be removed?\nskip = 0,                  # Skip X lines at the beginning of the file\nn_max = Inf,               # Only read X lines\nskip_empty_rows = TRUE,    # Should empty lines be ignored?\ncomment = \"\",              # Should comment lines be ignored?\nname_repair = \"unique\",    # How should \"broken\" column names be fixed\n...\n)\n\n\n\n7.3.3 What does readr produce? The tibble\nsample_table_path &lt;- \"/vol/volume/3b-1-introduction-to-r-and-the-tidyverse/ancientmetagenome-hostassociated_samples.tsv\"\nsample_table_url &lt;-\n\"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/samples/ancientmetagenome-hostassociated_samples.tsv\"\nsamples &lt;- readr::read_tsv(sample_table_url)\n\nThe tibble is a “data frame”, a tabular data structure with rows and columns\nUnlike a simple array, each column can have another data type\n\nprint(samples, n = 3)\n\n\n7.3.4 How to look at a tibble\nsamples          # Typing the name of an object will print it to the console\nstr(samples)     # A structural overview of an object\nsummary(samples) # A human-readable summary of an object\nView(samples)    # RStudio's interactive data browser\n\nR provides a very flexible indexing operation for data.frames and tibbles\nsamples[1,1]                         # Access the first row and column\nsamples[1,]                          # Access the first row\nsamples[,1]                          # Access the first column\nsamples[c(1,2,3),c(2,3,4)]           # Access values from rows and columns\nsamples[,-c(1,2)]                    # Remove the first two columns\nsamples[,c(\"site_name\", \"material\")] # Columns can be selected by name\ntibbles are mutable data structures, so their content can be overwritten\nsamples[1,1] &lt;- \"Cheesecake2015\"     # replace the first value in the first column"
  },
  {
    "objectID": "introduction-to-r.html#plotting-data-in-tibbles",
    "href": "introduction-to-r.html#plotting-data-in-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.4 Plotting data in tibbles",
    "text": "7.4 Plotting data in tibbles\n\n7.4.1 ggplot2 and the “grammar of graphics”\n\nggplot2 offers an unusual, but powerful and logical interface\nThe following example describes a stacked bar chart\nlibrary(ggplot2) # Loading a library to use its functions without ::\n\nggplot(          # Every plot starts with a call to the ggplot() function\ndata = samples # This function can also take the input tibble\n) +              # The plot consists of functions linked with +\ngeom_bar(        # \"geoms\" define the plot layers we want to draw\nmapping = aes( # The aes() function maps variables to visual properties\n    x = publication_year, # publication_year -&gt; x-axis\n    fill = community_type # community_type -&gt; fill color\n)\n)\ngeom_*: data + geometry (bars) + statistical transformation (sum)\n\n\n\n7.4.2 ggplot2 and the “grammar of graphics”\n\nThis is the plot described above: number of samples per community type through time\nggplot(samples) +\ngeom_bar(aes(x = publication_year, fill = community_type))\n\n\n\n7.4.3 ggplot2 features many geoms\n(){width=55%}\n\nRStudio shares helpful cheatsheets for the tidyverse and beyond: https://www.rstudio.com/resources/cheatsheets\n\n\n\n7.4.4 scales control the behaviour of visual elements\n\nAnother plot: Boxplots of sample age through time\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age))\nThis is not well readable, because extreme outliers dictate the scale\n\n\n\n7.4.5 scales control the behaviour of visual elements\n\nWe can change the scale of different visual elements - e.g. the y-axis\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age)) +\nscale_y_log10()\nThe log-scale improves readability\n\n\n\n7.4.6 scales control the behaviour of visual elements\n\n(Fill) color is a visual element of the plot and its scaling can be adjusted\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age,\n                fill = as.factor(publication_year))) +\nscale_y_log10() + scale_fill_viridis_d(option = \"C\")\n\n\n\n7.4.7 Defining plot matrices via facets\n\nSplitting up the plot by categories into facets is another way to visualize more variables at once\nggplot(samples) +\ngeom_count(aes(x = as.factor(publication_year), y = material)) +\nfacet_wrap(~archive)\nUnfortunately the x-axis became unreadable\n\n\n\n7.4.8 Setting purely aesthetic settings with theme\n\nAesthetic changes like this can be applied as part of the theme\nggplot(samples) +\ngeom_count(aes(x = as.factor(publication_year), y = material)) +\nfacet_wrap(~archive) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n7.4.9 Exercise 1\n\nLook at the mtcars dataset and read up on the meaning of its variables\n\nVisualize the relationship between Gross horsepower and 1/4 mile time\n\nIntegrate the Number of cylinders into your plot\n\n\n\n\n7.4.10 Possible solutions 1\n\nLook at the mtcars dataset and read up on the meaning of its variables\n?mtcars\nVisualize the relationship between Gross horsepower and 1/4 mile time\nggplot(mtcars) + geom_point(aes(x = hp, y = qsec))\nIntegrate the Number of cylinders into your plot\nggplot(mtcars) + geom_point(aes(x = hp, y = qsec, color = as.factor(cyl)))"
  },
  {
    "objectID": "introduction-to-r.html#conditional-queries-on-tibbles",
    "href": "introduction-to-r.html#conditional-queries-on-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.5 Conditional queries on tibbles",
    "text": "7.5 Conditional queries on tibbles\n\n7.5.1 Selecting columns and filtering rows with select and filter\n\nThe dplyr package includes powerful functions to subset data in tibbles based on conditions\ndplyr::select allows to select columns\ndplyr::select(samples, project_name, sample_age)   # reduce to two columns\ndplyr::select(samples, -project_name, -sample_age) # remove two columns\ndplyr::filter allows for conditional filtering of rows\ndplyr::filter(samples, publication_year == 2014)  # samples published in 2014\ndplyr::filter(samples, publication_year == 2014 |\n                    publication_year == 2018)  # samples from 2015 OR 2018\ndplyr::filter(samples, publication_year %in% c(2014, 2018)) # match operator: %in%\ndplyr::filter(samples, sample_host == \"Homo sapiens\" &\n                    community_type == \"oral\")  # oral samples from modern humans\n\n\n\n7.5.2 Chaining functions together with the pipe %&gt;%\n\nThe pipe %&gt;% in the magrittr package is a clever infix operator to chain data and operations\nlibrary(magrittr)\nsamples %&gt;% dplyr::filter(publication_year == 2014)\nIt forwards the LHS as the first argument of the function appearing on the RHS\nThat allows for sequences of functions (“tidyverse style”)\nsamples %&gt;%\ndplyr::select(sample_host, community_type) %&gt;%\ndplyr::filter(sample_host == \"Homo sapiens\" & community_type == \"oral\") %&gt;%\nnrow() # count the rows\nmagrittr also offers some more operators, among which the extraction %$% is particularly useful\nsamples %&gt;%\ndplyr::filter(material == \"tooth\") %$%\nsample_age %&gt;% # extract the sample_age column as a vector\nmax()          # get the maximum of said vector\n\n\n\n7.5.3 Summary statistics in base R\n\nSummarising and counting data is indispensable and R offers all operations you would expect in its base package\nnrow(samples)              # number of rows in a tibble\nlength(samples$site_name)  # length/size of a vector\nunique(samples$material)   # unique elements of a vector\nmin(samples$sample_age)    # minimum\nmax(samples$sample_age)    # maximum\nmean(samples$sample_age)   # mean\nmedian(samples$sample_age) # median\nvar(samples$sample_age)    # variance\nsd(samples$sample_age)     # standard deviation\nquantile(samples$sample_age, probs = 0.75) # sample quantiles for the given probs\nmany of these functions can ignore missing values with an option na.rm = TRUE\n\n\n\n7.5.4 Group-wise summaries with group_by and summarise\n\nThese summary statistics are particular useful when applied to conditional subsets of a dataset\ndplyr allows such summary operations with a combination of group_by and summarise\nsamples %&gt;%\ndplyr::group_by(material) %&gt;%  # group the tibble by the material column\ndplyr::summarise(\n    min_age = min(sample_age),   # a new column: min age for each group\n    median_age = median(sample_age), # a new column: median age for each group\n    max_age = max(sample_age)    # a new column: max age for each group\n)\ngrouping can be applied across multiple columns\nsamples %&gt;%\ndplyr::group_by(material, sample_host) %&gt;% # group by material and host\ndplyr::summarise(\n    n = dplyr::n(),  # a new column: number of samples for each group\n    .groups = \"drop\" # drop the grouping after this summary operation\n)\n\n\n\n7.5.5 Sorting and slicing tibbles with arrange and slice\n\ndplyr allows to arrange tibbles by one or multiple columns\nsamples %&gt;% dplyr::arrange(publication_year)        # sort by publication year\nsamples %&gt;% dplyr::arrange(publication_year,\n                        sample_age)              # ... and sample age\nsamples %&gt;% dplyr::arrange(dplyr::desc(sample_age)) # sort descending on sample age\nSorting also works within groups and can be paired with slice to extract extreme values per group\nsamples %&gt;%\ndplyr::group_by(publication_year) %&gt;%       # group by publication year\ndplyr::arrange(dplyr::desc(sample_age)) %&gt;% # sort by age within (!) groups\ndplyr::slice_head(n = 2) %&gt;%                # keep the first two samples per group\ndplyr::ungroup()                            # remove the still lingering grouping\nSlicing is also the relevant operation to take random samples from the observations in a tibble\nsamples %&gt;% dplyr::slice_sample(n = 20)\n\n\n\n7.5.6 Exercise 2\n\nDetermine the number of cars with four forward gears (gear) in the mtcars dataset\n\nDetermine the mean 1/4 mile time (qsec) per Number of cylinders (cyl) group\n\nIdentify the least efficient cars for both transmission types (am)\n\n\n\n\n7.5.7 Possible solutions 2\n\nDetermine the number of cars with four forward gears (gear) in the mtcars dataset\nmtcars %&gt;% dplyr::filter(gear == 4) %&gt;% nrow()\nDetermine the mean 1/4 mile time (qsec) per Number of cylinders (cyl) group\nmtcars %&gt;% dplyr::group_by(cyl) %&gt;% dplyr::summarise(qsec_mean = mean(qsec))\nIdentify the least efficient cars for both transmission types (am)\n#mtcars3 &lt;- tibble::rownames_to_column(mtcars, var = \"car\") %&gt;% tibble::as_tibble()\nmtcars %&gt;% dplyr::group_by(am) %&gt;% dplyr::arrange(mpg) %&gt;% dplyr::slice_head()"
  },
  {
    "objectID": "introduction-to-r.html#transforming-and-manipulating-tibbles",
    "href": "introduction-to-r.html#transforming-and-manipulating-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.6 Transforming and manipulating tibbles",
    "text": "7.6 Transforming and manipulating tibbles\n\n7.6.1 Renaming and reordering columns and values with rename, relocate and recode\n\nColumns in tibbles can be renamed with dplyr::rename and reordered with dplyr::relocate\nsamples %&gt;% dplyr::rename(country = geo_loc_name) # rename a column\nsamples %&gt;% dplyr::relocate(site_name, .before = project_name) # reorder columns\nValues in columns can also be changed with dplyr::recode\nsamples$sample_host %&gt;% dplyr::recode(`Homo sapiens` = \"modern human\")\nR supports explicitly ordinal data with factors, which can be reordered as well\nfactors can be handeld more easily with the forcats package\nggplot(samples) + geom_bar(aes(x = community_type)) # bars are alphabetically ordered\nsa2 &lt;- samples\nsa2$cto &lt;- forcats::fct_reorder(sa2$community_type, sa2$community_type, length)\n# fct_reorder: reorder the input factor by a summary statistic on an other vector\nggplot(sa2) + geom_bar(aes(x = community_type)) # bars are ordered by size\n\n\n\n7.6.2 Adding columns to tibbles with mutate and transmute\n\nA common application of data manipulation is adding derived columns. dplyr offers that with mutate\nsamples %&gt;%\ndplyr::mutate(                                               # add a column that\n    archive_summary = paste0(archive, \": \", archive_accession) # combines two other\n) %$% archive_summary                                        # columns\ndplyr::transmute removes all columns but the newly created ones\nsamples %&gt;%\ndplyr::transmute(\n    sample_name = tolower(sample_name), # overwrite this columns\n    publication_doi                     # select this column\n)\ntibble::add_column behaves as dplyr::mutate, but gives more control over column position\nsamples %&gt;% tibble::add_column(., id = 1:nrow(.), .before = \"project_name\")\n\n\n\n7.6.3 Conditional operations with ifelse and case_when\n\nifelse allows to implement conditional mutate operations, that consider information from other columns, but that gets cumbersome easily\nsamples %&gt;% dplyr::mutate(hemi = ifelse(latitude &gt;= 0, \"North\", \"South\")) %$% hemi\n\nsamples %&gt;% dplyr::mutate(\nhemi = ifelse(is.na(latitude), \"unknown\", ifelse(latitude &gt;= 0, \"North\", \"South\"))\n) %$% hemi\ndplyr::case_when is a much more readable solution for this application\nsamples %&gt;% dplyr::mutate(\nhemi = dplyr::case_when(\n    latitude &gt;= 0 ~ \"North\",\n    latitude &lt; 0  ~ \"South\",\n    TRUE          ~ \"unknown\" # TRUE catches all remaining cases\n)\n) %$% hemi\n\n\n\n7.6.4 Long and wide data formats\n\nFor different applications or to simplify certain analysis or plotting operations data often has to be transformed from a wide to a long format or vice versa\n\n\n\nA table in wide format has N key columns and N value columns\nA table in long format has N key columns, one descriptor column and one value column\n\n\n\n7.6.5 A wide dataset\ncarsales &lt;- tibble::tribble(\n  ~brand, ~`2014`, ~`2015`, ~`2016`, ~`2017`,\n  \"BMW\",  20,      25,      30,      45,\n  \"VW\",   67,      40,     120,      55\n)\n\ncarsales\n\nWide format becomes a problem, when the columns are semantically identical. This dataset is in wide format and we can not easily plot it\nWe generally prefer data in long format, although it is more verbose with more duplication. “Long” format data is more “tidy”\n\n\n\n7.6.6 Making a wide dataset long with pivot_longer\ncarsales_long &lt;- carsales %&gt;% tidyr::pivot_longer(\n  cols = tidyselect::num_range(\"\", range = 2014:2017), # set of columns to transform\n  names_to = \"year\",            # the name of the descriptor column we want\n  names_transform = as.integer, # a transformation function to apply to the names\n  values_to = \"sales\"           # the name of the value column we want\n)\n\ncarsales_long\n\n\n7.6.7 Making a long dataset wide with pivot_wider\ncarsales_wide &lt;- carsales_long %&gt;% tidyr::pivot_wider(\n  id_cols = \"brand\",  # the set of id columns that should not be changed\n  names_from = year,  # the descriptor column with the names of the new columns\n  values_from = sales # the value column from which the values should be extracted\n)\n\ncarsales_wide\n\nApplications of wide datasets are adjacency matrices to represent graphs, covariance matrices or other pairwise statistics\nWhen data gets big, then wide formats can be significantly more efficient (e.g. for spatial data)\n\n\n\n7.6.8 Exercise 3\n\nMove the column gear to the first position of the mtcars dataset\n\nMake a new dataset mtcars2 with the column mpg and an additional column am_v, which encodes the transmission type (am) as either \"manual\" or \"automatic\"\n\nCount the number of cars per transmission type (am_v) and number of gears (gear). Then transform the result to a wide format, with one column per transmission type.\n\n\n\n\n7.6.9 Possible solutions 3\n\nMove the column gear to the first position of the mtcars dataset\nmtcars %&gt;% dplyr::relocate(gear, .before = mpg)\nMake a new dataset mtcars2 with the column gear and an additional column am_v, which encodes the transmission type (am) as either \"manual\" or \"automatic\"\nmtcars2 &lt;- mtcars %&gt;% dplyr::mutate(\ngear, am_v = dplyr::case_when(am == 0 ~ \"automatic\", am == 1 ~ \"manual\")\n)\nCount the number of cars in mtcars2 per transmission type (am_v) and number of gears (gear). Then transform the result to a wide format, with one column per transmission type.\nmtcars2 %&gt;% dplyr::group_by(am_v, gear) %&gt;% dplyr::tally() %&gt;%\ntidyr::pivot_wider(names_from = am_v, values_from = n)"
  },
  {
    "objectID": "introduction-to-r.html#combining-tibbles-with-join-operations",
    "href": "introduction-to-r.html#combining-tibbles-with-join-operations",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.7 Combining tibbles with join operations",
    "text": "7.7 Combining tibbles with join operations\n\n7.7.1 Types of joins\nJoins combine two datasets x and y based on key columns\n\nMutating joins add columns from one dataset to the other\n\nLeft join: Take observations from x and add fitting information from y\nRight join: Take observations from y and add fitting information from x\nInner join: Join the overlapping observations from x and y\nFull join: Join all observations from x and y, even if information is missing\n\nFiltering joins remove observations from x based on their presence in y\n\nSemi join: Keep every observation in x that is in y\nAnti join: Keep every observation in x that is not in y\n\n\n\n\n7.7.2 A second dataset\nlibrary_table_path &lt;- \"/vol/volume/3b-1-introduction-to-r-and-the-tidyverse/ancientmetagenome-hostassociated_libraries.tsv\"\nlibrary_table_url &lt;-\n\"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/libraries/ancientmetagenome-hostassociated_libraries.tsv\"\n\nlibraries &lt;- readr::read_tsv(library_table_url)\nprint(libraries, n = 3)\n\n\n7.7.3 Meaningful subsets\nsamsub &lt;- samples %&gt;% dplyr::select(project_name, sample_name, sample_age)\nlibsub &lt;- libraries %&gt;% dplyr::select(project_name, sample_name, library_name, read_count)\nprint(samsub, n = 3)\nprint(libsub, n = 3)\n\n\n7.7.4 Left join\nTake observations from x and add fitting information from y\n\nleft &lt;- dplyr::left_join(\nx = samsub,                           # 1060 observations\ny = libsub,                           # 1657 observations\nby = c(\"project_name\", \"sample_name\") # the key columns by which to join\n)\n\nprint(left, n = 1)\n\nLeft joins are the most common join operation: Add information from another dataset\n\n\n\n7.7.5 Right join\nTake observations from y and add fitting information from x\n\nright &lt;- dplyr::right_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\n\nprint(right, n = 1)\n\nRight joins are almost identical to left joins – only x and y have reversed roles\n\n\n\n7.7.6 Inner join\nJoin the overlapping observations from x and y\n\ninner &lt;- dplyr::inner_join(\nx = samsub,                           # 1060 observations\ny = libsub,                           # 1657 observations\nby = c(\"project_name\", \"sample_name\")\n)\n\nprint(inner, n = 1)\n\nInner joins are a fast and easy way to check, to which degree two dataset overlap\n\n\n\n7.7.7 Full join\nJoin all observations from x and y, even if information is missing\n\nfull &lt;- dplyr::full_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\n\nprint(full, n = 1)\n\nFull joins allow to preserve every bit of information\n\n\n\n7.7.8 Semi join\nKeep every observation in x that is in y\n\nsemi &lt;- dplyr::semi_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\nprint(semi, n = 1)\n\nSemi joins are underused operations to filter datasets\n\n\n\n7.7.9 Anti join\nKeep every observation in x that is not in y\n\nanti &lt;- dplyr::anti_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\nprint(anti, n = 1)\n\nAnti joins allow to quickly specify incomplete datasets and missing information\n\n\n\n7.7.10 Exercise 4\nConsider the following additional dataset:\ngear_opinions &lt;- tibble::tibble(gear = c(3, 5), opinion = c(\"boring\", \"wow\"))\n\nAdd my opinions about gears to the mtcars dataset\n\nRemove all cars from the dataset for which I don’t have an opinion\n\n\n\n\n7.7.11 Possible Solutions 4\n\nAdd my opinions about gears to the mtcars dataset\ndplyr::left_join(mtcars, gear_opinions, by = \"gear\")\nRemove all cars from the dataset for which I don’t have an opinion\ndplyr::anti_join(mtcars, gear_opinions, by = \"gear\")"
  },
  {
    "objectID": "introduction-to-python.html#lecture",
    "href": "introduction-to-python.html#lecture",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.1 Lecture",
    "text": "8.1 Lecture\n\n\nPDF version of these slides can be downloaded from here.\nThis session is run using a Jupyter notebook. This can be found here. However, it will already be installed on compute nodes during the summer school.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe highly recommend viewing this walkthrough via the Jupyter notebook above! The output of commands on the website for this walkthrough are displayed in their own code blocks - be wary of what you copy-paste!\n\n\nfrom IPython.core.display import SVG"
  },
  {
    "objectID": "introduction-to-python.html#introduction-to-data-manipulation-in-python-with-pandas-and-visulization-with-plotnine",
    "href": "introduction-to-python.html#introduction-to-data-manipulation-in-python-with-pandas-and-visulization-with-plotnine",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.2 Introduction to data manipulation in Python with Pandas and visulization with plotnine",
    "text": "8.2 Introduction to data manipulation in Python with Pandas and visulization with plotnine\nMaxime Borry\nSPAAM Summer School 2022\nSVG(filename='img/whoami.svg')\n\n\n\nsvg\n\n\nOver the last few years, Python has gained an immense amount of popularity thanks to its numerous libraries in the field of machine learning, statistical data analysis, and bioinformatics. While a few years ago, it was often necessary to go back to R for performing routine data manipulation and analysis tasks, nowadays Python has a vast ecosystem of libraries for doing just that.\nToday, we will do a quick introduction of the most popular libraries for data analysis:\n\npandas, for reading and manipulation tabular data\nplotnine, the Python clone of ggplot2"
  },
  {
    "objectID": "introduction-to-python.html#overview",
    "href": "introduction-to-python.html#overview",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.3 Overview:",
    "text": "8.3 Overview:\n\n0 - Foreword, working in a jupyter environment\n1 - Loading required libraries\n2 - Foreword on Pandas\n3 - Reading data with Pandas\n4 - Dealing with missing data\n5 - Computing basic statistics\n6 - Filtering\n8 - GroupBy operations\n9 - Joining different tables\n10 - Visualization with Plotnine"
  },
  {
    "objectID": "introduction-to-python.html#foreword-working-in-a-jupyter-environment",
    "href": "introduction-to-python.html#foreword-working-in-a-jupyter-environment",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.4 0 - Foreword, working in a jupyter environment",
    "text": "8.4 0 - Foreword, working in a jupyter environment\n\n8.4.1 This is a markdown cell\nWith some features of the markdown syntax, such as:\n\nbold **bold**\nitalic *italic*\ninline code\n\n`inline code`\n\nlinks [links](https://www.google.com/)\nImages\n\n![](https://maximeborry.com/authors/maxime/avatar_hu4dc3c23d5a8c195732bbca11d7ce61be_114670_270x270_fill_lanczos_center_2.png)\nLatex code $ y = ax + b$\n$y = ax + b$\n\nprint(\"This is a code cell in Python\")\nThis is a code cell in Python\n! echo \"This is code cell in bash\"\nThis is code cell in bash\n%%bash\n\necho \"This a multiline code cell\"\necho \"in bash\"\nThis a multiline code cell\nin bash"
  },
  {
    "objectID": "introduction-to-python.html#loading-required-libraries",
    "href": "introduction-to-python.html#loading-required-libraries",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.5 1 - Loading required libraries",
    "text": "8.5 1 - Loading required libraries\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *\npd.__version__\n'1.4.3'\nnp.__version__\n'1.23.1'\n! conda list | grep plotnine\nplotnine                  0.9.0              pyhd8ed1ab_0    conda-forge"
  },
  {
    "objectID": "introduction-to-python.html#foreword-on-pandas",
    "href": "introduction-to-python.html#foreword-on-pandas",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.6 2 - Foreword on Pandas",
    "text": "8.6 2 - Foreword on Pandas\n\n8.6.1 Pandas terminology\n\n\nThe pandas getting started tutorial: pandas.pydata.org/docs/getting_started"
  },
  {
    "objectID": "introduction-to-python.html#reading-data-with-pandas",
    "href": "introduction-to-python.html#reading-data-with-pandas",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.7 3 - Reading data with Pandas",
    "text": "8.7 3 - Reading data with Pandas\nsample_table_url = \"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/samples/ancientmetagenome-hostassociated_samples.tsv\"\nlibrary_table_url = \"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/libraries/ancientmetagenome-hostassociated_libraries.tsv\"\nGetting help in Python\nhelp(pd.read_csv)\nHelp on function read_csv in module pandas.io.parsers.readers:\n\nread_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]',\nsep=&lt;no_default&gt;, delimiter=None, header='infer', names=&lt;no_default&gt;, index_col=None,\nusecols=None, squeeze=None, prefix=&lt;no_default&gt;, mangle_dupe_cols=True,\ndtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters=None,\ntrue_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0,\nnrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False,\nskip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False,\n date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None,\n compression: 'CompressionOptions' = 'infer', thousands=None, decimal: 'str' = '.',\n lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None,\n comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None,\n error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False,\n low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n    Read a comma-separated values (csv) file into DataFrame.\n\n    Also supports optionally iterating or breaking of the file\n    into chunks.\n\n    Additional help can be found in the online docs for\n    `IO Tools &lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html&gt;`_.\n\n    Parameters\n    ----------\n    filepath_or_buffer : str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n        expected. A local file could be: file://localhost/path/to/table.csv.\n\n        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n        By file-like object, we refer to objects with a ``read()`` method, such as\n        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n    sep : str, default ','\n        Delimiter to use. If sep is None, the C engine cannot automatically detect\n        the separator, but the Python parsing engine can, meaning the latter will\n        be used and automatically detect the separator by Python's builtin sniffer\n        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n        different from ``'\\s+'`` will be interpreted as regular expressions and\n        will also force the use of the Python parsing engine. Note that regex\n        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n    delimiter : str, default ``None``\n        Alias for sep.\n    header : int, list of int, None, default 'infer'\n        Row number(s) to use as the column names, and the start of the\n        data.  Default behavior is to infer the column names: if no names\n        are passed the behavior is identical to ``header=0`` and column\n        names are inferred from the first line of the file, if column\n        names are passed explicitly then the behavior is identical to\n        ``header=None``. Explicitly pass ``header=0`` to be able to\n        replace existing names. The header can be a list of integers that\n        specify row locations for a multi-index on the columns\n        e.g. [0,1,3]. Intervening rows that are not specified will be\n        skipped (e.g. 2 in this example is skipped). Note that this\n        parameter ignores commented lines and empty lines if\n        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n        data rather than the first line of the file.\n    names : array-like, optional\n        List of column names to use. If the file contains a header row,\n        then you should explicitly pass ``header=0`` to override the column names.\n        Duplicates in this list are not allowed.\n    index_col : int, str, sequence of int / str, or False, optional, default ``None``\n      Column(s) to use as the row labels of the ``DataFrame``, either given as\n      string name or column index. If a sequence of int / str is given, a\n      MultiIndex is used.\n\n      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n      column as the index, e.g. when you have a malformed file with delimiters at\n      the end of each line.\n    usecols : list-like or callable, optional\n        Return a subset of the columns. If list-like, all elements must either\n        be positional (i.e. integer indices into the document columns) or strings\n        that correspond to column names provided either by the user in `names` or\n        inferred from the document header row(s). If ``names`` are given, the document\n        header row(s) are not taken into account. For example, a valid list-like\n        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n        To instantiate a DataFrame from ``data`` with element order preserved use\n        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n        in ``['foo', 'bar']`` order or\n        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n        for ``['bar', 'foo']`` order.\n\n        If callable, the callable function will be evaluated against the column\n        names, returning names where the callable function evaluates to True. An\n        example of a valid callable argument would be ``lambda x: x.upper() in\n        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n        parsing time and lower memory usage.\n    squeeze : bool, default False\n        If the parsed data only contains one column then return a Series.\n\n        .. deprecated:: 1.4.0\n            Append ``.squeeze(\"columns\")`` to the call to ``read_csv`` to squeeze\n            the data.\n    prefix : str, optional\n        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n\n        .. deprecated:: 1.4.0\n           Use a list comprehension on the DataFrame's columns after calling ``read_csv``.\n    mangle_dupe_cols : bool, default True\n        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n        'X'...'X'. Passing in False will cause data to be overwritten if there\n        are duplicate names in the columns.\n    dtype : Type name or dict of column -&gt; type, optional\n        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n        'c': 'Int64'}\n        Use `str` or `object` together with suitable `na_values` settings\n        to preserve and not interpret dtype.\n        If converters are specified, they will be applied INSTEAD\n        of dtype conversion.\n    engine : {'c', 'python', 'pyarrow'}, optional\n        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n        is currently more feature-complete. Multithreading is currently only supported by\n        the pyarrow engine.\n\n        .. versionadded:: 1.4.0\n\n            The \"pyarrow\" engine was added as an *experimental* engine, and some features\n            are unsupported, or may not work correctly, with this engine.\n    converters : dict, optional\n        Dict of functions for converting values in certain columns. Keys can either\n        be integers or column labels.\n    true_values : list, optional\n        Values to consider as True.\n    false_values : list, optional\n        Values to consider as False.\n    skipinitialspace : bool, default False\n        Skip spaces after delimiter.\n    skiprows : list-like, int or callable, optional\n        Line numbers to skip (0-indexed) or number of lines to skip (int)\n        at the start of the file.\n\n        If callable, the callable function will be evaluated against the row\n        indices, returning True if the row should be skipped and False otherwise.\n        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n    skipfooter : int, default 0\n        Number of lines at bottom of file to skip (Unsupported with engine='c').\n    nrows : int, optional\n        Number of rows of file to read. Useful for reading pieces of large files.\n    na_values : scalar, str, list-like, or dict, optional\n        Additional strings to recognize as NA/NaN. If dict passed, specific\n        per-column NA values.  By default the following values are interpreted as\n        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n        '1.#IND', '1.#QNAN', '&lt;NA&gt;', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n        'nan', 'null'.\n    keep_default_na : bool, default True\n        Whether or not to include the default NaN values when parsing the data.\n        Depending on whether `na_values` is passed in, the behavior is as follows:\n\n        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n          is appended to the default NaN values used for parsing.\n        * If `keep_default_na` is True, and `na_values` are not specified, only\n          the default NaN values are used for parsing.\n        * If `keep_default_na` is False, and `na_values` are specified, only\n          the NaN values specified `na_values` are used for parsing.\n        * If `keep_default_na` is False, and `na_values` are not specified, no\n          strings will be parsed as NaN.\n\n        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n        `na_values` parameters will be ignored.\n    na_filter : bool, default True\n        Detect missing value markers (empty strings and the value of na_values). In\n        data without any NAs, passing na_filter=False can improve the performance\n        of reading a large file.\n    verbose : bool, default False\n        Indicate number of NA values placed in non-numeric columns.\n    skip_blank_lines : bool, default True\n        If True, skip over blank lines rather than interpreting as NaN values.\n    parse_dates : bool or list of int or names or list of lists or dict, default False\n        The behavior is as follows:\n\n        * boolean. If True -&gt; try parsing the index.\n        * list of int or names. e.g. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3\n          each as a separate date column.\n        * list of lists. e.g.  If [[1, 3]] -&gt; combine columns 1 and 3 and parse as\n          a single date column.\n        * dict, e.g. {'foo' : [1, 3]} -&gt; parse columns 1, 3 as date and call\n          result 'foo'\n\n        If a column or index cannot be represented as an array of datetimes,\n        say because of an unparsable value or a mixture of timezones, the column\n        or index will be returned unaltered as an object data type. For\n        non-standard datetime parsing, use ``pd.to_datetime`` after\n        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n        specify ``date_parser`` to be a partially-applied\n        :func:`pandas.to_datetime` with ``utc=True``. See\n        :ref:`io.csv.mixed_timezones` for more.\n\n        Note: A fast-path exists for iso8601-formatted dates.\n    infer_datetime_format : bool, default False\n        If True and `parse_dates` is enabled, pandas will attempt to infer the\n        format of the datetime strings in the columns, and if it can be inferred,\n        switch to a faster method of parsing them. In some cases this can increase\n        the parsing speed by 5-10x.\n    keep_date_col : bool, default False\n        If True and `parse_dates` specifies combining multiple columns then\n        keep the original columns.\n    date_parser : function, optional\n        Function to use for converting a sequence of string columns to an array of\n        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n        conversion. Pandas will try to call `date_parser` in three different ways,\n        advancing to the next if an exception occurs: 1) Pass one or more arrays\n        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n        string values from the columns defined by `parse_dates` into a single array\n        and pass that; and 3) call `date_parser` once for each row using one or\n        more strings (corresponding to the columns defined by `parse_dates`) as\n        arguments.\n    dayfirst : bool, default False\n        DD/MM format dates, international and European format.\n    cache_dates : bool, default True\n        If True, use a cache of unique, converted dates to apply the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especially ones with timezone offsets.\n\n        .. versionadded:: 0.25.0\n    iterator : bool, default False\n        Return TextFileReader object for iteration or getting chunks with\n        ``get_chunk()``.\n\n        .. versionchanged:: 1.2\n\n           ``TextFileReader`` is a context manager.\n    chunksize : int, optional\n        Return TextFileReader object for iteration.\n        See the `IO Tools docs\n        &lt;https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking&gt;`_\n        for more information on ``iterator`` and ``chunksize``.\n\n        .. versionchanged:: 1.2\n\n           ``TextFileReader`` is a context manager.\n    compression : str or dict, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer' and '%s' is\n        path-like, then detect compression from the following extensions: '.gz',\n        '.bz2', '.zip', '.xz', or '.zst' (otherwise no compression). If using\n        'zip', the ZIP file must contain only one data file to be read in. Set to\n        ``None`` for no decompression. Can also be a dict with key ``'method'`` set\n        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``} and other\n        key-value pairs are forwarded to ``zipfile.ZipFile``, ``gzip.GzipFile``,\n        ``bz2.BZ2File``, or ``zstandard.ZstdDecompressor``, respectively. As an\n        example, the following could be passed for Zstandard decompression using a\n        custom compression dictionary:\n        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n\n        .. versionchanged:: 1.4.0 Zstandard support.\n\n    thousands : str, optional\n        Thousands separator.\n    decimal : str, default '.'\n        Character to recognize as decimal point (e.g. use ',' for European data).\n    lineterminator : str (length 1), optional\n        Character to break file into lines. Only valid with C parser.\n    quotechar : str (length 1), optional\n        The character used to denote the start and end of a quoted item. Quoted\n        items can include the delimiter and it will be ignored.\n    quoting : int or csv.QUOTE_* instance, default 0\n        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n    doublequote : bool, default ``True``\n       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n       whether or not to interpret two consecutive quotechar elements INSIDE a\n       field as a single ``quotechar`` element.\n    escapechar : str (length 1), optional\n        One-character string used to escape other characters.\n    comment : str, optional\n        Indicates remainder of line should not be parsed. If found at the beginning\n        of a line, the line will be ignored altogether. This parameter must be a\n        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n        fully commented lines are ignored by the parameter `header` but not by\n        `skiprows`. For example, if ``comment='#'``, parsing\n        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n        treated as the header.\n    encoding : str, optional\n        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n        standard encodings\n        &lt;https://docs.python.org/3/library/codecs.html#standard-encodings&gt;`_ .\n\n        .. versionchanged:: 1.2\n\n           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n           This behavior was previously only the case for ``engine=\"python\"``.\n\n        .. versionchanged:: 1.3.0\n\n           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n           influence on how encoding errors are handled.\n\n    encoding_errors : str, optional, default \"strict\"\n        How encoding errors are treated. `List of possible values\n        &lt;https://docs.python.org/3/library/codecs.html#error-handlers&gt;`_ .\n\n        .. versionadded:: 1.3.0\n\n    dialect : str or csv.Dialect, optional\n        If provided, this parameter will override values (default or not) for the\n        following parameters: `delimiter`, `doublequote`, `escapechar`,\n        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n        override values, a ParserWarning will be issued. See csv.Dialect\n        documentation for more details.\n    error_bad_lines : bool, optional, default ``None``\n        Lines with too many fields (e.g. a csv line with too many commas) will by\n        default cause an exception to be raised, and no DataFrame will be returned.\n        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n        returned.\n\n        .. deprecated:: 1.3.0\n           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n           encountering a bad line instead.\n    warn_bad_lines : bool, optional, default ``None``\n        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n        \"bad line\" will be output.\n\n        .. deprecated:: 1.3.0\n           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n           encountering a bad line instead.\n    on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n        Specifies what to do upon encountering a bad line (a line with too many fields).\n        Allowed values are :\n\n            - 'error', raise an Exception when a bad line is encountered.\n            - 'warn', raise a warning when a bad line is encountered and skip that line.\n            - 'skip', skip bad lines without raising or warning when they are encountered.\n\n        .. versionadded:: 1.3.0\n\n            - callable, function with signature\n              ``(bad_line: list[str]) -&gt; list[str] | None`` that will process a single\n              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n              If the function returns ``None``, the bad line will be ignored.\n              If the function returns a new list of strings with more elements than\n              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n              Only supported when ``engine=\"python\"``\n\n        .. versionadded:: 1.4.0\n\n    delim_whitespace : bool, default False\n        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n        is set to True, nothing should be passed in for the ``delimiter``\n        parameter.\n    low_memory : bool, default True\n        Internally process the file in chunks, resulting in lower memory use\n        while parsing, but possibly mixed type inference.  To ensure no mixed\n        types either set False, or specify the type with the `dtype` parameter.\n        Note that the entire file is read into a single DataFrame regardless,\n        use the `chunksize` or `iterator` parameter to return the data in chunks.\n        (Only valid with C parser).\n    memory_map : bool, default False\n        If a filepath is provided for `filepath_or_buffer`, map the file object\n        directly onto memory and access the data directly from there. Using this\n        option can improve performance because there is no longer any I/O overhead.\n    float_precision : str, optional\n        Specifies which converter the C engine should use for floating-point\n        values. The options are ``None`` or 'high' for the ordinary converter,\n        'legacy' for the original lower precision pandas converter, and\n        'round_trip' for the round-trip converter.\n\n        .. versionchanged:: 1.2\n\n    storage_options : dict, optional\n        Extra options that make sense for a particular storage connection, e.g.\n        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n\n        .. versionadded:: 1.2\n\n    Returns\n    -------\n    DataFrame or TextParser\n        A comma-separated values (csv) file is returned as two-dimensional\n        data structure with labeled axes.\n\n    See Also\n    --------\n    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pd.read_csv('data.csv')  # doctest: +SKIP\nsample_df = pd.read_csv(sample_table_url, sep=\"\\t\")\nlibrary_df = pd.read_csv(library_table_url, sep=\"\\t\")\nsample_df.project_name.nunique()\n45\nlibrary_df.project_name.nunique()\n43\n\n8.7.1 Listing the columns of the sample dataframe\nsample_df.columns\nIndex(['project_name', 'publication_year', 'publication_doi', 'site_name',\n       'latitude', 'longitude', 'geo_loc_name', 'sample_name', 'sample_host',\n       'sample_age', 'sample_age_doi', 'community_type', 'material', 'archive',\n       'archive_project', 'archive_accession'],\n      dtype='object')\n\n\n8.7.2 Looking at the data type of the sample dataframe\nsample_df.dtypes\nproject_name          object\npublication_year       int64\npublication_doi       object\nsite_name             object\nlatitude             float64\nlongitude            float64\ngeo_loc_name          object\nsample_name           object\nsample_host           object\nsample_age             int64\nsample_age_doi        object\ncommunity_type        object\nmaterial              object\narchive               object\narchive_project       object\narchive_accession     object\ndtype: object\n\nint64 is for integers\nfloating64 is for floating point precision numbers, also known as double in some other programing languages\nobject is a general type in pandas for everything that is not a number, interval, categorical, or date\n\n\n\n8.7.3 Let’s inspect our data\nWhat is the size of our dataframe ?\nsample_df.shape\n(1060, 16)\nThis dataframe has 1060 rows, and 16 columns\nLet’s look at the first 5 rows\nsample_df.head()\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n2\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nGola Forest\n\n\n7.657\n\n\n-10.841\n\n\nSierra Leone\n\n\nChimp\n\n\nPan troglodytes\n\n\n100\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890499\n\n\n\n\n3\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.328\n\n\nSpain\n\n\nElSidron1\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890498\n\n\n\n\n4\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.329\n\n\nSpain\n\n\nElSidron2\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890496\n\n\n\n\n\n\nUnlike R, Python is 0 based language, meaning the first element is of index 0, not like R where it is 1.\nLet’s look at the last 5 rows\nsample_df.tail()\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n1055\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT2\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283094,ERS7283095\n\n\n\n\n1056\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283096,ERS7283097\n\n\n\n\n1057\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT9\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283098,ERS7283099\n\n\n\n\n1058\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nDom Square, Riga\n\n\n56.949\n\n\n24.104\n\n\nLatvia\n\n\nTZA3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283100,ERS7283101\n\n\n\n\n1059\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Peter’s Church, Riga\n\n\n56.947\n\n\n24.109\n\n\nLatvia\n\n\nTZA4\n\n\nHomo sapiens\n\n\n500\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283102,ERS7283103\n\n\n\n\n\n\nLet’s randomly inspect 5 rows\nsample_df.sample(n=5)\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n413\n\n\nNeukamm2020\n\n\n2020\n\n\n10.1186/s12915-020-00839-8\n\n\nAbusir el-Meleq\n\n\n29.240\n\n\n31.100\n\n\nEgypt\n\n\nAbusir1576\n\n\nHomo sapiens\n\n\n2200\n\n\n10.1038/ncomms15694\n\n\nskeletal tissue\n\n\nbone\n\n\nENA\n\n\nPRJEB33848\n\n\nERS3635981\n\n\n\n\n754\n\n\nRampelli2021\n\n\n2021\n\n\n10.1038/s42003-021-01689-y\n\n\nEl Salt\n\n\n38.687\n\n\n-0.508\n\n\nSpain\n\n\nV3\n\n\nHomo sapiens neanderthalensis\n\n\n44700\n\n\n10.1038/s42003-021-01689-y\n\n\ngut\n\n\nsediment\n\n\nENA\n\n\nPRJEB41665\n\n\nERS5428042\n\n\n\n\n436\n\n\nNeukamm2020\n\n\n2020\n\n\n10.1186/s12915-020-00839-8\n\n\nAbusir el-Meleq\n\n\n29.240\n\n\n31.100\n\n\nEgypt\n\n\nAbusir1606\n\n\nHomo sapiens\n\n\n2600\n\n\n10.1186/s12915-020-00839-8\n\n\nskeletal tissue\n\n\nbone\n\n\nENA\n\n\nPRJEB33848\n\n\nERS3635928\n\n\n\n\n474\n\n\nNeukamm2020\n\n\n2020\n\n\n10.1186/s12915-020-00839-8\n\n\nAbusir el-Meleq\n\n\n29.240\n\n\n31.100\n\n\nEgypt\n\n\nAbusir1654\n\n\nHomo sapiens\n\n\n2300\n\n\n10.1038/ncomms15694\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB33848\n\n\nERS3635960\n\n\n\n\n573\n\n\nPhilips2017\n\n\n2017\n\n\n10.1186/s12864-020-06810-9\n\n\nKowalewko\n\n\n52.699\n\n\n17.605\n\n\nPoland\n\n\nPCA0040\n\n\nHomo sapiens\n\n\n1900\n\n\n10.1186/s12864-020-06810-9\n\n\noral\n\n\ntooth\n\n\nSRA\n\n\nPRJNA354503\n\n\nSRS1815407\n\n\n\n\n\n\n\n8.7.3.0.0.1 Accessing the data by index/columns\nThe are different way of selecting of subset of a dataframe\nSelecting by the row index\n# selecting the 10th row, and all columns\nsample_df.iloc[9, :]\nproject_name                    Weyrich2017\npublication_year                       2017\npublication_doi         10.1038/nature21674\nsite_name            Stuttgart-Mühlhausen I\nlatitude                             48.839\nlongitude                             9.227\ngeo_loc_name                        Germany\nsample_name                        EuroLBK1\nsample_host                    Homo sapiens\nsample_age                             7400\nsample_age_doi          10.1038/nature21674\ncommunity_type                         oral\nmaterial                    dental calculus\narchive                                 SRA\narchive_project                 PRJNA685265\narchive_accession                SRS7890488\nName: 9, dtype: object\n# selecting the 10th to 12th row, and all columns\nsample_df.iloc[9:12, :]\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n9\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n48.839\n\n\n9.227\n\n\nGermany\n\n\nEuroLBK1\n\n\nHomo sapiens\n\n\n7400\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890488\n\n\n\n\n10\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n48.839\n\n\n9.227\n\n\nGermany\n\n\nEuroLBK2\n\n\nHomo sapiens\n\n\n7400\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890485\n\n\n\n\n11\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n48.839\n\n\n9.227\n\n\nGermany\n\n\nEuroLBK3\n\n\nHomo sapiens\n\n\n7400\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890490\n\n\n\n\n\n\n# selecting the 10th to 12th row, and the first to the 4th column\nsample_df.iloc[9:12, 0:4]\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\n\n\n\n\n9\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n\n\n10\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n\n\n11\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nStuttgart-Mühlhausen I\n\n\n\n\n\n\n# selecting the column site_name\nsample_df['site_name']\n0                           Dalheim\n1                           Dalheim\n2                       Gola Forest\n3                    El Sidrón Cave\n4                    El Sidrón Cave\n                   ...\n1055    St. Gertrude’s Church, Riga\n1056    St. Gertrude’s Church, Riga\n1057    St. Gertrude’s Church, Riga\n1058               Dom Square, Riga\n1059       St. Peter’s Church, Riga\nName: site_name, Length: 1060, dtype: object\n# Also valid, but less preferred\nsample_df.site_name\n0                           Dalheim\n1                           Dalheim\n2                       Gola Forest\n3                    El Sidrón Cave\n4                    El Sidrón Cave\n                   ...\n1055    St. Gertrude’s Church, Riga\n1056    St. Gertrude’s Church, Riga\n1057    St. Gertrude’s Church, Riga\n1058               Dom Square, Riga\n1059       St. Peter’s Church, Riga\nName: site_name, Length: 1060, dtype: object\n# Removing a row\nsample_df.drop(0)\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n2\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nGola Forest\n\n\n7.657\n\n\n-10.841\n\n\nSierra Leone\n\n\nChimp\n\n\nPan troglodytes\n\n\n100\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890499\n\n\n\n\n3\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.328\n\n\nSpain\n\n\nElSidron1\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890498\n\n\n\n\n4\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.329\n\n\nSpain\n\n\nElSidron2\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890496\n\n\n\n\n5\n\n\nWeyrich2017\n\n\n2017\n\n\n10.1038/nature21674\n\n\nSpy Cave\n\n\n50.480\n\n\n4.674\n\n\nBelgium\n\n\nSpy1\n\n\nHomo sapiens neanderthalensis\n\n\n35800\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890491\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1055\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT2\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283094,ERS7283095\n\n\n\n\n1056\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283096,ERS7283097\n\n\n\n\n1057\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT9\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283098,ERS7283099\n\n\n\n\n1058\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nDom Square, Riga\n\n\n56.949\n\n\n24.104\n\n\nLatvia\n\n\nTZA3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283100,ERS7283101\n\n\n\n\n1059\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Peter’s Church, Riga\n\n\n56.947\n\n\n24.109\n\n\nLatvia\n\n\nTZA4\n\n\nHomo sapiens\n\n\n500\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283102,ERS7283103\n\n\n\n\n\n\n1059 rows × 16 columns\n\n\n# Removing a columm\nsample_df.drop('project_name', axis=1)\n\n\n\n\n\n\n\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n2\n\n\n2017\n\n\n10.1038/nature21674\n\n\nGola Forest\n\n\n7.657\n\n\n-10.841\n\n\nSierra Leone\n\n\nChimp\n\n\nPan troglodytes\n\n\n100\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890499\n\n\n\n\n3\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.328\n\n\nSpain\n\n\nElSidron1\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890498\n\n\n\n\n4\n\n\n2017\n\n\n10.1038/nature21674\n\n\nEl Sidrón Cave\n\n\n43.386\n\n\n-5.329\n\n\nSpain\n\n\nElSidron2\n\n\nHomo sapiens neanderthalensis\n\n\n49000\n\n\n10.1038/nature21674\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA685265\n\n\nSRS7890496\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1055\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT2\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283094,ERS7283095\n\n\n\n\n1056\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283096,ERS7283097\n\n\n\n\n1057\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT9\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283098,ERS7283099\n\n\n\n\n1058\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nDom Square, Riga\n\n\n56.949\n\n\n24.104\n\n\nLatvia\n\n\nTZA3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283100,ERS7283101\n\n\n\n\n1059\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Peter’s Church, Riga\n\n\n56.947\n\n\n24.109\n\n\nLatvia\n\n\nTZA4\n\n\nHomo sapiens\n\n\n500\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283102,ERS7283103\n\n\n\n\n\n\n1060 rows × 15 columns\n\n\n\n\n8.7.3.0.1 4 - Dealing with missing data\nChecking is some entries if the table have missing data (NA or NaN)\nsample_df.isna()\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n2\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n3\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n4\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1055\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1056\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1057\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1058\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n1059\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\nFalse\n\n\n\n\n\n\n1060 rows × 16 columns\n\n\n# making the sum by row - axis=1\nsample_df.isna().sum(axis=1)\n0       0\n1       0\n2       0\n3       0\n4       0\n       ..\n1055    0\n1056    0\n1057    0\n1058    0\n1059    0\nLength: 1060, dtype: int64\nSorting by decreasing order to check which rows have missing values\nsample_df.isna().sum(axis=1).sort_values(ascending=False)\n800     2\n962     2\n992     2\n801     2\n802     2\n       ..\n362     0\n363     0\n364     0\n365     0\n1059    0\nLength: 1060, dtype: int64\nsample_df.iloc[800,:]\nproject_name                         FellowsYates2021\npublication_year                                 2021\npublication_doi               10.1073/pnas.2021655118\nsite_name                               Not specified\nlatitude                                          NaN\nlongitude                                         NaN\ngeo_loc_name         Democratic Republic of the Congo\nsample_name                                  GDC002.A\nsample_host                   Gorilla gorilla gorilla\nsample_age                                        200\nsample_age_doi                10.1073/pnas.2021655118\ncommunity_type                                   oral\nmaterial                              dental calculus\narchive                                           ENA\narchive_project                            PRJEB34569\narchive_accession                          ERS3774403\nName: 800, dtype: object\nWhat to do now ? The ideal scenario would be to correct or impute the data.\nHowever, sometimes, the only thing we can do is remove the row with missing data, with the .dropna() function.\nHere, we’re just going to ignore them, and deal with it individually if necessary"
  },
  {
    "objectID": "introduction-to-python.html#computing-basic-statistics",
    "href": "introduction-to-python.html#computing-basic-statistics",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.8 5 - Computing basic statistics",
    "text": "8.8 5 - Computing basic statistics\nTLDR: use the describe() function, the equivalent of summarize in R\nsample_df.describe()\n\n\n\n\n\n\n\n\n\npublication_year\n\n\nlatitude\n\n\nlongitude\n\n\nsample_age\n\n\n\n\n\n\ncount\n\n\n1060.000000\n\n\n1021.000000\n\n\n1021.000000\n\n\n1060.000000\n\n\n\n\nmean\n\n\n2019.377358\n\n\n40.600493\n\n\n3.749624\n\n\n3588.443396\n\n\n\n\nstd\n\n\n1.633877\n\n\n18.469421\n\n\n43.790316\n\n\n9862.416855\n\n\n\n\nmin\n\n\n2014.000000\n\n\n-34.030000\n\n\n-121.800000\n\n\n100.000000\n\n\n\n\n25%\n\n\n2018.000000\n\n\n29.240000\n\n\n-1.257000\n\n\n200.000000\n\n\n\n\n50%\n\n\n2020.000000\n\n\n45.450000\n\n\n14.381000\n\n\n1000.000000\n\n\n\n\n75%\n\n\n2021.000000\n\n\n52.699000\n\n\n23.892000\n\n\n2200.000000\n\n\n\n\nmax\n\n\n2021.000000\n\n\n79.000000\n\n\n159.346000\n\n\n102000.000000\n\n\n\n\n\n\nLet’s look at various individual summary statistics We can run them on the whole dataframe (for int or float columns), or on a subset of columns\nsample_df.mean()\n/var/folders/1c/l1qb09f15jddsh65f6xv1n_r0000gp/T/ipykernel_69168/2260452167.py:1:\nFutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None')\nis deprecated; in a future version this will raise TypeError.  Select only valid columns\nbefore calling the reduction.\n\n\n\n\n\npublication_year    2019.377358\nlatitude              40.600493\nlongitude              3.749624\nsample_age          3588.443396\ndtype: float64\nsample_df['publication_year'].describe()\ncount    1060.000000\nmean     2019.377358\nstd         1.633877\nmin      2014.000000\n25%      2018.000000\n50%      2020.000000\n75%      2021.000000\nmax      2021.000000\nName: publication_year, dtype: float64\n# The average publication year\nsample_df['publication_year'].mean()\n2019.377358490566\n# The median publication year\nsample_df['publication_year'].median()\n2020.0\n# The minimum, or oldest publication year\nsample_df['publication_year'].min()\n2014\n# The maximum, or most recent publication year\nsample_df['publication_year'].max()\n2021\n# The number of sites\nsample_df['site_name'].nunique()\n246\n# The number of samples from the different hosts\nsample_df['sample_host'].value_counts()\nHomo sapiens                      741\nUrsus arctos                       85\nAmbrosia artemisiifolia            46\nArabidopsis thaliana               34\nHomo sapiens neanderthalensis      32\nPan troglodytes schweinfurthii     26\nGorilla beringei beringei          15\nCanis lupus                        12\nGorilla gorilla gorilla             8\nMammuthus primigenius               8\nPan troglodytes verus               7\nRangifer tarandus                   6\nGorilla beringei graueri            6\nPan troglodytes ellioti             6\nPapio hamadryas                     5\nAlouatta palliata                   5\nConepatus chinga                    4\nGerbilliscus boehmi                 4\nStrigocuscus celebensis             4\nPapio anubis                        2\nGorilla beringei                    2\nPapio sp.                           1\nPan troglodytes                     1\nName: sample_host, dtype: int64\n# The quantile of the publication years\nsample_df['publication_year'].quantile(np.arange(0,1,0.1))\n0.0    2014.0\n0.1    2017.0\n0.2    2018.0\n0.3    2018.0\n0.4    2020.0\n0.5    2020.0\n0.6    2020.0\n0.7    2021.0\n0.8    2021.0\n0.9    2021.0\nName: publication_year, dtype: float64\n# We can also visualize it with built-in plot functions of pandas\nsample_df['publication_year'].plot.hist()\n&lt;AxesSubplot:ylabel='Frequency'&gt;\n\n\n\npng"
  },
  {
    "objectID": "introduction-to-python.html#filtering",
    "href": "introduction-to-python.html#filtering",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.9 6 - Filtering",
    "text": "8.9 6 - Filtering\nThere are different ways of filtering data with Pandas:\n\nThe classic method with bracket indexing/subsetting\nThe query() method\n\nThe classic method\n# Getting all the publications before 2015\nsample_df[sample_df['publication_year']  &lt; 2015 ]\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n272\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP4\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428959\n\n\n\n\n273\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP10\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428961\n\n\n\n\n274\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP18\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428962\n\n\n\n\n275\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP37\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428963\n\n\n\n\n276\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP9\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428960\n\n\n\n\n277\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP48\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428964\n\n\n\n\n278\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP02,TP10,TP15,TP26\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428958\n\n\n\n\n279\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP32,TP42,TP45,TP48\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428972\n\n\n\n\n500\n\n\nAppelt2014\n\n\n2014\n\n\n10.1128/AEM.03242-13\n\n\nPlace d’Armes, Namur\n\n\n50.460\n\n\n4.86\n\n\nBelgium\n\n\n4.453\n\n\nHomo sapiens\n\n\n600\n\n\n10.1128/AEM.03242-13\n\n\ngut\n\n\npalaeofaeces\n\n\nSRA\n\n\nPRJNA230469\n\n\nSRS510175\n\n\n\n\n\n\n# Getting all the publications before 2015, only in the Northern hemisphere\nsample_df[(sample_df['publication_year']  &lt; 2015) & (sample_df['longitude'] &gt; 0)]\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n500\n\n\nAppelt2014\n\n\n2014\n\n\n10.1128/AEM.03242-13\n\n\nPlace d’Armes, Namur\n\n\n50.460\n\n\n4.86\n\n\nBelgium\n\n\n4.453\n\n\nHomo sapiens\n\n\n600\n\n\n10.1128/AEM.03242-13\n\n\ngut\n\n\npalaeofaeces\n\n\nSRA\n\n\nPRJNA230469\n\n\nSRS510175\n\n\n\n\n\n\nThis syntax can rapidly become quite cumbersome, which is why we can also use the query() method\n# Getting all the publications before 2015\nsample_df.query(\"publication_year &lt; 2015\")\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n272\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP4\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428959\n\n\n\n\n273\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP10\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428961\n\n\n\n\n274\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP18\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428962\n\n\n\n\n275\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP37\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428963\n\n\n\n\n276\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP9\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428960\n\n\n\n\n277\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP48\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428964\n\n\n\n\n278\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP02,TP10,TP15,TP26\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428958\n\n\n\n\n279\n\n\nCampana2014\n\n\n2014\n\n\n10.1186/1756-0500-7-111\n\n\nTeposcolula Yucundaa\n\n\n17.490\n\n\n-97.46\n\n\nMexico\n\n\nTP32,TP42,TP45,TP48\n\n\nHomo sapiens\n\n\n400\n\n\n10.7183/1045-6635.23.4.467\n\n\nskeletal tissue\n\n\nbone\n\n\nSRA\n\n\nPRJNA205039\n\n\nSRS428972\n\n\n\n\n500\n\n\nAppelt2014\n\n\n2014\n\n\n10.1128/AEM.03242-13\n\n\nPlace d’Armes, Namur\n\n\n50.460\n\n\n4.86\n\n\nBelgium\n\n\n4.453\n\n\nHomo sapiens\n\n\n600\n\n\n10.1128/AEM.03242-13\n\n\ngut\n\n\npalaeofaeces\n\n\nSRA\n\n\nPRJNA230469\n\n\nSRS510175\n\n\n\n\n\n\n# Getting all the publications before 2015, only the Northern hemisphere\nsample_df.query(\"publication_year &lt; 2015 and longitude &gt; 0 \")\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742,SRS473743,SRS473744,SRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747,SRS473746,SRS473748,SRS473749,SRS473750\n\n\n\n\n500\n\n\nAppelt2014\n\n\n2014\n\n\n10.1128/AEM.03242-13\n\n\nPlace d’Armes, Namur\n\n\n50.460\n\n\n4.86\n\n\nBelgium\n\n\n4.453\n\n\nHomo sapiens\n\n\n600\n\n\n10.1128/AEM.03242-13\n\n\ngut\n\n\npalaeofaeces\n\n\nSRA\n\n\nPRJNA230469\n\n\nSRS510175"
  },
  {
    "objectID": "introduction-to-python.html#groupby-operations-and-computing-statistics-on-grouped-values",
    "href": "introduction-to-python.html#groupby-operations-and-computing-statistics-on-grouped-values",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.10 7 - GroupBy operations, and computing statistics on grouped values",
    "text": "8.10 7 - GroupBy operations, and computing statistics on grouped values\nThe “groupBy” operation, as the name suggests, allows us to group values by a grouping key, and perform a groupwise operation.\nFor example, we can group by the sample_host and get the age of the youngest sample in each group\nsample_df.groupby(\"sample_host\")['sample_age'].min()\nsample_host\nAlouatta palliata                   200\nAmbrosia artemisiifolia             100\nArabidopsis thaliana                100\nCanis lupus                         400\nConepatus chinga                    100\nGerbilliscus boehmi                 100\nGorilla beringei                    100\nGorilla beringei beringei           200\nGorilla beringei graueri            200\nGorilla gorilla gorilla             200\nHomo sapiens                        100\nHomo sapiens neanderthalensis     35800\nMammuthus primigenius             41800\nPan troglodytes                     100\nPan troglodytes ellioti             200\nPan troglodytes schweinfurthii      100\nPan troglodytes verus               200\nPapio anubis                        100\nPapio hamadryas                     100\nPapio sp.                           100\nRangifer tarandus                   100\nStrigocuscus celebensis             100\nUrsus arctos                        100\nName: sample_age, dtype: int64\nHere min() is a so-called aggregation function\nNotice that .value_counts() is actually a special case of .groupby()\nsample_df.groupby(\"sample_host\")[\"sample_host\"].count()\nsample_host\nAlouatta palliata                   5\nAmbrosia artemisiifolia            46\nArabidopsis thaliana               34\nCanis lupus                        12\nConepatus chinga                    4\nGerbilliscus boehmi                 4\nGorilla beringei                    2\nGorilla beringei beringei          15\nGorilla beringei graueri            6\nGorilla gorilla gorilla             8\nHomo sapiens                      741\nHomo sapiens neanderthalensis      32\nMammuthus primigenius               8\nPan troglodytes                     1\nPan troglodytes ellioti             6\nPan troglodytes schweinfurthii     26\nPan troglodytes verus               7\nPapio anubis                        2\nPapio hamadryas                     5\nPapio sp.                           1\nRangifer tarandus                   6\nStrigocuscus celebensis             4\nUrsus arctos                       85\nName: sample_host, dtype: int64"
  },
  {
    "objectID": "introduction-to-python.html#reshaping-data-from-wide-to-long-and-back",
    "href": "introduction-to-python.html#reshaping-data-from-wide-to-long-and-back",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.11 8 - Reshaping data, from wide to long and back",
    "text": "8.11 8 - Reshaping data, from wide to long and back\n\n\n8.11.1 From wide to long/tidy\nThe tidy format, or long format idea is that one column = one kind of data.\nUnfortunately for this tutorial, the AncientMetagenomeDir tables are already in the tidy format (good), so we’ll see an example or the wide format just below\nwide_df = pd.DataFrame(\n    [\n    [150,155,157,160],\n    [149,153,154,155]\n    ]\n    , index = ['John','Jack']\n    , columns = [1991,1992,1993, 1994]\n).rename_axis('individual').reset_index()\nwide_df\n\n\n\n\n\n\n\n\n\nindividual\n\n\n1991\n\n\n1992\n\n\n1993\n\n\n1994\n\n\n\n\n\n\n0\n\n\nJohn\n\n\n150\n\n\n155\n\n\n157\n\n\n160\n\n\n\n\n1\n\n\nJack\n\n\n149\n\n\n153\n\n\n154\n\n\n155\n\n\n\n\n\n\nIn this hypothetic dataframe, we have the years as column, the individual as index, and their height as value.\nWe’ll reformat to the tidy/long format using the .melt() function\ntidy_df = wide_df.melt(id_vars='individual', var_name='birthyear', value_name='height')\ntidy_df\n\n\n\n\n\n\n\n\n\nindividual\n\n\nbirthyear\n\n\nheight\n\n\n\n\n\n\n0\n\n\nJohn\n\n\n1991\n\n\n150\n\n\n\n\n1\n\n\nJack\n\n\n1991\n\n\n149\n\n\n\n\n2\n\n\nJohn\n\n\n1992\n\n\n155\n\n\n\n\n3\n\n\nJack\n\n\n1992\n\n\n153\n\n\n\n\n4\n\n\nJohn\n\n\n1993\n\n\n157\n\n\n\n\n5\n\n\nJack\n\n\n1993\n\n\n154\n\n\n\n\n6\n\n\nJohn\n\n\n1994\n\n\n160\n\n\n\n\n7\n\n\nJack\n\n\n1994\n\n\n155\n\n\n\n\n\n\n\nBonus: How to deal with a dataframe with the kind of data indicated in the column name, typically like so\n\nwide_df = pd.DataFrame(\n    [\n    [150,155,157,160],\n    [149,153,154,155]\n    ]\n    , index = ['John','Jack']\n    , columns = [\"year-1991\",\"year-1992\",\"year-1993\", \"year-1994\"]\n).rename_axis('individual').reset_index()\nwide_df\n\n\n\n\n\n\n\n\n\nindividual\n\n\nyear-1991\n\n\nyear-1992\n\n\nyear-1993\n\n\nyear-1994\n\n\n\n\n\n\n0\n\n\nJohn\n\n\n150\n\n\n155\n\n\n157\n\n\n160\n\n\n\n\n1\n\n\nJack\n\n\n149\n\n\n153\n\n\n154\n\n\n155\n\n\n\n\n\n\npd.wide_to_long(wide_df, ['year'], i='individual', j='birthyear', sep=\"-\").rename(columns={'year':'height'})\n\n\n\n\n\n\n\n\n\n\n\nheight\n\n\n\n\nindividual\n\n\nbirthyear\n\n\n\n\n\n\n\n\nJohn\n\n\n1991\n\n\n150\n\n\n\n\nJack\n\n\n1991\n\n\n149\n\n\n\n\nJohn\n\n\n1992\n\n\n155\n\n\n\n\nJack\n\n\n1992\n\n\n153\n\n\n\n\nJohn\n\n\n1993\n\n\n157\n\n\n\n\nJack\n\n\n1993\n\n\n154\n\n\n\n\nJohn\n\n\n1994\n\n\n160\n\n\n\n\nJack\n\n\n1994\n\n\n155\n\n\n\n\n\n\n\n\n8.11.2 From long/tidy to wide format using the .pivot() function.\ntidy_df.pivot(index='individual', columns='birthyear', values='height')\n/Users/maxime/mambaforge/envs/intro-data/lib/python3.10/site-packages/pandas/core/algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n\n\n\n\n\n\n\nbirthyear\n\n\n1991\n\n\n1992\n\n\n1993\n\n\n1994\n\n\n\n\nindividual\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJack\n\n\n149\n\n\n153\n\n\n154\n\n\n155\n\n\n\n\nJohn\n\n\n150\n\n\n155\n\n\n157\n\n\n160"
  },
  {
    "objectID": "introduction-to-python.html#joining-two-different-tables",
    "href": "introduction-to-python.html#joining-two-different-tables",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.12 9 - Joining two different tables",
    "text": "8.12 9 - Joining two different tables\nIn AncientMetagenomeDir, the information about each sample is located in sample table, and about the library in the library table.\nTo match these two together, we need to join the tables together.\nTo do so, we need a column in common between the two tables, the so-called joining key (this key can be the index)\n\nFor the samples and libraries dataframe, the joining key is the column sample_name\nsample_df.merge(library_df, on='sample_name').columns\nIndex(['project_name_x', 'publication_year_x', 'publication_doi', 'site_name',\n       'latitude', 'longitude', 'geo_loc_name', 'sample_name', 'sample_host',\n       'sample_age', 'sample_age_doi', 'community_type', 'material',\n       'archive_x', 'archive_project_x', 'archive_accession', 'project_name_y',\n       'publication_year_y', 'data_publication_doi', 'archive_y',\n       'archive_project_y', 'archive_sample_accession', 'library_name',\n       'strand_type', 'library_polymerase', 'library_treatment',\n       'library_concentration', 'instrument_model', 'library_layout',\n       'library_strategy', 'read_count', 'archive_data_accession',\n       'download_links', 'download_md5s', 'download_sizes'],\n      dtype='object')\nWe have some duplicate columns that we can get rid of:\nmerged_df = sample_df.merge(library_df.drop(['project_name', 'publication_year', 'archive_project', 'archive'], axis=1), on='sample_name')\nmerged_df\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\n…\n\n\nlibrary_treatment\n\n\nlibrary_concentration\n\n\ninstrument_model\n\n\nlibrary_layout\n\n\nlibrary_strategy\n\n\nread_count\n\n\narchive_data_accession\n\n\ndownload_links\n\n\ndownload_md5s\n\n\ndownload_sizes\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2000\n\n\nSINGLE\n\n\nWGS\n\n\n13228381\n\n\nSRR957738\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957738/…\n\n\n9c40c43b5d455e760ae8db924347f0b2\n\n\n953396663\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2000\n\n\nSINGLE\n\n\nWGS\n\n\n13260566\n\n\nSRR957739\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957739/…\n\n\ndec1507f742de109529638bf00e0732f\n\n\n1026825795\n\n\n\n\n2\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2000\n\n\nSINGLE\n\n\nWGS\n\n\n8869866\n\n\nSRR957740\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957740/…\n\n\nbc49c59f489b4009206f8abcb737d55d\n\n\n661500786\n\n\n\n\n3\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2000\n\n\nSINGLE\n\n\nWGS\n\n\n11275013\n\n\nSRR957741\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957741/…\n\n\ne02e3549ddd3ba6dc278a7f573c07321\n\n\n877360302\n\n\n\n\n4\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.84\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2000\n\n\nSINGLE\n\n\nWGS\n\n\n8978974\n\n\nSRR957742\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/SRR957/SRR957742/…\n\n\nb7c620b8ee165c08bee204529341ca5b\n\n\n690614774\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1802\n\n\nMaixner2021\n\n\n2021\n\n\n10.1016/j.cub.2021.09.031\n\n\nEdlersbergwerk - oben, Hallstatt\n\n\n47.560\n\n\n13.63\n\n\nAustria\n\n\n2612\n\n\nHomo sapiens\n\n\n150\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina MiSeq\n\n\nPAIRED\n\n\nWGS\n\n\n1858404\n\n\nERR5766179\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/ERR576/009/ERR576…\n\n\n542787c645b0aeebe15c66cc926d3f69;0bc58d56be3c3…\n\n\n86783041;98100690\n\n\n\n\n1803\n\n\nMaixner2021\n\n\n2021\n\n\n10.1016/j.cub.2021.09.031\n\n\nEdlersbergwerk - oben, Hallstatt\n\n\n47.560\n\n\n13.63\n\n\nAustria\n\n\n2612\n\n\nHomo sapiens\n\n\n150\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina MiSeq\n\n\nPAIRED\n\n\nWGS\n\n\n1603064\n\n\nERR5766180\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/ERR576/000/ERR576…\n\n\n022bb28da460e66590e974b4135bdd2e;f88acec67b648…\n\n\n74375931;77621627\n\n\n\n\n1804\n\n\nMaixner2021\n\n\n2021\n\n\n10.1016/j.cub.2021.09.031\n\n\nEdlersbergwerk - oben, Hallstatt\n\n\n47.560\n\n\n13.63\n\n\nAustria\n\n\n2612\n\n\nHomo sapiens\n\n\n150\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina MiSeq\n\n\nPAIRED\n\n\nWGS\n\n\n1075088\n\n\nERR5766181\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/ERR576/001/ERR576…\n\n\n57fc575d32db14f1d5c1ed7f6a106e91;4f57b9d978b53…\n\n\n51852071;56288763\n\n\n\n\n1805\n\n\nMaixner2021\n\n\n2021\n\n\n10.1016/j.cub.2021.09.031\n\n\nEdlersbergwerk - oben, Hallstatt\n\n\n47.560\n\n\n13.63\n\n\nAustria\n\n\n2612\n\n\nHomo sapiens\n\n\n150\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nIllumina HiSeq 2500\n\n\nPAIRED\n\n\nWGS\n\n\n138836358\n\n\nERR5766182\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/ERR576/002/ERR576…\n\n\n64e63df8da7542957d1d9eb08e764d38;3fc6cba02c74d…\n\n\n4332353625;4420486328\n\n\n\n\n1806\n\n\nMaixner2021\n\n\n2021\n\n\n10.1016/j.cub.2021.09.031\n\n\nEdlersbergwerk - oben, Hallstatt\n\n\n47.560\n\n\n13.63\n\n\nAustria\n\n\n2612\n\n\nHomo sapiens\n\n\n150\n\n\n…\n\n\nnone\n\n\nNaN\n\n\nHiSeq X Ten\n\n\nPAIRED\n\n\nWGS\n\n\n84192332\n\n\nERR5766183\n\n\nftp.sra.ebi.ac.uk/vol1/fastq/ERR576/003/ERR576…\n\n\n43ac661c4e211ed6ee2940dcab8b13cb;88de66a85df92…\n\n\n3128863954;3460789287\n\n\n\n\n\n\n1807 rows × 31 columns"
  },
  {
    "objectID": "introduction-to-python.html#visualizing-some-of-the-results-with-plotnine",
    "href": "introduction-to-python.html#visualizing-some-of-the-results-with-plotnine",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.13 10 - Visualizing some of the results with Plotnine",
    "text": "8.13 10 - Visualizing some of the results with Plotnine\nPlotnine is the Python clone of ggplot2, the syntax is identical, which makes it great if you’re working with data in tidy/long format, and are already familiar with the ggplot2 syntax\nggplot(merged_df, aes(x='publication_year')) + geom_histogram() + theme_classic()\n/Users/maxime/mambaforge/envs/intro-data/lib/python3.10/\nsite-packages/plotnine/stats/stat_bin.py:95:\nPlotnineWarning: 'stat_bin()' using 'bins = 15'. Pick better value with 'binwidth'.\n\n\n\npng\n\n\n&lt;ggplot: (366051178)&gt;\nWe can start to ask some questions, for example, is the sequencing depth increasing with the years ?\nmerged_df['publication_year'] = merged_df['publication_year'].astype('category')\nggplot(merged_df, aes(x='publication_year', y='np.log10(read_count)', fill='publication_year')) +\ngeom_jitter(alpha=0.1) + geom_boxplot(alpha=0.8) + theme_classic()\n\n\n\npng\n\n\n&lt;ggplot: (366112582)&gt;\nWe could ask the same question, but first grouping the samples by publication year\navg_read_count_by_year = merged_df.groupby('publication_year')['read_count'].mean().to_frame().reset_index()\navg_read_count_by_year\n\n\n\n\n\n\n\n\n\npublication_year\n\n\nread_count\n\n\n\n\n\n\n0\n\n\n2014\n\n\n1.437173e+07\n\n\n\n\n1\n\n\n2016\n\n\n3.653450e+04\n\n\n\n\n2\n\n\n2017\n\n\n5.712598e+06\n\n\n\n\n3\n\n\n2018\n\n\n9.273287e+06\n\n\n\n\n4\n\n\n2019\n\n\n2.211632e+07\n\n\n\n\n5\n\n\n2020\n\n\n1.111819e+07\n\n\n\n\n6\n\n\n2021\n\n\n2.547655e+07\n\n\n\n\n\n\nggplot(avg_read_count_by_year, aes(x='publication_year', y='np.log10(read_count)', fill='publication_year')) + geom_point()\n\n\n\npng\n\n\n&lt;ggplot: (366206706)&gt;\nYour turn ! Make a plot to investigate the relation between the type of library treatment throughout the publication years"
  },
  {
    "objectID": "introduction-to-python.html#bonus-dealing-with-ill-formatted-columns",
    "href": "introduction-to-python.html#bonus-dealing-with-ill-formatted-columns",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.14 11 - Bonus, dealing with ill-formatted columns",
    "text": "8.14 11 - Bonus, dealing with ill-formatted columns\nSometimes, colums can contains entries which could be split in multiple columns, typically values separated by a comma. In AncientMetagenomeDir, this is the case with the archive accession column.\nHere is how we would solve it with pandas\nsample_df.assign(archive_accession = sample_df.archive_accession.str.split(\",\")).explode('archive_accession')\n\n\n\n\n\n\n\n\n\nproject_name\n\n\npublication_year\n\n\npublication_doi\n\n\nsite_name\n\n\nlatitude\n\n\nlongitude\n\n\ngeo_loc_name\n\n\nsample_name\n\n\nsample_host\n\n\nsample_age\n\n\nsample_age_doi\n\n\ncommunity_type\n\n\nmaterial\n\n\narchive\n\n\narchive_project\n\n\narchive_accession\n\n\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473742\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473743\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473744\n\n\n\n\n0\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nB61\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473745\n\n\n\n\n1\n\n\nWarinner2014\n\n\n2014\n\n\n10.1038/ng.2906\n\n\nDalheim\n\n\n51.565\n\n\n8.840\n\n\nGermany\n\n\nG12\n\n\nHomo sapiens\n\n\n900\n\n\n10.1038/ng.2906\n\n\noral\n\n\ndental calculus\n\n\nSRA\n\n\nPRJNA216965\n\n\nSRS473747\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1057\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Gertrude’s Church, Riga\n\n\n56.958\n\n\n24.121\n\n\nLatvia\n\n\nT9\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283099\n\n\n\n\n1058\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nDom Square, Riga\n\n\n56.949\n\n\n24.104\n\n\nLatvia\n\n\nTZA3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283100\n\n\n\n\n1058\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nDom Square, Riga\n\n\n56.949\n\n\n24.104\n\n\nLatvia\n\n\nTZA3\n\n\nHomo sapiens\n\n\n400\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283101\n\n\n\n\n1059\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Peter’s Church, Riga\n\n\n56.947\n\n\n24.109\n\n\nLatvia\n\n\nTZA4\n\n\nHomo sapiens\n\n\n500\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283102\n\n\n\n\n1059\n\n\nKazarina2021b\n\n\n2021\n\n\n10.1016/j.jasrep.2021.103213\n\n\nSt. Peter’s Church, Riga\n\n\n56.947\n\n\n24.109\n\n\nLatvia\n\n\nTZA4\n\n\nHomo sapiens\n\n\n500\n\n\n10.1016/j.jasrep.2021.103213\n\n\noral\n\n\ntooth\n\n\nENA\n\n\nPRJEB47251\n\n\nERS7283103\n\n\n\n\n\n\n1262 rows × 16 columns"
  },
  {
    "objectID": "introduction-to-git.html#introduction",
    "href": "introduction-to-git.html#introduction",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn this walkthrough, we will introduce the version control system Git as well as Github, a remote hosting service for version controlled repositories. Git and Github are increasingly popular tools for tracking data, collaborating on research projects, and sharing data and code, and learning to use them will help in many aspects of your own research. For more information on the benefits of using version control systems, see the slides.\n\n\n\n\n\n\nTip\n\n\n\nFor this chapter’s exercises, if not already performed, you will need to create the conda environment from the yml file in the following archive, and activate the environment:\nconda activate git-eager"
  },
  {
    "objectID": "introduction-to-git.html#lecture",
    "href": "introduction-to-git.html#lecture",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.2 Lecture",
    "text": "9.2 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-git.html#ssh-setup",
    "href": "introduction-to-git.html#ssh-setup",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.3 SSH setup",
    "text": "9.3 SSH setup\nTo begin, you will set up an SSH key to facilitate easier authentication when transferring data between local and remote repositories. In other words, follow this section of the tutorial so that you never have to type in your github password again! Begin by activating the conda environment for this section (see Preparation above).\nconda activate git-eager\nNext, generate your own ssh key, replacing the email below with your own address.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nI recommend saving the file to the default location and skipping passphrase setup. To do this, simply press enter without typing anything.\nYou should now (hopefully!) have generated an ssh key. To check that it worked, run the following commands to list the files containing your public and private keys and check that the ssh program is running.\ncd ~/.ssh/\nls id*\neval \"$(ssh-agent -s)\"\nNow you need to give ssh your key to record:\nssh-add ~/.ssh/id_ed15519\nNext, open your webbrowser and navigate to your github account. Go to settings -&gt; SSH & GPG Keys -&gt; New SSH Key. Give you key a title and paste the public key that you just generated on your local machine.\ncat ~/.ssh/id_ed15519\nFinally, press Add SSH key. To check that it worked, run the following command on your local machine. You should see a message telling you that you’ve successfully authenticated.\nssh -T git@github.com\nFor more information about setting up the SSH key, including instructions for different operating systems, check out github’s documentation: https://docs.github.com/es/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent."
  },
  {
    "objectID": "introduction-to-git.html#the-only-6-commands-you-really-need-to-know",
    "href": "introduction-to-git.html#the-only-6-commands-you-really-need-to-know",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.4 The only 6 commands you really need to know",
    "text": "9.4 The only 6 commands you really need to know\nNow that you have set up your own SSH key, we can begin working on some version controlled data! Navigate to your github homepage and create a new repository. You can choose any name for your new repo (including the default). Add a README file, then select Create Repository.\n  \n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the session, replace the name of my repository (vigilant-octo-journey) with your own repo name.\n\n\n\n\nChange into the directory where you would like to work, and let’s get started! First, we will learn to clone a remote repository onto your local machine. Navigate to your new repo, select the Code dropdown menu, select SSH, and copy the address as shown below.\n\nBack at your command line, clone the repo as follows:\ngit clone git@github.com:meganemichel/vigilant-octo-journey.git\nNext, let’s add a new or modified file to our ‘staging area’ on our local machine.\ncd vigilant-octo-journey\necho \"test_file\" &gt; file_A.txt\necho \"Just an example repo\" &gt;&gt; README.md\ngit add file_A.txt\nNow we can check what files have been locally changed, staged, etc. with status.\ngit status\nYou should see that file_A.txt is staged to be committed, but README.md is NOT. Try adding README.md and check the status again.\nNow we need to package or save the changes into a commit with a message describing the changes we’ve made. Each commit comes with a unique hash ID and will be stored forever in git history.\ngit commit -m \"Add example file\"\nFinally, let’s push our local commit back to our remote repository.\ngit push\nWhat if we want to download new commits from our remote to our local repository?\ngit pull\nYou should see that your repository is already up-to-date, since we have not made new changes to the remote repo. Let’s try making a change to the remote repository’s README file (as below). Then, back on the command line, pull the repository again."
  },
  {
    "objectID": "introduction-to-git.html#working-collaboratively",
    "href": "introduction-to-git.html#working-collaboratively",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.5 Working collaboratively",
    "text": "9.5 Working collaboratively\nGithub facilitates simultaneous work by small teams through branching, which generates a copy of the main repository within the repository. This can be edited without breaking the ‘master’ version. First, back on github, make a new branch of your repository.\n\nFrom the command line, you can create a new branch as follows:\ngit switch -c new_branch\nTo switch back to the main branch, use\ngit switch main\nNote that you must commit changes for them to be saved to the desired branch!"
  },
  {
    "objectID": "introduction-to-git.html#pull-requests",
    "href": "introduction-to-git.html#pull-requests",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.6 Pull requests",
    "text": "9.6 Pull requests\nA Pull request (aka PR) is used to propose changes to a branch from another branch. Others can comment and make suggestinos before your changes are merged into the main branch. For more information on creating a pull request, see github’s documentation: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request."
  },
  {
    "objectID": "introduction-to-git.html#questions-to-think-about",
    "href": "introduction-to-git.html#questions-to-think-about",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.7 Questions to think about",
    "text": "9.7 Questions to think about\n\nWhy is using a version control software for tracking data and code important?\nHow can using Git(Hub) help me to collaborate on group projects?"
  },
  {
    "objectID": "section-ancient-metagenomics.html#taxonomic-profiling",
    "href": "section-ancient-metagenomics.html#taxonomic-profiling",
    "title": "Ancient Metagenomics",
    "section": "Taxonomic Profiling",
    "text": "Taxonomic Profiling\n\nTBC"
  },
  {
    "objectID": "section-ancient-metagenomics.html#functional-profiling",
    "href": "section-ancient-metagenomics.html#functional-profiling",
    "title": "Ancient Metagenomics",
    "section": "Functional Profiling",
    "text": "Functional Profiling\nThe value of microbial taxonomy lies in the implied biochemical properties of a given taxon. Historically taxonomy was determined by growth characteristics and cell properties, and more recently through genomic and genetic similarity.\nThe genomic content of microbial taxa, specifically the presence or absence of genes, determine how those taxa interact with their environment, including all the biochemical processes they participate in, both internally and externally. Strains within any microbial species may have different genetic content and therefore may behave strikingly differently in the same environment, which cannot be determined through taxonomic profiling. Functionally profiling a microbial community, or determining all of the genes present independent of the species they are derived from, reveals the biochemical reactions and metabolic products the community may perform and produce, respectively.\nThis approach may provide insights to community activity and environmental interactions that are hidden when using taxonomic approaches alone. In this chapter we will perform functional profiling of metagenomic communities to assess their genetic content and inferred metabolic pathways."
  },
  {
    "objectID": "section-ancient-metagenomics.html#de-novo-assembly",
    "href": "section-ancient-metagenomics.html#de-novo-assembly",
    "title": "Ancient Metagenomics",
    "section": "De novo Assembly",
    "text": "De novo Assembly\nDe novo assembly of ancient metagenomic samples enables the recovery of the genetic information of organisms without requiring any prior knowledge about their genomes. Therefore, this approach is very well suited to study the biological diversity of species that have not been studied well or are simply not known yet.\nIn this chapter, we will show you how to prepare your sequencing data and subsequently de novo assemble them. Furthermore, we will then learn how we can actually evaluate what organisms we might have assembled and whether we obtained enough data to reconstruct a whole metagenome-assembled genome. We will particularly focus on the quality assessment of these reconstructed genomes and how we can ensure that we obtained high-quality genomes."
  },
  {
    "objectID": "taxonomic-profiling.html#lecture",
    "href": "taxonomic-profiling.html#lecture",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.1 Lecture",
    "text": "10.1 Lecture\n\n\nPDF version of these slides can be downloaded from here.\nThis session is run using a Jupyter notebook. This can be found here. However, it will already be installed on compute nodes during the summer school.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe highly recommend viewing this walkthrough via the Jupyter notebook above! The output of commands on the website for this walkthrough are displayed in their own code blocks - be wary of what you copy-paste!"
  },
  {
    "objectID": "taxonomic-profiling.html#download-and-subsample",
    "href": "taxonomic-profiling.html#download-and-subsample",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.2 Download and Subsample",
    "text": "10.2 Download and Subsample\nimport subprocess\nimport glob\nfrom pathlib import Path\nFor this tutorial, we will be using the ERR5766177 library from the sample 2612 published by Maixner et al. 2021\n\n`\n\n10.2.1 Subsampling the sequencing files to make the analysis quicker for this tutorial\ndef subsample(filename, outdir, depth=1000000):\n    basename = Path(filename).stem\n    cmd = f\"seqtk sample -s42 {filename} {depth} &gt; {outdir}/{basename}_subsample_{depth}.fastq\"\n    print(cmd)\n    subprocess.check_output(cmd, shell=True)\nfor f in glob.glob(\"../data/raw/*\"):\n    outdir = \"../data/subsampled\"\n    subsample(f, outdir)\nseqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.fwd.fq.gz 1000000 &gt;\n../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq\nseqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.rev.fq.gz 1000000 &gt;\n../data/subsampled/ERR5766177_PE.mapped.hostremoved.rev.fq_subsample_1000000.fastq\n! gzip -f ../data/subsampled/*.fastq"
  },
  {
    "objectID": "taxonomic-profiling.html#hands-on-introduction-to-ancient-microbiome-analysis",
    "href": "taxonomic-profiling.html#hands-on-introduction-to-ancient-microbiome-analysis",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.3 Hands on introduction to ancient microbiome analysis",
    "text": "10.3 Hands on introduction to ancient microbiome analysis\nAuthor: Maxime Borry\nDate: 12/08/2021\nIn this tutorial, we’re going to go through the steps necessary to:\n\ngenerate a taxonomic profile table with metaphlan v3.0\nhave a look at metaphlan results with Pavian and generate a Krona plot\nCompare the diversity of our samples vs the diversity of modern human gut samples\nCompare the composition of our samples vs modern gut samples, and see where they fit in a lower dimensional space\n\n\n10.3.1 0. Quick intro to Jupyter Notebooks\nThis a markdown cell\nprint(\"This is a python cell\")\nThis is a python cell\n! echo \"This is how to run a single line bash command\"\nThis is how to run a single line bash command\n%%bash\necho \"This how to run a multi\"\necho \"lines bash command\"\nThis how to run a multi\nlines bash command\n\n\n10.3.2 1. Data pre-processing\nBefore starting to analyze our data, we will need to pre-process them to remove reads mapping to the host genome, here, Homo sapiens\nTo do so, I’ve used nf-core/eager\nI’ve already pre-processed the data, and the resulting cleaned files are available in the data/eager_cleaned, but the basic eager command to do so is:\nnextflow run nf-core/eager -profile &lt;docker/singularity/podman/conda/institute&gt; --input '*_R{1,2}.fastq.gz' \\\n--fasta 'human_genome.fasta' --hostremoval_input_fastq\n\n\n10.3.3 2. Adapter sequence trimming and low-quality bases trimming\nSequencing adapters are small DNA sequences adding prior to DNA sequencing to allow the DNA fragments to attach to the sequencing flow cells. Because these adapters could interfere with downtstream analyses, we need to remove them before proceeding any further. Furthermore, because the quality of the sequencing is not always optimal, we need to remove bases of lower sequencing quality to might lead to spurious results in downstream analyses.\nTo perform both of these tasks, we’ll use the program fastp.\n! fastp -h\noption needs value: --html\nusage: fastp [options] ...\noptions:\n  -i, --in1                            read1 input file name (string [=])\n  -o, --out1                           read1 output file name (string [=])\n  -I, --in2                            read2 input file name (string [=])\n  -O, --out2                           read2 output file name (string [=])\n      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1.\n                                       Default is to discard it. (string [=])\n      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2.\n                                       If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be\n                                       written to this same file. (string [=])\n      --overlapped_out                 for each read pair, output the overlapped region if it has no any mismatched\n                                       base. (string [=])\n      --failed_out                     specify the file to store reads that cannot pass the filters. (string [=])\n  -m, --merge                          for paired-end input, merge each pair of reads into a single read if they are\n                                       overlapped. The merged reads will be written\n                                       to the file given by --merged_out, the unmerged reads will be written to the\n                                       files specified by --out1 and --out2. The merging mode is disabled by default.\n      --merged_out                     in the merging mode, specify the file name to store merged output, or specify\n                                       --stdout to stream the merged output (string [=])\n      --include_unmerged               in the merging mode, write the unmerged or unpaired reads to the file specified\n                                       by --merge. Disabled by default.\n  -6, --phred64                        indicate the input is using phred64 scoring (it'll be converted to phred33,\n                                       so the output will still be phred33)\n  -z, --compression                    compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 4. (int [=4])\n      --stdin                          input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in.\n      --stdout                         stream passing-filters reads to STDOUT. This option will result in interleaved\n                                       FASTQ output for paired-end output. Disabled by default.\n      --interleaved_in                 indicate that &lt;in1&gt; is an interleaved FASTQ which contains both read1 and read2.\n                                       Disabled by default.\n      --reads_to_process               specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0])\n      --dont_overwrite                 don't overwrite existing files. Overwritting is allowed by default.\n      --fix_mgi_id                     the MGI FASTQ ID format is not compatible with many BAM operation tools, enable this option to fix it.\n  -V, --verbose                        output verbose log information (i.e. when every 1M reads are processed).\n  -A, --disable_adapter_trimming       adapter trimming is enabled by default. If this option is specified, adapter trimming is disabled\n  -a, --adapter_sequence               the adapter for read1. For SE data, if not specified, the adapter will be auto-detected.\n                                       For PE data, this is used if R1/R2 are found not overlapped. (string [=auto])\n      --adapter_sequence_r2            the adapter for read2 (PE data only). This is used if R1/R2 are found not overlapped.\n                                       If not specified, it will be the same as &lt;adapter_sequence&gt; (string [=auto])\n      --adapter_fasta                  specify a FASTA file to trim both read1 and read2 (if PE) by all the sequences in this FASTA file (string [=])\n      --detect_adapter_for_pe          by default, the auto-detection for adapter is for SE data input only, turn on this\n                                    option to enable it for PE data.\n  -f, --trim_front1                    trimming how many bases in front for read1, default is 0 (int [=0])\n  -t, --trim_tail1                     trimming how many bases in tail for read1, default is 0 (int [=0])\n  -b, --max_len1                       if read1 is longer than max_len1, then trim read1 at its tail to make it as\n                                       long as max_len1. Default 0 means no limitation (int [=0])\n  -F, --trim_front2                    trimming how many bases in front for read2. If it's not specified, it will follow read1's settings (int [=0])\n  -T, --trim_tail2                     trimming how many bases in tail for read2. If it's not specified, it will follow read1's settings (int [=0])\n  -B, --max_len2                       if read2 is longer than max_len2, then trim read2 at its tail to make it as long as max_len2.\n                                       Default 0 means no limitation. If it's not specified, it will follow read1's settings (int [=0])\n  -D, --dedup                          enable deduplication to drop the duplicated reads/pairs\n      --dup_calc_accuracy              accuracy level to calculate duplication (1~6), higher level uses more memory (1G, 2G, 4G, 8G, 16G, 24G).\n                                       Default 1 for no-dedup mode, and 3 for dedup mode. (int [=0])\n      --dont_eval_duplication          don't evaluate duplication rate to save time and use less memory.\n  -g, --trim_poly_g                    force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data\n      --poly_g_min_len                 the minimum length to detect polyG in the read tail. 10 by default. (int [=10])\n  -G, --disable_trim_poly_g            disable polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data\n  -x, --trim_poly_x                    enable polyX trimming in 3' ends.\n      --poly_x_min_len                 the minimum length to detect polyX in the read tail. 10 by default. (int [=10])\n  -5, --cut_front                      move a sliding window from front (5') to tail, drop the bases in the window if\n                                       its mean quality &lt; threshold, stop otherwise.\n  -3, --cut_tail                       move a sliding window from tail (3') to front, drop the bases in the window if\n                                       its mean quality &lt; threshold, stop otherwise.\n  -r, --cut_right                      move a sliding window from front to tail, if meet one window with mean quality\n                                       &lt; threshold, drop the bases in the window and the right part, and then stop.\n  -W, --cut_window_size                the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])\n  -M, --cut_mean_quality               the mean quality requirement option shared by cut_front, cut_tail or cut_sliding.\n                                       Range: 1~36 default: 20 (Q20) (int [=20])\n      --cut_front_window_size          the window size option of cut_front, default to cut_window_size if not specified (int [=4])\n      --cut_front_mean_quality         the mean quality requirement option for cut_front, default to cut_mean_quality if not specified (int [=20])\n      --cut_tail_window_size           the window size option of cut_tail, default to cut_window_size if not specified (int [=4])\n      --cut_tail_mean_quality          the mean quality requirement option for cut_tail, default to cut_mean_quality if not specified (int [=20])\n      --cut_right_window_size          the window size option of cut_right, default to cut_window_size if not specified (int [=4])\n      --cut_right_mean_quality         the mean quality requirement option for cut_right, default to cut_mean_quality if not specified (int [=20])\n  -Q, --disable_quality_filtering      quality filtering is enabled by default. If this option is specified, quality filtering is disabled\n  -q, --qualified_quality_phred        the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified. (int [=15])\n  -u, --unqualified_percent_limit      how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40% (int [=40])\n  -n, --n_base_limit                   if one read's number of N base is &gt;n_base_limit, then this read/pair is discarded. Default is 5 (int [=5])\n  -e, --average_qual                   if one read's average quality score &lt;avg_qual, then this read/pair is discarded.\n                                       Default 0 means no requirement (int [=0])\n  -L, --disable_length_filtering       length filtering is enabled by default. If this option is specified, length filtering is disabled\n  -l, --length_required                reads shorter than length_required will be discarded, default is 15. (int [=15])\n      --length_limit                   reads longer than length_limit will be discarded, default 0 means no limitation. (int [=0])\n  -y, --low_complexity_filter          enable low complexity filter. The complexity is defined as the percentage of base\n                                       that is different from its next base (base[i] != base[i+1]).\n  -Y, --complexity_threshold           the threshold for low complexity filter (0~100). Default is 30, which means 30% complexity is required. (int [=30])\n      --filter_by_index1               specify a file contains a list of barcodes of index1 to be filtered out, one barcode per line (string [=])\n      --filter_by_index2               specify a file contains a list of barcodes of index2 to be filtered out, one barcode per line (string [=])\n      --filter_by_index_threshold      the allowed difference of index barcode for index filtering, default 0 means completely identical. (int [=0])\n  -c, --correction                     enable base correction in overlapped regions (only for PE data), default is disabled\n      --overlap_len_require            the minimum length to detect overlapped region of PE reads. This will affect overlap analysis based PE merge,\n                                       adapter trimming and correction. 30 by default. (int [=30])\n      --overlap_diff_limit             the maximum number of mismatched bases to detect overlapped region of PE reads.\n                                       This will affect overlap analysis based PE merge, adapter trimming and correction. 5 by default. (int [=5])\n      --overlap_diff_percent_limit     the maximum percentage of mismatched bases to detect overlapped region of PE reads.\n                                       This will affect overlap analysis based PE merge, adapter trimming and correction. Default 20 means 20%. (int [=20])\n  -U, --umi                            enable unique molecular identifier (UMI) preprocessing\n      --umi_loc                        specify the location of UMI, can be (index1/index2/read1/read2/per_index/per_read, default is none (string [=])\n      --umi_len                        if the UMI is in read1/read2, its length should be provided (int [=0])\n      --umi_prefix                     if specified, an underline will be used to connect prefix and UMI (i.e.\n                                       prefix=UMI, UMI=AATTCG, final=UMI_AATTCG). No prefix by default (string [=])\n      --umi_skip                       if the UMI is in read1/read2, fastp can skip several bases following UMI, default is 0 (int [=0])\n  -p, --overrepresentation_analysis    enable overrepresented sequence analysis.\n  -P, --overrepresentation_sampling    one in (--overrepresentation_sampling) reads will be computed for overrepresentation\n                                       analysis (1~10000), smaller is slower, default is 20. (int [=20])\n  -j, --json                           the json format report file name (string [=fastp.json])\n  -h, --html                           the html format report file name (string [=fastp.html])\n  -R, --report_title                   should be quoted with ' or \", default is \"fastp report\" (string [=fastp report])\n  -w, --thread                         worker thread number, default is 3 (int [=3])\n  -s, --split                          split output by limiting total split file number with this option (2~999), a sequential number prefix\n                                       will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (int [=0])\n  -S, --split_by_lines                 split output by limiting lines of each file with this option(&gt;=1000), a sequential number prefix will be\n                                       added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (long [=0])\n  -d, --split_prefix_digits            the digits for the sequential number padding (1~10), default is 4, so the filename will be padded as\n                                       0001.xxx, 0 to disable padding (int [=4])\n      --cut_by_quality5                DEPRECATED, use --cut_front instead.\n      --cut_by_quality3                DEPRECATED, use --cut_tail instead.\n      --cut_by_quality_aggressive      DEPRECATED, use --cut_right instead.\n      --discard_unmerged               DEPRECATED, no effect now, see the introduction for merging.\n  -?, --help                           print this message\n%%bash\nfastp \\\n    --in1 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n    --in2 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n    --merge \\\n    --merged_out ../results/fastp/ERR5766177.merged.fastq.gz \\\n    --include_unmerged \\\n    --dedup \\\n    --json ../results/fastp/ERR5766177.fastp.json \\\n    --html ../results/fastp/ERR5766177.fastp.html \\\nRead1 before filtering:\ntotal reads: 1000000\ntotal bases: 101000000\nQ20 bases: 99440729(98.4562%)\nQ30 bases: 94683150(93.7457%)\n\nRead2 before filtering:\ntotal reads: 1000000\ntotal bases: 101000000\nQ20 bases: 99440729(98.4562%)\nQ30 bases: 94683150(93.7457%)\n\nMerged and filtered:\ntotal reads: 1994070\ntotal bases: 201397311\nQ20 bases: 198330392(98.4772%)\nQ30 bases: 188843169(93.7665%)\n\nFiltering result:\nreads passed filter: 1999252\nreads failed due to low quality: 728\nreads failed due to too many N: 20\nreads failed due to too short: 0\nreads with adapter trimmed: 282\nbases trimmed due to adapters: 18654\nreads corrected by overlap analysis: 0\nbases corrected by overlap analysis: 0\n\nDuplication rate: 0.2479%\n\nInsert size peak (evaluated by paired-end reads): 31\n\nRead pairs merged: 228\n% of original read pairs: 0.0228%\n% in reads after filtering: 0.0114339%\n\n\nJSON report: ../results/fastp/ERR5766177.fastp.json\nHTML report: ../results/fastp/ERR5766177.fastp.html\n\nfastp --in1 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n--in2 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz --merge \\\n--merged_out ../results/fastp/ERR5766177.merged.fastq.gz --include_unmerged --dedup \\\n--json ../results/fastp/ERR5766177.fastp.json --html ../results/fastp/ERR5766177.fastp.html\nfastp v0.23.2, time used: 11 seconds\n\n10.3.3.0.1 3. Taxonomic profiling with Metaphlan\n! metaphlan  --help\nusage: metaphlan --input_type {fastq,fasta,bowtie2out,sam} [--force]\n                 [--bowtie2db METAPHLAN_BOWTIE2_DB] [-x INDEX]\n                 [--bt2_ps BowTie2 presets] [--bowtie2_exe BOWTIE2_EXE]\n                 [--bowtie2_build BOWTIE2_BUILD] [--bowtie2out FILE_NAME]\n                 [--min_mapq_val MIN_MAPQ_VAL] [--no_map] [--tmp_dir]\n                 [--tax_lev TAXONOMIC_LEVEL] [--min_cu_len]\n                 [--min_alignment_len] [--add_viruses] [--ignore_eukaryotes]\n                 [--ignore_bacteria] [--ignore_archaea] [--stat_q]\n                 [--perc_nonzero] [--ignore_markers IGNORE_MARKERS]\n                 [--avoid_disqm] [--stat] [-t ANALYSIS TYPE]\n                 [--nreads NUMBER_OF_READS] [--pres_th PRESENCE_THRESHOLD]\n                 [--clade] [--min_ab] [-o output file] [--sample_id_key name]\n                 [--use_group_representative] [--sample_id value]\n                 [-s sam_output_file] [--legacy-output] [--CAMI_format_output]\n                 [--unknown_estimation] [--biom biom_output] [--mdelim mdelim]\n                 [--nproc N] [--install] [--force_download]\n                 [--read_min_len READ_MIN_LEN] [-v] [-h]\n                 [INPUT_FILE] [OUTPUT_FILE]\n\nDESCRIPTION\n MetaPhlAn version 3.1.0 (25 Jul 2022):\n METAgenomic PHyLogenetic ANalysis for metagenomic taxonomic profiling.\n\nAUTHORS: Francesco Beghini (francesco.beghini@unitn.it),Nicola Segata (nicola.segata@unitn.it), Duy Tin Truong,\nFrancesco Asnicar (f.asnicar@unitn.it), Aitor Blanco Miguez (aitor.blancomiguez@unitn.it)\n\nCOMMON COMMANDS\n\n We assume here that MetaPhlAn is installed using the several options available (pip, conda, PyPi)\n Also BowTie2 should be in the system path with execution and read permissions, and Perl should be installed)\n\n========== MetaPhlAn clade-abundance estimation =================\n\nThe basic usage of MetaPhlAn consists in the identification of the clades (from phyla to species )\npresent in the metagenome obtained from a microbiome sample and their\nrelative abundance. This correspond to the default analysis type (-t rel_ab).\n\n*  Profiling a metagenome from raw reads:\n$ metaphlan metagenome.fastq --input_type fastq -o profiled_metagenome.txt\n\n*  You can take advantage of multiple CPUs and save the intermediate BowTie2 output for re-running\n   MetaPhlAn extremely quickly:\n$ metaphlan metagenome.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq -o profiled_metagenome.txt\n\n*  If you already mapped your metagenome against the marker DB (using a previous MetaPhlAn run), you\n   can obtain the results in few seconds by using the previously saved --bowtie2out file and\n   specifying the input (--input_type bowtie2out):\n$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out -o profiled_metagenome.txt\n\n*  bowtie2out files generated with MetaPhlAn versions below 3 are not compatibile.\n   Starting from MetaPhlAn 3.0, the BowTie2 ouput now includes the size of the profiled metagenome and the average read length.\n   If you want to re-run MetaPhlAn using these file you should provide the metagenome size via --nreads:\n$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out --nreads 520000 -o profiled_metagenome.txt\n\n*  You can also provide an externally BowTie2-mapped SAM if you specify this format with\n   --input_type. Two steps: first apply BowTie2 and then feed MetaPhlAn with the obtained sam:\n$ bowtie2 --sam-no-hd --sam-no-sq --no-unal --very-sensitive -S metagenome.sam -x \\\n  ${mpa_dir}/metaphlan_databases/mpa_v30_CHOCOPhlAn_201901 -U metagenome.fastq\n$ metaphlan metagenome.sam --input_type sam -o profiled_metagenome.txt\n\n*  We can also natively handle paired-end metagenomes, and, more generally, metagenomes stored in\n  multiple files (but you need to specify the --bowtie2out parameter):\n$ metaphlan metagenome_1.fastq,metagenome_2.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq\n\n-------------------------------------------------------------------\n\n\n========== Marker level analysis ============================\n\nMetaPhlAn introduces the capability of characterizing organisms at the strain level using non\naggregated marker information. Such capability comes with several slightly different flavours and\nare a way to perform strain tracking and comparison across multiple samples.\nUsually, MetaPhlAn is first ran with the default -t to profile the species present in\nthe community, and then a strain-level profiling can be performed to zoom-in into specific species\nof interest. This operation can be performed quickly as it exploits the --bowtie2out intermediate\nfile saved during the execution of the default analysis type.\n\n*  The following command will output the abundance of each marker with a RPK (reads per kilo-base)\n   higher 0.0. (we are assuming that metagenome_outfmt.bz2 has been generated before as\n   shown above).\n$ metaphlan -t marker_ab_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n   The obtained RPK can be optionally normalized by the total number of reads in the metagenome\n   to guarantee fair comparisons of abundances across samples. The number of reads in the metagenome\n   needs to be passed with the '--nreads' argument\n\n*  The list of markers present in the sample can be obtained with '-t marker_pres_table'\n$ metaphlan -t marker_pres_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n   The --pres_th argument (default 1.0) set the minimum RPK value to consider a marker present\n\n*  The list '-t clade_profiles' analysis type reports the same information of '-t marker_ab_table'\n   but the markers are reported on a clade-by-clade basis.\n$ metaphlan -t clade_profiles metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n\n*  Finally, to obtain all markers present for a specific clade and all its subclades, the\n   '-t clade_specific_strain_tracker' should be used. For example, the following command\n   is reporting the presence/absence of the markers for the B. fragilis species and its strains\n   the optional argument --min_ab specifies the minimum clade abundance for reporting the markers\n\n$ metaphlan -t clade_specific_strain_tracker --clade s__Bacteroides_fragilis metagenome_outfmt.bz2 --input_typ\n  bowtie2out -o marker_abundance_table.txt\n\n-------------------------------------------------------------------\n\npositional arguments:\n  INPUT_FILE            the input file can be:\n                        * a fastq file containing metagenomic reads\n                        OR\n                        * a BowTie2 produced SAM file.\n                        OR\n                        * an intermediary mapping file of the metagenome generated by a previous MetaPhlAn run\n                        If the input file is missing, the script assumes that the input is provided using the standard\n                        input, or named pipes.\n                        IMPORTANT: the type of input needs to be specified with --input_type\n  OUTPUT_FILE           the tab-separated output file of the predicted taxon relative abundances\n                        [stdout if not present]\n\nRequired arguments:\n  --input_type {fastq,fasta,bowtie2out,sam}\n                        set whether the input is the FASTA file of metagenomic reads or\n                        the SAM file of the mapping of the reads against the MetaPhlAn db.\n\nMapping arguments:\n  --force               Force profiling of the input file by removing the bowtie2out file\n  --bowtie2db METAPHLAN_BOWTIE2_DB\n                        Folder containing the MetaPhlAn database. You can specify the location by exporting the\n                        DEFAULT_DB_FOLDER variable in the shell.\n                        [default /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/metaphlan/metaphlan_databases]\n  -x INDEX, --index INDEX\n                        Specify the id of the database version to use. If \"latest\", MetaPhlAn will get the latest version.\n                        If an index name is provided, MetaPhlAn will try to use it, if available, and skip the online check.\n                        If the database files are not found on the local MetaPhlAn installation they\n                        will be automatically downloaded [default latest]\n  --bt2_ps BowTie2 presets\n                        Presets options for BowTie2 (applied only when a FASTA file is provided)\n                        The choices enabled in MetaPhlAn are:\n                         * sensitive\n                         * very-sensitive\n                         * sensitive-local\n                         * very-sensitive-local\n                        [default very-sensitive]\n  --bowtie2_exe BOWTIE2_EXE\n                        Full path and name of the BowTie2 executable. This option allowsMetaPhlAn to reach the\n                        executable even when it is not in the system PATH or the system PATH is unreachable\n  --bowtie2_build BOWTIE2_BUILD\n                        Full path to the bowtie2-build command to use, deafult assumes that 'bowtie2-build is present in the system path\n  --bowtie2out FILE_NAME\n                        The file for saving the output of BowTie2\n  --min_mapq_val MIN_MAPQ_VAL\n                        Minimum mapping quality value (MAPQ) [default 5]\n  --no_map              Avoid storing the --bowtie2out map file\n  --tmp_dir             The folder used to store temporary files [default is the OS dependent tmp dir]\n\nPost-mapping arguments:\n  --tax_lev TAXONOMIC_LEVEL\n                        The taxonomic level for the relative abundance output:\n                        'a' : all taxonomic levels\n                        'k' : kingdoms\n                        'p' : phyla only\n                        'c' : classes only\n                        'o' : orders only\n                        'f' : families only\n                        'g' : genera only\n                        's' : species only\n                        [default 'a']\n  --min_cu_len          minimum total nucleotide length for the markers in a clade for\n                        estimating the abundance without considering sub-clade abundances\n                        [default 2000]\n  --min_alignment_len   The sam records for aligned reads with the longest subalignment\n                        length smaller than this threshold will be discarded.\n                        [default None]\n  --add_viruses         Allow the profiling of viral organisms\n  --ignore_eukaryotes   Do not profile eukaryotic organisms\n  --ignore_bacteria     Do not profile bacterial organisms\n  --ignore_archaea      Do not profile archeal organisms\n  --stat_q              Quantile value for the robust average\n                        [default 0.2]\n  --perc_nonzero        Percentage of markers with a non zero relative abundance for misidentify a species\n                        [default 0.33]\n  --ignore_markers IGNORE_MARKERS\n                        File containing a list of markers to ignore.\n  --avoid_disqm         Deactivate the procedure of disambiguating the quasi-markers based on the\n                        marker abundance pattern found in the sample. It is generally recommended\n                        to keep the disambiguation procedure in order to minimize false positives\n  --stat                Statistical approach for converting marker abundances into clade abundances\n                        'avg_g'  : clade global (i.e. normalizing all markers together) average\n                        'avg_l'  : average of length-normalized marker counts\n                        'tavg_g' : truncated clade global average at --stat_q quantile\n                        'tavg_l' : truncated average of length-normalized marker counts (at --stat_q)\n                        'wavg_g' : winsorized clade global average (at --stat_q)\n                        'wavg_l' : winsorized average of length-normalized marker counts (at --stat_q)\n                        'med'    : median of length-normalized marker counts\n                        [default tavg_g]\n\nAdditional analysis types and arguments:\n  -t ANALYSIS TYPE      Type of analysis to perform:\n                         * rel_ab: profiling a metagenomes in terms of relative abundances\n                         * rel_ab_w_read_stats: profiling a metagenomes in terms of relative abundances and estimate\n                                                the number of reads coming from each clade.\n                         * reads_map: mapping from reads to clades (only reads hitting a marker)\n                         * clade_profiles: normalized marker counts for clades with at least a non-null marker\n                         * marker_ab_table: normalized marker counts (only when &gt; 0.0 and normalized by metagenome size if --nreads is specified)\n                         * marker_counts: non-normalized marker counts [use with extreme caution]\n                         * marker_pres_table: list of markers present in the sample (threshold at 1.0 if not differently specified with --pres_th\n                         * clade_specific_strain_tracker: list of markers present for a specific clade, specified with --clade, and all its subclades\n                        [default 'rel_ab']\n  --nreads NUMBER_OF_READS\n                        The total number of reads in the original metagenome. It is used only when\n                        -t marker_table is specified for normalizing the length-normalized counts\n                        with the metagenome size as well. No normalization applied if --nreads is not\n                        specified\n  --pres_th PRESENCE_THRESHOLD\n                        Threshold for calling a marker present by the -t marker_pres_table option\n  --clade               The clade for clade_specific_strain_tracker analysis\n  --min_ab              The minimum percentage abundance for the clade in the clade_specific_strain_tracker analysis\n\nOutput arguments:\n  -o output file, --output_file output file\n                        The output file (if not specified as positional argument)\n  --sample_id_key name  Specify the sample ID key for this analysis. Defaults to 'SampleID'.\n  --use_group_representative\n                        Use a species as representative for species groups.\n  --sample_id value     Specify the sample ID for this analysis. Defaults to 'Metaphlan_Analysis'.\n  -s sam_output_file, --samout sam_output_file\n                        The sam output file\n  --legacy-output       Old MetaPhlAn2 two columns output\n  --CAMI_format_output  Report the profiling using the CAMI output format\n  --unknown_estimation  Scale relative abundances to the number of reads mapping to known clades in order to estimate unknowness\n  --biom biom_output, --biom_output_file biom_output\n                        If requesting biom file output: The name of the output file in biom format\n  --mdelim mdelim, --metadata_delimiter_char mdelim\n                        Delimiter for bug metadata: - defaults to pipe. e.g. the pipe in k__Bacteria|p__Proteobacteria\n\nOther arguments:\n  --nproc N             The number of CPUs to use for parallelizing the mapping [default 4]\n  --install             Only checks if the MetaPhlAn DB is installed and installs it if not. All other parameters are ignored.\n  --force_download      Force the re-download of the latest MetaPhlAn database.\n  --read_min_len READ_MIN_LEN\n                        Specify the minimum length of the reads to be considered when parsing the input file with\n                        'read_fastx.py' script, default value is 70\n  -v, --version         Prints the current MetaPhlAn version and exit\n  -h, --help            show this help message and exit\nmetaphlan ../results/fastp/ERR5766177.merged.fastq.gz  \\\n    --input_type fastq \\\n    --bowtie2out ../results/metaphlan/ERR5766177.bt2.out  \\\n    --nproc 4 \\\n    &gt; ../results/metaphlan/ERR5766177.metaphlan_profile.txt\nThe main results files that we’re interested in is located at ../results/metaphlan/ERR5766177.metaphlan_profile.txt\nIt’s a tab separated file, with taxons in rows, with their relative abundance in the sample\n! head ../results/metaphlan/ERR5766177.metaphlan_profile.txt\n#mpa_v30_CHOCOPhlAn_201901\n#/home/maxime_borry/.conda/envs/maxime/envs/summer_school_microbiome/bin/metaphlan ../results/fastp/ERR5766177.merged.fastq.gz \\\n--input_type fastq --bowtie2out ../results/metaphlan/ERR5766177.bt2.out --nproc 8\n#SampleID   Metaphlan_Analysis\n#clade_name NCBI_tax_id relative_abundance  additional_species\nk__Bacteria 2   82.23198\nk__Archaea  2157    17.76802\nk__Bacteria|p__Firmicutes   2|1239  33.47957\nk__Bacteria|p__Bacteroidetes    2|976   28.4209\nk__Bacteria|p__Actinobacteria   2|201174    20.33151\nk__Archaea|p__Euryarchaeota 2157|28890  17.76802\n\n\n\n10.3.4 4. Visualizing the taxonomic profile\n\n10.3.4.1 4.1 Visualizing metaphlan taxonomic profile with Pavian\nPavian is an interactive app to explore results of different taxonomic classifiers\nThere are different ways to run it:\n\nIf you have docker installed (and are somehow familiar with it)\n\ndocker pull 'florianbw/pavian'\ndocker run --rm -p 5000:80 florianbw/pavian\nThen open your browser and visit localhost:5000\n\nIf you are familiar with R\n\nif (!require(remotes)) { install.packages(\"remotes\") }\nremotes::install_github(\"fbreitwieser/pavian\")\n\npavian::runApp(port=5000)\nThen open your browser and visit localhost:5000\n\nOtherwise, just visit fbreitwieser.shinyapps.io/pavian.\n\n\n\n10.3.4.2 4.2 Visualizing metaphlan taxonomic profile with Krona\n%%bash\npython ../scripts/metaphlan2krona.py -p ../results/metaphlan/ERR5766177.metaphlan_profile.txt -k ../results/krona/ERR5766177_krona.out\nktImportText -o ../results/krona/ERR5766177_krona.html ../results/krona/ERR5766177_krona.out\nWriting ../results/krona/ERR5766177_krona.html...\n\n\n\n10.3.5 5. Getting modern reference data\nIn order to compare our sample with modern reference samples, I used the curatedMetagenomicsData package, which provides both curated metadata, and pre-computed metaphlan taxonomic profiles for published modern human samples. The full R code to get these data is available in curatedMetagenomics/get_sources.Rmd\nI pre-selected 200 gut microbiome samples from non-westernized (100) and westernized (100) from healthy, non-antibiotic users donors.\nlibrary(curatedMetagenomicData)\nlibrary(tidyverse)\n\nsampleMetadata %&gt;%\n  filter(body_site=='stool' & antibiotics_current_use  == 'no' & disease == 'healthy') %&gt;%\n  group_by(non_westernized) %&gt;%\n  sample_n(100) %&gt;%\n  ungroup() -&gt; selected_samples\n\nselected_samples %&gt;%\n  returnSamples(\"relative_abundance\") -&gt; rel_ab\n\ndata_ranks = splitByRanks(rel_ab)\n\nfor (r in names(data_ranks)){\n  print(r)\n  assay_rank = as.data.frame(assay(data_ranks[[r]]))\n  print(paste0(\"../../data/curated_metagenomics/modern_sources_\",tolower(r),\".csv\"))\n  write.csv(assay_rank, paste0(\"../../data/curated_metagenomics/modern_sources_\",tolower(r),\".csv\"))\n\n\nThe resulting metaphlan taxonomic profiles (split by taxonomic ranks) are available at\n../data/curated_metagenomics\nThe associated metadata is available at\n../data/metadata/curated_metagenomics_modern_sources.csv\n\n\n\n\n10.3.6 6. Bringing together ancient and modern data\nThis is the moment where we will the Pandas Python library to perform some data manipulation.\nWe will also use the Taxopy library to work to taxonomic informations.\n! pip install taxopy\nRequirement already satisfied: taxopy in /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages (0.10.0)\nimport pandas as pd\nimport taxopy\nimport pickle\nimport gzip\nwith gzip.open(\"../data/taxopy/taxdb.p.gz\", 'rb') as tdb:\n    taxo_db = pickle.load(tdb)\n! head ../results/metaphlan/ERR5766177.metaphlan_profile.txt\n#mpa_v30_CHOCOPhlAn_201901\n#/home/maxime_borry/.conda/envs/maxime/envs/summer_school_microbiome/bin/metaphlan ../results/fastp/ERR5766177.merged.fastq.gz\n--input_type fastq --bowtie2out ../results/metaphlan/ERR5766177.bt2.out --nproc 8\n#SampleID   Metaphlan_Analysis\n#clade_name NCBI_tax_id relative_abundance  additional_species\nk__Bacteria 2   82.23198\nk__Archaea  2157    17.76802\nk__Bacteria|p__Firmicutes   2|1239  33.47957\nk__Bacteria|p__Bacteroidetes    2|976   28.4209\nk__Bacteria|p__Actinobacteria   2|201174    20.33151\nk__Archaea|p__Euryarchaeota 2157|28890  17.76802\nancient_data = pd.read_csv(\"../results/metaphlan/ERR5766177.metaphlan_profile.txt\",\n                            comment=\"#\",\n                            delimiter=\"\\t\",\n                            names=['clade_name','NCBI_tax_id','relative_abundance','additional_species'])\nancient_data.head()\n\n\n\n\n\n\n\n\n\nclade_name\n\n\nNCBI_tax_id\n\n\nrelative_abundance\n\n\nadditional_species\n\n\n\n\n\n\n0\n\n\nk__Bacteria\n\n\n2\n\n\n82.23198\n\n\nNaN\n\n\n\n\n1\n\n\nk__Archaea\n\n\n2157\n\n\n17.76802\n\n\nNaN\n\n\n\n\n2\n\n\nk__Bacteria|p__Firmicutes\n\n\n2|1239\n\n\n33.47957\n\n\nNaN\n\n\n\n\n3\n\n\nk__Bacteria|p__Bacteroidetes\n\n\n2|976\n\n\n28.42090\n\n\nNaN\n\n\n\n\n4\n\n\nk__Bacteria|p__Actinobacteria\n\n\n2|201174\n\n\n20.33151\n\n\nNaN\n\n\n\n\n\n\nancient_data.sample(10)\n\n\n\n\n\n\n\n\n\nclade_name\n\n\nNCBI_tax_id\n\n\nrelative_abundance\n\n\nadditional_species\n\n\n\n\n\n\n1\n\n\nk__Archaea\n\n\n2157\n\n\n17.76802\n\n\nNaN\n\n\n\n\n46\n\n\nk__Bacteria|p__Bacteroidetes|c_Bacteroidia|o…\n\n\n2|976|200643|171549|171552|838|165179\n\n\n25.75544\n\n\nk__Bacteria|p__Bacteroidetes|c_Bacteroidia|o…\n\n\n\n\n55\n\n\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n2|1239|186801|186802|186803|189330|88431\n\n\n0.91178\n\n\nNaN\n\n\n\n\n18\n\n\nk__Archaea|p__Euryarchaeota|c_Halobacteria|o…\n\n\n2157|28890|183963|2235\n\n\n0.71177\n\n\nNaN\n\n\n\n\n36\n\n\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n\n\n2|201174|1760|85004|31953|1678\n\n\n9.39377\n\n\nNaN\n\n\n\n\n65\n\n\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n\n\n2|201174|1760|85004|31953|1678|216816\n\n\n0.05447\n\n\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n\n\n\n\n37\n\n\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n2|1239|186801|186802|186803|\n\n\n2.16125\n\n\nNaN\n\n\n\n\n38\n\n\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n2|1239|186801|186802|541000|216851\n\n\n1.24537\n\n\nNaN\n\n\n\n\n26\n\n\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n\n\n2|201174|1760|85004|31953\n\n\n9.39377\n\n\nNaN\n\n\n\n\n48\n\n\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n2|1239|186801|186802|541000|1263|40518\n\n\n14.96816\n\n\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n\n\n\n\nBecause for this analysis, we’re only going to look at the relative abundance, we’ll only this column, an the TAXID information\nancient_data = (\n    ancient_data\n    .rename(columns={'NCBI_tax_id': 'TAXID'})\n    .drop(['clade_name','additional_species'], axis=1)\n)\nAlways investigate your data at first !\nancient_data.relative_abundance.sum()\n700.00007\nPause and think: A relative abundance of 700%, really ?\nLet’s proceed further and try to understand what’s happening.\nancient_data.head()\n\n\n\n\n\n\n\n\n\nTAXID\n\n\nrelative_abundance\n\n\n\n\n\n\n0\n\n\n2\n\n\n82.23198\n\n\n\n\n1\n\n\n2157\n\n\n17.76802\n\n\n\n\n2\n\n\n2|1239\n\n\n33.47957\n\n\n\n\n3\n\n\n2|976\n\n\n28.42090\n\n\n\n\n4\n\n\n2|201174\n\n\n20.33151\n\n\n\n\n\n\nTo make sense of the TAXID, we will use taxopy to get all the taxonomic related informations such as:\n\nname of the taxon\nrank of the taxon\nlineage of the taxon\n\n#### This function is here to help us get the taxon information\n#### from the metaphlan taxonomic ID lineage, of the following form\n#### 2|976|200643|171549|171552|838|165179\n\ndef to_taxopy(taxid_entry, taxo_db):\n    \"\"\"Returns a taxopy taxon object\n    Args:\n        taxid_entry(str): metaphlan TAXID taxonomic lineage\n        taxo_db(taxopy database)\n    Returns:\n        (bool): Returns a taxopy taxon object\n    \"\"\"\n    taxid = taxid_entry.split(\"|\")[-1] # get the last element\n    try:\n        if len(taxid) &gt; 0:\n            return taxopy.Taxon(int(taxid), taxo_db) # if it's not empty, get the taxon corresponding to the taxid\n        else:\n            return taxopy.Taxon(12908, taxo_db) # otherwise, return the taxon associated with unclassified sequences\n    except taxopy.exceptions.TaxidError as e:\n        return taxopy.Taxon(12908, taxo_db)\nancient_data['taxopy'] = ancient_data['TAXID'].apply(to_taxopy, taxo_db=taxo_db)\nancient_data.head()\n\n\n\n\n\n\n\n\n\nTAXID\n\n\nrelative_abundance\n\n\ntaxopy\n\n\n\n\n\n\n0\n\n\n2\n\n\n82.23198\n\n\ns__Bacteria\n\n\n\n\n1\n\n\n2157\n\n\n17.76802\n\n\ns__Archaea\n\n\n\n\n2\n\n\n2|1239\n\n\n33.47957\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicutes\n\n\n\n\n3\n\n\n2|976\n\n\n28.42090\n\n\ns__Bacteria;c__FCB group;p__Bacteroidetes\n\n\n\n\n4\n\n\n2|201174\n\n\n20.33151\n\n\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\n\n\n\n\nancient_data = ancient_data.assign(\n    rank = ancient_data.taxopy.apply(lambda x: x.rank),\n    name = ancient_data.taxopy.apply(lambda x: x.name),\n    lineage = ancient_data.taxopy.apply(lambda x: x.name_lineage),\n)\nancient_data\n\n\n\n\n\n\n\n\n\nTAXID\n\n\nrelative_abundance\n\n\ntaxopy\n\n\nrank\n\n\nname\n\n\nlineage\n\n\n\n\n\n\n0\n\n\n2\n\n\n82.23198\n\n\ns__Bacteria\n\n\nsuperkingdom\n\n\nBacteria\n\n\n[Bacteria, cellular organisms, root]\n\n\n\n\n1\n\n\n2157\n\n\n17.76802\n\n\ns__Archaea\n\n\nsuperkingdom\n\n\nArchaea\n\n\n[Archaea, cellular organisms, root]\n\n\n\n\n2\n\n\n2|1239\n\n\n33.47957\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicutes\n\n\nphylum\n\n\nFirmicutes\n\n\n[Firmicutes, Terrabacteria group, Bacteria, ce…\n\n\n\n\n3\n\n\n2|976\n\n\n28.42090\n\n\ns__Bacteria;c__FCB group;p__Bacteroidetes\n\n\nphylum\n\n\nBacteroidetes\n\n\n[Bacteroidetes, Bacteroidetes/Chlorobi group, …\n\n\n\n\n4\n\n\n2|201174\n\n\n20.33151\n\n\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\nphylum\n\n\nActinobacteria\n\n\n[Actinobacteria, Terrabacteria group, Bacteria…\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n62\n\n\n2|1239|186801|186802|186803|572511|33039\n\n\n0.24910\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicut…\n\n\nspecies\n\n\n[Ruminococcus] torques\n\n\n[[Ruminococcus] torques, Mediterraneibacter, L…\n\n\n\n\n63\n\n\n2|201174|84998|84999|84107|1472762|1232426\n\n\n0.17084\n\n\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\nspecies\n\n\n[Collinsella] massiliensis\n\n\n[[Collinsella] massiliensis, Enorma, Coriobact…\n\n\n\n\n64\n\n\n2|1239|186801|186802|186803|189330|39486\n\n\n0.07690\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicut…\n\n\nspecies\n\n\nDorea formicigenerans\n\n\n[Dorea formicigenerans, Dorea, Lachnospiraceae…\n\n\n\n\n65\n\n\n2|201174|1760|85004|31953|1678|216816\n\n\n0.05447\n\n\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\nspecies\n\n\nBifidobacterium longum\n\n\n[Bifidobacterium longum, Bifidobacterium, Bifi…\n\n\n\n\n66\n\n\n2|1239|186801|186802|541000|1263|1262959\n\n\n0.01440\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicut…\n\n\nspecies\n\n\nRuminococcus sp. CAG:488\n\n\n[Ruminococcus sp. CAG:488, environmental sampl…\n\n\n\n\n\n\n67 rows × 6 columns\n\n\nBecause our modern data are split by ranks, we’ll first split our ancient sample by rank\nWhich of the entries are at the species rank level ?\nancient_species = ancient_data.query(\"rank == 'species'\")\nancient_species.head()\n\n\n\n\n\n\n\n\n\nTAXID\n\n\nrelative_abundance\n\n\ntaxopy\n\n\nrank\n\n\nname\n\n\nlineage\n\n\n\n\n\n\n46\n\n\n2|976|200643|171549|171552|838|165179\n\n\n25.75544\n\n\ns__Bacteria;c__FCB group;p__Bacteroidetes;c__B…\n\n\nspecies\n\n\nPrevotella copri\n\n\n[Prevotella copri, Prevotella, Prevotellaceae,…\n\n\n\n\n47\n\n\n2157|28890|183925|2158|2159|2172|2173\n\n\n17.05626\n\n\ns__Archaea;p__Euryarchaeota;c__Methanomada gro…\n\n\nspecies\n\n\nMethanobrevibacter smithii\n\n\n[Methanobrevibacter smithii, Methanobrevibacte…\n\n\n\n\n48\n\n\n2|1239|186801|186802|541000|1263|40518\n\n\n14.96816\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicut…\n\n\nspecies\n\n\nRuminococcus bromii\n\n\n[Ruminococcus bromii, Ruminococcus, Oscillospi…\n\n\n\n\n49\n\n\n2|1239|186801|186802|186803|841|301302\n\n\n13.57908\n\n\ns__Bacteria;c__Terrabacteria group;p__Firmicut…\n\n\nspecies\n\n\nRoseburia faecis\n\n\n[Roseburia faecis, Roseburia, Lachnospiraceae,…\n\n\n\n\n50\n\n\n2|201174|84998|84999|84107|102106|74426\n\n\n9.49165\n\n\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\nspecies\n\n\nCollinsella aerofaciens\n\n\n[Collinsella aerofaciens, Collinsella, Corioba…\n\n\n\n\n\n\nLet’s do a bit of renaming to prepare for what’s coming next\nancient_species = ancient_species[['relative_abundance','name']].set_index('name').rename(columns={'relative_abundance':'ERR5766177'})\nancient_species.head()\n\n\n\n\n\n\n\n\n\nERR5766177\n\n\n\n\nname\n\n\n\n\n\n\n\n\nPrevotella copri\n\n\n25.75544\n\n\n\n\nMethanobrevibacter smithii\n\n\n17.05626\n\n\n\n\nRuminococcus bromii\n\n\n14.96816\n\n\n\n\nRoseburia faecis\n\n\n13.57908\n\n\n\n\nCollinsella aerofaciens\n\n\n9.49165\n\n\n\n\n\n\nancient_phylums = ancient_data.query(\"rank == 'phylum'\")\nancient_phylums = ancient_phylums[['relative_abundance','name']].set_index('name').rename(columns={'relative_abundance':'ERR5766177'})\nancient_phylums\n\n\n\n\n\n\n\n\n\nERR5766177\n\n\n\n\nname\n\n\n\n\n\n\n\n\nFirmicutes\n\n\n33.47957\n\n\n\n\nBacteroidetes\n\n\n28.42090\n\n\n\n\nActinobacteria\n\n\n20.33151\n\n\n\n\nEuryarchaeota\n\n\n17.76802\n\n\n\n\n\n\nNow, let’s go back to the 700% relative abundance issue…\nancient_data.groupby('rank')['relative_abundance'].sum()\nrank\nclass            99.72648\nfamily           83.49854\ngenus            97.56524\nno rank          19.48331\norder            99.72648\nphylum          100.00000\nspecies         100.00002\nsuperkingdom    100.00000\nName: relative_abundance, dtype: float64\nSeems better, right ?\nPause and think: why don’t we get exactly 100% ?\nNow let’s load our modern reference samples\nmodern_phylums = pd.read_csv(\"../data/curated_metagenomics/modern_sources_phylum.csv\", index_col=0)\nmodern_phylums.head()\n\n\n\n\n\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nPNP_Main_283\n\n\nPNP_Validation_55\n\n\nG80275\n\n\nPNP_Main_363\n\n\nSAMEA7045572\n\n\nSAMEA7045355\n\n\nHD-13\n\n\nEGAR00001420773_9002000001423910\n\n\nSID5428-4\n\n\n…\n\n\nA46_02_1FE\n\n\nTZ_87532\n\n\nA94_01_1FE\n\n\nKHG_7\n\n\nLDK_4\n\n\nKHG_9\n\n\nA48_01_1FE\n\n\nKHG_1\n\n\nTZ_81781\n\n\nA09_01_1FE\n\n\n\n\n\n\nBacteroidetes\n\n\n0.00000\n\n\n17.44332\n\n\n82.86400\n\n\n69.99087\n\n\n31.93081\n\n\n51.76204\n\n\n53.32801\n\n\n74.59667\n\n\n8.81074\n\n\n26.39694\n\n\n…\n\n\n1.97760\n\n\n1.49601\n\n\n67.21410\n\n\n4.29848\n\n\n68.16890\n\n\n38.59709\n\n\n14.81828\n\n\n10.13908\n\n\n57.14031\n\n\n11.61544\n\n\n\n\nFirmicutes\n\n\n95.24231\n\n\n60.47031\n\n\n16.53946\n\n\n22.81977\n\n\n65.23075\n\n\n41.96928\n\n\n45.77661\n\n\n23.51065\n\n\n54.35341\n\n\n62.23094\n\n\n…\n\n\n76.68499\n\n\n78.13269\n\n\n29.72394\n\n\n33.51772\n\n\n19.11149\n\n\n46.87139\n\n\n72.68136\n\n\n35.43789\n\n\n40.57101\n\n\n24.72113\n\n\n\n\nProteobacteria\n\n\n4.49959\n\n\n0.77098\n\n\n0.05697\n\n\n4.07757\n\n\n0.27316\n\n\n3.33972\n\n\n0.02001\n\n\n1.72865\n\n\n0.00000\n\n\n1.81016\n\n\n…\n\n\n16.57250\n\n\n0.76159\n\n\n2.35058\n\n\n9.83772\n\n\n5.32392\n\n\n0.19699\n\n\n3.64655\n\n\n17.64151\n\n\n0.30580\n\n\n56.20177\n\n\n\n\nActinobacteria\n\n\n0.25809\n\n\n10.27631\n\n\n0.45187\n\n\n1.11902\n\n\n2.31075\n\n\n2.92715\n\n\n0.77667\n\n\n0.16403\n\n\n36.55138\n\n\n1.19951\n\n\n…\n\n\n3.01814\n\n\n19.20468\n\n\n0.69913\n\n\n46.99479\n\n\n7.39093\n\n\n14.26365\n\n\n5.47750\n\n\n36.77145\n\n\n1.16426\n\n\n7.40894\n\n\n\n\nVerrucomicrobia\n\n\n0.00000\n\n\n0.00784\n\n\n0.00000\n\n\n1.99276\n\n\n0.25451\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.09940\n\n\n3.29795\n\n\n…\n\n\n0.05011\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n\n\n\n\n5 rows × 200 columns\n\n\nmodern_species = pd.read_csv(\"../data/curated_metagenomics/modern_sources_species.csv\", index_col=0)\nmodern_species.head()\n\n\n\n\n\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nPNP_Main_283\n\n\nPNP_Validation_55\n\n\nG80275\n\n\nPNP_Main_363\n\n\nSAMEA7045572\n\n\nSAMEA7045355\n\n\nHD-13\n\n\nEGAR00001420773_9002000001423910\n\n\nSID5428-4\n\n\n…\n\n\nA46_02_1FE\n\n\nTZ_87532\n\n\nA94_01_1FE\n\n\nKHG_7\n\n\nLDK_4\n\n\nKHG_9\n\n\nA48_01_1FE\n\n\nKHG_1\n\n\nTZ_81781\n\n\nA09_01_1FE\n\n\n\n\n\n\nBacteroides vulgatus\n\n\n0.0\n\n\n0.60446\n\n\n1.59911\n\n\n4.39085\n\n\n0.04494\n\n\n4.66505\n\n\n2.99431\n\n\n29.30325\n\n\n1.48560\n\n\n0.98818\n\n\n…\n\n\n0.20717\n\n\n0.0\n\n\n0.00309\n\n\n0.48891\n\n\n0.00000\n\n\n0.02230\n\n\n0.00000\n\n\n0.15112\n\n\n0.0\n\n\n0.00836\n\n\n\n\nBacteroides stercoris\n\n\n0.0\n\n\n0.00546\n\n\n0.00000\n\n\n0.00000\n\n\n2.50789\n\n\n0.00000\n\n\n20.57498\n\n\n8.28443\n\n\n1.23261\n\n\n0.00000\n\n\n…\n\n\n0.00000\n\n\n0.0\n\n\n0.00000\n\n\n0.00693\n\n\n0.00000\n\n\n0.02603\n\n\n0.00000\n\n\n0.19318\n\n\n0.0\n\n\n0.00000\n\n\n\n\nAcidaminococcus intestini\n\n\n0.0\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.32822\n\n\n0.00000\n\n\n…\n\n\n0.00000\n\n\n0.0\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.0\n\n\n0.00000\n\n\n\n\nEubacterium sp CAG 38\n\n\n0.0\n\n\n0.06712\n\n\n0.81149\n\n\n0.05247\n\n\n0.26027\n\n\n0.00000\n\n\n0.00000\n\n\n2.62415\n\n\n0.46585\n\n\n0.23372\n\n\n…\n\n\n0.78140\n\n\n0.0\n\n\n0.00000\n\n\n0.00499\n\n\n0.00000\n\n\n0.02446\n\n\n0.00000\n\n\n0.00000\n\n\n0.0\n\n\n0.00000\n\n\n\n\nParabacteroides distasonis\n\n\n0.0\n\n\n1.34931\n\n\n2.00672\n\n\n5.85067\n\n\n0.59019\n\n\n7.00027\n\n\n1.28075\n\n\n0.61758\n\n\n0.07383\n\n\n2.80355\n\n\n…\n\n\n0.11423\n\n\n0.0\n\n\n0.01181\n\n\n0.01386\n\n\n0.03111\n\n\n0.07463\n\n\n0.15597\n\n\n0.07541\n\n\n0.0\n\n\n0.01932\n\n\n\n\n\n\n5 rows × 200 columns\n\n\nNow, let’s merge our ancient sample with the modern data in one single table\nall_species = ancient_species.merge(modern_species, left_index=True, right_index=True, how='outer').fillna(0)\nall_phylums = ancient_phylums.merge(modern_phylums, left_index=True, right_index=True, how='outer').fillna(0)\nFinally, let’s load the metadata\nmetadata = pd.read_csv(\"../data/metadata/curated_metagenomics_modern_sources.csv\")\nmetadata.head()\n\n\n\n\n\n\n\n\n\nstudy_name\n\n\nsample_id\n\n\nsubject_id\n\n\nbody_site\n\n\nantibiotics_current_use\n\n\nstudy_condition\n\n\ndisease\n\n\nage\n\n\ninfant_age\n\n\nage_category\n\n\n…\n\n\nhla_drb11\n\n\nbirth_order\n\n\nage_twins_started_to_live_apart\n\n\nzigosity\n\n\nbrinkman_index\n\n\nalcohol_numeric\n\n\nbreastfeeding_duration\n\n\nformula_first_day\n\n\nALT\n\n\neGFR\n\n\n\n\n\n\n0\n\n\nShaoY_2019\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nC01528_ba\n\n\nstool\n\n\nno\n\n\ncontrol\n\n\nhealthy\n\n\n0.0\n\n\n4.0\n\n\nnewborn\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\nZeeviD_2015\n\n\nPNP_Main_283\n\n\nPNP_Main_283\n\n\nstool\n\n\nno\n\n\ncontrol\n\n\nhealthy\n\n\nNaN\n\n\nNaN\n\n\nadult\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\nZeeviD_2015\n\n\nPNP_Validation_55\n\n\nPNP_Validation_55\n\n\nstool\n\n\nno\n\n\ncontrol\n\n\nhealthy\n\n\nNaN\n\n\nNaN\n\n\nadult\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\nVatanenT_2016\n\n\nG80275\n\n\nT014806\n\n\nstool\n\n\nno\n\n\ncontrol\n\n\nhealthy\n\n\n1.0\n\n\nNaN\n\n\nchild\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nZeeviD_2015\n\n\nPNP_Main_363\n\n\nPNP_Main_363\n\n\nstool\n\n\nno\n\n\ncontrol\n\n\nhealthy\n\n\nNaN\n\n\nNaN\n\n\nadult\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\n5 rows × 130 columns\n\n\n\n\n10.3.7 7. Comparing ancient and modern samples\n\n10.3.7.1 7.1 Taxonomic composition\nOne common plot in microbiome papers in a stacked barplot, often at the phylum or family level.\nFirst, we’ll do some renaming, to make the value of the metadata variables a bit easier to understand\ngroup_info = (\n    metadata['non_westernized']\n    .map({'no':'westernized','yes':'non_westernized'}) # for the non_westernized in the modern sample metadata, rename the value levels\n    .to_frame(name='group').set_index(metadata['sample_id']) # rename the column to group\n    .reset_index()\n    .append({'sample_id':'ERR5766177', 'group':'ancient'}, ignore_index=True) # add the ancient sample\n)\ngroup_info\n/var/folders/1c/l1qb09f15jddsh65f6xv1n_r0000gp/T/ipykernel_40830/27419655.py:2:\nFutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version.\nUse pandas.concat instead.\n  metadata['non_westernized']\n\n\n\n\n\n\n\n\n\nsample_id\n\n\ngroup\n\n\n\n\n\n\n0\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nwesternized\n\n\n\n\n1\n\n\nPNP_Main_283\n\n\nwesternized\n\n\n\n\n2\n\n\nPNP_Validation_55\n\n\nwesternized\n\n\n\n\n3\n\n\nG80275\n\n\nwesternized\n\n\n\n\n4\n\n\nPNP_Main_363\n\n\nwesternized\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n196\n\n\nA48_01_1FE\n\n\nnon_westernized\n\n\n\n\n197\n\n\nKHG_1\n\n\nnon_westernized\n\n\n\n\n198\n\n\nTZ_81781\n\n\nnon_westernized\n\n\n\n\n199\n\n\nA09_01_1FE\n\n\nnon_westernized\n\n\n\n\n200\n\n\nERR5766177\n\n\nancient\n\n\n\n\n\n\n201 rows × 2 columns\n\n\nWe need transform our data in tidy format to plot with plotnine, a python clone of ggplot.\nWe then add the group information (Westernized, non westernized, or ancient sample), and compute the mean abundance for each phylum.\nFirst we transpose the dataframe to have the samples as index, and the phylums as columns\nWe then add the metadata information\n(\n    all_phylums\n    .transpose()\n    .merge(group_info, left_index=True, right_on='sample_id')\n)\n\n\n\n\n\n\n\n\n\nActinobacteria\n\n\nApicomplexa\n\n\nAscomycota\n\n\nBacteroidetes\n\n\nBasidiomycota\n\n\nCandidatus Melainabacteria\n\n\nChlamydiae\n\n\nChloroflexi\n\n\nCyanobacteria\n\n\nDeferribacteres\n\n\n…\n\n\nFusobacteria\n\n\nLentisphaerae\n\n\nPlanctomycetes\n\n\nProteobacteria\n\n\nSpirochaetes\n\n\nSynergistetes\n\n\nTenericutes\n\n\nVerrucomicrobia\n\n\nsample_id\n\n\ngroup\n\n\n\n\n\n\n200\n\n\n20.33151\n\n\n0.0\n\n\n0.0\n\n\n28.42090\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n0.00000\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nERR5766177\n\n\nancient\n\n\n\n\n0\n\n\n0.25809\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n4.49959\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nwesternized\n\n\n\n\n1\n\n\n10.27631\n\n\n0.0\n\n\n0.0\n\n\n17.44332\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.01486\n\n\n0.0\n\n\n0.77098\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00784\n\n\nPNP_Main_283\n\n\nwesternized\n\n\n\n\n2\n\n\n0.45187\n\n\n0.0\n\n\n0.0\n\n\n82.86400\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n0.05697\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nPNP_Validation_55\n\n\nwesternized\n\n\n\n\n3\n\n\n1.11902\n\n\n0.0\n\n\n0.0\n\n\n69.99087\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n4.07757\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n1.99276\n\n\nG80275\n\n\nwesternized\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n195\n\n\n14.26365\n\n\n0.0\n\n\n0.0\n\n\n38.59709\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n0.19699\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nKHG_9\n\n\nnon_westernized\n\n\n\n\n196\n\n\n5.47750\n\n\n0.0\n\n\n0.0\n\n\n14.81828\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n3.64655\n\n\n0.09964\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nA48_01_1FE\n\n\nnon_westernized\n\n\n\n\n197\n\n\n36.77145\n\n\n0.0\n\n\n0.0\n\n\n10.13908\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n17.64151\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nKHG_1\n\n\nnon_westernized\n\n\n\n\n198\n\n\n1.16426\n\n\n0.0\n\n\n0.0\n\n\n57.14031\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n0.30580\n\n\n0.70467\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nTZ_81781\n\n\nnon_westernized\n\n\n\n\n199\n\n\n7.40894\n\n\n0.0\n\n\n0.0\n\n\n11.61544\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n…\n\n\n0.0\n\n\n0.00000\n\n\n0.0\n\n\n56.20177\n\n\n0.00000\n\n\n0.0\n\n\n0.0\n\n\n0.00000\n\n\nA09_01_1FE\n\n\nnon_westernized\n\n\n\n\n\n\n201 rows × 24 columns\n\n\nNow, we need it in the tidy format\ntidy_phylums = (\n    all_phylums\n    .transpose()\n    .merge(group_info, left_index=True, right_on='sample_id')\n    .melt(id_vars=['sample_id', 'group'], value_name='relative_abundance', var_name='Phylum', ignore_index=True)\n)\nFinally, we only want to keep the mean relative abundance for each phylum\ntidy_phylums = tidy_phylums.groupby(['group', 'Phylum']).mean().reset_index()\ntidy_phylums.groupby('group')['relative_abundance'].sum()\ngroup\nancient            100.000000\nnon_westernized     99.710255\nwesternized         99.905089\nName: relative_abundance, dtype: float64\nfrom plotnine import *\nggplot(tidy_phylums, aes(x='group', y='relative_abundance', fill='Phylum')) \\\n+ geom_bar(position='stack', stat='identity') \\\n+ ylab('mean abundance') \\\n+ xlab(\"\") \\\n+ theme_classic()\n\n&lt;ggplot: (406187548)&gt;\n\n\n10.3.7.2 7.2 Ecological diversity\n\n10.3.7.2.1 7.2.1 Alpha diversity\nAlpha diversity is the measure of diversity withing each sample. It is used to estimate how many species are present in a sample, and how diverse they are.\nWe’ll use the python library scikit-bio to compute it, and the plotnine library (a python port of ggplot2 to visualize the results).\nimport skbio\nLet’s compute the species richness, the Shannon, and Simpson index of diversity index\nshannon = skbio.diversity.alpha_diversity(metric='shannon', counts=all_species.transpose(), ids=all_species.columns)\nsimpson = skbio.diversity.alpha_diversity(metric='simpson', counts=all_species.transpose(), ids=all_species.columns)\nrichness = (all_species != 0).astype(int).sum(axis=0)\nalpha_diversity = (shannon.to_frame(name='shannon')\n                   .merge(simpson.to_frame(name='simpson'), left_index=True, right_index=True)\n                   .merge(richness.to_frame(name='richness'), left_index=True, right_index=True))\nalpha_diversity\n\n\n\n\n\n\n\n\n\nshannon\n\n\nsimpson\n\n\nrichness\n\n\n\n\n\n\nERR5766177\n\n\n3.032945\n\n\n0.844769\n\n\n21\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\n0.798112\n\n\n0.251280\n\n\n11\n\n\n\n\nPNP_Main_283\n\n\n5.092878\n\n\n0.954159\n\n\n118\n\n\n\n\nPNP_Validation_55\n\n\n3.670162\n\n\n0.812438\n\n\n72\n\n\n\n\nG80275\n\n\n3.831358\n\n\n0.876712\n\n\n66\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\nKHG_9\n\n\n3.884285\n\n\n0.861683\n\n\n87\n\n\n\n\nA48_01_1FE\n\n\n4.377755\n\n\n0.930024\n\n\n53\n\n\n\n\nKHG_1\n\n\n3.733834\n\n\n0.875335\n\n\n108\n\n\n\n\nTZ_81781\n\n\n2.881856\n\n\n0.719491\n\n\n44\n\n\n\n\nA09_01_1FE\n\n\n2.982322\n\n\n0.719962\n\n\n75\n\n\n\n\n\n\n201 rows × 3 columns\n\n\nLet’s load the group information from the metadata\nalpha_diversity = (\n    alpha_diversity\n    .merge(metadata[['sample_id', 'non_westernized']], left_index=True, right_on='sample_id', how='outer')\n    .set_index('sample_id')\n    .rename(columns={'non_westernized':'group'})\n)\nalpha_diversity['group'] = alpha_diversity['group'].replace({'yes':'non_westernized','no':'westernized', pd.NA:'ERR5766177'})\nalpha_diversity\n\n\n\n\n\n\n\n\n\nshannon\n\n\nsimpson\n\n\nrichness\n\n\ngroup\n\n\n\n\nsample_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERR5766177\n\n\n3.032945\n\n\n0.844769\n\n\n21\n\n\nERR5766177\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\n0.798112\n\n\n0.251280\n\n\n11\n\n\nwesternized\n\n\n\n\nPNP_Main_283\n\n\n5.092878\n\n\n0.954159\n\n\n118\n\n\nwesternized\n\n\n\n\nPNP_Validation_55\n\n\n3.670162\n\n\n0.812438\n\n\n72\n\n\nwesternized\n\n\n\n\nG80275\n\n\n3.831358\n\n\n0.876712\n\n\n66\n\n\nwesternized\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\nKHG_9\n\n\n3.884285\n\n\n0.861683\n\n\n87\n\n\nnon_westernized\n\n\n\n\nA48_01_1FE\n\n\n4.377755\n\n\n0.930024\n\n\n53\n\n\nnon_westernized\n\n\n\n\nKHG_1\n\n\n3.733834\n\n\n0.875335\n\n\n108\n\n\nnon_westernized\n\n\n\n\nTZ_81781\n\n\n2.881856\n\n\n0.719491\n\n\n44\n\n\nnon_westernized\n\n\n\n\nA09_01_1FE\n\n\n2.982322\n\n\n0.719962\n\n\n75\n\n\nnon_westernized\n\n\n\n\n\n\n201 rows × 4 columns\n\n\nalpha_diversity = alpha_diversity.melt(id_vars='group', value_name='alpha diversity', var_name='diversity_index', ignore_index=False)\nalpha_diversity\n\n\n\n\n\n\n\n\n\ngroup\n\n\ndiversity_index\n\n\nalpha diversity\n\n\n\n\nsample_id\n\n\n\n\n\n\n\n\n\n\n\n\nERR5766177\n\n\nERR5766177\n\n\nshannon\n\n\n3.032945\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\nwesternized\n\n\nshannon\n\n\n0.798112\n\n\n\n\nPNP_Main_283\n\n\nwesternized\n\n\nshannon\n\n\n5.092878\n\n\n\n\nPNP_Validation_55\n\n\nwesternized\n\n\nshannon\n\n\n3.670162\n\n\n\n\nG80275\n\n\nwesternized\n\n\nshannon\n\n\n3.831358\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\nKHG_9\n\n\nnon_westernized\n\n\nrichness\n\n\n87.000000\n\n\n\n\nA48_01_1FE\n\n\nnon_westernized\n\n\nrichness\n\n\n53.000000\n\n\n\n\nKHG_1\n\n\nnon_westernized\n\n\nrichness\n\n\n108.000000\n\n\n\n\nTZ_81781\n\n\nnon_westernized\n\n\nrichness\n\n\n44.000000\n\n\n\n\nA09_01_1FE\n\n\nnon_westernized\n\n\nrichness\n\n\n75.000000\n\n\n\n\n\n\n603 rows × 3 columns\n\n\ng = ggplot(alpha_diversity, aes(x='group', y='alpha diversity', color='group'))\ng += geom_violin()\ng += geom_jitter()\ng += theme_classic()\ng += facet_wrap('~diversity_index', scales = 'free')\ng += theme(axis_text_x=element_text(rotation=45, hjust=1))\ng += scale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\ng += theme(subplots_adjust={'wspace': 0.15})\ng\n\n\n\npng\n\n\n&lt;ggplot: (407407577)&gt;\nPause and think: Why do we observe a smaller species richness and diversity in our sample ?\n\n\n\n10.3.7.3 7.2.2 Beta diversity\nThe Beta diversity is the measure of diversity between a pair of samples. It is used to compare the diversity between samples and see how they relate.\nWe will compute the beta diversity using the bray-curtis dissimilarity\nbeta_diversity = skbio.diversity.beta_diversity(metric='braycurtis', counts=all_species.transpose(), ids=all_species.columns, validate=True)\nWe get a distance matrix\nprint(beta_diversity)\n201x201 distance matrix\nIDs:\n'ERR5766177', 'de028ad4-7ae6-11e9-a106-68b59976a384', 'PNP_Main_283', ...\nData:\n[[0.         1.         0.81508134 ... 0.85716612 0.69790092 0.8303726 ]\n [1.         0.         0.99988327 ... 0.99853413 0.994116   0.99877258]\n [0.81508134 0.99988327 0.         ... 0.82311942 0.87202543 0.91363156]\n ...\n [0.85716612 0.99853413 0.82311942 ... 0.         0.84253376 0.76616679]\n [0.69790092 0.994116   0.87202543 ... 0.84253376 0.         0.82409272]\n [0.8303726  0.99877258 0.91363156 ... 0.76616679 0.82409272 0.        ]]\nTo visualize this distance matrix in a lower dimensional space, we’ll use a PCoA, which is is a method very similar to a PCA, but taking a distance matrix as input.\npcoa = skbio.stats.ordination.pcoa(beta_diversity)\n/Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:143: RuntimeWarning:\nThe result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues.\nIf the negative ones are smaller, it's probably safe to ignore them, but if they are large in magnitude, the results won't be useful.\nSee the Notes section for more details. The smallest eigenvalue is -0.25334842745723996 and the largest is 10.204440747987945.\npcoa.samples\n\n\n\n\n\n\n\n\n\nPC1\n\n\nPC2\n\n\nPC3\n\n\nPC4\n\n\nPC5\n\n\nPC6\n\n\nPC7\n\n\nPC8\n\n\nPC9\n\n\nPC10\n\n\n…\n\n\nPC192\n\n\nPC193\n\n\nPC194\n\n\nPC195\n\n\nPC196\n\n\nPC197\n\n\nPC198\n\n\nPC199\n\n\nPC200\n\n\nPC201\n\n\n\n\n\n\nERR5766177\n\n\n0.216901\n\n\n-0.039778\n\n\n0.107412\n\n\n0.273272\n\n\n0.020540\n\n\n0.114876\n\n\n-0.256332\n\n\n-0.151069\n\n\n0.097451\n\n\n0.060211\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n\n\n-0.099355\n\n\n0.145224\n\n\n-0.191676\n\n\n0.127626\n\n\n0.119754\n\n\n-0.132209\n\n\n-0.097382\n\n\n0.036728\n\n\n0.081294\n\n\n-0.056686\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nPNP_Main_283\n\n\n-0.214108\n\n\n-0.147466\n\n\n0.116027\n\n\n0.090059\n\n\n0.076644\n\n\n0.111536\n\n\n0.092115\n\n\n0.026477\n\n\n-0.006460\n\n\n-0.018592\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nPNP_Validation_55\n\n\n0.244827\n\n\n-0.173996\n\n\n-0.311197\n\n\n-0.012836\n\n\n0.031759\n\n\n0.117548\n\n\n0.148715\n\n\n-0.135641\n\n\n0.034730\n\n\n-0.009395\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nG80275\n\n\n-0.261358\n\n\n-0.077147\n\n\n-0.254374\n\n\n-0.065932\n\n\n0.088538\n\n\n0.165970\n\n\n-0.005260\n\n\n-0.028739\n\n\n-0.002016\n\n\n0.015719\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\nKHG_9\n\n\n0.296057\n\n\n-0.150300\n\n\n0.013941\n\n\n0.032649\n\n\n-0.147692\n\n\n0.019663\n\n\n-0.063120\n\n\n-0.034453\n\n\n-0.073514\n\n\n0.070085\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nA48_01_1FE\n\n\n0.110621\n\n\n0.030971\n\n\n0.154231\n\n\n-0.185961\n\n\n-0.008512\n\n\n-0.103420\n\n\n0.028169\n\n\n-0.044530\n\n\n0.041902\n\n\n0.068597\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nKHG_1\n\n\n-0.100009\n\n\n0.167885\n\n\n0.009915\n\n\n0.076842\n\n\n-0.405582\n\n\n-0.039111\n\n\n-0.006421\n\n\n-0.009774\n\n\n-0.072252\n\n\n0.150000\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nTZ_81781\n\n\n0.405716\n\n\n-0.139297\n\n\n-0.075026\n\n\n-0.079716\n\n\n-0.053264\n\n\n-0.119271\n\n\n0.068261\n\n\n-0.018821\n\n\n0.198152\n\n\n-0.012792\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nA09_01_1FE\n\n\n0.089101\n\n\n0.471135\n\n\n0.069629\n\n\n-0.125644\n\n\n-0.036793\n\n\n0.115151\n\n\n0.060507\n\n\n-0.000912\n\n\n-0.027239\n\n\n-0.138436\n\n\n…\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n201 rows × 201 columns\n\n\nLet’s look at the variance explained by the first axes by using a scree plot\nvar_explained = pcoa.proportion_explained[:9].to_frame(name='variance explained').reset_index().rename(columns={'index':'PC'})\nggplot(var_explained, aes(x='PC', y='variance explained', group=1)) \\\n+ geom_point() \\\n+ geom_line() \\\n+ theme_classic()\n\n\n\npng\n\n\n&lt;ggplot: (407531271)&gt;\nIn this scree plot, we’re looking for the “elbow”, where there is a drop in the slope. Here, it seems that most of the variance is captures by the 3 first principal components\npcoa_embed = pcoa.samples[['PC1','PC2','PC3']].rename_axis('sample').reset_index()\npcoa_embed = (\n    pcoa_embed\n    .merge(metadata[['sample_id', 'non_westernized']], left_on='sample', right_on='sample_id', how='outer')\n    .drop('sample_id', axis=1)\n    .rename(columns={'non_westernized':'group'})\n)\npcoa_embed['group'] = pcoa_embed['group'].replace({'yes':'non_westernized','no':'westernized', pd.NA:'ERR5766177'})\nLet’s first look at these components with 2D plots\nggplot(pcoa_embed, aes(x='PC1', y='PC2', color='group')) \\\n+ geom_point() \\\n+ theme_classic() \\\n+ scale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\n\n\n\npng\n\n\n&lt;ggplot: (407572134)&gt;\nggplot(pcoa_embed, aes(x='PC1', y='PC3', color='group')) +\ngeom_point() +\ntheme_classic() +\nscale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\n\n\n\npng\n\n\n&lt;ggplot: (407612651)&gt;\nThen with a 3d plot\nimport plotly.express as px\n\nfig = px.scatter_3d(pcoa_embed, x=\"PC1\", y=\"PC2\", z=\"PC3\",\n                  color = \"group\",\n                  color_discrete_map={'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'},\n                  hover_name=\"sample\")\nfig.show()\n\n\n\n\n\n\nImportant\n\n\n\n3D PLOT HERE NOT DISPLAYED DUE TO RENDERING LIMITATIONS - PLEASE SEE JUPYTER NOTEBOOK\n\n\nPause and think: How do you think this embedding represents how our sample relates to modern reference samples ?\nWe can also visualize this distance matrix using a clustered heatmap, where pairs of sample with a small beta diversity are clustered together\nimport seaborn as sns\nimport scipy.spatial as sp, scipy.cluster.hierarchy as hc\npcoa_embed['colour'] = pcoa_embed['group'].map({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\nlinkage = hc.linkage(sp.distance.squareform(beta_diversity.to_data_frame()), method='average')\n\nsns.clustermap(\n    beta_diversity.to_data_frame(),\n    row_linkage=linkage,\n    col_linkage=linkage,\n    row_colors = pcoa_embed['colour'].to_list()\n)\n&lt;seaborn.matrix.ClusterGrid at 0x185b56100&gt;\n\n\n\npng\n\n\n\n\n\n10.3.8 8. Additional steps\n\n10.3.8.1 8.1 Source tracking\nSourcetracker is a program that can estimate the proportion of different sources (reference biomes) contained in a sample (a sink). However, because of the statistical framework that it uses (MCMC with Gibbs sampling), we recommend to limit the number of source samples to greatly reduce runtime\nFirst, you will need to transform our relative abundance table to counts for SourceTracker\nall_species_counts = all_species.multiply(1000000).astype(int)\nmin_count = all_species_counts.sum(axis=0).min()\nmin_count\n95327810\nExporting the count table to tsv\nall_species_counts.to_csv(\"../results/sourcetracker2/all_species_counts.tsv\", sep=\"\\t\", index_label='Taxon')\nConverting to biom format\n!biom convert -i ../results/sourcetracker2/all_species_counts.tsv \\\n-o ../results/sourcetracker2/all_species_counts.biom \\\n--table-type=\"Taxon table\" --to-hdf5\nConverting the metadata to Sourtracker format\nst2_metadata = metadata[['sample_id', 'non_westernized']].rename(columns={'non_westernized':'Env', 'sample_id':'#SampleID'})\nst2_metadata['Env'] = st2_metadata['Env'].replace({'yes':'non_westernized','no':'westernized'})\nst2_metadata['SourceSink'] = ['source'] * st2_metadata.shape[0]\nWe subset it to select only 10 samples from each source\nst2_metadata = st2_metadata.groupby('Env').sample(10).reset_index()\nst2_metadata = st2_metadata.append({'#SampleID':'ERR5766177', 'Env':'-','SourceSink':'sink'},\n                                   ignore_index=True)[['#SampleID','SourceSink','Env']].set_index('#SampleID')\n/var/folders/1c/l1qb09f15jddsh65f6xv1n_r0000gp/T/ipykernel_40830/2882312005.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nst2_metadata.to_csv(\"../results/sourcetracker2/labels_st2.tsv\", sep=\"\\t\", index_label='#SampleID')\nsourcetracker2 gibbs \\\n    -i ../results/sourcetracker2/all_species_counts.biom \\\n    -m ../results/sourcetracker2/labels_st2.tsv \\\n    -o ../results/sourcetracker2/st2 \\\n    --source_rarefaction_depth 95327810 \\\n    --sink_rarefaction_depth 95327810 \\\n    --jobs 10\nBecause SourceTracker is relying on MCMC sampling, it can very slow to run (which is why we won’t run it here)\nAmong alternative faster solutions for source tracking are (among others):\n\nFEAST (article, code),\nSourcepredict (article, code)\n\n\n\n10.3.8.2 8.2 The next steps:\n\nDamage Analysis (mapDamage, DamageProfiler, PyDamage)\nAssembly (megahit, metaSPAdes), binning (metabat2, maxbin2, dastool), and bin validation (checkm, gunc)\nFunctional analysis (Prokka, Humann)\nDifferential abundance (Maaslin2, Lefse, Songbird, GLM, Mixed effect models). Nice review by Wallen 2021\ngenotyping\nPhylogenies\n…"
  },
  {
    "objectID": "functional-profiling.html#lecture",
    "href": "functional-profiling.html#lecture",
    "title": "11  Functional Profiling",
    "section": "11.1 Lecture",
    "text": "11.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "functional-profiling.html#preparation",
    "href": "functional-profiling.html#preparation",
    "title": "11  Functional Profiling",
    "section": "11.2 Preparation",
    "text": "11.2 Preparation\nThe data and conda environment .yaml file for this practical session can be downloaded from here: https://doi.org/10.5281/zenodo.6983188. See instructions on page.\nChange into the session directory\ncd /&lt;path&gt;/&lt;to&gt;/5c-functional-genomics/\nLoad the conda environment.\nconda activate phylogenomics-functional\nOpen R Studio from within the conda environment, and we can load the required libraries for this walkthrough.\nlibrary(mixOmics) ## For PCA generation\n\n## Utility packages (pretty stuff)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(ggrepel)\nlibrary(viridis)\nlibrary(patchwork)"
  },
  {
    "objectID": "functional-profiling.html#humann3-pathways",
    "href": "functional-profiling.html#humann3-pathways",
    "title": "11  Functional Profiling",
    "section": "11.3 HUMAnN3 Pathways",
    "text": "11.3 HUMAnN3 Pathways\nFirst, we need to run HUMMAn3 to align reads against gene databases and convert to gene family names counts.\n\n\n\n\n\n\nWarning\n\n\n\nWe will not run HUMANn3 here as it requires very large databases and takes a long time to run, so we will give you the commands you normally would run but we provide with you pre-made results files before you (see below).\n## DO NOT RUN!\n\n# run humann3\nhumann3 --input file.fastq --output output --threads &lt;threads&gt;\n\n# join all output tables (can do for both gene and pathways)\nhumann_join_tables -i output/ -o genefamilies_joined.tsv --file_name unmapped_genefamilies\n\n# normalize the output (here by tss - total sum scaling, can do for both gene and pathways)\nhumann_renorm_table --input genefamilies_joined.tsv --output genefamilies_joined_cpm.tsv --units tss\n\n# regroup the table to combine gene families (standardise gene family IDs across taxa)\nhumann_regroup_table --input genefamilies_joined_cpm.tsv --output genefamilies_joined_cpm_ur90rxn.tsv --groups uniref90_rxn\n\n# give the gene families names\nhumann_rename_table --input genefamilies_joined_cpm_ur90rxn.tsv --output genefamilies_joined_cpm_ur90rxn_names.tsv -n metacyc-rxn"
  },
  {
    "objectID": "functional-profiling.html#humann3-tables",
    "href": "functional-profiling.html#humann3-tables",
    "title": "11  Functional Profiling",
    "section": "11.4 humann3 tables",
    "text": "11.4 humann3 tables\nFirst lets load a pre-made pathway abundance file\n## load the species and genus tables generated with humann3\nhumann3_path_full &lt;- fread(\"./pathabundance_joined_cpm.tsv\")\nhumann3_path_full &lt;- as_tibble(humann3_path_full)\n\n# clean the file names\nhumann3_path_full &lt;- rename(humann3_path_full, Pathway = `# Pathway`)\ncolnames(humann3_path_full) &lt;- gsub(\".unmapped_Abundance\",\"\", colnames(humann3_path_full))\ncolnames(humann3_path_full) &lt;- gsub(\".SG1\",\"\", colnames(humann3_path_full))\n\n# remove unmapped and ungrouped reads\nhumann3_path &lt;- humann3_path_full %&gt;% filter(!str_detect(Pathway, \"UNMAPPED|UNINTEGRATED\"))\nThen lets load associated sample metadata to help make it easier for comparative analysis and make actual informative inferences.\nThe data being used in this session, is from Velsko et al. 2022 (PNAS Nexus), where we tried to find associations between dental pathologies and taxonomic and genome content. We had a large skeletal collection from a single site in the Netherlands, with a lot of osteological metadata. The study aimed to see if there were any links between the oral microbiome and groups of dental pathologies.\n# load the metadata file\nfull_metadata &lt;- fread(\"full_combined_metadata.tsv\")\n\n\n## Example of metadata\ntibble(full_metadata %&gt;%\n    filter(Site_code == \"MID\") %&gt;%\n    select(Site, Time_period, Library_ID, Sequencing_instrument, Pipenotch, Max_Perio_Score, `%teeth_with_caries`))\nFirst step: we can pre-define various functions for generate PCAs we will use downstream - you don’t have to worry about these too much they are just custom functions to quickly plot PCAs from a mixOmics PCA output object with ggplot, but we leave the code here for if you’re curious.\n# plot PCA with colored dots and the title including the # of species or genera\nplot_pca &lt;- function(df, pc1, pc2, color_group, shape_group, ncomps) {\n    metadata_group_colors &lt;- get(paste(color_group, \"_colors\", sep = \"\"))\n    metadata_group_shapes &lt;- get(paste(shape_group, \"_shapes\", sep = \"\"))\n\n    pca.list &lt;- mixOmics::pca(df, ncomp = ncomps, logratio = 'CLR')\n\n    ## Pull out loadings\n    exp_var &lt;- paste0(round(pca.list$explained_variance * 100, 2), \"%\")\n    df_X &lt;- pca.list$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    color_group = df_X[[color_group]]\n    shape_group = df_X[[shape_group]]\n\n    ## Selecting which PCs to plot\n    if (pc1 == 'PC1') {\n        pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    }  else if (pc1 == 'PC2') {\n        pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n    }\n\n    if (pc2 == 'PC1') {\n        pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n    }  else if (pc2 == 'PC2') {\n        pc2 &lt;- df_X$PC2\n        exp_var_pc2 &lt;- exp_var[2]\n        yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n        pc2 &lt;- df_X$PC3\n        exp_var_pc2 &lt;- exp_var[3]\n        yaxis &lt;- c(\"PC3\")\n    }\n\n    ## Generate figure\n    pca_plot &lt;- ggplot(df_X, aes(pc1, pc2)) +\n     geom_point(aes(fill = color_group, shape = shape_group), size = 4.5, stroke = 0.3) +\n     scale_fill_manual(values = metadata_group_colors) +\n     scale_shape_manual(values = metadata_group_shapes) +\n     # stat_ellipse() +\n     xlab(paste(xaxis, \" - \", exp_var_pc1)) +\n     ylab(paste(yaxis, \" - \", exp_var_pc2)) +\n     theme_minimal(base_size = 16) +\n     theme(text = element_text(size=16)) +\n     theme(legend.title = element_blank(),\n           legend.key.size = unit(2,\"mm\"),\n           legend.text = element_text(size = 6)) +\n     theme(legend.position = \"top\")\n\n    return(pca_plot)\n}\n\n# for continuous data\nplot_pca_cont &lt;- function(df, pc1, pc2, color_group, shape_group, ncomps, title_text) {\n\n    pca.list &lt;- mixOmics::pca(df, ncomp = ncomps, logratio = 'CLR')\n\n    exp_var &lt;- paste0(round(pca.list$explained_variance * 100, 2), \"%\")\n    df_X &lt;- pca.list$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    color_group = df_X[[color_group]]\n    shape_group = df_X[[shape_group]]\n\n    if (pc1 == 'PC1') {\n        pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    }  else if (pc1 == 'PC2') {\n        pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n    }\n\n    if (pc2 == 'PC1') {\n        pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n    }  else if (pc2 == 'PC2') {\n        pc2 &lt;- df_X$PC2\n        exp_var_pc2 &lt;- exp_var[2]\n        yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n        pc2 &lt;- df_X$PC3\n        exp_var_pc2 &lt;- exp_var[3]\n        yaxis &lt;- c(\"PC3\")\n    }\n\n    pca_plot &lt;- ggplot(df_X, aes(pc1, pc2, fill = color_group, shape = shape_group)) +\n     geom_point(size = 5, color = \"black\") +\n     scale_fill_viridis_c(option = \"C\") +\n     scale_shape_manual(values = c(24,21)) +\n     # stat_ellipse() +\n     xlab(paste(xaxis, \" - \", exp_var_pc1)) +\n     ylab(paste(yaxis, \" - \", exp_var_pc2)) +\n     theme_minimal(base_size = 16) +\n     theme(text = element_text(size=16)) +\n     theme(legend.title = element_blank(),\n           legend.key.size = unit(2,\"mm\"),\n           legend.text = element_text(size = 6)) +\n     theme(legend.position = \"top\") +\n     ggtitle(title_text) + theme(plot.title = element_text(size = 10))\n\n    return(pca_plot)\n}\n\nplot_pca_bi &lt;- function(df, pc1, pc2, metadata_group, columntitle) {\n    metadata_group_colors &lt;- get(paste(metadata_group, \"_colors\", sep = \"\"))\n    metadata_group_shapes &lt;- get(paste(metadata_group, \"_shapes\", sep = \"\"))\n\n    arrow_pc &lt;- enquo(columntitle)\n\n    exp_var &lt;- paste0(round(df$explained_variance * 100, 2), \"%\") # explained variance for x- and y-labels\n\n    # select only the PCs from the PCA and add metadata\n    df_X &lt;- df$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    metadata_group = df_X[[metadata_group]]\n\n    corr_lam &lt;- df$sdev[c(\"PC1\", \"PC2\", \"PC3\")] * sqrt(nrow(df_X))\n\n    df_X &lt;- df_X %&gt;%\n      mutate(PC1 = PC1 / corr_lam[1],\n             PC2 = PC2 / corr_lam[2],\n             PC3 = PC3 / corr_lam[3])\n\n    # select the correct PC column and explained variance for PC1\n    if (pc1 == 'PC1') {\n        Pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    } else if (pc1 == 'PC2') {\n        Pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        Pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n   }\n\n    # select the correct PC column and explained variance for PC2\n    if (pc2 == 'PC1') {\n        Pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n } else if (pc2 == 'PC2') {\n       Pc2 &lt;- df_X$PC2\n       exp_var_pc2 &lt;- exp_var[2]\n       yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n       Pc2 &lt;- df_X$PC3\n       exp_var_pc2 &lt;- exp_var[3]\n       yaxis &lt;- c(\"PC3\")\n   }\n\n    # Identify the 10 pathways that have highest positive and negative loadings in the selected PC\n    pws_10 &lt;- df$loadings$X %&gt;%\n      as.data.frame(.) %&gt;%\n      rownames_to_column(var = \"Pathway\") %&gt;%\n      separate(Pathway, into = \"Pathway\", sep = \":\", extra = \"drop\") %&gt;%\n      top_n(10, !!arrow_pc)\n\n    neg_10 &lt;- df$loadings$X %&gt;%\n      as.data.frame(.) %&gt;%\n      rownames_to_column(var = \"Pathway\") %&gt;%\n      separate(Pathway, into = \"Pathway\", sep = \":\", extra = \"drop\") %&gt;%\n      top_n(-10, !!arrow_pc)\n\n\n    pca_plot_bi &lt;- ggplot(df_X, aes(x = Pc1, y = Pc2)) +\n      geom_point(size = 3.5, aes(shape = metadata_group, fill = metadata_group))+\n      geom_segment(data = pws_10,\n                   aes(xend = get(paste(pc1)), yend = get(paste(pc2))),\n                   x = 0, y = 0, colour = \"black\",\n                   size = 0.5,\n                   arrow = arrow(length = unit(0.03, \"npc\"))) +\n      geom_label_repel(data = pws_10,\n                   aes(x = get(paste(pc1)), y = get(paste(pc2)), label = Pathway),\n                   size = 2.5, colour = \"grey20\", label.padding = 0.2, force = 5, max.overlaps = 20) +\n      geom_segment(data = neg_10,\n                   aes(xend = get(paste(pc1)), yend = get(paste(pc2))),\n                   x = 0, y = 0, colour = \"grey50\",\n                   size = 0.5,\n                   arrow = arrow(length = unit(0.03, \"npc\"))) +\n      geom_label_repel(data = neg_10,\n                   aes(x = get(paste(pc1)), y = get(paste(pc2)), label = Pathway),\n                   size = 2.5, colour = \"grey20\", label.padding = 0.2, max.overlaps = 12) +\n      labs(x = paste(xaxis, \" - \", exp_var_pc1),\n           y = paste(yaxis, \" - \", exp_var_pc2)) +\n      scale_fill_manual(values = metadata_group_colors) +\n      scale_shape_manual(values = metadata_group_shapes) +\n      theme_minimal() + theme(text = element_text(size = 16)) +\n      theme(text = element_text(size=16)) +\n      theme(legend.position = \"top\")\n\n    return(pca_plot_bi)\n}\nAs we are dealing with aDNA, and we often have bad samples, its sometimes interesting to see differences between well/badly preserved samples at all stages of analysis.\nTherefore we may generate results for all samples. However for actual analysis where we want to intepret biological differences, should exclude outliers (in this case highly contaminated samples - as identified by the decontam package - see Velsko et al. 2022 _PNAS Nexus for more details).\nWe can make a list the outliers from the previous authentication analyses.\noutliers_mpa3 &lt;- c(\"EXB059.A2101\",\"EXB059.A2501\",\"EXB015.A3301\",\"EXB034.A2701\",\n                   \"EXB059.A2201\",\"EXB059.A2301\",\"EXB059.A2401\",\"LIB058.A0103\",\"LIB058.A0106\",\"LIB058.A0104\")\npoor_samples_mpa3 &lt;- c(\"CS28\",\"CS38\",\"CSN\",\"ELR003.A0101\",\"ELR010.A0101\",\n                       \"KT09calc\",\"MID024.A0101\",\"MID063.A0101\",\"MID092.A0101\")\n\noutliersF &lt;- str_c(outliers_mpa3, collapse = \"|\")"
  },
  {
    "objectID": "functional-profiling.html#sample-clustering-with-pca",
    "href": "functional-profiling.html#sample-clustering-with-pca",
    "title": "11  Functional Profiling",
    "section": "11.5 Sample Clustering with PCA",
    "text": "11.5 Sample Clustering with PCA\n\n11.5.1 Pathway abundance analyses\nOnce we’ve removed outlier samples, our first simple question is - what is the functional relationships of the groups?\nCan we already see distinctive patterns between the different groups in our dataset?\nTo do this lets clean up the data a bit (cleaning names, removing samples with no metadata etc.), normalise (via a ‘centered-log-ratio’ transform ), and run a PCA.\nOnce we’ve done this we should always check our PCA’s Scree plot first.\n\nhumann3_path_l1 &lt;- humann3_path %&gt;%\n  filter(!str_detect(Pathway, \"\\\\|\")) %&gt;%\n  # no full_metadata, remove these\n  select(-c(\"MID025.A0101\",\"MID033.A0101\",\"MID052.A0101\",\"MID056.A0101\",\n            \"MID065.A0101\",\"MID068.A0101\",\"MID076.A0101\",\"MID078.A0101\")) %&gt;%\n  # remove poorly preserved saples\n  select(-c(\"MID024.A0101\",\"MID063.A0101\",\"MID092.A0101\")) %&gt;%\n  select(-matches(\"EXB|LIB\")) %&gt;%\n  # inner_join(., humann3_path.decontam_noblanks_presence_more_30, by = \"Pathway\") %&gt;%\n  gather(\"Library_ID\",\"Counts\",2:ncol(.)) %&gt;%\n  mutate(Counts = Counts + 1) %&gt;%\n  spread(Pathway,Counts) %&gt;%\n  column_to_rownames(\"Library_ID\")\n\n# prepare to run a PCA\n# check the number of components to retain by tuning the PCA\nmixOmics::tune.pca(humann3_path_l1, logratio = 'CLR')\n\n\nhumann3_all_otu.pca &lt;- mixOmics::pca(humann3_path_l1, ncomp = 3, logratio = 'CLR')\nhumann3_all_pca_values &lt;- humann3_all_otu.pca$variates$X %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Library_ID\") %&gt;%\n  inner_join(., full_metadata, by = \"Library_ID\")\nWe can see the first couple of PCs in the scree plot account for a good chunk of the variation of our dataset, so lets visualise the PCA itself.\nWe visualise the PCA with one of our custom functions defined above, and colour by the Pipe notch metadata column.\n# pipenotch colors/shapes\nPipenotch_colors = c(\"#311068\",\"#C83E73\")\nPipenotch_shapes = c(16,17)\n\n# by minimum number of pipenotches\npipenotch &lt;- plot_pca_cont(humann3_path_l1, \"PC1\", \"PC2\",\"Min_no_Pipe_notches\",\"Pipenotch\", 3,\"Min. No. Pipe Notches\")\npipenotch\nWe can see there is a slight separation between the groups, but how do we find out which pathways are maybe driving this pattern?\nFor this we can generate a PCA bi-plot which show what loadings are driving the spread of the samples.\nPipenotch_colors = c(\"#311068\",\"#C83E73\")\nPipenotch_shapes = c(24,21)\n\nbiplot &lt;- plot_pca_bi(humann3_all_otu.pca, \"PC1\", \"PC2\", \"Pipenotch\", PC1)\nbiplot\nFrom the biplot we can see which pathways are differentiating along PC1.\nWe can pull these IDs out to find out what pathways there are from the biplot object itself.\n# make a table of the pathways to save, to use again later in another R notebook\nhumann3_pathway_biplot_list &lt;- biplot$plot_env$pws_10 %&gt;% arrange(desc(PC1)) %&gt;% select(Pathway, PC1, PC2) %&gt;% mutate(Direction = \"PC1+\")\nhumann3_pathway_biplot_list &lt;- humann3_pathway_biplot_list %&gt;%\n  bind_rows(biplot$plot_env$neg_10 %&gt;% arrange(desc(PC1)) %&gt;% select(Pathway, PC1, PC2)%&gt;% mutate(Direction = \"PC1-\"))\n\n\n11.5.2 Species contributions to pathways\nHowever, this ID numbers aren’t very informative to us. At this point we have to do a bit of literature review/database scraping to pull the human-readable names/descriptions of the IDs - which we have already done for you.\nWe can load these back into our environment\n# PC biplot loading top 10s\nhumann3_pathway_biplot_list &lt;- fread(\"./humann3_pathway_biplot_list.tsv\")\nhumann3_pathway_biplot_list &lt;- humann3_pathway_biplot_list %&gt;%\n  rename(Pathway = pathway) %&gt;%\n  mutate(Path = sapply(Pathway, function(f) {\n                                  unlist(str_split(f, \":\"))[1]\n                                  })) %&gt;%\n  select(Pathway, Path, everything()) %&gt;%\n  # remove 3 of the 4 ubiquinol pathways w/identical loadings\n  filter(!str_detect(Pathway, \"5856|5857|6708\"))\n\ntibble(humann3_pathway_biplot_list)\nWe now have the pathway ID, and a pathway description for each of the loadings of the PCA.\n\n11.5.2.0.1 PC1\nWhile we have the pathways, we don’t who contributed these.\nFor this, we can join our pathway table back onto the original output from HUMANn3 we loaded at the beginning, which includes the taxa information.\n# list the 10 orthologs with strongest loading in PC1 + values\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1+\") %&gt;%\n  pull(Path) %&gt;%\n  str_c(., collapse = \"|\") # need this format for filtering in the next step\n\n\n# select only those 10 pathways from the list, and split the column with names into 3 (Pathway, Genus, Species)\nhumann3_path_pc1pws &lt;- humann3_path %&gt;%\n  filter(str_detect(Pathway, humann3_path_biplot_pc)) %&gt;%\n  filter(str_detect(Pathway, \"\\\\|\")) %&gt;%\n  gather(\"SampleID\", \"CPM\", 2:ncol(.)) %&gt;%\n  mutate(Pathway = str_replace_all(Pathway, \"\\\\.s__\", \"|s__\")) %&gt;%\n  separate(., Pathway, into = c(\"Pathway\", \"Genus\", \"Species\"), sep = \"\\\\|\") %&gt;%\n  mutate(Species = replace_na(Species, \"unclassified\"),\n         Genus = str_replace_all(Genus, \"g__\", \"\"),\n         Species = str_replace_all(Species, \"s__\", \"\")) %&gt;%\n  inner_join(., humann3_pathway_biplot_list %&gt;%\n              select(Pathway, Path) %&gt;%\n               distinct(.), by = \"Pathway\") %&gt;%\n  inner_join(.,  humann3_pathway_biplot_list %&gt;%\n               filter(Direction == \"PC1+\") %&gt;%\n               select(Path), by = \"Path\") %&gt;%\n  # select(-Pathway) %&gt;%\n  select(Path, everything()) %&gt;%\n  arrange(Path)\n\ntibble(humann3_path_pc1pws)\nWe can now see who contributed which pathway, and also the abundance information (CPM)!\nGiven many taxa may contribute to the same pathway, we may want to see which taxa are more ‘dominantly’ contributing to this.\nFor this we can calculate of all copies of a given pathway what fraction comes from which taxa (you can imagine this like ‘depth’ coverage in genomic analysis), based on the percentage of the total copies per million for that pathway.\n# calculate the % for each pathway contributed by each genus\nhumann3_path_pc1pws_stats &lt;- humann3_path_pc1pws %&gt;%\n  group_by(Path, Genus) %&gt;%\n  summarize(Sum = sum(CPM)) %&gt;%\n  mutate(Percent = Sum/sum(Sum)*100) %&gt;%\n  ungroup(.)\n\n# create the list of 10 orthologs again, but don't collapse the list as above\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1+\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path)\n\n# calculate the total % of all genera that contribute &lt; X% to each ortholog\nhumann3_path_pc1pws_stats_extra &lt;- lapply(humann3_path_biplot_pc, function(eclass) {\n high_percent &lt;- humann3_path_pc1pws_stats %&gt;%\n   filter(Path == eclass) %&gt;%\n   filter(Percent &lt; 5) %&gt;%\n   summarise(Remaining = sum(Percent)) %&gt;%\n   mutate(Path = eclass,\n          Genus = \"Other\")\n}) %&gt;%\n bind_rows(.)\n\n# add this additional % to the main table\nhumann3_path_pcbi_bar_df &lt;- humann3_path_pc1pws_stats_extra %&gt;%\n  rename(Percent = Remaining) %&gt;%\n  bind_rows(., humann3_path_pc1pws_stats %&gt;%\n              select(-Sum)) %&gt;%\n  select(Path, Genus, Percent) %&gt;%\n  mutate(Direction = \"PC1+\") %&gt;%\n  distinct()\nAnd we can visualize the contributors to the top 10 pathways) driving the main variation along PC1 (with the assumption these maybe the most biological significant, and to reduce the numbers of pathways we have to research.\nFor the loadings falling in the positive direction of the PC1:\n# plot the values in a bar chart\npaths_sp_pc1 &lt;- humann3_path_pcbi_bar_df %&gt;%\n  # filter(Direction == \"PC1+\", Genus != \"Other\") %&gt;% # removing Other plots all species/unassigned - no need to filter the pathways\n  filter(Percent &gt;= 5 | (Percent &lt;= 5 & Genus == \"Other\")) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  # filter(Percent &gt;= 5) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  mutate(\n    Genus = fct_relevel(Genus, \"Other\",\"unclassified\",\"Aggregatibacter\",\"Capnocytophaga\",\"Cardiobacterium\",\n                        \"Eikenella\",\"Haemophilus\",\"Kingella\",\"Lautropia\",\"Neisseria\",\"Ottowia\",\"Streptococcus\"),\n         Path = fct_relevel(Path, humann3_pathway_biplot_list %&gt;%\n                              filter(Direction == \"PC1+\") %&gt;%\n                              pull(Path))) %&gt;%\n  ggplot(., aes(x=Path, y=Percent, fill = Genus)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0D0887FF\",\"#969696\",\"#5D01A6FF\",\"#7E03A8FF\",\n                                 \"#9C179EFF\",\"#B52F8CFF\",\"#CC4678FF\",\"#DE5F65FF\",\n                                 \"#ED7953FF\",\"#F89441FF\",\"#FDB32FFF\",\"#FBD424FF\",\"#F0F921FF\")) +\n    theme(text = element_text(size=18),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    ylab(\"Percent\") +\n    ggtitle(\"Metacyc pathways - PC1 positive\") +\n    theme(title = element_text(size=10))\n\n# viridis_pal(option = \"B\")(13)\npaths_sp_pc1\n\n\n\n\n\n\nWarning\n\n\n\nPWY-5345 has no species assignment to that pathway.\n\n\nAnd the negative loadings:\n# list the 10 orthologs with strongest loading in PC1 + values\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1-\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path) %&gt;%\n  str_c(., collapse = \"|\") # need this format for filtering in the next step\n\n# select only those 10 orthologs from the list, and split the column with names into 3 (Ortholog, Genus, Species)\nhumann3_path_pc1neg &lt;- humann3_path %&gt;%\n  filter(str_detect(Pathway, humann3_path_biplot_pc)) %&gt;%\n  filter(str_detect(Pathway, \"\\\\|\")) %&gt;%\n  gather(\"SampleID\", \"CPM\", 2:ncol(.)) %&gt;%\n  mutate(Pathway = str_replace_all(Pathway, \"\\\\.s__\", \"|s__\")) %&gt;%\n  separate(., Pathway, into = c(\"Pathway\", \"Genus\", \"Species\"), sep = \"\\\\|\") %&gt;%\n  mutate(Species = replace_na(Species, \"unclassified\"),\n         Genus = str_replace_all(Genus, \"g__\", \"\"),\n         Species = str_replace_all(Species, \"s__\", \"\")) %&gt;%\n  inner_join(., humann3_pathway_biplot_list %&gt;%\n              select(Pathway, Path) %&gt;%\n               distinct(.), by = \"Pathway\") %&gt;%\n  inner_join(.,  humann3_pathway_biplot_list %&gt;%\n               filter(Direction == \"PC1-\") %&gt;%\n               select(Path), by = \"Path\") %&gt;%\n  select(-Pathway) %&gt;%\n  select(Path, everything()) %&gt;%\n  arrange(Path)\n\n# calculate the % for each ortholog contributed by each genus\nhumann3_path_pc1neg_stats &lt;- humann3_path_pc1neg %&gt;%\n  group_by(Path, Genus) %&gt;%\n  summarize(Sum = sum(CPM)) %&gt;%\n  mutate(Percent = Sum/sum(Sum)*100) %&gt;%\n  ungroup(.)\n\n# create the list of 10 orthologs again, but don't collapse the list as above\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1-\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path)\n\n# calculate the total % of all genera that contribute &lt; X% to each ortholog\nhumann3_path_pc1neg_stats_extra &lt;- lapply(humann3_path_biplot_pc, function(eclass) {\n high_percent &lt;- humann3_path_pc1neg_stats %&gt;%\n   filter(Path == eclass) %&gt;%\n   filter(Percent &lt; 5) %&gt;%\n   summarise(Remaining = sum(Percent)) %&gt;%\n   mutate(Path = eclass,\n          Genus = \"Other\")\n}) %&gt;%\n bind_rows(.)\n\n# add this additional % to the main table\nhumann3_path_pcbi_bar_df &lt;- humann3_path_pcbi_bar_df %&gt;%\n  bind_rows(humann3_path_pc1neg_stats_extra %&gt;%\n            rename(Percent = Remaining) %&gt;%\n            bind_rows(., humann3_path_pc1neg_stats %&gt;%\n                      select(-Sum)) %&gt;%\n            select(Path, Genus, Percent) %&gt;%\n            mutate(Direction = \"PC1-\")) %&gt;%\n  distinct()\n\n# plot the values in a bar chart\npaths_sp_pc2 &lt;- humann3_path_pcbi_bar_df %&gt;%\n  filter(Direction == \"PC1-\") %&gt;% # removing Other plots all species/unassigned - no need to filter the pathways\n  filter(Percent &gt;= 5 | (Percent &lt;= 5 & Genus == \"Other\")) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  mutate(Genus = fct_relevel(Genus, \"Other\",\"unclassified\",\"Desulfobulbus\",\"Desulfomicrobium\",\"Methanobrevibacter\"),\n         Path = fct_relevel(Path, humann3_pathway_biplot_list %&gt;%\n                              filter(Direction == \"PC1-\") %&gt;%\n                              pull(Path))) %&gt;%\n  ggplot(., aes(x=Path, y=Percent, fill = Genus)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0D0887FF\",\"#969696\",\"#B52F8CFF\",\"#ED7953FF\",\"#FCFFA4FF\")) +\n    theme(text = element_text(size=18),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    # facet_wrap(~pathrtholog, nrow=2) +\n    ylab(\"Percent\") +\n    ggtitle(\"Metacyc pathways - PC1 negative\") +\n    theme(title = element_text(size=12))\n\npaths_sp_pc2\n\n\n\n11.5.3 Final Visualisation\nFinally, we can stick the biplot and the taxon contribution plots together!\nh3biplots &lt;- biplot + paths_sp_pc2 + paths_sp_pc1 +\n  plot_layout(widths = c(2, 1,1))\n\nggsave(\"./h3_paths_biplots.pdf\", plot = h3biplots,\n        device = \"pdf\", scale = 1, width = 20, height = 9.25, units = c(\"in\"), dpi = 300)\n\nsystem(paste0('firefox \"h3_paths_biplots.pdf\"'))\nThis allows us to evaluate all the information together.\nFrom this point onwards, we would have to do manual research/literature reviews into each of the pathways, see if they make ‘sense’ to the sample type and associated groups of samples, and evaluate whether they are interesting or not.."
  },
  {
    "objectID": "denovo-assembly.html#introduction",
    "href": "denovo-assembly.html#introduction",
    "title": "12  Introduction to de novo Genome Assembly",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction"
  },
  {
    "objectID": "decontamination-authentication.html#introduction",
    "href": "decontamination-authentication.html#introduction",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nIn ancient metagenomics we typically try to answer two questions: “Who is there?” and “How ancient?”, meaning we would like to detect an organism and investigate whether this organism is ancient. There are three typical ways to identify the presence of an organism in a metagenomic sample:\n\nalignment of DNA fragments to a reference genome (Bowtie, BWA, Malt etc.)\ntaxonomic (kmer-based) classification of DNA fragments (Kraken, MetaPhlan, Centrifuge etc.)\nde-novo genome assembly (Megahit, metaSPAdes etc.)\n\nThe first two are reference-based, i.e. they assume a similarity of a query ancient DNA fragment to a modern reference genome in a database. This is a strong assumption, which might not be true for very old or very diverged ancient organisms. This is the case when the reference-free de-novo assembly approach becomes prowerful. However, de-novo assembly has its own computational challenges for low-coverage ancient metagenomic samples that typically contain very short DNA fragments.\n\nWhile all the three types of metagenomic analysis are suitable for exploring composition of metagenomic samples, they do not directly validate the findings or provide information about ancient or endogenous status of the detected organims. It can happen that the detected organism 1) was mis-identified (the DNA belongs to another organism than initially thought), 2) has a modern origin (for example, lab or sequencing contaminant), or 3) is of exogenous origin (for example, an ancient microbe that entered the host post-mortem). Therefore, additional analysis is needed to follow-up each hit and demonstrate its ancient origin. Below, we describe a few steps that can help ancient metagenomic researchers to verify their findings and put them into biological context."
  },
  {
    "objectID": "decontamination-authentication.html#evennes-of-coverage",
    "href": "decontamination-authentication.html#evennes-of-coverage",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.2 Evennes of coverage",
    "text": "13.2 Evennes of coverage\nConcluding organism presence by relying solely on the numbers of assigned sequenced reads (aka depth of coverage metric) turns out to be not optimal and too permissive, which may result in a large amount of false-positive discoveries. For example, when using mapping to a reference database, the aligned reads may demonstrate non-uniform coverage as visualized in the Integrative Genomics Viewer (IGV) below.\n\nIn this case, DNA reads originating from another microbe were (mis-)aligned to Yersina pestis reference genome. It can be observed that a large numer the reads align only to a few conserved genomic loci. Therefore, even if many thousands of DNA reads are capable of aligning to the reference genome, the overall uneven alignment pattern suggests no presence of Yersina pestis in the metagenomic sample. Thus, not only the number of assigned reads (proportinal to depth of coverage metric) but also the breadth and evenness of coverage metrics become of particular importance for veryfication of metagenomic findings, i.e. hits with DNA reads uniformly aligned across the reference genome are more likely to be true-positive detections.\n\nPractically, the breadth / evenness of coverage can be computed from the BAM-alignments via samtools depth as follows:\nsamtools depth -a sorted.bam &gt; sorted.bam.breadth_of_coverage\nand visualized using for example the following R code snippet (alternatively aDNA-BAMPlotter can be used):\n# Read output of samtools depth commans\ndf &lt;- read.delim(\"sorted.bam.breadth_of_coverage\", header = FALSE, sep = \"\\t\")\nnames(df) &lt;- c(\"Ref\", \"Pos\", \"N_reads\")\n\n# Split reference genome in tiles, compute breadth of coverage for each tile\nN_tiles &lt;- 100\nstep &lt;- (max(df$Pos) - min(df$Pos)) / N_tiles; tiles &lt;- c(0:N_tiles) * step\nboc &lt;- vector()\nfor(i in 1:length(tiles))\n{\n  df_loc &lt;- df[df$Pos &gt;= tiles[i] & df$Pos &lt; tiles[i+1], ]\n  boc &lt;- append(boc, rep(sum(df_loc$N_reads &gt; 0) / length(df_loc$N_reads),\n  dim(df_loc)[1]))\n}\nboc[is.na(boc)]&lt;-0; df$boc &lt;- boc\n\n# Plot evenness of coverage\nplot(df$boc ~ df$Pos, type = \"s\", xlab = \"Genome position\", ylab = \"Coverage\")\nabline(h = 0, col = \"red\", lty = 2)\nmtext(paste0(round((sum(df$N_reads &gt; 0) / length(df$N_reads)) * 100, 2), \n\"% of genome covered\"), cex = 0.8)\n\nIn the R script above, we simply split the reference genome into N_tiles tiles and compute the breadth of coverage (number of reference nucleotides covered by at least one read normalized by the total length) locally in each tile. By visualizing how the local breadth of coverage changes from tile to tile, we can monitor the distribution of the reads across the reference genome. In the evenness of coverage figure above, the reads seem to cover all parts of the reference genome uniformly, which is a good evidence of true-positive detection, even though the total mean breadth of coverage is low due to the low total number of reads.\nAlthough taxonomic classifiers do not perform alignment, some of them, such as KrakenUniq and Kraken2 provide a way to infer breadth of coverage in addition to the number of assigned reads to a taxon. This allows for immediate filtering out a lot of false positive hits. Since Kraken-family classifiers are typically faster and less memory-demanding, i.e. can work with very large reference databases, compared to genome aligners, they provide a robust and fairly unbiased initial taxonomic profiling, which can still later be followed-up with proper alignment and computing evenness of coverage as described above.\nKrakenUniq by default delivers a proxy metric for breadth of coverage called the number of unique kmers (in the 4th column of its output table) assigned to a taxon. KrakenUniq can be run with the following command line, and the output can be easily filtered with respect to both depth and breadth of coverage, which substantially reduces the number of false-positive hits.\nDBNAME = KrakenUniq_DB_directory\nKRAKEN_INPUT = sample.fastq.gz\nKRAKEN_OUTPUT = KrakenUniq_output_directory\nkrakenuniq --db $DBNAME --fastq-input $KRAKEN_INPUT --threads 20 \\\n--classified-out $KRAKEN_OUTPUT/classified_sequences.krakenuniq \\\n--unclassified-out $KRAKEN_OUTPUT/unclassified_sequences.krakenuniq \\\n--output $KRAKEN_OUTPUT/sequences.krakenuniq \\\n--report-file $KRAKEN_OUTPUT/krakenuniq.output\n\nWhile KrakenUniq delivers information about breadth of coverage by default, one has to use a special flag –report-minimizer-data when running Kraken2 in order to get the breadth of coverage proxy which is called the number of distrinct minimizers for the case of Kraken2.\nDBNAME = Kraken2_DB_directory\nKRAKEN_INPUT = sample.fastq.gz\nKRAKEN_OUTPUT = Kraken2_output_directory\nkraken2 --db $DBNAME --fastq-input $KRAKEN_INPUT --threads 20 \\\n--classified-out $KRAKEN_OUTPUT/classified_sequences.kraken2 \\\n--unclassified-out $KRAKEN_OUTPUT/unclassified_sequences.kraken2 \\\n--output $KRAKEN_OUTPUT/sequences.kraken2 \\\n--report $KRAKEN_OUTPUT/kraken2.output \\\n--use-names --report-minimizer-data\nThen the filtering of Kraken2 output with respect to breadth and depth of coverage can be done by analogy with filtering KrakenUniq output table. In case of de-novo assembly, the original DNA reads are typically alligned back to the assembled contigs, and the evennes / breadth of coverage can be computed from these alignments."
  },
  {
    "objectID": "decontamination-authentication.html#alignment-quality",
    "href": "decontamination-authentication.html#alignment-quality",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.3 Alignment quality",
    "text": "13.3 Alignment quality\nIn addition to evenness and breadth of coverage, it is very informative to monitor how well the metagenomic reads map to a reference genome. Here one can control for mapping quality (MAPQ field in the BAM-alignments) and the number of mismatches for each read, i.e. edit distance.\nMapping quality (MAPQ) can be extracted from the 5th column of BAM-alignments. It is also recommended to remove multi-mapping reads, i.e. the ones that have MAPQ = 0, at least for Bowtie and BWA aligners that are commonly used in ancient metagenomics. Samtools with -q flag can be used to extract reads with MAPQ &gt; = 1.\nsamtools view -q 1 -h -b -@ 4 original.bam &gt; filtered.bam\nsamtools view temp.bam | cut -f5 &gt; mapq.txt\nThen the 5th column of the filtered BAM-alignment can be visualized via a simple histogram in R as below for two random metagenomic samples.\nmapq &lt;- as.numeric(readLines(\"mapq.txt\"))\nhist(mapq, breaks = 100)\n\nNote that MAPQ scores are computed slightly differently for Bowtie and BWA, so they are not directly comparable, however, for both MAPQ ~ 10-30, as in the histograms above, indicates good affinity of the DNA reads to the reference genome.\nEdit distance can be extracted by gathering information in the NM-tag inside BAM-alignemnts, which reports the number of mismatches for each aligned read. This can be done either in bash / awk, or using handy functions from Rsamtools R package:\nlibrary(\"Rsamtools\")\nparam &lt;- ScanBamParam(tag = \"NM\")\nbam &lt;- scanBam(\"filtered.bam\", param = param)\nbarplot(table(bam[[1]]$tag$NM), ylab=\"Number of reads\", xlab=\"Mismatches\")\n\nIn the barplot above we can see that the majority of reads align either without or with very few mismatches, which is an evidence of high affinity of the aligned reads with respect to the reference genome. For a true-positive finding, the edit distance barplot typically has a decreasing profile. However, for a very degraded DNA, it can have a mode around 1 or 2, which can also be reasonable. A fasle-positive hit would have a mode of the edit distance barplot shifted toward higher numbers of mismatches."
  },
  {
    "objectID": "decontamination-authentication.html#ancient-status",
    "href": "decontamination-authentication.html#ancient-status",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.4 Ancient status",
    "text": "13.4 Ancient status\nChecking evenness of coverage and alignment quality can help us to make sure that the organism we are thinking about is really present in the metagenomic sample. However, we still need to address the question “How ancinet?”. For this purpose we need to compute deamination profile and read length distribution of the aligned reads in order to prove that they demonstrate damage pattern and are sufficiently fragmented, which would be a good evidence of ancient origin of the detected organisms.\nDeamination profile of a damaged DNA demonstrate an enrichment of C / T polymorphisms at the ends of the reads compared to all other single nucleotide substitutions. There are several tools for computing demination profile, but perhaps the most popular is mapDamage. The tool can be run using the following command line:\nmapDamage -i ${INPUT_BAM} -r ${REFERENCE_SEQUENCE} --no-stats \\\n--merge-reference-sequences -d ${OUTPUT_DIRECTORY}\n\nmaDamage delivers a bunch of useful statistics, among other read length distribution can be checked. A typical mode of DNA reads should be within a range 30-70 base-pairs in order to be a good evidence of DNA fragmentation. Reads longer tha 100 base-pairs are more likely to originate from modern contamination.\n\nAnother useful tool that can be applied to assess how DNA is damaged is PMDtools which is a maximum-likelihood probabilistic model that calculates an ancient score, PMD score, for each read. The ability of PMDtools to infer ancient status with a single read resolution is quite unique and different from mapDamage that can only assess deamination based on a number of reads. PMD scores can be computed using the following command line, please note that Python2 is needed for this purpose.\nsamtools view -h input.bam | python2 pmdtools.0.60.py \\\n--printDS &gt; PMDscores.txt\nThe distribution of PMD scores can be visualized via a histogram in R as follows:\npmd_scores &lt;- read.delim(\"PMDscores.txt\", header = FALSE, sep = \"\\t\")\nhist(pmd_scores$V4, breaks = 1000, xlab = \"PMDscores\")\n\nTypcally, reads with PMD scores greater than 3 are considered to be reliably ancient, i.e. damaged, and can be extracted for taking a closer look. Therefore PMDtools is great for separating ancient reads from modern contaminant reads.\nAs mapDamage, PMDtools can also compute demination profile. However, the advantage of PMDtools that it can compute demination profile for UDG / USER treated samples (with the flag –CpG). For this purpose, PMDtools uses only CpG sites which escape the treatment, so deamination is not gone completely and there is a chance to authenticate treated samples. Computing demination pattern with PMDtoools can be achieved with the following command line (please note that the scripts pmdtools.0.60.py and plotPMD.v2.R can be downloaded from the github repository here https://github.com/pontussk/PMDtools):\nsamtools view mybam.bam | python2 pmdtools.0.60.py --platypus \\\n--requirebaseq 30 &gt; PMD_temp.txt\nR CMD BATCH plotPMD.v2.R"
  },
  {
    "objectID": "decontamination-authentication.html#decontamination",
    "href": "decontamination-authentication.html#decontamination",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.5 Decontamination",
    "text": "13.5 Decontamination\nModern contamination is one of the major problems in ancient metagenomics analysis. Large fractions of modern bacterial, animal or human DNA in metagenomic samples can lead to false biological and historical conclusions. A lot of scientific literature is dedicated to this topic, and comprehensive tables and sources of potential contamination (e.g. animal and bacterial DNA present in PCR reagensts) are available.\n\nA good practice to discriminate between endogenous and contaminant organisms is to sequence negative controls, so-called blanks. Organisms detected on blanks, like the microbial genera reported in the table below, can substantially facilitate making more informed decision about true metagenomic profile of a sample. Nevertheless, the table below may seem rather conservative since in addition to well-known environmental contaminants as Burkholderia and Pseudomonas it includes also human oral genera as Streptococcus, which are probably less likely to be of environmental origin.\n\nIt is typically assumed that an organism found on a blank has a lower confidence to be endogenous to the studied metagenomic sample, and sometimes it is even expluded from the downstream analysis as an unreliable hit. Despite there are attempts to automate filtering out modern contaminants (we will discuss them below), decontamination process still remains to be a tidious manual work where each candidate should be carefully investigated from different contexts in order to prove its ancient and endogenous origin.\nIf negative control samples (balnks) are available, contaminating organisms can be detected by comparing their abundances in the negative controls with true samples. In this case, contaminant organisms stand out by their high prevalence in both types of samples if one simply plots mean across samples abundance of each detected organism in true samples and negative controls against each other as in the figure below:\ndf&lt;-read.delim(\"krakenuniq_abundance_matrix.txt\",header=TRUE,\nrow.names = 1, check.names = FALSE, sep = \"\\t\")\n\ntrue_sample &lt;- subset(df,select=colnames(df)[!grepl(\"control\",colnames(df))])\nnegative_control &lt;- subset(df,select=colnames(df)[grepl(\"control\",colnames(df))])\n\nplot(log10(rowMeans(true_sample)+1) ~ log10(rowMeans(negative_control)+1),\nxlab = \"Log10 ( Negative controls )\", ylab = \"Log10 ( True samples )\",\nmain = \"Organism abundance in true samples vs. negative controls\",\npch = 19, col = \"blue\")\npoints(log10(rowMeans(true_sample)+1)[log10(rowMeans(negative_control)+1)&gt;1] ~ \nlog10(rowMeans(negative_control)+1)[log10(rowMeans(negative_control)+1)&gt;1],\npch = 19,col = \"red\")\n\nIn the figure above, one point indicates an organism detected in a group of metagenomic samples. The points highlighted by red have high abundance in negative control samples, and therefore they are likely contamiannts.\nIn addition to PCR reagents and lab contaminants, reference databses can also be contaminanted by various, often microbial, organisms. A typical example that when screening environmental or sedimentary ancient DNA samples, a fish Cyprinos carpio can pop up if adapter trimming procedure was not successful for some reason.\n\nIt was noticed that the Cyprinos carpio reference genome available at NCBI contains large fraction of Illumina sequncing adapters. Therefore, appearence of this organism in your analysis may falsely lead your conclusion toward potential lake or river present in the excavation site.\nLet us now discuss a few available computational approaches to decontaminate metagenomic samples. One of them is decontam R package that offers a simple statistical test for whether a detected organism is likely contaminant. This approach is useful when DNA quantitation data recording the concentration of DNA in each sample (e.g. PicoGreen fluorescent intensity measures) is available. The idea of the decontam is that contaminant DNA is expected to be present in approximately equal and low concentrations across samples, while sample DNA concentrations can vary widely. As a result, the expected frequency of contaminant DNA varies inversely with total sample DNA concentration (red line in the figure below), while the expected frequency of non-contaminant DNA does not (blue line).\n\nAnother popular tool for detecting contaminating microorganisms is Recentrifuge. It works as a classifier that is trained to recognize contaminant microbial organisms. In case of Recentrifuge, one has to use blanks or other negative controls and provide microbial names and abundances on the blanks in order to train Recentrifuge to recognize endogenous vs. contaminant sources.\nIf one wants to assess the degree of contamination for each sample, there is a handy tool cuperdec, which is an R package that allows a quick comparison of microbial profiles in a query metagenomic sample against a database. The idea of cuperdec is to rank organisms in each sample by their abundance and then using an “expanding window” approach to compute their enrichment in a reference database that contains a comprehensive list of microbial organisms which are specific to a tissue / environment in question. The tool produces so-called Cumulative Percent Decay curves that aim to represent the level of endogenous content of microbiome samples, such as ancient dental calculus, to help to identify samples with low levels of preservation that should be discarded for downstream analysis.\nlibrary(\"cuperdec\")\nlibrary(\"magrittr\")\nlibrary(\"dplyr\")\n\n# Load database (in this case oral database)\ndata(cuperdec_database_ex)\ndatabase &lt;- load_database(cuperdec_database_ex, target = \"oral\") %&gt;% print()\n\n# Load abundance matrix and metadata\ntaxatable &lt;- load_taxa_table(\"abund_matrix.txt\")  %&gt;% print()\nmetadata &lt;- as_tibble(data.frame(Sample = unique(taxatable$Sample), \nSample_Source = \"Oral\"))\n\n# Compute cumulative percent decay curves, filter and plot results\ncurves &lt;- calculate_curve(taxatable, database = database) %&gt;% print()\nfilter_result &lt;- simple_filter(curves, percent_threshold = 50) %&gt;% print()\nplot_cuperdec(curves, metadata = my_metadata_map, filter_result)\n\nIn the figure above, one curve represents one sample, and the red curves have a very high amount of contamination and very low amount of endogenous DNA. These samples might be considered to be dropped from the downstream analysis."
  },
  {
    "objectID": "decontamination-authentication.html#source-tracking",
    "href": "decontamination-authentication.html#source-tracking",
    "title": "13  Decontamination and authentication analysis",
    "section": "13.6 Source tracking",
    "text": "13.6 Source tracking\nFor the case of ancient microbiome profiling, in addition to traditional inspection of the list of detected organisms and comparing it with the ones detected on blanks, it is useful to use pre-trained classifiers that can give a prediction on what environment the detected organisms most likely come from. The most popular and widely used tool is called SourceTracker. SourceTracker is a Bayesian version of the Gaussian Mixture Model (GMM) clustering algorithm that is trained on a reference data called Sources, i.e. different classes such as Soil or Human Oral or Human Gut microbial communities etc., and then it can estimate proportion / contribution of each Source into test samples called Sinks.\n\nOriginally, SourceTracker was developed for 16S data, i.e. using only 16S ribosomal RNA genes, but it can be easily trained using also shotgun metagenomics data, which was demonstrated in its metagenomic extension called mSourceTracker and its faster and more scalable version FEAST. The input data for SourceTracker are metadata, i.e. each sample has to have “source” or “sink” annotation as well as environmental label (e.g. Oral, Gut, Soil etc.), and microbial abundances (OTU abundances) quantified in some way, for example through QIIME pipeline, MetaPhlan or Kraken. The SourceTracker R script can be downloaded from https://github.com/danknights/sourcetracker, it expects two input data frames: metadata with at least sample name, environment and source / sink labels, and abundance matrix. Note that source and sink metadata and abundances have to be merged together prior to using SourceTracker. Next, training SourceTracker on source samples and running predictions on sink samples can be done using following command lines:\nlibrary(\"ggplot2\")\nsource('/sourcetracker/src/SourceTracker.r')\n\n# Read metadata and abundance (OTU) table\notus &lt;- read.table(\"otus.txt\", sep='\\t', header=T, row.names=1, check=F)\notus &lt;- as.data.frame(t(as.matrix(otus)))\notus[otus&gt;0]&lt;-1\notus &lt;- otus[rowSums(otus) != 0, ]\nmetadata &lt;- read.table('metadata.txt', sep='\\t', h=T, row.names=1, check=F)\nmetadata&lt;-metadata[as.character(metadata$Env)!=\"Vaginal\",]\nenvs &lt;- metadata$Env # environments can be e.g. Oral, Gut, Soil etc.\n\n# Extract common samples between metadata and abundance\ncommon.sample.ids &lt;- intersect(rownames(metadata), rownames(otus))\notus &lt;- otus[common.sample.ids,]\nmetadata &lt;- metadata[common.sample.ids,]\n\n# Split data into train (source) and test (sink) subsets\ntrain.ix &lt;- which(metadata$SourceSink == 'source')\ntest.ix &lt;- which(metadata$SourceSink == 'sink')\n\n# Train SourceTracker on sources (HMP) and run predictions on sinks\nst &lt;- sourcetracker(otus[train.ix,], envs[train.ix])\nresults &lt;- predict(st, otus[test.ix,], alpha1 = 0.001, alpha2 = 0.001)\n\n# Prepare output for plotting\nresults$proportions &lt;- results$proportions[order(-results$proportions[,\"Oral\"]),]\nname &lt;- rep(rownames(results$proportions), each = 4)\nvalue &lt;- as.numeric(t(results$proportions))\ncondition &lt;- rep(c(\"Gut\" , \"Oral\" , \"Skin\", \"Unknown\"), length(test.ix))\ndata &lt;- data.frame(name, condition, value)\n\n# Plot SourceTracker inference as a barplot\nggplot(data, aes(fill=condition, y=value, x=reorder(name,seq(1:length(name))))) + \ngeom_bar(position = \"fill\", stat = \"identity\") + \ntheme(axis.text.x = element_text(angle = 90, size = 5, hjust = 1, vjust = 0.5)) + \nxlab(\"Sample\") + ylab(\"Fraction\")\n\nIn the figure above the SourceTracker was trained on Human Microbiome Project (HMP) data, and was capable of predicting the fractions of oral, gut, skin or other microbial composition on the query sink samples. In a similar way, environmental soil or marine microbes can be used as Sources. In this way, environmental percentage of contamination can be detected per sample.\nA drawback of SourceTracker, mSourceTracker and FEAST is that they require a microbial abundance table after a taxonomic classification with e.g. QIIME or Kraken has been performed. Such taxonomic classification can be biased since it is computed against a reference database with known taxonomic annotation. In contrast, a novel microbial source tracjing tool decOM aims at moving away from database-dependent methods and using unsupervised approaches exploiting read-level sequence composition.\n\ndecOM uses kmtricks to compute a matrix of k-mer counts on raw reads (fastq-files) from source samples, and then uses the source k-mer abundance matrix for looking up k-mer composition of sink samples. This allows decOM to calculate microbial contributions / fractions from the sources. For example, for estimating contributions from ancient Oral (aOral), modern Oral (mOral), Skin and Sediment / Soil environments one can use an already computed source matrix from here https://github.com/CamilaDuitama/decOM/ and provide it as a -p_sources parameter. Then decOM can be run using the following command line:\ndecOM -p_sources decOM_sources/ -p_sinks FASTQ_NAMES_LIST.txt \\\n-p_keys decOM/FASTQ -mem 900GB -t 15\nIn the command line above, he -p_sinks parameter provides a list of sink samples, for example SRR13355807. The sink fastq-files are placed in decOM/FASTQ together with keys fof-files containing the mapping between fastq file names and locations of the fastq-files, for example SRR13355807 : decOM/FASTQ/SRR13355807.fastq.gz. The contributions from the sources to the sink samples, which are recorded in the decOM_output.csv output file, can then be processed and plotted as follows:\ndf&lt;-read.csv(\"decOM_output.csv\", check.names=FALSE)\n\nresult &lt;- subset(df, select = c(\"Sink\", \"Sediment/Soil\", \"Skin\", \n\"aOral\", \"mOral\", \"Unknown\"))\nrownames(result) &lt;- result$Sink\nresult$Sink &lt;- NULL\nresult &lt;- result / rowSums(result)\nresult&lt;-result[order(-result$aOral),]\nhead(result)\n\nname &lt;- rep(rownames(result), each = 5)\nvalue &lt;- as.numeric(t(result))\ncondition &lt;- rep(c(\"Sediment/Soil\",\"Skin\",\"aOral\",\"mOral\",\"Unknown\"), \ndim(result)[1])\ndata &lt;- data.frame(name, condition, value)\nhead(data)\n\n\nlibrary(\"ggplot2\")\nlibrary(\"viridis\")\nggplot(data, aes(fill=condition, y=value, x=reorder(name,seq(1:length(name))))) + \n  geom_bar(position = \"fill\", stat = \"identity\") + \n  theme(axis.text.x = element_text(angle=90, size = 5, hjust = 1, vjust = 0.5)) + \n  xlab(\"Sample\") + ylab(\"Fraction\")\n\ndecOM has certain advantages compared to SourceTracker as its is a taxonomic classification / database free approach. However, it also appears to be very sensitive to the particular training / source data set. In the example above it can be seen that the microbial source tracking of sink samples is very much dominated by the Oral community, which was the training / source data set."
  },
  {
    "objectID": "section-ancient-genomics.html#genome-mapping",
    "href": "section-ancient-genomics.html#genome-mapping",
    "title": "Ancient Genomics",
    "section": "Genome Mapping",
    "text": "Genome Mapping\nAn important step in the reconstruction of full genomic sequences is mapping. Even relatively short genomes usually cannot be sequenced as a single consecutive piece. Instead, millions of short sequence reads are generated from genomic fragments. These reads can be several hundred nucleotides in length but are considerably shorter for ancient DNA (aDNA).\nFor many applications involving comparative genomics these ‘reads’ have to be aligned to one or multiple already-reconstructed reference genomes in order to identify differences between the sequenced genome and any given contextual dataset. Aligning millions of short reads to much longer genome sequences in a time-efficient and accurate manner is a bioinformatics challenge for which numerous algorithms and tools have been developed. Each of these programs comes with a variety of parameters that can significantly alter the results and default settings are often not optimal when working with aDNA. Furthermore, read mapping procedures are often part of complex computational genomics pipelines and are therefore not directly applied by many users.\nIn this chapter we will take a look at specific challenges during read mapping when dealing with aDNA. We will get an overview of common input and output formats and manually apply a read mapper to aDNA data studying the direct effects of variation in mapping parameters. We will conclude the session with an outlook on genotyping, which is an important follow-up analysis step, that in turn is very relevant for down-stream analyses such as phylogenetics."
  },
  {
    "objectID": "section-ancient-genomics.html#phylogenomics",
    "href": "section-ancient-genomics.html#phylogenomics",
    "title": "Ancient Genomics",
    "section": "Phylogenomics",
    "text": "Phylogenomics\nPhylogenetic trees are central tools for studying the evolution of microorganisms, as they provide essential information about their relationships and timing of divergence between microbial strains.\nIn this chapter, we will introduce basic phylogenetic concepts and definitions, and provide guidance on how to interpret phylogenetic trees. We will then learn how to reconstruct phylogenetic trees from DNA sequences using various methods ranging from distance-based methods to probabilistic approaches, including maximum likelihood and Bayesian phylogenetics. In particular, we will learn how to use ancient genomic data to reconstruct time-calibrated trees with BEAST2."
  },
  {
    "objectID": "genome-mapping.html#lecture",
    "href": "genome-mapping.html#lecture",
    "title": "14  Genome Mapping",
    "section": "14.1 Lecture",
    "text": "14.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "genome-mapping.html#mapping-to-a-reference-genome",
    "href": "genome-mapping.html#mapping-to-a-reference-genome",
    "title": "14  Genome Mapping",
    "section": "14.2 Mapping to a Reference Genome",
    "text": "14.2 Mapping to a Reference Genome\nOne way of reconstructing genomic information from DNA sequencing reads is mapping/aligning them to a reference genome. This allows for identification of differences between the genome from your sample and the reference genome. This information can be used for example for comparative analyses such as in phylogenetics. For a detailed explanation of the read alignment problem and an overview of concepts for solving it, please see https://doi.org/10.1146/annurev-genom-090413-025358.\nIn this session we will map two samples to the Yersinia pestis (plague) genome using different parameter sets. We will do this “manually” in the sense that we will use all necessary commands one by one in the terminal. These commands usually run in the back when you apply DNA sequencing data processing pipelines.\n\n14.2.1 Preparation\nThe data and conda environment .yaml file for this practical session can be downloaded from here: https://doi.org/10.5281/zenodo.6983174. See instructions on page.\nWe will open a terminal and then navigate to the working directory of this session:\ncd /&lt;path&gt;/&lt;to&gt;/4b-genome-mapping/\nThen, we need to activate the conda environment of this session. By this all the necessary tools can be accessed in the current terminal session:\nconda activate microbial-genomics\nWe will be using the Burrows-Wheeler Aligner (Li et al. 2009 – http://bio-bwa.sourceforge.net). There are different algorithms implemented for different types of data (e.g. different read lengths). Here, we use BWA backtrack (bwa aln), which is well suitable for Illumina sequences up to 100bp. Other algorithms are bwa mem and bwa sw for longer reads.\n\n\n14.2.2 Reference Genome\nFor mapping we need a reference genome in FASTA format. Ideally we use a genome from the same species that our data relates to or, if not available, a closely related species. The selection of the correct reference genome is highly relevant. E.g. if the chosen genome differs too much from the organism the data relates to, it might not be possible to map most of the reads. Reference genomes can be retrieved from comprehensive databases such as NCBI.\nIn your directory, you can find 2 samples and your reference. As a first step we will index our reference genome (make sure you are inside your directory).\nThe first index we will generate is for bwa.\nbwa index YpestisCO92.fa\nThe second index will be used by the genome browser we will apply to our results later on:\nsamtools faidx YpestisCO92.fa\nWe need to build a third index that is necessary for the genotyping step, which comes later after mapping:\npicard CreateSequenceDictionary R=YpestisCO92.fa\n\n\n14.2.3 Mapping Parameters\nWe will be using bwa aln, but we need to specify parameters. For now we will concentrate on the “seed length” and the “maximum edit distance”. We will use the default setting for all other parameters during this session. The choice of the right parameters depend on many factors such as the type of data and the specific use case. One aspect is the mapping sensitivity, i.e. how different a read can be from the chosen reference and still be mapped. In this context we generally differentiate between strict and lenient mapping parameters.\nAs many other mapping algorithms bwa uses a so-called “seed-and-extend” approach. I.e. it initially maps the first N nucleotides of each read to the genome with relatively few mismatches and thereby determines candidate positions for the more time-intensive full alignment.\nA short seed length will generate more such candidate positions and therefore mapping will take longer, but it will also be more sensitive, i.e. there can be more differences between the read and the genome. Long seeds are less sensitive but the mapping procedure is faster.\nIn this session we will use the following two parameter sets:\nLenient\nAllow for more mismatches → -n 0.01\nShort seed length → -l 16\nStrict\nAllow for less mismatches → -n 0.1\nLong seed length → -l 32\nWe will be working with pre-processed files (sample1.fastq.gz, sample2.fastq.gz), i.e. any quality filtering and removal of sequencing adapters is already done.\nWe will map each file once with lenient and once with strict parameters. For this, we will make 4 separate directories, to avoid mixing up files:\nmkdir sample1_lenient sample2_lenient sample1_strict sample2_strict\n\n\n14.2.4 Mapping Sample1\nLet’s begin with a lenient mapping of sample1.\nGo into the corresponding folder:\ncd sample1_lenient\nPerform the bwa alignment, here for sample1, and specify lenient mapping parameters:\nbwa aln -n 0.01 -l 16 ../YpestisCO92.fa ../sample1.fastq.gz &gt; reads_file.sai\nProceed with writing the mapping in sam format (https://en.wikipedia.org/wiki/SAM_(file_format)):\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample1.fastq.gz &gt; reads_mapped.sam\nNote that we have specified the sequencing platform (Illumina) by creating a so-called “Read Group” (-r). This information is used later during the genotyping step.\nConvert SAM file to binary format (BAM file):\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\nFor processing of sam and bam files we use SAMtools (Li et al. 2009 – http://samtools.sourceforge.net/).\n-b specifies to output in BAM format. (-S specifies input is SAM, can be omitted in recent versions.)\nNow we sort the bam file → Sort alignments by leftmost coordinates:\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\nThe sorted bam file needs to be indexed → more efficient for further processing:\nsamtools index reads_mapped_sorted.bam\nDeduplication → Removal of reads from duplicated fragments:\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\nsamtools index reads_mapped_sorted_dedup.bam\nDuplicated reads are usually a consequence of amplification of the DNA fragments in the lab. Therefore, they are not biologically meaningful.\nWe have now completed the mapping procedure. Let’s have a look at our mapping results:\nsamtools view reads_mapped_sorted_dedup.bam | less -S\n(exit by pressing q)\nWe can also get a summary about the number of mapped reads. For this we use the samtools idxstats command (http://www.htslib.org/doc/samtools-idxstats.html):\nsamtools idxstats reads_mapped_sorted_dedup.bam\n\n\n14.2.5 Genotyping\nThe next step we need to perform is genotyping, i.e. the identification of all SNPs that differentiate the sample from the reference. For this we use the Genome Analysis Toolkit (GATK) (DePristo et al. 2011 – http://www.broadinstitute.org/gatk/)\nIt uses the reference genome and the mapping as input and produces an output in Variant Call Format (VCF) (https://en.wikipedia.org/wiki/Variant_Call_Format).\nPerform genotyping on the mapping file:\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\nLet’s have a look…\ncat mysnps.vcf | less -S\n(exit by pressing q)\n\n\n14.2.6 Mapping and Genotyping for the other Samples/Parameters\nLet’s now continue with mapping and genotyping for the other samples and parameter settings.\n\n14.2.6.1 Sample2 Lenient\ncd ..\ncd sample2_lenient\n\nbwa aln -n 0.01 -l 16 ../YpestisCO92.fa ../sample2.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample2.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\n\n\n14.2.6.2 Sample1 Strict\ncd ..\ncd sample1_strict\n\nbwa aln -n 0.1 -l 32 ../YpestisCO92.fa ../sample1.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample1.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\n\n\n14.2.6.3 Sample2 Strict\ncd ..\ncd sample2_strict\n\nbwa aln -n 0.1 -l 32 ../YpestisCO92.fa ../sample2.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample2.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\n\n\n\n14.2.7 Comparing Genotypes\nIn order to combine the results from multiple samples and parameter settings we need to agregate and comparatively analyse the information from all the vcf files. For this we will use the software MultiVCFAnalyzer (https://github.com/alexherbig/MultiVCFAnalyzer).\nIt produces various output files and summary statistics and can integrate gene annotations for SNP effect analysis as done by the program SnpEff (Cingolani et al. 2012 - http://snpeff.sourceforge.net/).\nRun MultiVCFAnalyzer on all 4 files at once. First cd one level up (if you type ls you should see your 4 directories, reference, etc.):\ncd ..\nThen make a new directory…\nmkdir vcf_out\n…and run the programme:\nmultivcfanalyzer NA YpestisCO92.fa NA vcf_out F 30 3 0.9 0.9 NA sample1_lenient/mysnps.vcf sample1_strict/mysnps.vcf sample2_lenient/mysnps.vcf sample2_strict/mysnps.vcf\nLet’s have a look in the ‘vcf_out’ directory (cd into it):\ncd vcf_out\nCheck the parameters we set earlier:\nless -S info.txt\n(exit by pressing q)\nCheck results:\nless -S snpStatistics.tsv\n(exit by pressing q)\nThe file content should look like this:\nSNP statistics for 4 samples.\nQuality Threshold: 30.0\nCoverage Threshold: 3\nMinimum SNP allele frequency: 0.9\nsample  SNP Calls (all) SNP Calls (het) coverage(fold)  coverage(percent)\nrefCall allPos  noCall  discardedRefCall    discardedVarCall    filteredVarCall unhandledGenotype\nsample1_lenient 213 0   16.38   92.69\n4313387 4653728 293297  46103   728 0   0\nsample1_strict  207 0   16.33   92.71\n4314060 4653728 293403  45633   425 0   0\nsample2_lenient 1274    0   9.01    83.69\n3893600 4653728 453550  297471  7829    0   4\nsample2_strict  1218    0   8.94    83.76\n3896970 4653728 455450  295275  4815    0   0\nFirst we find the most important parameter settings and then the table of results. The first column contains the dataset name and the second column the number of called SNPs. The genome coverage and the fraction of the genome covered with the used threshold can be found in columns 4 and 5, respectively. For example, sample1 had 207 SNP calls with strict parameters. The coverage is about 16-fold and about 93% of the genome are covered 3 fold or higher (The coverage threshold we set was 3).\n\n\n14.2.8 Exploring the Results\nFor visual exploration of mapping results so-called “Genome Browsers” are used. Here we will use the Integrative Genomics Viewer (IGV) (https://software.broadinstitute.org/software/igv/).\nTo open IGV, simply type the following command and the app will open:\nigv\nNote that you cannot use the terminal while IGV is open. If you want to use it anyways, open a second terminal via the bar on the bottom.\nLoad your reference (YpestisCO92.fa):\n→ Genomes → Load Genome from File\n\nLoad your bam files (do this 4 times, once for each mapping):\n→ File → Load from File\n\nTry to explore the mapping results yourself. Here are some questions to guide you. Please also have a look at the examples below.\nWhat differences do you observe between the samples and parameters?\nDifferences in number of mapped reads, coverage, number of SNPs\nDo you see any global patterns?\nWhich sample is more affected by changing the parameters?\nWhich of the two samples might be ancient, which is modern?\nLet’s examine some SNPs. Have a look at snpTable.tsv.\nCan you identify SNPs that were called with lenient but not with strict parameters or vice versa?\nLet’s check out some of these in IGV. Do you observe certain patterns in these genomic regions?\n\n\n14.2.9 Examples\nPlease find here a few examples for exploration. To get a better visualization we have loaded here only sample2_lenient (top track) and sample2_strict (bottom track):\n\nYou can see all aligned reads in the current genomic region as stacks of grey arrows. In the middle of the image you see brown dashes in all of the reads. This is a SNP. You also see sporadically green or red dashes in some reads but not all of them at a given position. These sporadic differences are DNA damage such as we typically find it for ancient DNA.\nFor jumping to a specific coordinate you need to enter it into the coordinate field at the top:\n\nE.g. if you enter 12326942 after the colon in the coordinate field and hit enter, you will jump to the same position as in the screenshot above.\nLet’s have a look at some positions.\nFor example position 36472:\n\nIn the middle of the image you see a SNP (T) that was called with strict parameters (bottom) but not with lenient parameters (top). But why would it not be called in the top track? It is not called because there are three reads that cover the same position, but do not contain the T. We can see that these reads have other difference to the reference at other positions. That’s why they are not mapped with strict parameters. It is quite likely that they originate from a different species. This example demonstrates that sensitive mapping parameters might actually lead to a loss of certain SNP calls.\nDoes this mean that stricter parameters will always give us a clean mapping? Let’s have a look at position 219200:\n\nYou might need to zoom out a bit using the slider in the upper right corner.\nSo, what is going on here? We see a lot of variation in most of the reads. This is reduced a bit with strict mapping parameters (bottom track) but the effect is still quite pronounced. Here, we see a region that seems to be conserved in other species as well, so we have a lot of mapping from other organisms. We can’t compensate that with stricter mapping parameters and we would have to apply some filtering on genotype level to remove this variation from our genotyping. Removing false positive SNP calls is important as it would interfere with downstream analysis such as phylogenomics.\nSuch regions can be fairly large. For example, see this 20 kb region around position 224750:\n\n\n\n14.2.10 Conclusions\n\nMapping DNA sequencing reads to a reference genome is a complex procedure that requires multiple steps.\nMapping results are the basis for genotyping, i.e. the detection of differences to the reference.\nThe genotyping results can be aggregated from multiple samples and comparatively analysed e.g. in the context of phylogenomics.\nThe chosen mapping parameters can have a strong influence on the results of any downstream analysis.\nThis is particularly true when dealing with ancient DNA samples as they tend to contain DNA from multiple organisms. This can lead to mismapped reads and therefore incorrect genotypes, which can further influence downstream analyses."
  },
  {
    "objectID": "phylogenomics.html#lecture",
    "href": "phylogenomics.html#lecture",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.1 Lecture",
    "text": "15.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "phylogenomics.html#preparation",
    "href": "phylogenomics.html#preparation",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.2 Preparation",
    "text": "15.2 Preparation\nThe data and conda environment .yaml file for this practical session can be downloaded from here: https://doi.org/10.5281/zenodo.6983184. See instructions on page.\nChange into the session directory\ncd /&lt;path&gt;/&lt;to&gt;/5b-phylogenomics/\nThe data in this folder should contain an alignment (snpAlignment_session5.fasta) and a txt file with the ages of the samples that we are going to be working with in this session (samples.ages.txt)\nLoad the conda environment.\nconda activate phylogenomics-functional"
  },
  {
    "objectID": "phylogenomics.html#basic-concepts-in-phylogenomics",
    "href": "phylogenomics.html#basic-concepts-in-phylogenomics",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.3 Basic concepts in phylogenomics",
    "text": "15.3 Basic concepts in phylogenomics\nBefore we start building trees, we are going to recap a few basic concepts in phylogenomics.\n\n15.3.1 What is a phylogenetic tree\nThe Oxford Dictionary defines phylogenetics as “the branch of biology that deals with phylogeny, especially with the deduction of the historical relationships between groups of organisms”.\nWhen we are building/computing a phylogeny we are inferring the relationship between organisms based on a set of homologous characters, which are characters that have been inherited from a common ancestor. Those characters can be morphological, which is common in paleontological studies, or molecular characters, which in our case they will be DNA sequences.\nTo display the relationships between the organism we use phylogenetic trees, which are composed of different elements.\n\n\n15.3.2 Parts of a phylogenetic tree: Nodes and Branches\nA phylogenetic tree has different parts:\n\nThe tips, which can also be called leaves, represent the sampled “individuals”. Each of the sequences that you used to reconstruct a tree will be displayed in your tree as tips. In the example, we used sequences A,B,C,D and E, and those appear at the tips of the tree.\nA node in the tree represents an ancestor (or ancestral sequences) shared by one or more tips. In the tree we see different nodes displayed, the most rightwards ones represent the ancestors between A and B; and C and D, and if we move toward the left we will see a node representing the ancestor of A,B,C and D, and the most leftwards node represent the common ancestor of all the sequences in our tree.\nBranches are the part of the tree that connects each node to other nodes or to the leaves. These represent evolutionary paths between nodes/leaves. The length of these branches is called branch length and it can represent different measures depending on the algorithm that you use to infer the phylogenetic tree, such as the number of changes (in the case of for example Maximum Parsimony tree, see Character-based phylogenetic methods), genetic/evolutionary distance (for Neighbour-Joining, see distance-based phylogenetic methods, or for Maximum Likelihood method, see Character-based phylogenetic methods), or the time between two taxa or nodes (for example, trees inferred by BEAST, see Bayesian phylogenetic inference using BEAST2).\n\n\n\n\n15.3.3 Types of trees: Ultrametric vs. non-ultrametric\nWe can have different types of trees depending on how the distance to the root (represents the ancestor shared by all the taxa in the tree):\n\nIn ultrametric trees the distance from the root to any of the tips in the tree is the same. One typical example of ultrametric tree will be a tree where the branch length represents time and all of our tips were samples in the present.\nIn non-ultrametric trees the distance from the root to the tips differ from tip to tip. We will find this type of tree when we use algorithms where the branch lengths are calculated based on genetic distance/number of changes."
  },
  {
    "objectID": "phylogenomics.html#the-start-dna-sequence-alignment",
    "href": "phylogenomics.html#the-start-dna-sequence-alignment",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.4 The start: DNA sequence alignment",
    "text": "15.4 The start: DNA sequence alignment\n\n15.4.1 DNA sequence aligment\nFor this course, our start would be a DNA sequence alignment, however phylogenies can be built using any other source of information that contain homologous characters, such as phenotypic characters inherited from the same ancestor, protein sequences, etc.\nWe start by collecting the DNA sequences of our organisms/individuals of interest, and now we need to make sure that the positions are homologous, which means that they were inherited from the same ancestor, to ensure that they share the same evolutionary history. A way to ensure that our positions are homologous is by aligning our DNA sequences.\nThere are different ways to produce an alignment from our sequences, and this will depend on the type of data that you have:\n\nMultiple Sequence Alignment (MSA) can be computed from complete genomes. This can be achieved with MAFFT or Clustal Omega. However, this will be computationally expensive to perform on large genomes.\nReference based alignment: this consists of aligning your reads to a reference genome, and it was covered during the Practical 4B: Genome mapping.\n\n\n\n\n\n\n\nNote\n\n\n\nFor large genomic datasets, we often use Single Nucleotide Polymorphims (SNPs) alignments, i.e. alignments that contain only variable genomic positions. This is to reduce the computational time since the variable positions are the most informative for reconstructing phylogenetic trees.\n\n\n\n\n15.4.2 DNA sequence alignment\nIn this practical session, we will be working with an alignment produced as you learned in the Practical 4B:Genome mapping.\nWe are going to start by exploring the alignment in MEGA. So open the MEGA desktop application and load the alignment by clicking on File -&gt; Open A File/Session -&gt; Select the snpAligment_session5.fasta.\n\nIt will you ask what you want to do with the alignment. In MEGA you can also produce an alignment, however, since our sequences are already aligned we will press on Analyze.\nThen we will select Nucleotide Sequences since we are working with a DNA alignment. Note that MEGA can also work with Protein Sequences as well as Pairwise Distance Matrix (which we will cover shortly). In the same window, we will change the character for Missing Data to N and click in OK.\n\nA window would open up asking if our alignment contains protein encoding sequences, and we will select No.\n\n\n\n\n\n\nTip\n\n\n\nIf you had protein encoding sequences, you would have selected Yes. This will allow you to treat different positions with different evolutionary modes depending on their codon position. One can do this to take in account that the third codon position can change to different nucleotides without resulting in a different amino acid, while position one and two of the codon are more restricted.\n\n\nTo explore the alignment, you will then click on the box with TA\n\nYou will see an alignment containing sequences from the bacterial pathogen Yersinia pestis. Within the alignment, we have our sequences of interest (VLI092, CHC004, KZL002) that date between 5000-2000 years Before Present (BP), and we want to know how they relate to the rest of the Yersinia pestis genomes in the alignment.\n\nWhat do you think the dots represent?\n\nThey represent positions that are same as the reference\n\nWhat are the Ns in the sequences?\n\nThey represent positions where we have missing data. We told MEGA to encode missing positions as N\n\nHow many sequences are we analysing?\n\nWe are analysing 33 sequences."
  },
  {
    "objectID": "phylogenomics.html#distance-based-phylogenetic-methods",
    "href": "phylogenomics.html#distance-based-phylogenetic-methods",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.5 Distance-based phylogenetic methods",
    "text": "15.5 Distance-based phylogenetic methods\nDistance-based methods calculate the tree from a pairwise distance matrix. The distances in the pairwise distance matrix can represent either:\n\nNumber of differences between to sequences\np-distance that is calculated as number of differences / total number of sites\n\nFor any of the phylogenetic methods, we can also take into account that different sites may evolve differently depending on their nucleotide composition. To take this into account, you will need to use substitution models. By applying different evolutionary models, one can take into account multiple consecutive mutations as well as different probabilities to observe a specific mutation given the specific character in this position. There are tools that allow you to choose the substitution model that it is more fitting to you specific alignment, such as jModelTest, ModelGenerator, etc.\n\n\n15.5.1 Calculate a pairwise distance matrix\nTo calculate a pairwise distance matrix, one will have to check the number of differences between all the possible pair combination between the sequences in the alignment, and in the case of a p-distance normalise the number of differences by the total number of sites. All this differences will be then stored in a matrix.\nMEGA also provides a function to calculate Pairwise Distances, for that you will have to click into the Distance symbol, and select Compute Pairwise Distances, the output will be a matrix with all the pairwise distances. This will be the first step you will do if you wanted to compute a distance-based phylogeny with methods such as UPGMA or Neighbour-Joining. We will be covering the second method.\n\n\n\n15.5.2 Calculating a Neighbour-Joining tree\nIn the slide you have an example on how a small Neighbour-Joining (NJ), with 6 taxa is calculated.\nIn essence, this method will be grouping taxa that have the shortest distance together first, and will be doing this iteratively until all the taxa/sequences included in your alignment have been placed in a tree. Luckily, you won’t have to do this by hand since MEGA allows you to build a NJ tree.\n\n\n15.5.3 Let’s make our own NJ tree\nFor that go back to MEGA and click on the Phylogeny symbol and then select Construct Neighbour Joining Tree. In the window that would pop up, you will then chance the Model/Method to p-distance since we want to normalise by the total number of sites shared by sequences. Then press OK and a window with the calculated phylogenetic tree will pop up.\n\nSince the tree is not easily visualised in MEGA, we will export it in newick format (an “standarised” format to write a tree in a computer-readable form) and explore our tree in FigTree. This tool has a better interface for visually manipulating trees and allows us to interact with the phylogenetic tree.\n\nTo do that you will click on File, then Export current tree (Newick) and click on Branch Lengths to include those in the newick annotation. When you press OK, a new window with the tree in newick format will pop up and you will then press File -&gt; Save and saved it as NJ_tree.nwk (If you are doing this for your own project, please give your files informative names).\nAs said above, we will explore own NJ tree in FigTree. Open the software and then open the NJ tree by clicking on File -&gt; Open and selecting the file with the NJ tree NJ_tree.nwk\n\nSo, which type of tree is this? Before you answer this question, let me introduce you to another type of trees\n\n\n15.5.4 Type of trees: Rooted vs Unrooted trees\n\nAn unrooted tree does not contain a root. It displays the relationships between our sequences but not the direction in time. In the example, we can not tell if human or chimpanzee or gorilla are more ancestral to each other.\nA rooted tree does contain a root. This root can be calculated based on the inclusion of an outgroup, a known sequenced to be older than any of the taxa in our tree, or by computational methods that can place the root by various methods, being one of such methods the Mid Point Rooting. A rooted tree represents the relationships and the direction in time. By rooting our example tree with the outgroup, we now can tell that gorilla is ancestral to both human and chimpanzees.\n\n ##### Let’s make our own NJ tree\nSo if we go back to our question, which type of tree is this?\n\nEven though a root is displayed by default, this is actually an unrooted tree. We know that Yersinia pseudotuberculosis (labelled here as Y. pseudotuberculosis) is an outgroup to Yersinia pestis. You can reroot the tree by selecting Y.pseudotuberculosis and pressing Reroot.\n\nNow we have the correct tree.\nHow many leaves/tips has our tree?\n\nThat’s right, it is 33, the same number of sequences that was in our original SNP alignment.\n\nIs it an ultrametric tree?\n\nNo, we can see that root to tip distance is different between taxa. The branch lengths in the NJ tree will represent genetic distance between the different taxa.\n\nWhere are our taxa of interest? (KZL002, GZL002, CHC004 and VLI092)\n\nThey all fall ancestral to the rest of Yersinia pestis in this tree.\n\nDo they form a clade?\n\nAll the genomes have a common ancestor and form a single branch, meaning that they form a clade.\n\nWhich type of clade?\n\nTo answer this question, let’s look at what types of clades we can have in a phylogenetic tree\n\n\n\n15.5.5 Types of Clades\n\nA Monophyletic clade is a group of taxa that contain all the taxa that share a common ancestor.\nA Paraphyletic clade is a group of taxa including all the taxa with a common recent ancestor except one or more taxa. In the example, since A is missing from the clade selected, it won’t be anymore a paraphyletic clade rather than a monophyletic. Another common example of paraphyletic clade would be selecting all the reptiles but excluding birds.\nA Polyphyletic clade is a group of taxa from different monophyletic clades.\n\n\n\n\n15.5.6 Lets make our own NJ tree\nNow regarding our question, which type of clade form KZL002, GZL002, CHC004 and VLI092?\nSince they all share a common ancestor,they form a monophyletic clade.\nUntil now we have learned how to explore a SNP alignment, the different part of the a phylogenetic tree and how to make a NJ tree. The NJ tree is based on pairwise distances, which is one of the simplest algorithms to build a tree and now you will see more complex algorithms for phylogenetic tree building."
  },
  {
    "objectID": "phylogenomics.html#character-based-phylogenetic-methods-maximum-parsimony-and-probabilistic-approaches",
    "href": "phylogenomics.html#character-based-phylogenetic-methods-maximum-parsimony-and-probabilistic-approaches",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.6 Character-based phylogenetic methods: maximum parsimony and probabilistic approaches",
    "text": "15.6 Character-based phylogenetic methods: maximum parsimony and probabilistic approaches\nCharacter-based methods are not based on pairwise distances but rather model the complete evolution of each character (e.g. DNA nucleotides at each position) along the phylogenetic tree.\nOne of these methods is maximum parsimony and it consists in choosing the tree that underlies an evolutionary history with the least number of character changes.\n\n\nOther types of character-based methods which are more commonly used today are probabilistic methods. In general, these are statistical techniques that are based on probabilistic models under which the data that we observe is generated following a probability distribution depending on a set of parameters which we want to estimate.\nIn a phylogenetic probabilistic model, the data is the sequence alignment and the parameters, are the substitution matrix and the phylogenetic tree. The probability of the data given the model parameters is called the likelihood.\n\n\n\n15.6.1 Maximum likelihood estimation and bootstrapping\nOne way we can make inferences from a probabilistic model is by finding the combination of parameters which maximises the likelihood. These parameter values are called maximum likelihood (ML) estimates. We are usually not able to compute the likelihood value for all possible combinations of parameters and have to rely on heuristic algorithms to find the maximum likelihood estimates.\n\nThe Maximum likelihood estimates are point estimates, i.e. single parameter values (for example, a tree), which does not allow to measure uncertainty. A classic method to measure the uncertainty of ML tree estimates is bootstrapping, which consists in repeatedly disturbing the alignment by masking sites from it and estimating a tree from each of these bootstrap alignments.\n\nFor each clade in the ML tree, a bootstrap support value is computed which corresponds to the proportion of bootstrap trees containing the clade. This gives an indication of how robustly the clade is supported by the data (i.e. whether it holds even after disturbing the dataset). Bootstrapping can be used to measure the topology uncertainty of trees estimated with any inference method.\nHere is a command to estimate an ML phylogenetic tree using RAxML (you may find the list of parameters in the RAxML manual):\nraxmlHPC-PTHREADS -m GTRGAMMA -T 3 -f a -x 12345 -p 12345 -N autoMRE -s snpAlignment_session5.fasta -n full_dataset.tre\nOnce the analysis has been completed, you can open the tree using Figtree (RAxML_bipartitions… file, change “label” to “bootstrap support” at the prompt).\nThe tree estimated using this model is a substitution tree (branch lengths represent genetic distances in subst./site), and it is not oriented in time: this is an unrooted tree (displayed with a random root in Figtree). You can reroot the tree in Figtree using Y. pseudotuberculosis as an outgroup (click on the branch leading to Y. pseudo and then “Reroot”).\nWhere do our genomes of interest from the LNBA period fall (VLI092, GZL002, CHC004, KZL002), compared to the rest of Yersinia pestis diversity? Is that placement well-supported? (look at the bootstrap support value: click on the “Node Labels” box and open the drop-down menu, change “Node ages” to “bootstrap support”)\nYou can notice that the phylogeny is difficult to visualize due to the long branch leading to Y. pseudotuberculosis. Having a very distant outgroup can also have deleterious effects on the estimated phylogeny (due to long branch attraction). We can construct a new phylogeny after removing the outgroup (go back to the alignment in mega, unclick Y.pseudotuberculosis, and export in fasta format), and then reroot the tree based on our previous knowledge: place the root on the branch leading to the LNBA genomes.\nThen, export the rooted tree: file &gt; export trees &gt; “save as currently displayed”.\n\n\n15.6.2 Estimating a time-tree using Bayesian phylogenetics\nNow, we will try to use reconstruct a phylogeny in which the branch lengths do not represent a number of mutations but instead represent the time of evolution. To do so, we will use the sample ages (C14 dates) to calibrate the tree in time. This assumes a molecular clock hypothesis in which substitutions occur at a rate that is relatively constant in time so that the time of evolution can be estimated based on the number of substitutions.\n\n15.6.2.1 Temporal signal testing\nIt is a good practice to assess if the genetic sequences that we analyse do indeed behave like molecular clocks before trying to estimate a time tree. A classic test to do this is called root-to-tip regression, which consists in verifying that the oldest a sequence is, the closer it should be to the root because there was less time for mutations to accumulate before this sequence was sampled. The correlation between sample age and distance to the root (root-to-tip regression) can be assessed using a rooted substitution tree and the program tempest:\n\nopen tempest and load the re-rooted ML tree that we produced previously\nclick on “import dates” in the “sample dates” tab, select the sample_age.txt file, and then change to “dates specified as years before the present”\nlook at the root-to-tip regression: is there a positive correlation?\n\n\n\n\n15.6.2.2 Bayesian phylogenetic inference using BEAST2\nWe will estimate a time tree from our alignment using Bayesian inference instead of maximum likelihood.\nBayesian inference is a type of inference which is based on a probability distribution that is different from the likelihood: the posterior probability. The posterior probability is the probability of the parameters given the data. The posterior distribution is easier to interpret than the likelihood because it contains all the information about the parameters: point estimates such as the median or the mean can be directly estimated from it, but also percentile intervals which can be used to measure uncertainty.\n\nThe Bayes theorem tells us that is proportional to the product of the likelihood and the “prior” probability of the data:\n\nTherefore, for Bayesian inference, we need to complement our probabilistic model with prior distributions for all the parameters. Because we want to estimate a time tree, we also add another parameter: the molecular clock (average substitution rate in time units).\n\nTo characterize the full posterior distribution of each parameter, we would need in theory to compute the posterior probability for each possible combination of parameters. This is impossible, and we will instead use an algorithm called Markov chain Monte Carlo (MCMC) to approximate the posterior distribution. The MCMC is an algorithm which iteratively samples values of the parameters from the posterior distribution. Therefore, if the MCMC has run long enough, the (marginal) posterior distribution of the parameters can be approximated by a histogram of the sampled values.\n\nThe different components of the BEAST2 analysis can be set up in the program BEAUti:\n\n\nload the alignment in the “Partitions” tab\nset the sampling dates in the “Tip dates” tab\nchoose the substitution model in the “Site model” tab\nchoose the molecular clock model in the “Clock model” tab\nchoose the prior distribution of parameters in the “Priors” tab\nset up the MCMC in the “MCMC” tab\n\n    \nThe “taming the beast” website has great tutorials to learn setting a BEAST2 analysis. In particular, the “Introduction to BEAST2”, “Prior selection” and “Time-stamped data” are good starts.\nTry running an analysis on the alignment without outgroup using the following:\n\ndon’t forget to specify the sample ages correctly\nuse a GTR substitution matrix with a Gamma site model and 4 Gamma categories\nuse a relaxed clock lognormal model with an initial value of 1E-4\nuse a Bayesian Skyline Coalescent tree prior\nchange the mean clock prior to a uniform distribution between 1E-6 and 1E-3 subst/site/year\nuse 300M iterations for the MCMC chain, and log the parameters and trees each 10,000th iteration\n\nOnce the analysis is completed, assess the posterior distribution sampling and parameter estimates by loading the obtained log file into Tracer\nFirst, look at the trace of the posterior to check if the MCMC has passed the burn-in phase, and if you have remove all burn-in iterations\n\nIf so you can look at the trace and effective sample size (ESS) value of all parameters, to check that the MCMC has run long enough. The traces should look (more or less) like a “hairy caterpillar”, and a rule of thumb is that all ESS values should be above 200. If this is not the case, you should run the MCMC longer (BEAST2 has a -resume option that you can use to extend the MCMC sampling without starting everything from the beginning).\n\nYou can then look at the estimates of your parameter in the top-right panel (mean, median, 95% HPD interval, …). Note that these are marginal estimates, i.e. integrated over all other parameters.\n\nWhat is your estimate of the substitution (mean clock) rate?\nYou can then generate a maximum clade credibility (MCC) tree using treeAnnotator.\n\nWhat is your mean estimate for the age of the common ancestor of all Yersinia pestis strains? To which parameter (displayed in beauti) does this corresponds?"
  },
  {
    "objectID": "section-ancient-metagenomic-resources.html#accessing-ancient-metagenome-data",
    "href": "section-ancient-metagenomic-resources.html#accessing-ancient-metagenome-data",
    "title": "Ancient Metagenomic Resources",
    "section": "Accessing Ancient Metagenome Data",
    "text": "Accessing Ancient Metagenome Data\nFinding relevant comparative data for your ancient metagenomic analysis is not trivial. While palaeogenomicists are very good at uploading their raw sequencing data to large sequencing data repositories such as the EBI’s ENA or NCBI’s SRA archives in standardised file formats, these files often have limited metadata. This often makes it difficult for researchers to search for and download relevent published data they wish to use use to augment their own analysis.\nAncientMetagenomeDir is a community project from the SPAAM community to make ancient metagenomic data more accessible. We curate a list of standardised metadata of all published ancient metagenomic samples and libraries, hosted on GitHub. In this chapter we will go through how to use the AncientMetagenomeDir repository and associated tools to find and download data for your own analyses. We will also discuss important things to consider when publishing your own data to make it more accessible for other researchers."
  },
  {
    "objectID": "section-ancient-metagenomic-resources.html#ancient-metagenomic-pipelines",
    "href": "section-ancient-metagenomic-resources.html#ancient-metagenomic-pipelines",
    "title": "Ancient Metagenomic Resources",
    "section": "Ancient Metagenomic Pipelines",
    "text": "Ancient Metagenomic Pipelines\nAnalyses in the field of ancient DNA are growing, both in terms of the number of samples processed and in the diversity of our research questions and analytical methods. Computational pipelines are a solution to the challenges of big data, helping researchers to perform analyses efficiently and in a reproducible fashion. Today we will introduce nf-core/eager, one of several pipelines designed specifically for the preprocessing, analysis, and authentication of ancient next-generation sequencing data.\nIn this chapter we will learn how to practically perform basic analyses with nf-core/eager, starting from raw data and performing preprocessing, alignment, and genotyping of several Yersinia pestis-positive samples. We will gain an appreciation of the diversity of analyses that can be performed within nf-core eager, as well as where to find additional information for customizing your own nf-core/eager runs. Finally, we will learn how to use nf-core/eager to evaluate the quality and authenticity of our ancient samples. After this session, you will be ready to strike out into the world of nf-core/eager and build your own analyses from scratch!"
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#lecture",
    "href": "accessing-ancientmetagenomic-data.html#lecture",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.1 Lecture",
    "text": "16.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#introduction",
    "href": "accessing-ancientmetagenomic-data.html#introduction",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.2 Introduction",
    "text": "16.2 Introduction\nIn most bioinformatic projects, we need to include publicly available comparative data to expand or compare our newly generated data with.\nIncluding public data can benefit ancient metagenomic studies in a variety of ways. It can help increase our sample sizes (a common problem when dealing with rare archaeological samples) - thus providing stronger statistical power. Comparison with a range of previously published data of different preservational levels can allow an estimate on the quality of the new samples. When considering solely (re)using public data, we can consider that this can also spawn new ideas, projects, and meta analyses to allow further deeper exploration of ancient metagenomic data (e.g., looking for correlations between various environmental factors and preservation).\nFortunately for us, genomicists and particularly palaeogenomicists have been very good at uploading raw sequencing data to well-established databases.\nIn the vast majority of cases you will be able to find publically available sequencing data on the INSDC association of databases, namely the EBI’s European Nucleotide Archive (ENA), and NCBI or DDBJ’s Sequence Read Archives (SRA). However, you may in some cases find ancient metagenomic data on institutional FTP servers, domain specific databases (e.g. OAGR), Zenodo, Figshare, or GitHub.\nBut while the data is publicly available, we need to ask whether it is ‘FAIR’."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#finding-ancient-metagenomic-data",
    "href": "accessing-ancientmetagenomic-data.html#finding-ancient-metagenomic-data",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.3 Finding Ancient Metagenomic Data",
    "text": "16.3 Finding Ancient Metagenomic Data\nFAIR principles were defined by researchers, librarians, and industry in 2016 to improve the quality of data uploads - primarily by making data uploads more ‘machine readable’. FAIR standards for:\n\nFindable\nAccessible\nInteroperable\nReproducible\n\nAnd when we consider ancient metagenomic data, we are pretty close to this. Sequencing data is in most cases accessible (via the public databases like ENA, SRA), interoperable and reproducible because we use field standard formats such as FASTQ or BAM files. However Findable remains an issue.\nThis is because the metadata about each data file is dispersed over many places, and very often not with the data files themselves.\nIn this case I am referring to metadata such as: What is the sample’s name? How old is it? Where is it from? Which enzymes were used for library construction? What sequencing machine was this library sequenced on?\nTo find this information about a given data file, you have to search many places (main text, supplementary information, the database itself), for different types of metadata (as authors report different things), and also in different formats (text, tables, figures.\nThis very heterogenous landscape makes it difficult for machines to index all this information (if at all), and thus means you cannot search for the data you want to use for your own reserch in online search engines."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#ancientmetagenomedir",
    "href": "accessing-ancientmetagenomic-data.html#ancientmetagenomedir",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.4 AncientMetagenomeDir",
    "text": "16.4 AncientMetagenomeDir\nThis is where the SPAAM community project AncientMetagenomeDir comes in. AncientMetagenomeDir is a resource of lists of metadata of all publishing and publically available ancient metagenomes and microbial genome-level enriched samples.\nBy aggregating and standardising metadata and accession codes of ancient metagenomic samples, the project aims to make it easier for people to find comparative data for their own projects, as well as help track the field over time and facilitate meta analyses.\nCurrently the project is split over three main tables: host-associated metagenomes (e.g. ancient microbiomes), host-associated single-genomes (e.g. ancient pathogens), and environmental metagenomes (e.g. lakebed cores or cave sediment sequences).\nThe repository already contains more than a thousand samples and span the entire globe and as far back as hundreds of thousands of years.\nTo make the lists of samples and their metadata as accessible and interoperable as possible, we utilise simple text (TSV - tab separate value) files - files that can be opened by pretty much all spreadsheet tools (e.g., Microsoft Office excel, LibreOffice Calc) and languages (R, Python etc.).\n\nCriticially, by standardising the recorded all metadata across all publications this makes it much easier for researchers to filter for particular time periods, geographical regions, or sample types of their interest - and then use the also recorded accession numbers to efficiently download the data.\nAt their core all different AncientMetagenomeDir tables must have at 6 minimum metadata sets:\n\nPublication information (doi)\nSample name(s)\nGeographic location (e.g. country, coordinates)\nAge\nSample type (e.g. bone, sediment, etc.)\nData Archive and accessions\n\nEach table then has additional columns depending on the context (e.g. what time of microbiome is expected for host-associated metagenoes, or species name of the genome that was reconstructed).\nThe AncientMetagenomeDir project already has 3 releases, and will continued to be regularly updated as the community continues to submit new metadata of samples of new publications as they come out."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#further-improving-metadata-reporting-in-ancient-metagenomics",
    "href": "accessing-ancientmetagenomic-data.html#further-improving-metadata-reporting-in-ancient-metagenomics",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.5 Further Improving Metadata Reporting in Ancient Metagenomics",
    "text": "16.5 Further Improving Metadata Reporting in Ancient Metagenomics\nHowever, for researchers, sample-level metadata likely will not include all the information that is needed to include and process public data in their own projects.\nThe SPAAM community have been busy over the last few months extending the types of metadata included in the AncientMetagenomeDir project, to include library level metadata.\nThis metadata includes things such as whether a given set of data files contain sequencing data sequenced on which platform, whether the libraries have undergone damage treatment in the lab, or whether the uploaded data contains all or only mapped reads.\nWe have also started a new project - a MIxS checklist currently entitled ‘MINAS’ - which we aim to make the standard metadata reporting sheet for all ancient metagenomics and even for any ancient DNA sample. Such a checklist would be interegrated into services such as the ENA or SRA, and therefore would standardise metadata alongside the raw data, and make ancient metagenomic data much more findable with search engines.\nFinally, to make it easier for researchers who are not familiar with sequencing database infrastucture, we are in the process of building a new tool (something already in a usable state) called AMDirT. This allows a web browser-based GUI to filter and select data, and produce scripts for you to download all the selected data (without having to go to the databases themselves).\nThis is something we are going to try out now!"
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#running-amdirt",
    "href": "accessing-ancientmetagenomic-data.html#running-amdirt",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.6 Running AMDirT",
    "text": "16.6 Running AMDirT\n\nFirst, we will need to activate a conda environment, and then install the latest development version of the tool for you.\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial will require a web-browser! Make sure to run on your local laptop/PC or on a server with X11 forwarding\n\n\nOpen your terminal, and run the following two commands:\nconda activate git-eager\npip install --upgrade --force-reinstall git+https://github.com/SPAAM-community/AMDirT.git@dev\nOnce that (hopefully) installs correctly, we can load the tool by running\nAMDirT filter\nYour web browser should now load, and you should see a two panel page.\n\nUnder Select a table use the dropdown menu to select ‘ancientsinglegenome-hostassociated’.\nYou should then see a table, pretty similar what you are familiar with with spreadsheet tools such as Microsoft Excel or LibreOffice calc.\n\nTo navigate, you can scroll down to see more rows, and press shift and scroll to see more columns, or use click on a cell and use your arrow keys (⬆,⬇,⬅,➡) to move around the table.\nYou can reorder columns by clicking on the column name, and also filter by pressing the little ‘burger’ icon that appears on the column header when you hover over a given column.\nAs an exercise, we will try filtering to a particular set of samples, then generate some download scripts, and download the files.\nFirst, filter the project_name column to ‘Kocher2021’.\n\nThen scroll to the right, and filter the geo_loc_name to ‘United Kingdom’.\n\nYou should be left with 4 rows.\nFinally, scroll back to the first column and tick the boxes of these four samples.\n\nOnce you’ve selected the samples you want, you can press Validate selection. You should then see a series loading-spinner, and new buttons should appear!\n\nYou should have three main buttons:\n\nDownload Curl sample download script\nDownload nf-core/eager input TSV\nDownload Citations as BibText\n\nThe first button is for generating a download script that will allow you to immediately download all sequencing data of the samples you selected. The second button is a pre-configured input file for use in the nf-core/eager ancient DNA pipeline, and finally, the third button generates a text file with (in most cases) all the citations of the data you downloaded, in a format accepted by most reference/citation managers.\nIt’s important to note you are not necessarily restricted to Curl for downloading the data, or nf-core/eager for running the files. AMDirT aims to add support for whatever tools or pipelines requested by the community. For example, an already supported download alternative is with the nf-core/fetchNGS pipeline. You can select these using the drop-down menus on the left hand-side.\nPress the three buttons to make sure you download the files. And once this is done, you can close the tab of the web browser, and in the terminal you can press ctrl + c to shutdown the tool."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#inspecting-amdirt-output",
    "href": "accessing-ancientmetagenomic-data.html#inspecting-amdirt-output",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.7 Inspecting AMDirT Output",
    "text": "16.7 Inspecting AMDirT Output\nLets look at the files that AMDirT has generated for you.\nFirst you should cd into the directory that your web browser downloaded the files into (e.g. cd ~/Downloads/), then look inside the directory. You should see the following three files\n$ ls\nancientMetagenomeDir_curl_download_script.sh\nancientMetagenomeDir_citations.bib\nancientMetagenomeDir_eager_input.csv\nWe can simple run cat on each file to look inside. If you run cat on the curl download script, you should see a series of curl commands with the correct ENA links for you for each of the samples you wish to download.\n$ cat ancientMetagenomeDir_curl_download_script.sh\n#!/usr/bin/env bash\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz -o ERR6053619.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz -o ERR6053618.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz -o ERR6053675.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz -o ERR6053686.fastq.gz\nBy providing this script for you, AMDirT facilitates fast download of files of interest by replacing the one-by-one download commands for each sample with a single command!\n$ bash ancientMetagenomeDir_curl_download_script.sh\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz -o ERR6053619.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz -o ERR6053618.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz -o ERR6053675.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz -o ERR6053686.fastq.gz\nRunning this command should result in progress logs of the downloading of the data of the four selected samples!\nOnce the four samples are downloaded, AMDirT then facilitates fast processing of the data, as the eager script can be given directly to nf-core/eager as input. Importantly by including the library metadata (mentioned above), researchers can leverage the complex automated processing that nf-core/eager can perform when given such relevant metadata.\n$ cat ancientMetagenomeDir_eager_input.csv\nSample_Name Library_ID  Lane    Colour_Chemistry    SeqType Organism    Strandedness    UDG_Treatment   R1  R2  BAM\nI0157   ERR6053618  0   4   SE  Homo sapiens    double  unknown ERX5692504_ERR6053618.fastq.gz  NA  NA\nI0161   ERR6053619  0   4   SE  Homo sapiens    double  unknown ERX5692505_ERR6053619.fastq.gz  NA  NA\nOAI017  ERR6053675  0   4   SE  Homo sapiens    double  half    ERX5692561_ERR6053675.fastq.gz  NA  NA\nSED009  ERR6053686  0   4   SE  Homo sapiens    double  half    ERX5692572_ERR6053686.fastq.gz  NA  NA\nFinally, we can look into the citations file which will provide you with the citation information of all the downloaded data and AncientMetagenomeDir itself.\n\n\n\n\n\n\nWarning\n\n\n\nThe contents of this file is reliant on indexing of publications on CrossRef. In some cases not all citations will be present, so this should be double checked!\n\n\n$ cat ancientMetagenomeDir_citations.bib\n\n@article{Fellows_Yates_2021,\n    doi = {10.1038/s41597-021-00816-y},\n    url = {https://doi.org/10.1038%2Fs41597-021-00816-y},\n    year = 2021,\n    month = {jan},\n    publisher = {Springer Science and Business Media {LLC}},\n    volume = {8},\n    number = {1},\n    author = {James A. Fellows Yates and Aida Andrades Valtue{\\~{n}}a and {\\AA}shild J. V{\\aa}gene and\n    Becky Cribdon and Irina M. Velsko and Maxime Borry and Miriam J. Bravo-Lopez and Antonio Fernandez-Guerra\n    and Eleanor J. Green and Shreya L. Ramachandran and Peter D. Heintzman and Maria A. Spyrou and Alexander\n    Hübner and Abigail S. Gancz and Jessica Hider and Aurora F. Allshouse and Valentina Zaro and Christina Warinner},\n    title = {Community-curated and standardised metadata of published ancient metagenomic samples with {AncientMetagenomeDir}},\n    journal = {Scientific Data}\n}\nThis file can be easily loaded into most reference managers and then have all the citations quickly added to your manuscripts."
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#git-practise",
    "href": "accessing-ancientmetagenomic-data.html#git-practise",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.8 Git Practise",
    "text": "16.8 Git Practise\nA critical factor of AncientMetagenomeDir is that it is community-based. The community curates all new submissions to the repository, and this all occurs with Git.\nThe data is hosted and maintained on GitHub - new publications are evaluated on issues, submissions created on branches, made by pull requests, and PRs reviewed by other members of the community.\nYou can see the workflow in the image below from the AncientMetageomeDir publication, and read more about the workflow on the AncientMetagenomeDir wiki\n\nThis means we can also use this repository to practise git!\nYour task (with git terms removed):\n\nMake a ‘copy’ the jfy133/AncientMetagenomeDir repository to your account\n‘Download’ the copied repo to your local machine\n‘Change’ to the dev branch\nModify ‘ancientsinglegenome-hostassociated_samples.tsv’\n\nClick here to get some example data to copy in to the end of the TSV file\n\n‘Send’ back to Git(Hub)\nOpen a ‘request’ adding changes to the original repo\n\nMake sure to put ‘Summer school’ in the title of the ‘Request’\n\n\n\n\n\n\n\n\nClick me to reveal the correct terminology\n\n\n\n\n\n\nFork the jfy133/AncientMetagenomeDir repository to your account\nClone the copied repo to your local machine\nSwitch to the dev branch\nModify ‘ancientsinglegenome-hostassociated_samples.tsv’\n\nClick here to get some example data to copy in to the end of the TSV file\n\nCommit and Push back to your Fork on Git(Hub)\nOpen a Pull Request adding changes to the original jfy133/AncientMetagenomeDir repo\n\nMake sure to put ‘Summer school’ in the title of the pull request"
  },
  {
    "objectID": "accessing-ancientmetagenomic-data.html#summary",
    "href": "accessing-ancientmetagenomic-data.html#summary",
    "title": "16  Introduction to AncientMetagenomeDir",
    "section": "16.9 Summary",
    "text": "16.9 Summary\n\nReporting of metadata messy! Consider when publishing your own work!\n\nUse AncientMetagenomeDir as a template\n\nUse AncientMetagenomeDir and AMDirT (beta) to rapidly find public ancient metagenomic data\nContribute to AncientMetagenomeDir with git\n\nCommunity curated!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#lecture",
    "href": "ancient-metagenomic-pipelines.html#lecture",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.1 Lecture",
    "text": "17.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#introduction",
    "href": "ancient-metagenomic-pipelines.html#introduction",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.2 Introduction",
    "text": "17.2 Introduction\nA pipeline is a series of linked computational steps, where the output of one process becomes the input of the next. Pipelines are critical for managing the huge quantities of data that are now being generated regularly as part of ancient DNA analyses. Today we will discuss one option for managing computational analyses of ancient next-generation sequencing datasets, nf-core/eager. Keep in mind that other tools, like the Paleomix pipeline, can also be used for similar applications."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "href": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.3 What is nf-core/eager?",
    "text": "17.3 What is nf-core/eager?\nnf-core/eager is a computational pipeline specifically designed for preprocessing and analysis of ancient DNA data. It is a reimplementation of the previously published EAGER (Efficient Ancient Genome Reconstruction) pipeline (Peltzer et al. 2016) using Nextflow. The nf-core/eager pipeline was designed with the following aims in mind:\n\nPortability- In order for our analyses to be reproducible, others should be able to easily implement our computational pipelines. nf-core/eager is highly portable, providing easy access to pipeline tools and facilitating use across multiple platforms. nf-core eager utilizes Docker, Conda, and Singularity for containerization, enabling distrubition of the pipeline in a self-contained bundle containing all the code, packages, and libraries needed to run it.\nReproducibility- nf-core/eager uses custom configuration profiles to specify both HPC-level parameters and analyses-specific options. These profiles can be shared alongside your publication, making it easier for others to reproduce your methodology!\nNew Tools- Finally, nf-core/eager includes additional, novel methods and tools for analysis of ancient DNA data that were not included in previous versions. This is especially good news for folks interested in microbial sciences, who can take advantage of new analytical pathways for metagenomic analysis and pathogen screening."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "href": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.4 Steps in the pipeline",
    "text": "17.4 Steps in the pipeline\n\nA detailed description of steps in the pipeline is available as part of nf-core/eager’s extensive documentation. For more information, check out the usage documentation here.\nBriefly, nf-core/eager takes standard input file types that are shared across the genomics field, including raw fastq files, aligned reads in bam format, and a reference fasta. nf-core/eager can perform preprocessing of this raw data, including adapter clipping, read merging, and quality control of adapter-trimmed data. Note that input files can be specified using wildcards OR a standardized tsv format file; the latter facilitates streamlined integration of multpile data types within a single EAGER run! More on this later.\nnf-core/eager facilitates mapping using a variety of field-standard alignment tools with configurable parameters. An exciting new addition in nf-core/eager also enables analysis of off-target host DNA for all of you metagenomics folks out there. Be sure to check out the functionality available for metagenomic profiling (blue route in the ‘tube map’ above).\nnf-core/eager incorporates field-standard quality control tools designed for use with ancient DNA so that you can easily evaluate the success of your experiments. Multiple genotyping approaches and additional analyses are available depending on your input datatype, organism, and research questions. Importantly, all of these processes generate data that we need to compile and analyze in a coherent way. nf-core eager uses MultiQC. to create an integrated html report that summarizes the output/results from each of the pipeline steps. Stay tuned for the practical portion of the walkthrough!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "href": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.5 How to build an nf-core/eager command: A practical introduction",
    "text": "17.5 How to build an nf-core/eager command: A practical introduction\nFor the practical portion of the walkthrough, we will utilize sequencing data from four aDNA libraries, which you should have already downloaded from NCBI. If not, please see the Preparation section above.\nThese four libraries come from from two ancient individuals, GLZ002 and KZL002. GLZ002 comes from the Neolithic Siberian site of Glazkovskoe predmestie adn was radiocarbon dated to 3081-2913 calBCE. KZL002 is an Iron Age individual from Kazakhstan, radiocarbon dated to 2736-2457 calBCE. Both individuals were infected with the so-called ‘Stone Age Plague’ of Yersinia pestis, and libraries from these individuals were processed using hybridization capture to increase the number of Y. pestis sequences available for analysis.\nOur aims in the following tutorial are to:\n\nPreprocess the fastq files by trimming adapters and merging paired-end reads\nAlign reads to the Y. pestis reference and compute the endogenous DNA percentage\nFilter the aligned reads to remove host DNA\nRemove duplicate reads for accurate coverage estimation and genotyping\nMerge data by sample and perform genotyping on the combined dataset\nReview quality control data to evaluate the success of the previous steps\n\nLet’s get started!\nFirst, activate the conda environment that we downloaded during setup:\nconda activate git-eager\nNext, download the latest version of the nf-core/eager repo (or check for updates if you have a previously-installed version):\nnextflow pull nf-core/eager\nFinally, we will build our eager command:\nnextflow run nf-core/eager \\ #Tells nextflow to execute the EAGER pipeline\n-r 2.4.5 -ds1l \\ #Specifies which pipeline and Nextflow versions to run for reproducibility\n-profile conda  \\ #Profiles configure your analysis for specific computing environments/analyses\n--fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna \\ #Specify reference in fasta format\n--input ancientMetagenomeDir_eager_input.tsv \\ #Specify input in tsv format or wildcards\n--run_bam_filtering --bam_unmapped_type fastq \\ #Filter unmapped reads and save in fastq format\n--run_genotyping --genotyping_tool ug --gatk_ug_out_mode EMIT_ALL_SITES \\ #Run genotyping with the GATK UnifiedGenotyper\n--run_bcftools_stats #Generate variant calling statistics\nFor full parameter documentation, click here.\nAnd now we wait…"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "href": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.6 Top Tips for nf-core/eager success",
    "text": "17.6 Top Tips for nf-core/eager success\n\nScreen sessions\n\nDepending on your input data, infrastructre, and analyses, running nf-core/eager can take hours or even days. To avoid crashed due to loss of power or network connectivity, try running nf-core/eager in a screen or tmux session:\nscreen -R eager\n\nMultiple ways to supply input data\n\nIn this tutorial, a tsv file to specify our input data files and formats. This is a powerful approach that allows nf-core eager to intelligently apply analyses to certain files only (e.g. merging for paired-end but not single-end libraries). Check out the contents of our tsv input file using the following command:\ncat ancientMetagenomeDir_eager_input.tsv\nInputs can also be specified using wildcards, which can be useful for fast analyses with simple input data types (e.g. same sequencing configuration, file location, etc.).\nnextflow run nf-core/eager -r 2.4.5 -ds1l -profile conda --fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna\n--input \"data/*fastq.gz\" &lt;...&gt;\nSee the online nf-core/eager documentation for more details.\n\nGet your MultiQC report via email\n\nIf you have GNU mail or sendmail set up on your system, you can add the following flag to send the MultiQC html to your email upon run completion:\n--email \"your_address@something.com\"\n\nCheck out the EAGER GUI\n\nFor folks who might be less comfortable with the command line, check out the nf-core/eager GUI! The GUI also provides a full list of options with short explanations for those interested in learning more about what the pipeline can do.\n\nWhen something fails, all is not lost!\n\nWhen individual jobs fail, nf-coreager will try to automatically resubmit that job with increased memory and CPUs (up to two times per job). When the whole pipeline crashes, you can save time and computational resources by resubmitting with the -resume flag. nf-core/eager will retrieve cached results from previous steps as long as the input is the same.\n\nMonitor your pipeline in real time with the Nextflow Tower\n\nRegular users may be interested in checking out the Nextflow Tower, a tool for monitoring the progress of Nextflow pipelines in real time. Check here for more information."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "href": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.7 Questions to think about",
    "text": "17.7 Questions to think about\n\nWhy is it important to use a pipeline for genomic analysis of ancient data?\nHow can the design of the nf-core/eager pipeline help researchers comply with the FAIR princples for management of scientific data?\nWhat metrics do you use to evaluate the success/failure of ancient DNA sequencing experiments? How can these measures be evaluated when using nf-core/eager for data preprocessing and analysis?"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resources.html#introduction-to-ngs-sequencing",
    "href": "resources.html#introduction-to-ngs-sequencing",
    "title": "18  Resources",
    "section": "18.1 Introduction to NGS Sequencing",
    "text": "18.1 Introduction to NGS Sequencing\n\nhttps://www.youtube.com/watch?v=fCd6B5HRaZ8\nhttps://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dijk, Erwin L van, Hélène Auger, Yan Jaszczyszyn, and Claude Thermes.\n2014. “Ten Years of Next-Generation Sequencing Technology.”\nTrends in Genetics 30 (9): 418–26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. “Double\nIndexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina\nPlatform.” Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder,\nMichael N Edmonson, Yu Liu, et al. 2019. “Analysis of Error\nProfiles in Deep Next-Generation Sequencing Data.” Genome\nBiology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. “Illumina Sequencing\nLibrary Preparation for Highly Multiplexed Target Capture and\nSequencing.” Cold Spring Harbor Protocols 2010 (6):\ndb.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. “Next-Generation Sequencing Transforms\nToday’s Biology.” Nature Methods 5 (1): 16–18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. “Next-Generation\nDNA Sequencing.” Nature Biotechnology 26\n(10): 1135–45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle\nJoseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017.\n“Index Switching Causes ‘Spreading-of-Signal’ Among\nMultiplexed Samples in Illumina HiSeq 4000 DNA\nSequencing.” bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018.\n“Overview of Next-Generation Sequencing Technologies.”\nCurrent Protocols in Molecular Biology / Edited by Frederick M.\nAusubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dalén, and\nKaterina Guschanski. 2019. “Index Hopping on the Illumina\nHiseqX Platform and Its Consequences for Ancient\nDNA Studies.” Molecular Ecology Resources,\nMarch. https://doi.org/10.1111/1755-0998.13009."
  },
  {
    "objectID": "tools.html#introduction-to-r-and-the-tidyverse",
    "href": "tools.html#introduction-to-r-and-the-tidyverse",
    "title": "19  Tools",
    "section": "19.1 Introduction to R and the Tidyverse",
    "text": "19.1 Introduction to R and the Tidyverse\n\nr\nr studio (desktop)\ntidyverse"
  },
  {
    "objectID": "tools.html#introduction-to-python-and-pandas",
    "href": "tools.html#introduction-to-python-and-pandas",
    "title": "19  Tools",
    "section": "19.2 Introduction to Python and Pandas",
    "text": "19.2 Introduction to Python and Pandas\n\npython\njupyter"
  },
  {
    "objectID": "tools.html#introduction-to-github",
    "href": "tools.html#introduction-to-github",
    "title": "19  Tools",
    "section": "19.3 Introduction to Git(Hub)",
    "text": "19.3 Introduction to Git(Hub)\n\ngit (normally installed by default on all UNIX based operating systems e.g. Linux, OSX)"
  },
  {
    "objectID": "tools.html#functional-profiling",
    "href": "tools.html#functional-profiling",
    "title": "19  Tools",
    "section": "19.4 Functional Profiling",
    "text": "19.4 Functional Profiling\n\nr\nr studio (desktop)\ntidyverse\nhumann3"
  },
  {
    "objectID": "tools.html#de-novo-assembly",
    "href": "tools.html#de-novo-assembly",
    "title": "19  Tools",
    "section": "19.5 De novo assembly",
    "text": "19.5 De novo assembly\n\nfastp\nmegahit\nbowtie2\nsamtools\nbioawk\ndiamond\nmetabat2\nmaxbin2\nconcoct\nmetawrap\ncheckm-genome\ngunc\npydamage\nprokka"
  },
  {
    "objectID": "tools.html#genome-mapping",
    "href": "tools.html#genome-mapping",
    "title": "19  Tools",
    "section": "19.6 Genome Mapping",
    "text": "19.6 Genome Mapping\n\nbwa\nigv\ngatk"
  },
  {
    "objectID": "tools.html#phylogenomics",
    "href": "tools.html#phylogenomics",
    "title": "19  Tools",
    "section": "19.7 Phylogenomics",
    "text": "19.7 Phylogenomics\n\nbeast2_\ntracer\ntempest\nmega"
  },
  {
    "objectID": "tools.html#ancient-metagenomic-pipelines",
    "href": "tools.html#ancient-metagenomic-pipelines",
    "title": "19  Tools",
    "section": "19.8 Ancient Metagenomic Pipelines",
    "text": "19.8 Ancient Metagenomic Pipelines\n\nnextflow\nnf-core tools\nnf-core/eager"
  }
]