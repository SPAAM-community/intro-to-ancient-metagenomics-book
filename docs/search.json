[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Ancient Metagenomics",
    "section": "",
    "text": "Introduction\nAncient metagenomics applies cutting-edge metagenomic methods to the degraded DNA content of archaeological and palaeontological specimens. The rapidly growing field is currently uncovering a wealth of novel information for both human and natural history, from identifying the causes of devastating pandemics such as the Black Death, to revealing how past ecosystems changed in response to long-term climatic and anthropogenic change, to reconstructing the microbiomes of extinct human relatives. However, as the field grows, the techniques, methods, and workflows used to analyse such data are rapidly changing and improving.\nIn this book we will go through the main steps of ancient metagenomic bioinformatic workflows, familiarising students with the command line, demonstrating how to process next-generation-sequencing (NGS) data, and showing how to perform de novo metagenomic assembly. Focusing on host-associated ancient metagenomics, the book consists of a combination of theory and hands-on exercises, allowing readers to become familiar with the types of questions and data researchers work with.\nBy the end of the textbook, readers will have an understanding of how to effectively carry out the major bioinformatic components of an ancient metagenomic project in an open and transparent manner.\n\n\n\n\n\n\nNote\n\n\n\nIf you export the PDF or ePub versions of this book, some sections maybe excluded (such as videos, and embedded slide decks). Always refer to this website in doubt.\n\n\nAll material was originally developed for the SPAAM Summer School: Introduction to Ancient Metagenomics"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Please note if you export the PDF or ePub versions of this book, some sections maybe excluded (such as videos, and embedded slide decks)."
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "1¬† Authors",
    "section": "",
    "text": "The creation of this text book was developed through a series of ‚Ä¶\n\n\n\n\n\n\n\n\n2022\n\nüá¨üáß James Fellows Yates is an archaeology-trained biomolecular archaeologist and convert to palaeogenomics, and is recently pivoting to bioinformatics. He specialises in ancient metagenomics analysis, generating tools and high-throughput approaches and high-quality pipelines for validating and analysing ancient (oral) microbiomes and palaeogenomic data.\n\n\n2022\n\nüá∫üá∏ Christina Warinner is Group Leader of Microbiome Sciences at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and Associate Professor of Anthropology at Harvard University. She serves on the Leadership Team of the Max Planck-Harvard Research Center for the Archaeoscience of the Ancient Mediterranean (MHAAM), and is a Professor in the Faculty of Biological Sciences at Friedrich Schiller University in Jena, Germany. Her research focuses on the use of metagenomics and paleoproteomics to better understand past human diet, health, and the evolution of the human microbiome.\n\n\n2022\n\nüá™üá∏ Aida Andrades Valtue√±a is a geneticist interested in pathogen evolution, with particular interest in prehistoric pathogens. She has been exploring new methods to analyse ancient pathogen data to understand their past function and ecology to inform models of pathogen emergence.\n\n\n2022\n\nüá©üá™ Alexander Herbig is a bioinformatician and group leader for Computational Pathogenomics at the Max Planck Institute for Evolutionary Anthropology. His main interest is in studying the evolution of human pathogens and in methods development for pathogen detection and bacterial genomics.\n\n\n2022\n\nüá©üá™ Alex H√ºbner is a computational biologist, who originally studied biotechnology, before switching to evolutionary biology during his PhD. For his postdoc in the Warinner lab, he focuses on investigating whether new methods in the field of modern metagenomics can be directly applied to ancient DNA data. Here, he is particularly interested in the de novo assembly of ancient metagenomic sequencing data and the subsequent analysis of its results.\n\n\n2022\n\nüá©üá™ Alina Hiss is a PhD student in the Computational Pathogenomics group at the Max Planck Institute for Evolutionary Anthropology. She is interested in the evolution of human pathogens and working on material from the Carpathian basin to gain insights about the presence and spread of pathogens in the region during the Early Medieval period.\n\n\n2022\n\nüá´üá∑ Arthur Kocher initially trained as a veterinarian. He then pursued a PhD in the field of disease ecology, during which he studied the impact of biodiversity changes on the transmission of zoonotic diseases using molecular tools such as DNA metabarcoding. During his Post-Docs, he extended his research focus to evolutionary aspects of pathogens, which he currently investigates using ancient genomic data and Bayesian phylogenetics.\n\n\n2022\n\nüá©üá™ Clemens Schmid is a computational archaeologist pursuing a PhD in the group of Stephan Schiffels at the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. He is trained both in archaeology and computer science and currently develops computational methods for the spatiotemporal co-analysis of archaeological and ancient genomic data. He worked in research projects on the European Neolithic, Copper and Bronze age and maintains research software in R, C++ and Haskell.\n\n\n2022\n\nüá∫üá∏ Irina Velsko is a postdoc in the Microbiome group of the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. She did her PhD work on oral microbiology and immunology of the living, and now works on oral microbiomes of the living and the dead. Her work focuses on the evolution and ecology of dental plaque biofilms, both modern and ancient, and the complex interplay between microbiomes and their hosts.\n\n\n2022\n\nüá´üá∑ Maxime Borry is a doctoral researcher in bioinformatics at the Max Planck Institute for Evolutionary Anthropology in Germany. After an undergraduate in life sciences and a master in Ecology, followed by a master in bioinformatics, he is now working on the completion of his PhD, focused on developing new tools and data analysis of ancient metagenomic samples.\n\n\n2022\n\nüá∫üá∏ Megan Michel is a PhD student jointly affiliated with the Archaeogenetics Department at the Max Planck Institute for Evolutionary Anthropology and the Human Evolutionary Biology Department at Harvard University. Her research focuses on using computational genomic analyses to understand how pathogens have co-evolved with their hosts over the course of human history.\n\n\n2022\n\nüá∑üá∫ Nikolay Oskolkov is a bioinformatician at Lund University and the bioinformatics platform of SciLifeLab, Sweden. He defended his PhD in theoretical physics in 2007, and switched to life sciences in 2012. His research interests include mathematical statistics and machine learning applied to genetics and genomics, single cell and ancient metagenomics data analysis.\n\n\n2022\n\nüá¶üá∫ Sebastian Duchene is an Australian Research Council Fellow at the Doherty Institute for Infection and Immunity at the University of Melbourne, Australia. Prior to joining the University of Melbourne he obtained his PhD and conducted postdoctoral work at the University of Sydney. His research is in molecular evolution and epidemiology of infectious pathogens, notably viruses and bacteria, and developing Bayesian phylodynamic methods.\n\n\n2022\n\nüá¨üá∑ Thiseas Lamnidis is a human population geneticist interested in European population history after the Bronze Age. To gain the required resolution to differentiate between Iron Age European populations, he is developing analytical methods based on the sharing of rare variation between individuals. He has also contributed to pipelines that streamline the processing and analysis of genetic data in a reproducible manner, while also facilitating dissemination of information among interdisciplinary colleagues."
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#introduction",
    "href": "introduction-to-ngs-sequencing.html#introduction",
    "title": "3¬† Introduction to NGS Sequencing",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this section, I will introduce how we are able to convert DNA molecules to human readable sequences of A, C, T, and Gs, which we can subsequently can computationally analyse.\nThe field of Ancient DNA was revolutionised by the development of ‚ÄòNext Generation Sequencing‚Äô (NGS), which relies on sequencing of millions of short fragments of DNA in parallel. The global leading DNA sequencing company is Illumina, and the technology used by Illumina is also most popular by palaeogeneticists. Therefore I will describe how the various technologies behind Illumina next-generation sequencing machines.\nI will also describe some important differences in the way different models of Illumina sequences work, and how this can influence ancient DNA research. Finally I will introduce the structure of ‚ÄòFASTQ‚Äô files, the most popular file format for representing the DNA sequence output of NGS sequencing machines."
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#lecture",
    "href": "introduction-to-ngs-sequencing.html#lecture",
    "title": "3¬† Introduction to NGS Sequencing",
    "section": "3.2 Lecture",
    "text": "3.2 Lecture\n\n\nPDF version of the slide lectures can be downloaded from here."
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#readings",
    "href": "introduction-to-ngs-sequencing.html#readings",
    "title": "3¬† Introduction to NGS Sequencing",
    "section": "3.3 Readings",
    "text": "3.3 Readings\n\nReviews\n(Schuster 2008)\n(Shendure and Ji 2008)\n(Slatko, Gardner, and Ausubel 2018)\n(Dijk et al. 2014)\n\n\nSequencing Library Construction\n(Kircher, Sawyer, and Meyer 2012)\n(Meyer and Kircher 2010)\n\n\nErrors and Considerations\n(Ma et al. 2019)\n(Sinha et al. 2017)\n(Valk et al. 2019)"
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "href": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "title": "3¬† Introduction to NGS Sequencing",
    "section": "3.4 Questions to think about",
    "text": "3.4 Questions to think about\n\nWhy is Illumina sequencing technologies useful for aDNA?\nWhat problems can the 2-colour chemistry technology of NextSeq and NovaSeqs cause in downstream analysis?\nWhy is ‚ÄòIndex-Hopping‚Äô a problem?\nWhat is good software to evaluate the quality of your sequencing runs?\n\n\n\n\n\nDijk, Erwin L van, H√©l√®ne Auger, Yan Jaszczyszyn, and Claude Thermes. 2014. ‚ÄúTen Years of Next-Generation Sequencing Technology.‚Äù Trends in Genetics 30 (9): 418‚Äì26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. ‚ÄúDouble Indexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina Platform.‚Äù Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder, Michael N Edmonson, Yu Liu, et al. 2019. ‚ÄúAnalysis of Error Profiles in Deep Next-Generation Sequencing Data.‚Äù Genome Biology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. ‚ÄúIllumina Sequencing Library Preparation for Highly Multiplexed Target Capture and Sequencing.‚Äù Cold Spring Harbor Protocols 2010 (6): db.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. ‚ÄúNext-Generation Sequencing Transforms Today‚Äôs Biology.‚Äù Nature Methods 5 (1): 16‚Äì18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. ‚ÄúNext-Generation DNA Sequencing.‚Äù Nature Biotechnology 26 (10): 1135‚Äì45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle Joseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017. ‚ÄúIndex Switching Causes ‚ÄòSpreading-of-Signal‚Äô Among Multiplexed Samples in Illumina HiSeq 4000 DNA Sequencing.‚Äù bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018. ‚ÄúOverview of Next-Generation Sequencing Technologies.‚Äù Current Protocols in Molecular Biology / Edited by Frederick M. Ausubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dal√©n, and Katerina Guschanski. 2019. ‚ÄúIndex Hopping on the Illumina HiseqX Platform and Its Consequences for Ancient DNA Studies.‚Äù Molecular Ecology Resources, March. https://doi.org/10.1111/1755-0998.13009."
  },
  {
    "objectID": "introduction-to-git.html#overview",
    "href": "introduction-to-git.html#overview",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.1 Overview",
    "text": "11.1 Overview\nAs the size and complexity of metagenomic analyses continues to expand, effectively organizing and tracking changes to scripts, code, and even data, continues to be a critical part of ancient metagenomic analyses. Furthermore, this complexity is leading to ever more collaborative projects, with input from multiple researchers.\nIn this practical session, we will introduce ‚ÄòGit‚Äô, an extremely popular version control system used in bioinformatics and software development to store, track changes, and collaborate on scripts and code. We will also introduce, GitHub, a cloud-based service for Git repositories for sharing data and code, and where many bioinformatic tools are stored. We will learn how to access and navigate course materials stored on GitHub through the web interface as well as the command line, and we will create our own repositories to store and share the output of upcoming sessions.\n\n11.1.0.1 Preparation\nThe conda environment .yaml file for this practical session can be downloaded from here: https://zenodo.org/record/6983120#.YxdEaOxBz0o. See instructions on page.\n\n\n11.1.0.2 Introduction\nIn this walkthrough, we will introduce the version control system Git as well as Github, a remote hosting service for version controlled repositories. Git and Github are increasingly popular tools for tracking data, collaborating on research projects, and sharing data and code, and learning to use them will help in many aspects of your own research. For more information on the benefits of using version control systems, see the slides.\n\n\n11.1.0.3 SSH setup\nTo begin, you will set up an SSH key to facilitate easier authentication when transferring data between local and remote repositories. In other words, follow this section of the tutorial so that you never have to type in your github password again! Begin by activating the conda environment for this section (see Preparation above).\nconda activate git-eager\nNext, generate your own ssh key, replacing the email below with your own address.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nI recommend saving the file to the default location and skipping passphrase setup. To do this, simply press enter without typing anything.\nYou should now (hopefully!) have generated an ssh key. To check that it worked, run the following commands to list the files containing your public and private keys and check that the ssh program is running.\ncd ~/.ssh/\nls id*\neval \"$(ssh-agent -s)\"\nNow you need to give ssh your key to record:\nssh-add ~/.ssh/id_ed15519\nNext, open your webbrowser and navigate to your github account. Go to settings -&gt; SSH & GPG Keys -&gt; New SSH Key. Give you key a title and paste the public key that you just generated on your local machine.\ncat ~/.ssh/id_ed15519\nFinally, press Add SSH key. To check that it worked, run the following command on your local machine. You should see a message telling you that you‚Äôve successfully authenticated.\nssh -T git@github.com\nFor more information about setting up the SSH key, including instructions for different operating systems, check out github‚Äôs documentation: https://docs.github.com/es/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent.\n\n\n11.1.0.4 The only 6 commands you really need to know\nNow that you have set up your own SSH key, we can begin working on some version controlled data! Navigate to your github homepage and create a new repository. You can choose any name for your new repo (including the default). Add a README file, then select Create Repository.\n  \n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the session, replace the name of my repository (vigilant-octo-journey) with your own repo name.\n\n\n\n\nChange into the directory where you would like to work, and let‚Äôs get started! First, we will learn to clone a remote repository onto your local machine. Navigate to your new repo, select the Code dropdown menu, select SSH, and copy the address as shown below.\n\nBack at your command line, clone the repo as follows:\ngit clone git@github.com:meganemichel/vigilant-octo-journey.git\nNext, let‚Äôs add a new or modified file to our ‚Äòstaging area‚Äô on our local machine.\ncd vigilant-octo-journey\necho \"test_file\" &gt; file_A.txt\necho \"Just an example repo\" &gt;&gt; README.md\ngit add file_A.txt\nNow we can check what files have been locally changed, staged, etc. with status.\ngit status\nYou should see that file_A.txt is staged to be committed, but README.md is NOT. Try adding README.md and check the status again.\nNow we need to package or save the changes into a commit with a message describing the changes we‚Äôve made. Each commit comes with a unique hash ID and will be stored forever in git history.\ngit commit -m \"Add example file\"\nFinally, let‚Äôs push our local commit back to our remote repository.\ngit push\nWhat if we want to download new commits from our remote to our local repository?\ngit pull\nYou should see that your repository is already up-to-date, since we have not made new changes to the remote repo. Let‚Äôs try making a change to the remote repository‚Äôs README file (as below). Then, back on the command line, pull the repository again.\n\n\n\n11.1.0.5 Working collaboratively\nGithub facilitates simultaneous work by small teams through branching, which generates a copy of the main repository within the repository. This can be edited without breaking the ‚Äòmaster‚Äô version. First, back on github, make a new branch of your repository.\n\nFrom the command line, you can create a new branch as follows:\ngit switch -c new_branch\nTo switch back to the main branch, use\ngit switch main\nNote that you must commit changes for them to be saved to the desired branch!\n\n\n11.1.0.6 Pull requests\nA Pull request (aka PR) is used to propose changes to a branch from another branch. Others can comment and make suggestinos before your changes are merged into the main branch. For more information on creating a pull request, see github‚Äôs documentation: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request.\n\n\n11.1.1 Resources\n\nhttps://www.atlassian.com/git/tutorials\nhttps://ohshitgit.com/\n\n\n\n11.1.2 Readings\n\nChacon, Scott, and Ben Straub. 2022. Pro Git. Second Edition. The Expert‚Äôs Voice. Apress.\n\n\n\n11.1.3 Questions to think about\n\nWhy is using a version control software for tracking data and code important?\nHow can using Git(Hub) help me to collaborate on group projects?"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "12¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resources.html#introduction-to-ngs-sequence",
    "href": "resources.html#introduction-to-ngs-sequence",
    "title": "13¬† Resources",
    "section": "13.1 Introduction to NGS Sequence",
    "text": "13.1 Introduction to NGS Sequence\n\nhttps://www.youtube.com/watch?v=fCd6B5HRaZ8\nhttps://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dijk, Erwin L van, H√©l√®ne Auger, Yan Jaszczyszyn, and Claude Thermes.\n2014. ‚ÄúTen Years of Next-Generation Sequencing Technology.‚Äù\nTrends in Genetics 30 (9): 418‚Äì26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. ‚ÄúDouble\nIndexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina\nPlatform.‚Äù Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder,\nMichael N Edmonson, Yu Liu, et al. 2019. ‚ÄúAnalysis of Error\nProfiles in Deep Next-Generation Sequencing Data.‚Äù Genome\nBiology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. ‚ÄúIllumina Sequencing\nLibrary Preparation for Highly Multiplexed Target Capture and\nSequencing.‚Äù Cold Spring Harbor Protocols 2010 (6):\ndb.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. ‚ÄúNext-Generation Sequencing Transforms\nToday‚Äôs Biology.‚Äù Nature Methods 5 (1): 16‚Äì18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. ‚ÄúNext-Generation\nDNA Sequencing.‚Äù Nature Biotechnology 26\n(10): 1135‚Äì45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle\nJoseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017.\n‚ÄúIndex Switching Causes ‚ÄòSpreading-of-Signal‚Äô Among\nMultiplexed Samples in Illumina HiSeq 4000 DNA\nSequencing.‚Äù bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018.\n‚ÄúOverview of Next-Generation Sequencing Technologies.‚Äù\nCurrent Protocols in Molecular Biology / Edited by Frederick M.\nAusubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dal√©n, and\nKaterina Guschanski. 2019. ‚ÄúIndex Hopping on the Illumina\nHiseqX Platform and Its Consequences for Ancient\nDNA Studies.‚Äù Molecular Ecology Resources,\nMarch. https://doi.org/10.1111/1755-0998.13009."
  },
  {
    "objectID": "acknowledgements.html#financial-support",
    "href": "acknowledgements.html#financial-support",
    "title": "2¬† Acknowledgements",
    "section": "2.1 Financial Support",
    "text": "2.1 Financial Support\n\nThe content of this textbook was developed from the SPAAM Summer School: Introduction to Ancient Metagenomics summer school series, sponsored by the Werner Siemens-Stiftung (Grant: Paleobiotechnology, awarded to Pierre Stallforth, Hans-Kn√∂ll Institute, and Christina Warinner, Max Planck Institute for Evolutionary Anthropology)"
  },
  {
    "objectID": "acknowledgements.html#institutional-support",
    "href": "acknowledgements.html#institutional-support",
    "title": "2¬† Acknowledgements",
    "section": "2.2 Institutional Support",
    "text": "2.2 Institutional Support"
  },
  {
    "objectID": "acknowledgements.html#infrastructural-support",
    "href": "acknowledgements.html#infrastructural-support",
    "title": "2¬† Acknowledgements",
    "section": "2.3 Infrastructural Support",
    "text": "2.3 Infrastructural Support\n\nThe practical sessions of the summers schools work was supported by the BMBF-funded de.NBI Cloud within the German Network for Bioinformatics Infrastructure (de.NBI) (031A532B, 031A533A, 031A533B, 031A534A, 031A535A, 031A537A, 031A537B, 031A537C, 031A537D, 031A538A). z"
  },
  {
    "objectID": "introduction-to-ancient-dna.html#introduction",
    "href": "introduction-to-ancient-dna.html#introduction",
    "title": "4¬† Introduction to Ancient DNA",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThis chapter introduces you to ancient DNA and the enormous technological changes that have taken place since the field‚Äôs origins in 1984. Starting with the quagga and proceeding to microbes, we discuss where ancient microbial DNA can be found in the archaeological record and examime how ancient DNA is defined by its condition, not by a fixed age. We next cover genome basics and take an in-depth look at the way DNA degrades over time. We detail the fundamentals of DNA damage, including the specific chemical processes that lead to DNA fragmentation and C-&gt;T miscoding lesions. We then demystify the DNA damage ‚Äúsmile plot‚Äù and explain the how the plot‚Äôs symmetry or asymmetry is related to the specific enzymes used to repair DNA during library construction. We discuss how DNA damage is and is not clock-like, how to interpret and troubleshoot DNA damage plots, and how DNA damage patters can be used to authenticate ancient samples, specific taxa, and even sequences. We cover laboratory strategies for removing or reducing damage for greater accuracy for genotype calling, and we discuss the pros and cons of single-stranded library protocols. We then take a closer look at proofreading and non-proofreading polymerases and note key steps in NGS library preparation during which enzyme selection is critical in ancient DNA studies. Finally, we examine the big picture of why DNA damage matters in ancient microbial studies, and its effects on taxonomic identification of sequences, accurate genome mapping, and metagenomic assembly."
  },
  {
    "objectID": "introduction-to-ancient-dna.html#lecture",
    "href": "introduction-to-ancient-dna.html#lecture",
    "title": "4¬† Introduction to Ancient DNA",
    "section": "4.2 Lecture",
    "text": "4.2 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-ancient-dna.html#questions-to-think-about",
    "href": "introduction-to-ancient-dna.html#questions-to-think-about",
    "title": "4¬† Introduction to Ancient DNA",
    "section": "4.3 Questions to think about",
    "text": "4.3 Questions to think about\n\nWhat is ancient DNA?\nWhere do we find ancient DNA from microbes?\nHow does DNA degrade?\nHow do I interpret a DNA damage plot?\nHow is DNA damage used to authenticate ancient genomes and samples?\nWhat methods are available for managing DNA damage?\nHow does DNA damage matter for my analyses?"
  },
  {
    "objectID": "introduction-to-metagenomics.html#introduction",
    "href": "introduction-to-metagenomics.html#introduction",
    "title": "5¬† Introduction to Metagenomics",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis chapter introduces you to the basics of metagenomics, with an emphasis on tools and approaches that are used to study ancient metagenomes. We begin by covering the basic terminology used in metagenomics and microbiome research and discuss how the field has changed over time. We examine the species concept for microbes and challenges that arise in classifying microbial species with respect to taxonomy and phylogeny. We then proceed to taxonomic profiling and discuss the pros and cons of different taxonomic profilers. Afterwards, we explain how to estimate preservation in ancient metagenomic samples and how to clean up your datasets and remove contaminants. Finally, we discuss strategies for exploring and comparing the ecological diversity in your samples, including different strategies for data normalization, distance calculation, and ordination."
  },
  {
    "objectID": "introduction-to-metagenomics.html#lecture",
    "href": "introduction-to-metagenomics.html#lecture",
    "title": "5¬† Introduction to Metagenomics",
    "section": "5.2 Lecture",
    "text": "5.2 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#introduction",
    "href": "introduction-to-microbial-genomics.html#introduction",
    "title": "6¬† Introduction to Microbial Genomics",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nThe field of microbial genomics aims at the reconstruction and comparative analyses of genomes for gaining insights into the genetic foundation and evolution of various functional aspects such as virulence mechanisms in pathogens.\nIncluding data from ancient samples into this comparative assessment allows for studying these evolutionary changes through time. This, for example, provides insights into the emergence of human pathogens and their development in conjunction with human cultural transitions.\nIn this lecture I will provide examples for how to utilise data from ancient genomes in comparative studies of human pathogens and today‚Äôs practical sessions will highlight methodologies for the reconstruction of microbial genomes."
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#lecture",
    "href": "introduction-to-microbial-genomics.html#lecture",
    "title": "6¬† Introduction to Microbial Genomics",
    "section": "6.2 Lecture",
    "text": "6.2 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-evolutionary-biology.html#introduction",
    "href": "introduction-to-evolutionary-biology.html#introduction",
    "title": "7¬† Introduction to Evolutionary Biology",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nPathogen genome data are an invaluable source of information about the evolution and spread of these organisms. This chapter will focus on molecular phylogenetic methods and the insight that they can reveal from improving our understanding of ancient evolution to the epidemiological dynamics of current outbreaks.\nThe first section will introduce phylognenetic trees and a set of core terms and concepts for their interpretation. Next, it will focus on some of the most popular approaches to inferring phylogenetic trees; those based on genetic distance, maximum likelihood, and Bayesian inference. These methods carry important considerations regarding the process that generated the data, computational capability, and data quality, all of which will be discussed here. Finally, we will direct our attention to examples of analyses of ancient and modern pathogens (e.g.¬†Yersinia pestis, Hepatitis B virus, SARS-CoV-2) and critically assess appropriate choice of models and methods.\n\n7.1.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "introduction-to-metagenomics.html#questions-to-think-about",
    "href": "introduction-to-metagenomics.html#questions-to-think-about",
    "title": "5¬† Introduction to Metagenomics",
    "section": "5.3 Questions to think about",
    "text": "5.3 Questions to think about\n\nWhat is a metagenome? a microbiota? a microbiome?\nWhat is ancient metagenomics?\nWhat challenges do DNA degradation and sample decay pose for ancient metagenomics\nHow do you find out ‚Äúwho‚Äôs there‚Äù in your samples?\nHow do alignment based and k-mer based taxonomic profilers differ? What are the advantages and disadvantages of each?\nWhy does database selection matter?\nHow do you estimate the preservation and integrity of your ancient metagenome?\nWhat are tools you can use to identify poorly preserved samples and remove contaminant taxa?\nWhat aspects of diversity are important in investigating microbial communities?\nWhich distance metrics are commonly used to compare the beta-diversity of microbial communities and why? What are some advantages and disadvantages to these different approaches?"
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "href": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "title": "6¬† Introduction to Microbial Genomics",
    "section": "6.3 Questions to think about",
    "text": "6.3 Questions to think about"
  },
  {
    "objectID": "introduction-to-git.html#introduction",
    "href": "introduction-to-git.html#introduction",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nAs the size and complexity of metagenomic analyses continues to expand, effectively organizing and tracking changes to scripts, code, and even data, continues to be a critical part of ancient metagenomic analyses. Furthermore, this complexity is leading to ever more collaborative projects, with input from multiple researchers.\nIn this chapter, we will introduce ‚ÄòGit‚Äô, an extremely popular version control system used in bioinformatics and software development to store, track changes, and collaborate on scripts and code. We will also introduce, GitHub, a cloud-based service for Git repositories for sharing data and code, and where many bioinformatic tools are stored. We will learn how to access and navigate course materials stored on GitHub through the web interface as well as the command line, and we will create our own repositories to store and share the output of upcoming sessions."
  },
  {
    "objectID": "introduction-to-git.html#ssh-setup",
    "href": "introduction-to-git.html#ssh-setup",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.2 SSH setup",
    "text": "11.2 SSH setup\nTo begin, you will set up an SSH key to facilitate easier authentication when transferring data between local and remote repositories. In other words, follow this section of the tutorial so that you never have to type in your github password again! Begin by activating the conda environment for this section (see Preparation above).\nconda activate git-eager\nNext, generate your own ssh key, replacing the email below with your own address.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nI recommend saving the file to the default location and skipping passphrase setup. To do this, simply press enter without typing anything.\nYou should now (hopefully!) have generated an ssh key. To check that it worked, run the following commands to list the files containing your public and private keys and check that the ssh program is running.\ncd ~/.ssh/\nls id*\neval \"$(ssh-agent -s)\"\nNow you need to give ssh your key to record:\nssh-add ~/.ssh/id_ed15519\nNext, open your webbrowser and navigate to your github account. Go to settings -&gt; SSH & GPG Keys -&gt; New SSH Key. Give you key a title and paste the public key that you just generated on your local machine.\ncat ~/.ssh/id_ed15519\nFinally, press Add SSH key. To check that it worked, run the following command on your local machine. You should see a message telling you that you‚Äôve successfully authenticated.\nssh -T git@github.com\nFor more information about setting up the SSH key, including instructions for different operating systems, check out github‚Äôs documentation: https://docs.github.com/es/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent."
  },
  {
    "objectID": "introduction-to-git.html#the-only-6-commands-you-really-need-to-know",
    "href": "introduction-to-git.html#the-only-6-commands-you-really-need-to-know",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.3 The only 6 commands you really need to know",
    "text": "11.3 The only 6 commands you really need to know\nNow that you have set up your own SSH key, we can begin working on some version controlled data! Navigate to your github homepage and create a new repository. You can choose any name for your new repo (including the default). Add a README file, then select Create Repository.\n  \n\n\n\n\n\n\nNote\n\n\n\nFor the remainder of the session, replace the name of my repository (vigilant-octo-journey) with your own repo name.\n\n\n\n\nChange into the directory where you would like to work, and let‚Äôs get started! First, we will learn to clone a remote repository onto your local machine. Navigate to your new repo, select the Code dropdown menu, select SSH, and copy the address as shown below.\n\nBack at your command line, clone the repo as follows:\ngit clone git@github.com:meganemichel/vigilant-octo-journey.git\nNext, let‚Äôs add a new or modified file to our ‚Äòstaging area‚Äô on our local machine.\ncd vigilant-octo-journey\necho \"test_file\" &gt; file_A.txt\necho \"Just an example repo\" &gt;&gt; README.md\ngit add file_A.txt\nNow we can check what files have been locally changed, staged, etc. with status.\ngit status\nYou should see that file_A.txt is staged to be committed, but README.md is NOT. Try adding README.md and check the status again.\nNow we need to package or save the changes into a commit with a message describing the changes we‚Äôve made. Each commit comes with a unique hash ID and will be stored forever in git history.\ngit commit -m \"Add example file\"\nFinally, let‚Äôs push our local commit back to our remote repository.\ngit push\nWhat if we want to download new commits from our remote to our local repository?\ngit pull\nYou should see that your repository is already up-to-date, since we have not made new changes to the remote repo. Let‚Äôs try making a change to the remote repository‚Äôs README file (as below). Then, back on the command line, pull the repository again."
  },
  {
    "objectID": "introduction-to-git.html#working-collaboratively",
    "href": "introduction-to-git.html#working-collaboratively",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.4 Working collaboratively",
    "text": "11.4 Working collaboratively\nGithub facilitates simultaneous work by small teams through branching, which generates a copy of the main repository within the repository. This can be edited without breaking the ‚Äòmaster‚Äô version. First, back on github, make a new branch of your repository.\n\nFrom the command line, you can create a new branch as follows:\ngit switch -c new_branch\nTo switch back to the main branch, use\ngit switch main\nNote that you must commit changes for them to be saved to the desired branch!"
  },
  {
    "objectID": "introduction-to-git.html#pull-requests",
    "href": "introduction-to-git.html#pull-requests",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.5 Pull requests",
    "text": "11.5 Pull requests\nA Pull request (aka PR) is used to propose changes to a branch from another branch. Others can comment and make suggestinos before your changes are merged into the main branch. For more information on creating a pull request, see github‚Äôs documentation: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request."
  },
  {
    "objectID": "introduction-to-git.html#questions-to-think-about",
    "href": "introduction-to-git.html#questions-to-think-about",
    "title": "11¬† Introduction to Git(Hub)",
    "section": "11.6 Questions to think about",
    "text": "11.6 Questions to think about\n\nWhy is using a version control software for tracking data and code important?\nHow can using Git(Hub) help me to collaborate on group projects?"
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#abstract",
    "href": "introduction-to-ancientmetagenomedir.html#abstract",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.1 Abstract",
    "text": "12.1 Abstract\n\n\nFinding relevant comparative data for your ancient metagenomic analysis is not trivial. While palaeogenomicists are very good at uploading their raw sequencing data to large sequencing data repositories such as the EBI‚Äôs ENA or NCBI‚Äôs SRA archives in standardised file formats, these files often have limited metadata. This often makes it difficult for researchers to search for and download relevent published data they wish to use use to augment their own analysis.\nAncientMetagenomeDir is a community project from the SPAAM community to make ancient metagenomic data more accessible. We curate a list of standardised metadata of all published ancient metagenomic samples and libraries, hosted on GitHub. In this chapter we will go through how to use the AncientMetagenomeDir repository and associated tools to find and download data for your own analyses. We will also discuss important things to consider when publishing your own data to make it more accessible for other researchers."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#introduction",
    "href": "introduction-to-ancientmetagenomedir.html#introduction",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.2 Introduction",
    "text": "12.2 Introduction\nIn most bioinformatic projects, we need to include publicly available comparative data to expand or compare our newly generated data with.\nIncluding public data can benefit ancient metagenomic studies in a variety of ways. It can help increase our sample sizes (a common problem when dealing with rare archaeological samples) - thus providing stronger statistical power. Comparison with a range of previously published data of different preservational levels can allow an estimate on the quality of the new samples. When considering solely (re)using public data, we can consider that this can also spawn new ideas, projects, and meta analyses to allow further deeper exploration of ancient metagenomic data (e.g., looking for correlations between various environmental factors and preservation).\nFortunately for us, genomicists and particularly palaeogenomicists have been very good at uploading raw sequencing data to well-established databases.\nIn the vast majority of cases you will be able to find publically available sequencing data on the INSDC association of databases, namely the EBI‚Äôs European Nucleotide Archive (ENA), and NCBI or DDBJ‚Äôs Sequence Read Archives (SRA). However, you may in some cases find ancient metagenomic data on institutional FTP servers, domain specific databases (e.g.¬†OAGR), Zenodo, Figshare, or GitHub.\nBut while the data is publicly available, we need to ask whether it is ‚ÄòFAIR‚Äô."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#finding-ancient-metagenomic-data",
    "href": "introduction-to-ancientmetagenomedir.html#finding-ancient-metagenomic-data",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.3 Finding Ancient Metagenomic Data",
    "text": "12.3 Finding Ancient Metagenomic Data\nFAIR principles were defined by researchers, librarians, and industry in 2016 to improve the quality of data uploads - primarily by making data uploads more ‚Äòmachine readable‚Äô. FAIR standards for:\n\nFindable\nAccessible\nInteroperable\nReproducible\n\nAnd when we consider ancient metagenomic data, we are pretty close to this. Sequencing data is in most cases accessible (via the public databases like ENA, SRA), interoperable and reproducible because we use field standard formats such as FASTQ or BAM files. However Findable remains an issue.\nThis is because the metadata about each data file is dispersed over many places, and very often not with the data files themselves.\nIn this case I am referring to metadata such as: What is the sample‚Äôs name? How old is it? Where is it from? Which enzymes were used for library construction? What sequencing machine was this library sequenced on?\nTo find this information about a given data file, you have to search many places (main text, supplementary information, the database itself), for different types of metadata (as authors report different things), and also in different formats (text, tables, figures.\nThis very heterogenous landscape makes it difficult for machines to index all this information (if at all), and thus means you cannot search for the data you want to use for your own reserch in online search engines."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#ancientmetagenomedir",
    "href": "introduction-to-ancientmetagenomedir.html#ancientmetagenomedir",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.4 AncientMetagenomeDir",
    "text": "12.4 AncientMetagenomeDir\nThis is where the SPAAM community project AncientMetagenomeDir comes in. AncientMetagenomeDir is a resource of lists of metadata of all publishing and publically available ancient metagenomes and microbial genome-level enriched samples.\nBy aggregating and standardising metadata and accession codes of ancient metagenomic samples, the project aims to make it easier for people to find comparative data for their own projects, as well as help track the field over time and facilitate meta analyses.\nCurrently the project is split over three main tables: host-associated metagenomes (e.g.¬†ancient microbiomes), host-associated single-genomes (e.g.¬†ancient pathogens), and environmental metagenomes (e.g.¬†lakebed cores or cave sediment sequences).\nThe repository already contains more than a thousand samples and span the entire globe and as far back as hundreds of thousands of years.\nTo make the lists of samples and their metadata as accessible and interoperable as possible, we utilise simple text (TSV - tab separate value) files - files that can be opened by pretty much all spreadsheet tools (e.g., Microsoft Office excel, LibreOffice Calc) and languages (R, Python etc.).\n\nCriticially, by standardising the recorded all metadata across all publications this makes it much easier for researchers to filter for particular time periods, geographical regions, or sample types of their interest - and then use the also recorded accession numbers to efficiently download the data.\nAt their core all different AncientMetagenomeDir tables must have at 6 minimum metadata sets:\n\nPublication information (doi)\nSample name(s)\nGeographic location (e.g.¬†country, coordinates)\nAge\nSample type (e.g.¬†bone, sediment, etc.)\nData Archive and accessions\n\nEach table then has additional columns depending on the context (e.g.¬†what time of microbiome is expected for host-associated metagenoes, or species name of the genome that was reconstructed).\nThe AncientMetagenomeDir project already has 3 releases, and will continued to be regularly updated as the community continues to submit new metadata of samples of new publications as they come out."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#further-improving-metadata-reporting-in-ancient-metagenomics",
    "href": "introduction-to-ancientmetagenomedir.html#further-improving-metadata-reporting-in-ancient-metagenomics",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.5 Further Improving Metadata Reporting in Ancient Metagenomics",
    "text": "12.5 Further Improving Metadata Reporting in Ancient Metagenomics\nHowever, for researchers, sample-level metadata likely will not include all the information that is needed to include and process public data in their own projects.\nThe SPAAM community have been busy over the last few months extending the types of metadata included in the AncientMetagenomeDir project, to include library level metadata.\nThis metadata includes things such as whether a given set of data files contain sequencing data sequenced on which platform, whether the libraries have undergone damage treatment in the lab, or whether the uploaded data contains all or only mapped reads.\nWe have also started a new project - a MIxS checklist currently entitled ‚ÄòMINAS‚Äô - which we aim to make the standard metadata reporting sheet for all ancient metagenomics and even for any ancient DNA sample. Such a checklist would be interegrated into services such as the ENA or SRA, and therefore would standardise metadata alongside the raw data, and make ancient metagenomic data much more findable with search engines.\nFinally, to make it easier for researchers who are not familiar with sequencing database infrastucture, we are in the process of building a new tool (something already in a usable state) called AMDirT. This allows a web browser-based GUI to filter and select data, and produce scripts for you to download all the selected data (without having to go to the databases themselves).\nThis is something we are going to try out now!"
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#running-amdirt",
    "href": "introduction-to-ancientmetagenomedir.html#running-amdirt",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.6 Running AMDirT",
    "text": "12.6 Running AMDirT\n\nFirst, we will need to activate a conda environment, and then install the latest development version of the tool for you.\n\n‚ö†Ô∏è this tutorial will require a web-browser! Make sure to run on your local laptop/PC or on a server with X11 forwarding\n\nOpen your terminal, and run the following two commands:\nconda activate git-eager\npip install --upgrade --force-reinstall git+https://github.com/SPAAM-community/AMDirT.git@dev\nOnce that (hopefully) installs correctly, we can load the tool by running\nAMDirT filter\nYour web browser should now load, and you should see a two panel page.\n\nUnder Select a table use the dropdown menu to select ‚Äòancientsinglegenome-hostassociated‚Äô.\nYou should then see a table, pretty similar what you are familiar with with spreadsheet tools such as Microsoft Excel or LibreOffice calc.\n\nTo navigate, you can scroll down to see more rows, and press shift and scroll to see more columns, or use click on a cell and use your arrow keys (‚¨Ü,‚¨á,‚¨Ö,‚û°) to move around the table.\nYou can reorder columns by clicking on the column name, and also filter by pressing the little ‚Äòburger‚Äô icon that appears on the column header when you hover over a given column.\nAs an exercise, we will try filtering to a particular set of samples, then generate some download scripts, and download the files.\nFirst, filter the project_name column to ‚ÄòKocher2021‚Äô.\n\nThen scroll to the right, and filter the geo_loc_name to ‚ÄòUnited Kingdom‚Äô.\n\nYou should be left with 4 rows.\nFinally, scroll back to the first column and tick the boxes of these four samples.\n\nOnce you‚Äôve selected the samples you want, you can press Validate selection. You should then see a series loading-spinner, and new buttons should appear!\n\nYou should have three main buttons:\n\nDownload Curl sample download script\nDownload nf-core/eager input TSV\nDownload Citations as BibText\n\nThe first button is for generating a download script that will allow you to immediately download all sequencing data of the samples you selected. The second button is a pre-configured input file for use in the nf-core/eager ancient DNA pipeline, and finally, the third button generates a text file with (in most cases) all the citations of the data you downloaded, in a format accepted by most reference/citation managers.\nIt‚Äôs important to note you are not necessarily restricted to Curl for downloading the data, or nf-core/eager for running the files. AMDirT aims to add support for whatever tools or pipelines requested by the community. For example, an already supported download alternative is with the nf-core/fetchNGS pipeline. You can select these using the drop-down menus on the left hand-side.\nPress the three buttons to make sure you download the files. And once this is done, you can close the tab of the web browser, and in the terminal you can press ctrl + c to shutdown the tool."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#inspecting-amdirt-output",
    "href": "introduction-to-ancientmetagenomedir.html#inspecting-amdirt-output",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.7 Inspecting AMDirT Output",
    "text": "12.7 Inspecting AMDirT Output\nLets look at the files that AMDirT has generated for you.\nFirst you should cd into the directory that your web browser downloaded the files into (e.g.¬†cd ~/Downloads/), then look inside the directory. You should see the following three files\n$ ls\nancientMetagenomeDir_curl_download_script.sh\nancientMetagenomeDir_citations.bib\nancientMetagenomeDir_eager_input.csv\nWe can simple run cat on each file to look inside. If you run cat on the curl download script, you should see a series of curl commands with the correct ENA links for you for each of the samples you wish to download.\n$ cat ancientMetagenomeDir_curl_download_script.sh\n#!/usr/bin/env bash\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz -o ERR6053619.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz -o ERR6053618.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz -o ERR6053675.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz -o ERR6053686.fastq.gz\nBy providing this script for you, AMDirT facilitates fast download of files of interest by replacing the one-by-one download commands for each sample with a single command!\n$ bash ancientMetagenomeDir_curl_download_script.sh\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz -o ERR6053619.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz -o ERR6053618.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz -o ERR6053675.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz -o ERR6053686.fastq.gz\nRunning this command should result in progress logs of the downloading of the data of the four selected samples!\nOnce the four samples are downloaded, AMDirT then facilitates fast processing of the data, as the eager script can be given directly to nf-core/eager as input. Importantly by including the library metadata (mentioned above), researchers can leverage the complex automated processing that nf-core/eager can perform when given such relevant metadata.\n$ cat ancientMetagenomeDir_eager_input.csv\nSample_Name Library_ID  Lane    Colour_Chemistry    SeqType Organism    Strandedness    UDG_Treatment   R1  R2  BAM\nI0157   ERR6053618  0   4   SE  Homo sapiens    double  unknown ERX5692504_ERR6053618.fastq.gz  NA  NA\nI0161   ERR6053619  0   4   SE  Homo sapiens    double  unknown ERX5692505_ERR6053619.fastq.gz  NA  NA\nOAI017  ERR6053675  0   4   SE  Homo sapiens    double  half    ERX5692561_ERR6053675.fastq.gz  NA  NA\nSED009  ERR6053686  0   4   SE  Homo sapiens    double  half    ERX5692572_ERR6053686.fastq.gz  NA  NA\nFinally, we can look into the citations file which will provide you with the citation information of all the downloaded data and AncientMetagenomeDir itself.\n\n‚ö†Ô∏è the contents of this file is reliant on indexing of publications on CrossRef. In some cases not all citations will be present, so this should be double checked!\n\n$ cat ancientMetagenomeDir_citations.bib\n\n@article{Fellows_Yates_2021,\n    doi = {10.1038/s41597-021-00816-y},\n    url = {https://doi.org/10.1038%2Fs41597-021-00816-y},\n    year = 2021,\n    month = {jan},\n    publisher = {Springer Science and Business Media {LLC}},\n    volume = {8},\n    number = {1},\n    author = {James A. Fellows Yates and Aida Andrades Valtue{\\~{n}}a and {\\AA}shild J. V{\\aa}gene and\n    Becky Cribdon and Irina M. Velsko and Maxime Borry and Miriam J. Bravo-Lopez and Antonio Fernandez-Guerra\n    and Eleanor J. Green and Shreya L. Ramachandran and Peter D. Heintzman and Maria A. Spyrou and Alexander\n    H√ºbner and Abigail S. Gancz and Jessica Hider and Aurora F. Allshouse and Valentina Zaro and Christina Warinner},\n    title = {Community-curated and standardised metadata of published ancient metagenomic samples with {AncientMetagenomeDir}},\n    journal = {Scientific Data}\n}\nThis file can be easily loaded into most reference managers and then have all the citations quickly added to your manuscripts."
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#git-practise",
    "href": "introduction-to-ancientmetagenomedir.html#git-practise",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.8 Git Practise",
    "text": "12.8 Git Practise\nA critical factor of AncientMetagenomeDir is that it is community-based. The community curates all new submissions to the repository, and this all occurs with Git.\nThe data is hosted and maintained on GitHub - new publications are evaluated on issues, submissions created on branches, made by pull requests, and PRs reviewed by other members of the community.\nYou can see the workflow in the image below from the AncientMetageomeDir publication, and read more about the workflow on the AncientMetagenomeDir wiki\n\nThis means we can also use this repository to practise git!\nYour task (with git terms removed):\n\nMake a ‚Äòcopy‚Äô the jfy133/AncientMetagenomeDir repository to your account\n‚ÄòDownload‚Äô the copied repo to your local machine\n‚ÄòChange‚Äô to the dev branch\nModify ‚Äòancientsinglegenome-hostassociated_samples.tsv‚Äô\n\nClick here to get some example data to copy in to the end of the TSV file\n\n‚ÄòSend‚Äô back to Git(Hub)\nOpen a ‚Äòrequest‚Äô adding changes to the original repo\n\nMake sure to put ‚ÄòSummer school‚Äô in the title of the ‚ÄòRequest‚Äô\n\n\n\n\nClick me to reveal the correct terminology\n\n\nFork the jfy133/AncientMetagenomeDir repository to your account\nClone the copied repo to your local machine\nSwitch to the dev branch\nModify ‚Äòancientsinglegenome-hostassociated_samples.tsv‚Äô\n\nClick here to get some example data to copy in to the end of the TSV file\n\nCommit and Push back to your Fork on Git(Hub)\nOpen a Pull Request adding changes to the original jfy133/AncientMetagenomeDir repo\n\nMake sure to put ‚ÄòSummer school‚Äô in the title of the pull request"
  },
  {
    "objectID": "introduction-to-ancientmetagenomedir.html#summary",
    "href": "introduction-to-ancientmetagenomedir.html#summary",
    "title": "12¬† Introduction to AncientMetagenomeDir",
    "section": "12.9 Summary",
    "text": "12.9 Summary\n\nReporting of metadata messy! Consider when publishing your own work!\n\nUse AncientMetagenomeDir as a template\n\nUse AncientMetagenomeDir and AMDirT (beta) to rapidly find public ancient metagenomic data\nContribute to AncientMetagenomeDir with git\n\nCommunity curated!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#abstract",
    "href": "ancient-metagenomic-pipelines.html#abstract",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.1 Abstract",
    "text": "13.1 Abstract\nAnalyses in the field of ancient DNA are growing, both in terms of the number of samples processed and in the diversity of our research questions and analytical methods. Computational pipelines are a solution to the challenges of big data, helping researchers to perform analyses efficiently and in a reproducible fashion. Today we will introduce nf-core/eager, one of several pipelines designed specifically for the preprocessing, analysis, and authentication of ancient next-generation sequencing data.\nIn this practical session we will learn how to perform basic analyses with nf-core/eager, starting from raw data and performing preprocessing, alignment, and genotyping of several Yersinia pestis-positive samples. We will gain an appreciation of the diversity of analyses that can be performed within nf-core eager, as well as where to find additional information for customizing your own nf-core/eager runs. Finally, we will learn how to use nf-core/eager to evaluate the quality and authenticity of our ancient samples. After this session, you will be ready to strike out into the world of nf-core/eager and build your own analyses from scratch!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#introduction",
    "href": "ancient-metagenomic-pipelines.html#introduction",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.2 Introduction",
    "text": "13.2 Introduction\nA pipeline is a series of linked computational steps, where the output of one process becomes the input of the next. Pipelines are critical for managing the huge quantities of data that are now being generated regularly as part of ancient DNA analyses. Today we will discuss one option for managing computational analyses of ancient next-generation sequencing datasets, nf-core/eager. Keep in mind that other tools, like the Paleomix pipeline, can also be used for similar applications."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "href": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.3 What is nf-core/eager?",
    "text": "13.3 What is nf-core/eager?\nnf-core/eager is a computational pipeline specifically designed for preprocessing and analysis of ancient DNA data. It is a reimplementation of the previously published EAGER (Efficient Ancient Genome Reconstruction) pipeline (Peltzer et al. 2016) using Nextflow. The nf-core/eager pipeline was designed with the following aims in mind:\n\nPortability- In order for our analyses to be reproducible, others should be able to easily implement our computational pipelines. nf-core/eager is highly portable, providing easy access to pipeline tools and facilitating use across multiple platforms. nf-core eager utilizes Docker, Conda, and Singularity for containerization, enabling distrubition of the pipeline in a self-contained bundle containing all the code, packages, and libraries needed to run it.\nReproducibility- nf-core/eager uses custom configuration profiles to specify both HPC-level parameters and analyses-specific options. These profiles can be shared alongside your publication, making it easier for others to reproduce your methodology!\nNew Tools- Finally, nf-core/eager includes additional, novel methods and tools for analysis of ancient DNA data that were not included in previous versions. This is especially good news for folks interested in microbial sciences, who can take advantage of new analytical pathways for metagenomic analysis and pathogen screening."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "href": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.4 Steps in the pipeline",
    "text": "13.4 Steps in the pipeline\n\nA detailed description of steps in the pipeline is available as part of nf-core/eager‚Äôs extensive documentation. For more information, check out the usage documentation here.\nBriefly, nf-core/eager takes standard input file types that are shared across the genomics field, including raw fastq files, aligned reads in bam format, and a reference fasta. nf-core/eager can perform preprocessing of this raw data, including adapter clipping, read merging, and quality control of adapter-trimmed data. Note that input files can be specified using wildcards OR a standardized tsv format file; the latter facilitates streamlined integration of multpile data types within a single EAGER run! More on this later.\nnf-core/eager facilitates mapping using a variety of field-standard alignment tools with configurable parameters. An exciting new addition in nf-core/eager also enables analysis of off-target host DNA for all of you metagenomics folks out there. Be sure to check out the functionality available for metagenomic profiling (blue route in the ‚Äòtube map‚Äô above).\nnf-core/eager incorporates field-standard quality control tools designed for use with ancient DNA so that you can easily evaluate the success of your experiments. Multiple genotyping approaches and additional analyses are available depending on your input datatype, organism, and research questions. Importantly, all of these processes generate data that we need to compile and analyze in a coherent way. nf-core eager uses MultiQC. to create an integrated html report that summarizes the output/results from each of the pipeline steps. Stay tuned for the practical portion of the walkthrough!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "href": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.5 How to build an nf-core/eager command: A practical introduction",
    "text": "13.5 How to build an nf-core/eager command: A practical introduction\nFor the practical portion of the walkthrough, we will utilize sequencing data from four aDNA libraries, which you should have already downloaded from NCBI. If not, please see the Preparation section above.\nThese four libraries come from from two ancient individuals, GLZ002 and KZL002. GLZ002 comes from the Neolithic Siberian site of Glazkovskoe predmestie adn was radiocarbon dated to 3081-2913 calBCE. KZL002 is an Iron Age individual from Kazakhstan, radiocarbon dated to 2736-2457 calBCE. Both individuals were infected with the so-called ‚ÄòStone Age Plague‚Äô of Yersinia pestis, and libraries from these individuals were processed using hybridization capture to increase the number of Y. pestis sequences available for analysis.\nOur aims in the following tutorial are to:\n\nPreprocess the fastq files by trimming adapters and merging paired-end reads\nAlign reads to the Y. pestis reference and compute the endogenous DNA percentage\nFilter the aligned reads to remove host DNA\nRemove duplicate reads for accurate coverage estimation and genotyping\nMerge data by sample and perform genotyping on the combined dataset\nReview quality control data to evaluate the success of the previous steps\n\nLet‚Äôs get started!\nFirst, activate the conda environment that we downloaded during setup:\nconda activate git-eager\nNext, download the latest version of the nf-core/eager repo (or check for updates if you have a previously-installed version):\nnextflow pull nf-core/eager\nFinally, we will build our eager command:\nnextflow run nf-core/eager \\ #Tells nextflow to execute the EAGER pipeline\n-r 2.4.5 -ds1l \\ #Specifies which pipeline and Nextflow versions to run for reproducibility\n-profile conda  \\ #Profiles configure your analysis for specific computing environments/analyses\n--fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna \\ #Specify reference in fasta format\n--input ancientMetagenomeDir_eager_input.tsv \\ #Specify input in tsv format or wildcards\n--run_bam_filtering --bam_unmapped_type fastq \\ #Filter unmapped reads and save in fastq format\n--run_genotyping --genotyping_tool ug --gatk_ug_out_mode EMIT_ALL_SITES \\ #Run genotyping with the GATK UnifiedGenotyper\n--run_bcftools_stats #Generate variant calling statistics\nFor full parameter documentation, click here.\nAnd now we wait‚Ä¶"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "href": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.6 Top Tips for nf-core/eager success",
    "text": "13.6 Top Tips for nf-core/eager success\n\nScreen sessions\n\nDepending on your input data, infrastructre, and analyses, running nf-core/eager can take hours or even days. To avoid crashed due to loss of power or network connectivity, try running nf-core/eager in a screen or tmux session:\nscreen -R eager\n\nMultiple ways to supply input data\n\nIn this tutorial, a tsv file to specify our input data files and formats. This is a powerful approach that allows nf-core eager to intelligently apply analyses to certain files only (e.g.¬†merging for paired-end but not single-end libraries). Check out the contents of our tsv input file using the following command:\ncat ancientMetagenomeDir_eager_input.tsv\nInputs can also be specified using wildcards, which can be useful for fast analyses with simple input data types (e.g.¬†same sequencing configuration, file location, etc.).\nnextflow run nf-core/eager -r 2.4.5 -ds1l -profile conda --fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna\n--input \"data/*fastq.gz\" &lt;...&gt;\nSee the online nf-core/eager documentation for more details.\n\nGet your MultiQC report via email\n\nIf you have GNU mail or sendmail set up on your system, you can add the following flag to send the MultiQC html to your email upon run completion:\n--email \"your_address@something.com\"\n\nCheck out the EAGER GUI\n\nFor folks who might be less comfortable with the command line, check out the nf-core/eager GUI! The GUI also provides a full list of options with short explanations for those interested in learning more about what the pipeline can do.\n\nWhen something fails, all is not lost!\n\nWhen individual jobs fail, nf-coreager will try to automatically resubmit that job with increased memory and CPUs (up to two times per job). When the whole pipeline crashes, you can save time and computational resources by resubmitting with the -resume flag. nf-core/eager will retrieve cached results from previous steps as long as the input is the same.\n\nMonitor your pipeline in real time with the Nextflow Tower\n\nRegular users may be interested in checking out the Nextflow Tower, a tool for monitoring the progress of Nextflow pipelines in real time. Check here for more information."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "href": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "title": "13¬† Ancient Metagenomic Pipelines",
    "section": "13.7 Questions to think about",
    "text": "13.7 Questions to think about\n\nWhy is it important to use a pipeline for genomic analysis of ancient data?\nHow can the design of the nf-core/eager pipeline help researchers comply with the FAIR princples for management of scientific data?\nWhat metrics do you use to evaluate the success/failure of ancient DNA sequencing experiments? How can these measures be evaluated when using nf-core/eager for data preprocessing and analysis?"
  },
  {
    "objectID": "introduction-to-r.html#abstract",
    "href": "introduction-to-r.html#abstract",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.1 Abstract",
    "text": "9.1 Abstract\nR is an interpreted programming language with a particular focus on data manipulation and analysis. It is very well established for scientific computing and supported by an active community developing and maintaining a huge ecosystem of software packages for both general and highly derived applications.\nIn this session we will explore how to use R for a simple, standard data science workflow. We will import, clean, and visualise context and summary data for and from our ancient metagenomics analysis workflow. On the way we will learn about the RStudio integrated development environment, dip into the basic logic and syntax of R and finally write some first useful code within the tidyverse framework for tidy, readable and reproducible data analysis.\nThis session will be targeted at beginners without much previous experience with R or programming and will kickstart your journey to master this powerful tool.\n\n\n\n\n\n\nNote\n\n\n\nThis session is typically ran held in parallel to the Introduction to Python and Pandas. Participants of the summer schools chose which to attend based on their prior experience. We recommend the introduction to R session if you have no experience with neither R nor Python."
  },
  {
    "objectID": "introduction-to-r.html#getting-started-for-this-workshop",
    "href": "introduction-to-r.html#getting-started-for-this-workshop",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.2 Getting started for this workshop",
    "text": "9.2 Getting started for this workshop\n\nActivate the relevant conda environment (don‚Äôt forget to deactivate it later!)\nconda activate r-python\nNavigate to\n/&lt;path&gt;/&lt;to&gt;/3b-1-introduction-to-r-and-the-tidyverse/spaam_r_tidyverse_intro_2h\nPull the latest changes in this Git repository\ngit pull\nOpen RStudio\nLoad the project with File &gt; Open Project...\nOpen the file presentation.Rmd in RStudio"
  },
  {
    "objectID": "introduction-to-r.html#table-of-contents",
    "href": "introduction-to-r.html#table-of-contents",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.3 Table of Contents",
    "text": "9.3 Table of Contents\n\nThe working environment\nLoading data into tibbles\nPlotting data in tibbles\nConditional queries on tibbles\nTransforming and manipulating tibbles\nCombining tibbles with join operations"
  },
  {
    "objectID": "introduction-to-r.html#the-working-environment",
    "href": "introduction-to-r.html#the-working-environment",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.2 The working environment",
    "text": "9.2 The working environment\n\n9.2.1 R, RStudio and the tidyverse\n\nR is a fully featured programming language, but it excels as an environment for (statistical) data analysis (https://www.r-project.org)\nRStudio is an integrated development environment (IDE) for R (and other languages): (https://www.rstudio.com/products/rstudio)\nThe tidyverse is a collection of R packages with well-designed and consistent interfaces for the main steps of data analysis: loading, transforming and plotting data (https://www.tidyverse.org)\n\nThis introduction works with tidyverse ~v1.3.0\nWe will learn about readr, tibble, ggplot2, dplyr, magrittr and tidyr\nforcats will be briefly mentioned\npurrr and stringr are left out"
  },
  {
    "objectID": "introduction-to-r.html#loading-data-into-tibbles",
    "href": "introduction-to-r.html#loading-data-into-tibbles",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.3 Loading data into tibbles",
    "text": "9.3 Loading data into tibbles\n\n9.3.1 Reading data with readr\n\nWith R we usually operate on data in our computer‚Äôs memory\nThe tidyverse provides the package readr to read data from text files into the memory\nreadr can read from our file system or the internet\nIt provides functions to read data in almost any (text) format:\nreadr::read_csv()   # .csv files\nreadr::read_tsv()   # .tsv files\nreadr::read_delim() # tabular files with an arbitrary separator\nreadr::read_fwf()   # fixed width files\nreadr::read_lines() # read linewise to parse yourself\nreadr automatically detects column types ‚Äì but you can also define them manually\n\n\n\n9.3.2 How does the interface of read_csv work\n\nWe can learn more about a function with ?. To open a help file: ?readr::read_csv\nreadr::read_csv has many options to specify how to read a text file\nread_csv(\nfile,                      # The path to the file we want to read\ncol_names = TRUE,          # Are there column names?\ncol_types = NULL,          # Which types do the columns have? NULL -&gt; auto\nlocale = default_locale(), # How is information encoded in this file?\nna = c(\"\", \"NA\"),          # Which values mean \"no data\"\ntrim_ws = TRUE,            # Should superfluous white-spaces be removed?\nskip = 0,                  # Skip X lines at the beginning of the file\nn_max = Inf,               # Only read X lines\nskip_empty_rows = TRUE,    # Should empty lines be ignored?\ncomment = \"\",              # Should comment lines be ignored?\nname_repair = \"unique\",    # How should \"broken\" column names be fixed\n...\n)\n\n\n\n9.3.3 What does readr produce? The tibble\nsample_table_path &lt;- \"/vol/volume/3b-1-introduction-to-r-and-the-tidyverse/ancientmetagenome-hostassociated_samples.tsv\"\nsample_table_url &lt;-\n\"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/samples/ancientmetagenome-hostassociated_samples.tsv\"\nsamples &lt;- readr::read_tsv(sample_table_url)\n\nThe tibble is a ‚Äúdata frame‚Äù, a tabular data structure with rows and columns\nUnlike a simple array, each column can have another data type\n\nprint(samples, n = 3)\n\n\n9.3.4 How to look at a tibble\nsamples          # Typing the name of an object will print it to the console\nstr(samples)     # A structural overview of an object\nsummary(samples) # A human-readable summary of an object\nView(samples)    # RStudio's interactive data browser\n\nR provides a very flexible indexing operation for data.frames and tibbles\nsamples[1,1]                         # Access the first row and column\nsamples[1,]                          # Access the first row\nsamples[,1]                          # Access the first column\nsamples[c(1,2,3),c(2,3,4)]           # Access values from rows and columns\nsamples[,-c(1,2)]                    # Remove the first two columns\nsamples[,c(\"site_name\", \"material\")] # Columns can be selected by name\ntibbles are mutable data structures, so their content can be overwritten\nsamples[1,1] &lt;- \"Cheesecake2015\"     # replace the first value in the first column"
  },
  {
    "objectID": "introduction-to-r.html#plotting-data-in-tibbles",
    "href": "introduction-to-r.html#plotting-data-in-tibbles",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.4 Plotting data in tibbles",
    "text": "9.4 Plotting data in tibbles\n\n9.4.1 ggplot2 and the ‚Äúgrammar of graphics‚Äù\n\nggplot2 offers an unusual, but powerful and logical interface\nThe following example describes a stacked bar chart\nlibrary(ggplot2) # Loading a library to use its functions without ::\n\nggplot(          # Every plot starts with a call to the ggplot() function\ndata = samples # This function can also take the input tibble\n) +              # The plot consists of functions linked with +\ngeom_bar(        # \"geoms\" define the plot layers we want to draw\nmapping = aes( # The aes() function maps variables to visual properties\n    x = publication_year, # publication_year -&gt; x-axis\n    fill = community_type # community_type -&gt; fill color\n)\n)\ngeom_*: data + geometry (bars) + statistical transformation (sum)\n\n\n\n9.4.2 ggplot2 and the ‚Äúgrammar of graphics‚Äù\n\nThis is the plot described above: number of samples per community type through time\nggplot(samples) +\ngeom_bar(aes(x = publication_year, fill = community_type))\n\n\n\n9.4.3 ggplot2 features many geoms\n(){width=55%}\n\nRStudio shares helpful cheatsheets for the tidyverse and beyond: https://www.rstudio.com/resources/cheatsheets\n\n\n\n9.4.4 scales control the behaviour of visual elements\n\nAnother plot: Boxplots of sample age through time\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age))\nThis is not well readable, because extreme outliers dictate the scale\n\n\n\n9.4.5 scales control the behaviour of visual elements\n\nWe can change the scale of different visual elements - e.g.¬†the y-axis\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age)) +\nscale_y_log10()\nThe log-scale improves readability\n\n\n\n9.4.6 scales control the behaviour of visual elements\n\n(Fill) color is a visual element of the plot and its scaling can be adjusted\nggplot(samples) +\ngeom_boxplot(aes(x = as.factor(publication_year), y = sample_age,\n                fill = as.factor(publication_year))) +\nscale_y_log10() + scale_fill_viridis_d(option = \"C\")\n\n\n\n9.4.7 Defining plot matrices via facets\n\nSplitting up the plot by categories into facets is another way to visualize more variables at once\nggplot(samples) +\ngeom_count(aes(x = as.factor(publication_year), y = material)) +\nfacet_wrap(~archive)\nUnfortunately the x-axis became unreadable\n\n\n\n9.4.8 Setting purely aesthetic settings with theme\n\nAesthetic changes like this can be applied as part of the theme\nggplot(samples) +\ngeom_count(aes(x = as.factor(publication_year), y = material)) +\nfacet_wrap(~archive) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n9.4.9 Exercise 1\n\nLook at the mtcars dataset and read up on the meaning of its variables\n\nVisualize the relationship between Gross horsepower and 1/4 mile time\n\nIntegrate the Number of cylinders into your plot\n\n\n\n\n9.4.10 Possible solutions 1\n\nLook at the mtcars dataset and read up on the meaning of its variables\n?mtcars\nVisualize the relationship between Gross horsepower and 1/4 mile time\nggplot(mtcars) + geom_point(aes(x = hp, y = qsec))\nIntegrate the Number of cylinders into your plot\nggplot(mtcars) + geom_point(aes(x = hp, y = qsec, color = as.factor(cyl)))"
  },
  {
    "objectID": "introduction-to-r.html#conditional-queries-on-tibbles",
    "href": "introduction-to-r.html#conditional-queries-on-tibbles",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.5 Conditional queries on tibbles",
    "text": "9.5 Conditional queries on tibbles\n\n9.5.1 Selecting columns and filtering rows with select and filter\n\nThe dplyr package includes powerful functions to subset data in tibbles based on conditions\ndplyr::select allows to select columns\ndplyr::select(samples, project_name, sample_age)   # reduce to two columns\ndplyr::select(samples, -project_name, -sample_age) # remove two columns\ndplyr::filter allows for conditional filtering of rows\ndplyr::filter(samples, publication_year == 2014)  # samples published in 2014\ndplyr::filter(samples, publication_year == 2014 |\n                    publication_year == 2018)  # samples from 2015 OR 2018\ndplyr::filter(samples, publication_year %in% c(2014, 2018)) # match operator: %in%\ndplyr::filter(samples, sample_host == \"Homo sapiens\" &\n                    community_type == \"oral\")  # oral samples from modern humans\n\n\n\n9.5.2 Chaining functions together with the pipe %&gt;%\n\nThe pipe %&gt;% in the magrittr package is a clever infix operator to chain data and operations\nlibrary(magrittr)\nsamples %&gt;% dplyr::filter(publication_year == 2014)\nIt forwards the LHS as the first argument of the function appearing on the RHS\nThat allows for sequences of functions (‚Äútidyverse style‚Äù)\nsamples %&gt;%\ndplyr::select(sample_host, community_type) %&gt;%\ndplyr::filter(sample_host == \"Homo sapiens\" & community_type == \"oral\") %&gt;%\nnrow() # count the rows\nmagrittr also offers some more operators, among which the extraction %$% is particularly useful\nsamples %&gt;%\ndplyr::filter(material == \"tooth\") %$%\nsample_age %&gt;% # extract the sample_age column as a vector\nmax()          # get the maximum of said vector\n\n\n\n9.5.3 Summary statistics in base R\n\nSummarising and counting data is indispensable and R offers all operations you would expect in its base package\nnrow(samples)              # number of rows in a tibble\nlength(samples$site_name)  # length/size of a vector\nunique(samples$material)   # unique elements of a vector\nmin(samples$sample_age)    # minimum\nmax(samples$sample_age)    # maximum\nmean(samples$sample_age)   # mean\nmedian(samples$sample_age) # median\nvar(samples$sample_age)    # variance\nsd(samples$sample_age)     # standard deviation\nquantile(samples$sample_age, probs = 0.75) # sample quantiles for the given probs\nmany of these functions can ignore missing values with an option na.rm = TRUE\n\n\n\n9.5.4 Group-wise summaries with group_by and summarise\n\nThese summary statistics are particular useful when applied to conditional subsets of a dataset\ndplyr allows such summary operations with a combination of group_by and summarise\nsamples %&gt;%\ndplyr::group_by(material) %&gt;%  # group the tibble by the material column\ndplyr::summarise(\n    min_age = min(sample_age),   # a new column: min age for each group\n    median_age = median(sample_age), # a new column: median age for each group\n    max_age = max(sample_age)    # a new column: max age for each group\n)\ngrouping can be applied across multiple columns\nsamples %&gt;%\ndplyr::group_by(material, sample_host) %&gt;% # group by material and host\ndplyr::summarise(\n    n = dplyr::n(),  # a new column: number of samples for each group\n    .groups = \"drop\" # drop the grouping after this summary operation\n)\n\n\n\n9.5.5 Sorting and slicing tibbles with arrange and slice\n\ndplyr allows to arrange tibbles by one or multiple columns\nsamples %&gt;% dplyr::arrange(publication_year)        # sort by publication year\nsamples %&gt;% dplyr::arrange(publication_year,\n                        sample_age)              # ... and sample age\nsamples %&gt;% dplyr::arrange(dplyr::desc(sample_age)) # sort descending on sample age\nSorting also works within groups and can be paired with slice to extract extreme values per group\nsamples %&gt;%\ndplyr::group_by(publication_year) %&gt;%       # group by publication year\ndplyr::arrange(dplyr::desc(sample_age)) %&gt;% # sort by age within (!) groups\ndplyr::slice_head(n = 2) %&gt;%                # keep the first two samples per group\ndplyr::ungroup()                            # remove the still lingering grouping\nSlicing is also the relevant operation to take random samples from the observations in a tibble\nsamples %&gt;% dplyr::slice_sample(n = 20)\n\n\n\n9.5.6 Exercise 2\n\nDetermine the number of cars with four forward gears (gear) in the mtcars dataset\n\nDetermine the mean 1/4 mile time (qsec) per Number of cylinders (cyl) group\n\nIdentify the least efficient cars for both transmission types (am)\n\n\n\n\n9.5.7 Possible solutions 2\n\nDetermine the number of cars with four forward gears (gear) in the mtcars dataset\nmtcars %&gt;% dplyr::filter(gear == 4) %&gt;% nrow()\nDetermine the mean 1/4 mile time (qsec) per Number of cylinders (cyl) group\nmtcars %&gt;% dplyr::group_by(cyl) %&gt;% dplyr::summarise(qsec_mean = mean(qsec))\nIdentify the least efficient cars for both transmission types (am)\n#mtcars3 &lt;- tibble::rownames_to_column(mtcars, var = \"car\") %&gt;% tibble::as_tibble()\nmtcars %&gt;% dplyr::group_by(am) %&gt;% dplyr::arrange(mpg) %&gt;% dplyr::slice_head()"
  },
  {
    "objectID": "introduction-to-r.html#transforming-and-manipulating-tibbles",
    "href": "introduction-to-r.html#transforming-and-manipulating-tibbles",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.6 Transforming and manipulating tibbles",
    "text": "9.6 Transforming and manipulating tibbles\n\n9.6.1 Renaming and reordering columns and values with rename, relocate and recode\n\nColumns in tibbles can be renamed with dplyr::rename and reordered with dplyr::relocate\nsamples %&gt;% dplyr::rename(country = geo_loc_name) # rename a column\nsamples %&gt;% dplyr::relocate(site_name, .before = project_name) # reorder columns\nValues in columns can also be changed with dplyr::recode\nsamples$sample_host %&gt;% dplyr::recode(`Homo sapiens` = \"modern human\")\nR supports explicitly ordinal data with factors, which can be reordered as well\nfactors can be handeld more easily with the forcats package\nggplot(samples) + geom_bar(aes(x = community_type)) # bars are alphabetically ordered\nsa2 &lt;- samples\nsa2$cto &lt;- forcats::fct_reorder(sa2$community_type, sa2$community_type, length)\n# fct_reorder: reorder the input factor by a summary statistic on an other vector\nggplot(sa2) + geom_bar(aes(x = community_type)) # bars are ordered by size\n\n\n\n9.6.2 Adding columns to tibbles with mutate and transmute\n\nA common application of data manipulation is adding derived columns. dplyr offers that with mutate\nsamples %&gt;%\ndplyr::mutate(                                               # add a column that\n    archive_summary = paste0(archive, \": \", archive_accession) # combines two other\n) %$% archive_summary                                        # columns\ndplyr::transmute removes all columns but the newly created ones\nsamples %&gt;%\ndplyr::transmute(\n    sample_name = tolower(sample_name), # overwrite this columns\n    publication_doi                     # select this column\n)\ntibble::add_column behaves as dplyr::mutate, but gives more control over column position\nsamples %&gt;% tibble::add_column(., id = 1:nrow(.), .before = \"project_name\")\n\n\n\n9.6.3 Conditional operations with ifelse and case_when\n\nifelse allows to implement conditional mutate operations, that consider information from other columns, but that gets cumbersome easily\nsamples %&gt;% dplyr::mutate(hemi = ifelse(latitude &gt;= 0, \"North\", \"South\")) %$% hemi\n\nsamples %&gt;% dplyr::mutate(\nhemi = ifelse(is.na(latitude), \"unknown\", ifelse(latitude &gt;= 0, \"North\", \"South\"))\n) %$% hemi\ndplyr::case_when is a much more readable solution for this application\nsamples %&gt;% dplyr::mutate(\nhemi = dplyr::case_when(\n    latitude &gt;= 0 ~ \"North\",\n    latitude &lt; 0  ~ \"South\",\n    TRUE          ~ \"unknown\" # TRUE catches all remaining cases\n)\n) %$% hemi\n\n\n\n9.6.4 Long and wide data formats\n\nFor different applications or to simplify certain analysis or plotting operations data often has to be transformed from a wide to a long format or vice versa\n\n\n\nA table in wide format has N key columns and N value columns\nA table in long format has N key columns, one descriptor column and one value column\n\n\n\n9.6.5 A wide dataset\ncarsales &lt;- tibble::tribble(\n  ~brand, ~`2014`, ~`2015`, ~`2016`, ~`2017`,\n  \"BMW\",  20,      25,      30,      45,\n  \"VW\",   67,      40,     120,      55\n)\n\ncarsales\n\nWide format becomes a problem, when the columns are semantically identical. This dataset is in wide format and we can not easily plot it\nWe generally prefer data in long format, although it is more verbose with more duplication. ‚ÄúLong‚Äù format data is more ‚Äútidy‚Äù\n\n\n\n9.6.6 Making a wide dataset long with pivot_longer\ncarsales_long &lt;- carsales %&gt;% tidyr::pivot_longer(\n  cols = tidyselect::num_range(\"\", range = 2014:2017), # set of columns to transform\n  names_to = \"year\",            # the name of the descriptor column we want\n  names_transform = as.integer, # a transformation function to apply to the names\n  values_to = \"sales\"           # the name of the value column we want\n)\n\ncarsales_long\n\n\n9.6.7 Making a long dataset wide with pivot_wider\ncarsales_wide &lt;- carsales_long %&gt;% tidyr::pivot_wider(\n  id_cols = \"brand\",  # the set of id columns that should not be changed\n  names_from = year,  # the descriptor column with the names of the new columns\n  values_from = sales # the value column from which the values should be extracted\n)\n\ncarsales_wide\n\nApplications of wide datasets are adjacency matrices to represent graphs, covariance matrices or other pairwise statistics\nWhen data gets big, then wide formats can be significantly more efficient (e.g.¬†for spatial data)\n\n\n\n9.6.8 Exercise 3\n\nMove the column gear to the first position of the mtcars dataset\n\nMake a new dataset mtcars2 with the column mpg and an additional column am_v, which encodes the transmission type (am) as either \"manual\" or \"automatic\"\n\nCount the number of cars per transmission type (am_v) and number of gears (gear). Then transform the result to a wide format, with one column per transmission type.\n\n\n\n\n9.6.9 Possible solutions 3\n\nMove the column gear to the first position of the mtcars dataset\nmtcars %&gt;% dplyr::relocate(gear, .before = mpg)\nMake a new dataset mtcars2 with the column gear and an additional column am_v, which encodes the transmission type (am) as either \"manual\" or \"automatic\"\nmtcars2 &lt;- mtcars %&gt;% dplyr::mutate(\ngear, am_v = dplyr::case_when(am == 0 ~ \"automatic\", am == 1 ~ \"manual\")\n)\nCount the number of cars in mtcars2 per transmission type (am_v) and number of gears (gear). Then transform the result to a wide format, with one column per transmission type.\nmtcars2 %&gt;% dplyr::group_by(am_v, gear) %&gt;% dplyr::tally() %&gt;%\ntidyr::pivot_wider(names_from = am_v, values_from = n)"
  },
  {
    "objectID": "introduction-to-r.html#combining-tibbles-with-join-operations",
    "href": "introduction-to-r.html#combining-tibbles-with-join-operations",
    "title": "9¬† Introduction to R and the Tidyverse",
    "section": "9.7 Combining tibbles with join operations",
    "text": "9.7 Combining tibbles with join operations\n\n9.7.1 Types of joins\nJoins combine two datasets x and y based on key columns\n\nMutating joins add columns from one dataset to the other\n\nLeft join: Take observations from x and add fitting information from y\nRight join: Take observations from y and add fitting information from x\nInner join: Join the overlapping observations from x and y\nFull join: Join all observations from x and y, even if information is missing\n\nFiltering joins remove observations from x based on their presence in y\n\nSemi join: Keep every observation in x that is in y\nAnti join: Keep every observation in x that is not in y\n\n\n\n\n9.7.2 A second dataset\nlibrary_table_path &lt;- \"/vol/volume/3b-1-introduction-to-r-and-the-tidyverse/ancientmetagenome-hostassociated_libraries.tsv\"\nlibrary_table_url &lt;-\n\"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/b187df6ebd23dfeb42935fd5020cb615ead3f164/\nancientmetagenome-hostassociated/libraries/ancientmetagenome-hostassociated_libraries.tsv\"\n\nlibraries &lt;- readr::read_tsv(library_table_url)\nprint(libraries, n = 3)\n\n\n9.7.3 Meaningful subsets\nsamsub &lt;- samples %&gt;% dplyr::select(project_name, sample_name, sample_age)\nlibsub &lt;- libraries %&gt;% dplyr::select(project_name, sample_name, library_name, read_count)\nprint(samsub, n = 3)\nprint(libsub, n = 3)\n\n\n9.7.4 Left join\nTake observations from x and add fitting information from y\n\nleft &lt;- dplyr::left_join(\nx = samsub,                           # 1060 observations\ny = libsub,                           # 1657 observations\nby = c(\"project_name\", \"sample_name\") # the key columns by which to join\n)\n\nprint(left, n = 1)\n\nLeft joins are the most common join operation: Add information from another dataset\n\n\n\n9.7.5 Right join\nTake observations from y and add fitting information from x\n\nright &lt;- dplyr::right_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\n\nprint(right, n = 1)\n\nRight joins are almost identical to left joins ‚Äì only x and y have reversed roles\n\n\n\n9.7.6 Inner join\nJoin the overlapping observations from x and y\n\ninner &lt;- dplyr::inner_join(\nx = samsub,                           # 1060 observations\ny = libsub,                           # 1657 observations\nby = c(\"project_name\", \"sample_name\")\n)\n\nprint(inner, n = 1)\n\nInner joins are a fast and easy way to check, to which degree two dataset overlap\n\n\n\n9.7.7 Full join\nJoin all observations from x and y, even if information is missing\n\nfull &lt;- dplyr::full_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\n\nprint(full, n = 1)\n\nFull joins allow to preserve every bit of information\n\n\n\n9.7.8 Semi join\nKeep every observation in x that is in y\n\nsemi &lt;- dplyr::semi_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\nprint(semi, n = 1)\n\nSemi joins are underused operations to filter datasets\n\n\n\n9.7.9 Anti join\nKeep every observation in x that is not in y\n\nanti &lt;- dplyr::anti_join(\n  x = samsub,                           # 1060 observations\n  y = libsub,                           # 1657 observations\n  by = c(\"project_name\", \"sample_name\")\n)\nprint(anti, n = 1)\n\nAnti joins allow to quickly specify incomplete datasets and missing information\n\n\n\n9.7.10 Exercise 4\nConsider the following additional dataset:\ngear_opinions &lt;- tibble::tibble(gear = c(3, 5), opinion = c(\"boring\", \"wow\"))\n\nAdd my opinions about gears to the mtcars dataset\n\nRemove all cars from the dataset for which I don‚Äôt have an opinion\n\n\n\n\n9.7.11 Possible Solutions 4\n\nAdd my opinions about gears to the mtcars dataset\ndplyr::left_join(mtcars, gear_opinions, by = \"gear\")\nRemove all cars from the dataset for which I don‚Äôt have an opinion\ndplyr::anti_join(mtcars, gear_opinions, by = \"gear\")"
  }
]