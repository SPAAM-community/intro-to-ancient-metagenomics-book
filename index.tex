% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[osf,p]{libertinus}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{soul}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{3}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.89,0.38,0.04}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{graphicx}
\begin{document}
\titlehead{\centering\includegraphics[width=6cm]{assets/images/cover.png}}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Introduction to Ancient Metagenomics},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Introduction to Ancient Metagenomics}
\author{}
\date{2023-08-06}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, enhanced, sharp corners, boxrule=0pt, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

Ancient metagenomics applies cutting-edge metagenomic methods to the
degraded DNA content of archaeological and palaeontological specimens.
The rapidly growing field is currently uncovering a wealth of novel
information for both human and natural history, from identifying the
causes of devastating pandemics such as the Black Death, to revealing
how past ecosystems changed in response to long-term climatic and
anthropogenic change, to reconstructing the microbiomes of extinct human
relatives. However, as the field grows, the techniques, methods, and
workflows used to analyse such data are rapidly changing and improving.

In this book we will go through the main steps of ancient metagenomic
bioinformatic workflows, familiarising students with the command line,
demonstrating how to process next-generation-sequencing (NGS) data, and
showing how to perform de novo metagenomic assembly. Focusing on
host-associated ancient metagenomics, the book consists of a combination
of theory and hands-on exercises, allowing readers to become familiar
with the types of questions and data researchers work with.

By the end of the textbook, readers will have an understanding of how to
effectively carry out the major bioinformatic components of an ancient
metagenomic project in an open and transparent manner.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

If you export the PDF or ePub versions of this book, some sections maybe
excluded (such as videos, and embedded slide decks). Always refer to
this website in doubt.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

The PDF/ePub version of the book is currently still under construction,
and is likely misformatted and missing much of the content. It is not
recommended for use.

\end{tcolorbox}

\emph{All material was originally developed for the
\href{https://spaam-community-github.io}{SPAAM} Summer School:
Introduction to Ancient Metagenomics}

\bookmarksetup{startatroot}

\hypertarget{citing-this-book}{%
\chapter*{Citing this book}\label{citing-this-book}}
\addcontentsline{toc}{chapter}{Citing this book}

\markboth{Citing this book}{Citing this book}

The source material for this book is located on GitHub:

\url{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book}.

If you wish to cite this book, please use the following bibliographic
information

\begin{quote}
James A. Fellows Yates, Christina Warinner, Alina Hiß, Arthur Kocher,
Clemens Schmid, Irina Velsko, Maxime Borry, Megan Michel, Nikolay
Oskolkov, Sebastian Duchene, Thiseas Lamnidis, Aida Andrades Valtueña,
Alexander Herbig, \& Alexander Hübner. (2023). Introduction to Ancient
Metagenomics. In Introduction to Ancient Metagenomics (Version 2022).
Zenodo. DOI:
\href{https://doi.org/10.5281/zenodo.8027281}{10.5281/zenodo.8027281}
\end{quote}

This work is licensed under a Creative Commons Attribution 4.0
International License.

\bookmarksetup{startatroot}

\hypertarget{authors}{%
\chapter*{Authors}\label{authors}}
\addcontentsline{toc}{chapter}{Authors}

\markboth{Authors}{Authors}

The creation of this text book was developed through a series of summer
schools run by the SPAAM community, and financially supported by the
\href{https://www.wernersiemens-stiftung.ch/}{Werner Siemens-Stiftung}.
The have contributed to the development of this textbook.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/FELLOWS_YATES_James.jpg}
& 🇬🇧 \textbf{James Fellows Yates} is an archaeology-trained biomolecular
archaeologist and convert to palaeogenomics, and is recently pivoting to
bioinformatics. He specialises in ancient metagenomics analysis,
generating tools and high-throughput approaches and high-quality
pipelines for validating and analysing ancient (oral) microbiomes and
palaeogenomic data. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/WARINNER_Christina.png}
& 🇺🇸 \textbf{Christina Warinner} is Group Leader of Microbiome Sciences
at the Max Planck Institute for Evolutionary Anthropology in Leipzig,
Germany, and Associate Professor of Anthropology at Harvard University.
She serves on the Leadership Team of the Max Planck-Harvard Research
Center for the Archaeoscience of the Ancient Mediterranean (MHAAM), and
is a Professor in the Faculty of Biological Sciences at Friedrich
Schiller University in Jena, Germany. Her research focuses on the use of
metagenomics and paleoproteomics to better understand past human diet,
health, and the evolution of the human microbiome. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/ANDRADES_VALTUENA_Aida.jpg}
& 🇪🇸 \textbf{Aida Andrades Valtueña} is a geneticist interested in
pathogen evolution, with particular interest in prehistoric pathogens.
She has been exploring new methods to analyse ancient pathogen data to
understand their past function and ecology to inform models of pathogen
emergence. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/HERBIG_Alexander.jpeg}
& 🇩🇪 \textbf{Alexander Herbig} is a bioinformatician and group leader
for Computational Pathogenomics at the Max Planck Institute for
Evolutionary Anthropology. His main interest is in studying the
evolution of human pathogens and in methods development for pathogen
detection and bacterial genomics. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/HUEBNER_Alex.jpg}
& 🇩🇪 \textbf{Alex Hübner} is a computational biologist, who originally
studied biotechnology, before switching to evolutionary biology during
his PhD. For his postdoc in the Warinner lab, he focuses on
investigating whether new methods in the field of modern metagenomics
can be directly applied to ancient DNA data. Here, he is particularly
interested in the \emph{de novo} assembly of ancient metagenomic
sequencing data and the subsequent analysis of its results. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/HISS_Alina.jpg}
& 🇩🇪 \textbf{Alina Hiss} is a PhD student in the Computational
Pathogenomics group at the Max Planck Institute for Evolutionary
Anthropology. She is interested in the evolution of human pathogens and
working on material from the Carpathian basin to gain insights about the
presence and spread of pathogens in the region during the Early Medieval
period. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/KOCHER_Arthur.jpg}
& 🇫🇷 \textbf{Arthur Kocher} initially trained as a veterinarian. He then
pursued a PhD in the field of disease ecology, during which he studied
the impact of biodiversity changes on the transmission of zoonotic
diseases using molecular tools such as DNA metabarcoding. During his
Post-Docs, he extended his research focus to evolutionary aspects of
pathogens, which he currently investigates using ancient genomic data
and Bayesian phylogenetics. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/SCHMID_Clemens.JPG}
& 🇩🇪 \textbf{Clemens Schmid} is a computational archaeologist pursuing a
PhD in the group of Stephan Schiffels at the department of
Archaeogenetics at the Max Planck Institute for Evolutionary
Anthropology. He is trained both in archaeology and computer science and
currently develops computational methods for the spatiotemporal
co-analysis of archaeological and ancient genomic data. He worked in
research projects on the European Neolithic, Copper and Bronze age and
maintains research software in R, C++ and Haskell. \\
2022 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/VELSKO_Irina.jpeg}
& 🇺🇸 \textbf{Irina Velsko} is a postdoc in the Microbiome group of the
department of Archaeogenetics at the Max Planck Institute for
Evolutionary Anthropology. She did her PhD work on oral microbiology and
immunology of the living, and now works on oral microbiomes of the
living and the dead. Her work focuses on the evolution and ecology of
dental plaque biofilms, both modern and ancient, and the complex
interplay between microbiomes and their hosts. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/BORRY_Maxime.png}
& 🇫🇷 \textbf{Maxime Borry} is a doctoral researcher in bioinformatics at
the Max Planck Institute for Evolutionary Anthropology in Germany. After
an undergraduate in life sciences and a master in Ecology, followed by a
master in bioinformatics, he is now working on the completion of his
PhD, focused on developing new tools and data analysis of ancient
metagenomic samples. \\
2022 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/MICHEL_Megan.jpg}
& 🇺🇸 \textbf{Megan Michel} is a PhD student jointly affiliated with the
Archaeogenetics Department at the Max Planck Institute for Evolutionary
Anthropology and the Human Evolutionary Biology Department at Harvard
University. Her research focuses on using computational genomic analyses
to understand how pathogens have co-evolved with their hosts over the
course of human history. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/OSKOLKOV_Nikolay.jpg}
& 🇷🇺 \textbf{Nikolay Oskolkov} is a bioinformatician at Lund University
and the bioinformatics platform of SciLifeLab, Sweden. He defended his
PhD in theoretical physics in 2007, and switched to life sciences in
2012. His research interests include mathematical statistics and machine
learning applied to genetics and genomics, single cell and ancient
metagenomics data analysis. \\
2022 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/DUCHENE_Sebastian.jpeg}
& 🇦🇺 \textbf{Sebastian Duchene} is an Australian Research Council Fellow
at the Doherty Institute for Infection and Immunity at the University of
Melbourne, Australia. Prior to joining the University of Melbourne he
obtained his PhD and conducted postdoctoral work at the University of
Sydney. His research is in molecular evolution and epidemiology of
infectious pathogens, notably viruses and bacteria, and developing
Bayesian phylodynamic methods. \\
2022-2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/LAMNIDIS_Thiseas.jpg}
& 🇬🇷 \textbf{Thiseas Lamnidis} is a human population geneticist
interested in European population history after the Bronze Age. To gain
the required resolution to differentiate between Iron Age European
populations, he is developing analytical methods based on the sharing of
rare variation between individuals. He has also contributed to pipelines
that streamline the processing and analysis of genetic data in a
reproducible manner, while also facilitating dissemination of
information among interdisciplinary colleagues. \\
2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/NOTA_Kevin.jpg}
& 🇳🇱 \textbf{Kevin Nota} has a PhD in molecular paleoecology from
Uppsala University. Currently he is a postdoc in the Max Planck Research
Group for Ancient Environmental Genomics. His main research interest is
in population genomics from ancient environmental samples. \\
2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/GUELLIL_Meriam.jpg}
& 🇦🇹 Meriam Guellil is an expert in ancient microbial phylogenomics and
metagenomics, particularly of human pathogens. She is particularly
interested in the study of diseases that are invisible in the
archaeological and osteological record, and the study of their evolution
throughout human history. Her previous research includes studies on
microbial species such as Yersinia pestis, Haemophilus influenzae,
Borrelia recurrentis and Herpes simplex 1. \\
2023 &
\includegraphics[width=1.04167in,height=\textheight]{assets/images/headshots/WARNER_Robin.jpg}
& 🇩🇪 Robin Warner is a MSc bioinformatics student at the Leipzig
University. He is currently writing his master's thesis in the Max
Planck Research Group for Ancient Environmental Genomics about the
comparison of ancient sedimentary DNA capture methods and shotgun
sequencing. \\
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{acknowledgements}{%
\chapter*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{Acknowledgements}

\markboth{Acknowledgements}{Acknowledgements}

We would like to thank the following supporters of the original summer
schools and eventual textbook.

\hypertarget{financial-support}{%
\section*{\texorpdfstring{\textbf{Financial
Support}}{Financial Support}}\label{financial-support}}
\addcontentsline{toc}{section}{\textbf{Financial Support}}

\markright{\textbf{Financial Support}}

\includegraphics[width=2.08333in,height=\textheight]{assets/images/logos/WSS_Logo_16mm_600_rgb.png}

The content of this textbook was developed from the SPAAM Summer School:
Introduction to Ancient Metagenomics summer school series, sponsored by
the Werner Siemens-Stiftung (Grant: Paleobiotechnology, awarded to
Pierre Stallforth, Hans-Knöll Institute, and Christina Warinner, Max
Planck Institute for Evolutionary Anthropology)

\hypertarget{institutional-support}{%
\section*{\texorpdfstring{\textbf{Institutional
Support}}{Institutional Support}}\label{institutional-support}}
\addcontentsline{toc}{section}{\textbf{Institutional Support}}

\markright{\textbf{Institutional Support}}

\begin{figure}

\begin{minipage}[t]{0.20\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics[width=2.08333in,height=\textheight]{assets/images/logos/leibniz_hki.png}

}

}

\end{minipage}%
%
\begin{minipage}[t]{0.20\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics[width=2.08333in,height=\textheight]{assets/images/logos/mhaam.png}

}

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics[width=4.16667in,height=\textheight]{assets/images/logos/MPI_Logo_DE_CMYK_green.png}

}

}

\end{minipage}%
%
\begin{minipage}[t]{0.20\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics[width=2.08333in,height=\textheight]{assets/images/logos/JSMC Logo.png}

}

}

\end{minipage}%

\end{figure}

\hypertarget{infrastructural-support}{%
\section*{\texorpdfstring{\textbf{Infrastructural
Support}}{Infrastructural Support}}\label{infrastructural-support}}
\addcontentsline{toc}{section}{\textbf{Infrastructural Support}}

\markright{\textbf{Infrastructural Support}}

\includegraphics[width=2.08333in,height=\textheight]{index_files/mediabag/assets/images/logos/denbi-logo-color.pdf}

The practical sessions of the summers schools work was supported by the
BMBF-funded de.NBI Cloud within the German Network for Bioinformatics
Infrastructure (de.NBI) (031A532B, 031A533A, 031A533B, 031A534A,
031A535A, 031A537A, 031A537B, 031A537C, 031A537D, 031A538A). z

\bookmarksetup{startatroot}

\hypertarget{before-you-start}{%
\chapter*{Before you Start}\label{before-you-start}}
\addcontentsline{toc}{chapter}{Before you Start}

\markboth{Before you Start}{Before you Start}

The summer school course that this textbook is derived from was designed
to be as practical as possible. This means that most of the chapters are
designed to act as a walkthrough to guide you through the steps on how
to generate and analyse data for each of the major steps of an ancient
metagenomics project.

The summer school utilised cloud computing to provide a consistent
computing platform for all participants, however all tools and data
demonstrated are open-source and publicly available.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

Bioinformatics often involve large computing resource requirements!
While we aim to make example data and processing as efficient as
possible, we cannot guarantee that they will all be able to work on
standard laptops or desktop computing - most likely due to memory/RAM
requirements. As a guide, the cloud nodes used during the summer school
had 16 cores and 32 GB of RAM.

\end{tcolorbox}

To following the practical chapters of this text book, you will require:

\begin{itemize}
\tightlist
\item
  A unix based operating system (e.g., Linux, MacOS, or possibly Windows
  with Linux Subsystem - however the latter has not be tested )
\item
  A corresponding Unix terminal
\item
  An internet connection
\item
  A web browser
\item
  A
  \href{https://docs.conda.io/en/latest/miniconda.html}{\texttt{conda}}
  installation with
  \href{https://bioconda.github.io/\#usage}{\texttt{bioconda}}
  configured.

  \begin{itemize}
  \tightlist
  \item
    Conda is a very popular package manager for installing software in
    bioinformatics. \texttt{bioconda} is a the main source of
    bioinformatics software for conda.
  \item
    To speed up installation, we would also highly recommend setting up
    the
    \href{https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community}{\texttt{libmamba-solver}}
  \end{itemize}
\end{itemize}

For each chapter we will provide a link to a \texttt{tar} archive that
will contain the raw data and a conda \texttt{yml} file that specifies
the software environment for that chapter.

Before loading the environment for the exercises, the environment will
need to be installed using the \texttt{yml} with the instructions below,
and then activated. A list of the software in each chapter's environment
can be found in the \protect\hyperlink{conda-environments}{Appendix}.

\hypertarget{creating-a-conda-environment}{%
\section*{Creating a conda
environment}\label{creating-a-conda-environment}}
\addcontentsline{toc}{section}{Creating a conda environment}

\markright{Creating a conda environment}

Once \texttt{conda} is installed and \texttt{bioconda} configured, at
the beginning of each chapter, to create the \texttt{conda} environment
from the \texttt{yml} file, you will need to run the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Download the \texttt{conda} env on
\item
  Within the resulting directory run the following conda command to
  install the software into it's dedicated environment

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ env create }\AttributeTok{{-}f} \OperatorTok{\textless{}}\NormalTok{env\_file}\OperatorTok{\textgreater{}}\NormalTok{.yml}
\end{Highlighting}
\end{Shaded}

  ::: \{.callout-note\} Note: you only have to run the environment
  creation once. :::
\item
  Follow the instructions as prompted. Once created, you can see a list
  of environments with

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ env list}
\end{Highlighting}
\end{Shaded}
\item
  To load the relevant environment, you can run

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate }\OperatorTok{\textless{}}\NormalTok{name\_of\_envonment}\OperatorTok{\textgreater{}}\NormalTok{.yml}
\end{Highlighting}
\end{Shaded}
\item
  Once finished with the chapter, you can deactivate the environment
  with

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ deactivate}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

To reuse the environment, just run step 4 and 5 as necessary.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

To delete a conda software environment, just get the path listed on
\texttt{conda\ env\ list} and delete the folder with
\texttt{rm\ -rf\ \textless{}path\textgreater{}}.

\end{tcolorbox}

\hypertarget{additional-software}{%
\section*{Additional Software}\label{additional-software}}
\addcontentsline{toc}{section}{Additional Software}

\markright{Additional Software}

For some chapters you may need the following software/and or data
manually installed, which are not available on \texttt{bioconda}:

\begin{itemize}
\item
  Docker (installation method will vary depending on your OS)

  \begin{itemize}
  \item
    \href{https://docs.docker.com/engine/install/ubuntu/}{Standard
    install}
  \item
    Linux-nerd install

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ install }\AttributeTok{{-}m}\NormalTok{ 0755 }\AttributeTok{{-}d}\NormalTok{ /etc/apt/keyrings}
\ExtensionTok{curl} \AttributeTok{{-}fsSL}\NormalTok{ https://download.docker.com/linux/ubuntu/gpg }\KeywordTok{|} \FunctionTok{sudo}\NormalTok{ gpg }\AttributeTok{{-}{-}dearmor} \AttributeTok{{-}o}\NormalTok{ /etc/apt/keyrings/docker.gpg}
\FunctionTok{sudo}\NormalTok{ chmod a+r /etc/apt/keyrings/docker.gpg}
\BuiltInTok{echo} \DataTypeTok{\textbackslash{}}
\StringTok{"deb [arch="}\VariableTok{$(}\ExtensionTok{dpkg} \AttributeTok{{-}{-}print{-}architecture}\VariableTok{)}\StringTok{" signed{-}by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu }\DataTypeTok{\textbackslash{}}
\StringTok{"}\VariableTok{$(}\BuiltInTok{.}\NormalTok{ /etc/os{-}release }\KeywordTok{\&\&} \BuiltInTok{echo} \StringTok{"}\VariableTok{$VERSION\_CODENAME}\StringTok{"}\VariableTok{)}\StringTok{" stable"} \KeywordTok{|} \DataTypeTok{\textbackslash{}}
\FunctionTok{sudo}\NormalTok{ tee /etc/apt/sources.list.d/docker.list }\OperatorTok{\textgreater{}}\NormalTok{ /dev/null}
\FunctionTok{sudo}\NormalTok{ apt{-}get update}
\FunctionTok{sudo}\NormalTok{ apt{-}get install docker{-}ce docker{-}ce{-}cli containerd.io docker{-}buildx{-}plugin docker{-}compose{-}plugin}
\CommentTok{\#\# May need to do a reboot or something here}
\FunctionTok{sudo}\NormalTok{ groupadd docker}
\FunctionTok{sudo}\NormalTok{ usermod }\AttributeTok{{-}aG}\NormalTok{ docker }\VariableTok{$USER}
\ExtensionTok{newgrp}\NormalTok{ docker}
\FunctionTok{sudo}\NormalTok{ reboot }\CommentTok{\#\# will kick you out, but it\textquotesingle{}ll be back in a minute or two}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  rename (if not already installed, e.g.~on OSX)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sudo}\NormalTok{ apt install rename}
\end{Highlighting}
\end{Shaded}
\item
  \emph{De novo} assembly

  \begin{itemize}
  \tightlist
  \item
    MetaWrap
  \end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ create }\AttributeTok{{-}n}\NormalTok{ metawrap{-}env python=2.7}
\ExtensionTok{conda}\NormalTok{ activate metawrap{-}env}
\ExtensionTok{conda}\NormalTok{ install biopython bwa maxbin2=2.2.7 metabat2 samtools=1.9}
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/bin/}
\FunctionTok{git}\NormalTok{ clone https://github.com/bxlab/metaWRAP.git}
\BuiltInTok{echo} \StringTok{"export PATH=}\VariableTok{$PATH}\StringTok{:\textasciitilde{}/bin/metaWRAP/bin"} \OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ \textasciitilde{}/.bashrc}
\end{Highlighting}
\end{Shaded}
\item
  Functional Profiling

  \begin{itemize}
  \item
    HUMAnN3 UniRef database (where the functional providing conda
    environment is already activated - see the Functional Profiling
    chapter for more details)

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{humann3\_databases} \AttributeTok{{-}{-}download}\NormalTok{ uniref uniref90\_ec\_filtered\_diamond /vol/volume/5c{-}functional{-}genomics/humann3\_db}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  Phylogenomics

  \begin{itemize}
  \item
    \href{http://tree.bio.ed.ac.uk/download.html?name=tempest\&id=102\&num=3}{Tempest}
    (v1.5.3)
  \item
    It is also recommended to assign the following \texttt{bash}
    variable so you can access the tool without the full path

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{export} \VariableTok{tempest}\OperatorTok{=}\StringTok{\textquotesingle{}bash /home/ubuntu/bin/TempEst\_v1.5.3/bin/tempest\textquotesingle{}}
\end{Highlighting}
\end{Shaded}
  \item
    \href{https://www.megasoftware.net}{MEGAX} (v11.0.11)
  \end{itemize}
\item
  Authentication and Decontamination

  \begin{itemize}
  \item
    Sourcetracker

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/sourcetracker/}
\FunctionTok{git}\NormalTok{ clone https://github.com/danknights/sourcetracker.git}

\CommentTok{\#\# patch due to R update}
\FunctionTok{sed} \AttributeTok{{-}i} \StringTok{\textquotesingle{}538,539s/\^{}/\#/\textquotesingle{}}\NormalTok{ sourcetracker/src/SourceTracker.r}
\end{Highlighting}
\end{Shaded}
  \item
    decOM

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/decom}
\FunctionTok{git}\NormalTok{ clone https://github.com/CamilaDuitama/decOM.git  }
\BuiltInTok{cd}\NormalTok{ decOM  }
\ExtensionTok{conda}\NormalTok{ env create }\AttributeTok{{-}n}\NormalTok{ decOM }\AttributeTok{{-}{-}file}\NormalTok{ environment.yml  }
\ExtensionTok{conda}\NormalTok{ deactivate}
\ExtensionTok{conda}\NormalTok{ activate decOM  }

\BuiltInTok{export} \VariableTok{PATH}\OperatorTok{=}\NormalTok{/absolute/path/to/decOM:}\VariableTok{$\{PATH\}}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  Ancient Metagenomic Pipelines

\begin{verbatim}
  ```bash
  cd /<path>/<to>/ancient-metagenomic-pipelines/
  git clone https://github.com/NBISweden/aMeta
  cd aMeta
  conda env create -f workflow/envs/environment.yaml
  conda activate aMeta
  ```
\end{verbatim}
\end{itemize}

\part{Theory}

In the first section of this book we will introduce the basic concepts
of a range of topics related to ancient DNA, from how Next Generation
Sequencing (NGS) sequencing works, to the fundamental biochemistry of
ancient DNA, to the phylogenomic analysis of reconstructed genomes.

The content of this section of the book were originally delivered as
lectures, and each chapter will have a recording of the lectures and the
accompanying slides.

\hypertarget{lectures}{%
\section*{Lectures}\label{lectures}}
\addcontentsline{toc}{section}{Lectures}

\markright{Lectures}

\hypertarget{introduction-to-ngs-sequencing}{%
\subsection*{\texorpdfstring{\protect\hyperlink{introduction-to-ngs-sequencing-1}{Introduction
to NGS
Sequencing}}{Introduction to NGS Sequencing}}\label{introduction-to-ngs-sequencing}}
\addcontentsline{toc}{subsection}{\protect\hyperlink{introduction-to-ngs-sequencing-1}{Introduction
to NGS Sequencing}}

In this chapter, we will introduce how we are able to convert DNA
molecules to human readable sequences of A, C, T, and Gs, which we can
subsequently can computationally analyse.

The field of Ancient DNA was revolutionised by the development of `Next
Generation Sequencing' (NGS), which relies on sequencing of millions of
\emph{short} fragments of DNA in parallel. The global leading DNA
sequencing company is Illumina, and the technology used by Illumina is
also most popular by palaeogeneticists. Therefore we will go through the
various technologies behind Illumina next-generation sequencing
machines.

We will also look at some important differences in the way different
models of Illumina sequences work, and how this can influence ancient
DNA research. Finally we will cover the structure of `FASTQ' files, the
most popular file format for representing the DNA sequence output of NGS
sequencing machines.

\hypertarget{introduction-to-ancient-dna}{%
\subsection*{\texorpdfstring{\protect\hyperlink{introduction-to-ancient-dna-1}{Introduction
to Ancient
DNA}}{Introduction to Ancient DNA}}\label{introduction-to-ancient-dna}}
\addcontentsline{toc}{subsection}{\protect\hyperlink{introduction-to-ancient-dna-1}{Introduction
to Ancient DNA}}

This chapter introduces you to ancient DNA and the enormous
technological changes that have taken place since the field's origins in
1984. Starting with the quagga and proceeding to microbes, we discuss
where ancient microbial DNA can be found in the archaeological record
and examine how ancient DNA is defined by its condition, not by a fixed
age.

We next cover genome basics and take an in-depth look at the way DNA
degrades over time. We detail the fundamentals of DNA damage, including
the specific chemical processes that lead to DNA fragmentation and
C-\textgreater T miscoding lesions. We then demystify the DNA damage
``smiley plot'' and explain the how the plot's symmetry or asymmetry is
related to the specific enzymes used to repair DNA during library
construction. We discuss how DNA damage is and is not clock-like, how to
interpret and troubleshoot DNA damage plots, and how DNA damage patters
can be used to authenticate ancient samples, specific taxa, and even
sequences. We cover laboratory strategies for removing or reducing
damage for greater accuracy for genotype calling, and we discuss the
pros and cons of single-stranded library protocols. We then take a
closer look at proofreading and non-proofreading polymerases and note
key steps in NGS library preparation during which enzyme selection is
critical in ancient DNA studies.

Finally, we examine the big picture of why DNA damage matters in ancient
microbial studies, and its effects on taxonomic identification of
sequences, accurate genome mapping, and metagenomic assembly.

\hypertarget{introduction-to-metagenomics}{%
\subsection*{\texorpdfstring{\protect\hyperlink{introduction-to-metagenomics-1}{Introduction
to
Metagenomics}}{Introduction to Metagenomics}}\label{introduction-to-metagenomics}}
\addcontentsline{toc}{subsection}{\protect\hyperlink{introduction-to-metagenomics-1}{Introduction
to Metagenomics}}

This chapter introduces you to the basics of metagenomics, with an
emphasis on tools and approaches that are used to study ancient
metagenomes. We begin by covering the basic terminology used in
metagenomics and microbiome research and discuss how the field has
changed over time. We examine the species concept for microbes and
challenges that arise in classifying microbial species with respect to
taxonomy and phylogeny. We then proceed to taxonomic profiling and
discuss the pros and cons of different taxonomic profilers.

Afterwards, we explain how to estimate preservation in ancient
metagenomic samples and how to clean up your datasets and remove
contaminants. Finally, we discuss strategies for exploring and comparing
the ecological diversity in your samples, including different strategies
for data normalization, distance calculation, and ordination.

\hypertarget{introduction-to-microbial-genomics}{%
\subsection*{\texorpdfstring{\protect\hyperlink{introduction-to-microbial-genomics-1}{Introduction
to Microbial
Genomics}}{Introduction to Microbial Genomics}}\label{introduction-to-microbial-genomics}}
\addcontentsline{toc}{subsection}{\protect\hyperlink{introduction-to-microbial-genomics-1}{Introduction
to Microbial Genomics}}

The field of microbial genomics aims at the reconstruction and
comparative analyses of genomes for gaining insights into the genetic
foundation and evolution of various functional aspects such as virulence
mechanisms in pathogens.

Including data from ancient samples into this comparative assessment
allows for studying these evolutionary changes through time. This, for
example, provides insights into the emergence of human pathogens and
their development in conjunction with human cultural transitions.

In this chapter we will look examples for how to utilise data from
ancient genomes in comparative studies of human pathogens and today's
practical sessions will highlight methodologies for the reconstruction
of microbial genomes.

\hypertarget{introduction-to-evolutionary-biology}{%
\subsection*{\texorpdfstring{\protect\hyperlink{introduction-to-evolutionary-biology-1}{Introduction
to Evolutionary
Biology}}{Introduction to Evolutionary Biology}}\label{introduction-to-evolutionary-biology}}
\addcontentsline{toc}{subsection}{\protect\hyperlink{introduction-to-evolutionary-biology-1}{Introduction
to Evolutionary Biology}}

Pathogen genome data are an invaluable source of information about the
evolution and spread of these organisms. This chapter will focus on
molecular phylogenetic methods and the insight that they can reveal from
improving our understanding of ancient evolution to the epidemiological
dynamics of current outbreaks.

The first section will introduce phylognenetic trees and a set of core
terms and concepts for their interpretation. Next, it will focus on some
of the most popular approaches to inferring phylogenetic trees; those
based on genetic distance, maximum likelihood, and Bayesian inference.
These methods carry important considerations regarding the process that
generated the data, computational capability, and data quality, all of
which will be discussed here. Finally, we will direct our attention to
examples of analyses of ancient and modern pathogens (e.g.~Yersinia
pestis, Hepatitis B virus, SARS-CoV-2) and critically assess appropriate
choice of models and methods.

\hypertarget{introduction-to-ngs-sequencing-1}{%
\chapter{Introduction to NGS
Sequencing}\label{introduction-to-ngs-sequencing-1}}

\hypertarget{lecture}{%
\section{Lecture}\label{lecture}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of the slide lectures can be downloaded from
\href{https://raw.githubusercontent.com/SPAAM-community/wss-summer-school/main/docs/assets/slides/2022/1a-intro-to-ngs/SPAAM\%20Summer\%20School\%202022\%20-\%201A\%20-\%20Introduction\%20to\%20NGS\%20Data.pdf}{here}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-important-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-important-color!10!white, opacityback=0, opacitybacktitle=0.6]

Text is a raw transcription of the lecture above - extensive editing is
still underway. Readability will be low.

\end{tcolorbox}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

We will make a start. So in this session, introduction to NGS data, I
want to very briefly recap the basics of DNA for a good reason. Then
introduce what DNA sequence is and how that works, and then also explain
how Illumina NGS data, sequencing data is generated. The reason why I'm
focusing on Illumina is because this is what the vast, very vast
majority of ancient DNA will be sequenced on for reasons which I will
explain later.

\hypertarget{basic-structure-of-dna}{%
\section{Basic structure of DNA}\label{basic-structure-of-dna}}

So first, to actually understand how sequencing works, we need to look
at, just do a very, very briefly recap of what DNA is. So DNA, as you
probably, everybody knows, is a double helix molecule which is stored in
every single cell in your body.

And this double helix is actually made up of four main components called
nucleotides, which you can see here
(Figure~\ref{fig-intro-ngs-fig-2dhelix}), and they consist of two
strands in which these four nucleotides will come together and bind
together in a particular order. And these four nucleotides are made up
of two groups, pyrimidines and purines. So pyrimidines are cytosines and
thiamines, and purines are guanines and adenines.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{assets/images/chapters/intro-to-ngs/fig-intro-ngs-fig-2dhelix.png}

}

\caption{\label{fig-intro-ngs-fig-2dhelix}2D molecular diagram of DNA
helix, with sugar-phosphate backbone and amine groups labelled
indicated. Source:
\href{https://commons.wikimedia.org/wiki/File:Structure_ADN.png}{Pradana
Aumars, CC BY-SA 4.0, via Wikimedia Commons}}

}

\end{minipage}%

\end{figure}

And this base pairing, which brings the two strands of the DNA together,
always consists of one pyrimidine and one pyrimine. I always remember
which group go together, and so I remember it as C always goes with G,
think CGI, and then go A with T. What's some of the best CGI you've ever
seen? It's from Star Wars, so think AT-AT Walker
(Figure~\ref{fig-intro-ngs-fig-atatwalker}), Antid Walker, you have been
able to remember that. So what this means, and because they go together,
it always means they're complementary. So whenever you find a C on one
strand of the DNA molecule, you will see a G on the other and vice
versa, and again, A with a T on the other. So depending on which strand
you are reading, you can always get the order of the base on the other
strand because of this complementary base pairing.

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=3.125in]{assets/images/chapters/intro-to-ngs/fig-intro-ngs-fig-atatwalker.jpg}

}

\caption{\label{fig-intro-ngs-fig-atatwalker}Lego construction of AT AT
walker from Star Wars. Source:
\href{https://commons.wikimedia.org/wiki/File:LEGO_Star_Wars_AT-AT_Walker_(6740909421).jpg}{Tim
Moreillon, CC BY-SA 2.0 Generic, via Wikimedia Commons}}

\end{figure}

\hypertarget{dna-replication}{%
\section{DNA replication}\label{dna-replication}}

And this is important because this is essentially how replication
occurs, so when you basically make a copy of the DNA strand. And in very
simple terms, what we do is, or what the body does, is unwinds the
multiple various macro structures, including double helix. You then
separate the strands into two, so you have, all of your nucleotides are
basically exposed, so you don't have them binding together. And then you
get an enzyme called DNA polymerase, which basically attaches onto the
strands that's reading along it, and when it finds an exposed
nucleotide, it will say, well, okay, I recognize, for example, this is a
C that's being exposed, and it waits to basically pick up a free
nucleotide floating around in the cellular gunk, and then basically
allows it to bind together in that position. Then it basically will move
along to the next exposed nucleotide, see what it is, let's say it's an
A, and then it will basically wait to find the T and fix it on the
strand, and eventually another enzyme, I believe, comes along, I can't
remember the name, I didn't name this, comes along and basically repairs
the backbone, like, no, it doesn't matter, prepares the backbone to
basically then have your two, strands from your original strand. And
just remember basically having this enzyme picking up free nucleotides
and adding it to the new strand, because this is the important thing the
sequencing is within a minute.

\hypertarget{extracting-dna}{%
\section{Extracting DNA}\label{extracting-dna}}

\hypertarget{modern-dna}{%
\subsection{Modern DNA}\label{modern-dna}}

As a reminder, how we get DNA, so when you do this, you basically get
your sample, you then have to break down the cells, the cell walls, and
membranes, and then you basically have to destroy a lot of stuff inside
the cell, there's not the DNAs, things like RNAs, proteins, and things
like this, because this can basically inhibit your DNA replication
downstream in your molecular steps. You then separate out the DNA from
the rest of the broken stuff, which you can then pull out
(Figure~\ref{fig-intro-ngs-fig-dnareplication}). And normally in modern
DNA, this can actually look like a long, this sort of spaghetti-like
thing.

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=3.125in]{assets/images/chapters/intro-to-ngs/fig-intro-ngs-fig-dnareplication.jpg}

}

\caption{\label{fig-intro-ngs-fig-dnareplication}Schematic diagram of
DNA replication, with a helicase unwinding DNA strand with DNA
polymerases on the leading and lagging strand incorporating free
nucleotides. Source:
\href{https://commons.wikimedia.org/wiki/File:DNA_Polymerase_DNA_Replication.png}{Christine
Imiller, CC BY-SA 4.0 Generic, via Wikimedia Commons}}

\end{figure}

\hypertarget{ancient-dna}{%
\subsection{Ancient DNA}\label{ancient-dna}}

However, ancient DNA is a bit different. The process is the same, you
basically have to break down the tissue, in this case it's, let's say,
bone, so you have to demineralize it to release all of the biomolecules,
and you have to degrade all the other stuff, but the DNA molecules are
also degraded, so they're already fragmented, so they're very short,
they're very damaged, so they have modified nucleotides, and they also
have contamination. So basically, your small fragments DNA is sitting in
a super-modern DNA, so a lot of these things will be covered in more
detail in other later sessions, but you have to remember that they're
damaged, they're old, they're very, very short.

\hypertarget{dna-sequencing}{%
\section{DNA sequencing}\label{dna-sequencing}}

And this brings us on to DNA sequencing, which is essentially the
conversion of the chemical, that's better, chemical nucleotides of a DNA
molecule to the human-readable ACTG on your computer screen, and what we
basically do all of our analyses on in genetics and genomics. And the
way this works is pretty common across most methods, which is you
replicate a strand, as I described a minute ago, but instead of adding
just a standard nucleotide, you add a fluorophore-modified nucleotide,
so a fluorophore is a little molecule which essentially, when you excite
it somehow, will emit a color.

And in the case of DNA, you can have four different nucleotides, and
each one will emit a different color
Figure~\ref{fig-intro-ngs-fig-colourednucleotides}. And so quite often,
the way you excite the fluorophore is firing a laser, which then emits
the light, and you record the color. And so when you're basically adding
your nucleotide each time, you fire a laser, take a picture of the color
being emitted, and then you know which base it is, and then you repeat
on the next base, and next base, and next base.

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=6.25in]{assets/images/chapters/intro-to-ngs/fig-intro-ngs-fig-colourednucleotides.jpeg}

}

\caption{\label{fig-intro-ngs-fig-colourednucleotides}Molecular diagram
of the four DNA nucleotides with coloured fluorophore amine groups.
Source: {[}@Ju2006-cl{]}}

\end{figure}

\hypertarget{sanger-sequencing}{%
\subsection{Sanger sequencing}\label{sanger-sequencing}}

So historically, the first, let's say, mass production sequencing method
was called Sanger sequencing
(Figure~\ref{fig-intro-ngs-fig-sangersequencing}). So what this involved
was taking a DNA molecule, making lots and lots of copies of it, but
also fragmenting it in a random manner, so all of the, oh sorry,
fragmenting it, no. I'm getting confused, one step ahead. Sorry, taking
a DNA molecule, making lots of copies of it. Then you start extending
the molecule, but instead of adding just no standard nucleotides, you
mix in a few special modified nucleotides, which include essentially a
blocker. And what this means is that once the polymerase gets to this
particular blocking nucleotide, it will not extend it any further.
However, as you added a mixture of standard nucleotides and also these
blocker nucleotides, your DNA molecules will be extended to somewhat a
random length each time, because you have many, many different copies.

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=3.125in]{assets/images/chapters/intro-to-ngs/fig-intro-ngs-fig-sangersequencing.png}

}

\caption{\label{fig-intro-ngs-fig-sangersequencing}Diagram of sanger
sequencing, with a primer, template and free blocking nucleotides being
added to ends of molecules at random lengths, and then being sent
through a size-selecting capillary with a laser dector to identify which
of the four blocking nucleotides are passing through the capillary and
thus ready ina chromatopgraph. Source:
\href{https://commons.wikimedia.org/wiki/File:Sanger-sequencing.svg}{Estevezj,
CC-BY-SA 3.0 Unported via Wikimedia Commons}}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=3.125in]{assets/images/chapters/intro-to-ngs/fig-intro-ngs-chromatogram.jpg}

}

\caption{\label{fig-intro-ngs-fig-chromatogram}Example of gel-based
chromatogram with four columns with bands indicating which colour is the
blocking nucleotide of that particular sized DNA molecule, and on the
size an intensity graph of each colour given off by each flourophore to
indicate which colour it is. Source:
\href{https://commons.wikimedia.org/wiki/File:Radioactive_Fluorescent_Seq.jpg}{Morse
Phoque (Abizar), CC-BY-SA 3.0 Unported via Wikimedia Commons}}

\end{figure}

In the end, you will essentially have the entirety of your original
molecule covered, as you can sort of see in this sort of step-like
manner here. And what would happen once you basically have your randomly
extended molecules, but with these blockers, you would then send it
through a capillary gel, which basically separates out the DNA molecules
based on its length, and then you would fire a laser. And the important
thing about these blocking nucleotides is also they were essentially
fluorophores, so when you fire the laser, it'll emit a light. And as you
basically have your DNA molecules going through the capillary gel, so
the shorter ones going faster through the gel because of resistance, and
the longer ones going slower, you can basically record the order of the
molecules going through the capillary gel. And then according to the
light, you can basically see the different colors, and then with that,
you can basically count which base is in your DNA molecule, as it goes
through the capillary gel, and basically reconstruct the sequence by
this method (Figure~\ref{fig-intro-ngs-fig-chromatogram}). However, this
is, the approach was actually not so good for high-throughput DNA, so
trying to reconstruct whole genomes, a lot of the original human genome
was generated with this, but this tooyears, was extremely expensive, and
it's also very, very resource-hungry, you have to use a lot of
preliminary, you have to use a lot of DNA, which again, when we deal
with ancient DNA, which you get very small amounts, because it's very
degraded, it doesn't really work.

\hypertarget{next-generation-sequencing}{%
\subsection{Next generation
sequencing}\label{next-generation-sequencing}}

And then in about 2005, next-generation sequencing, which is a bit of a
misnomer, to be honest, came along, where you can see sequence billions
and billions of DNA molecules at once. It was very fast and cheap, and
very much revolutionized genetics, and pushed us into a near of
genomics. And the market leader was, and still is, is a company called
Illumina. There are others called PacBio and IonTorrent, but really
Illumina is the one that pretty much everybody uses nowadays for at
least short read sequencing. And as I'm sure you're all aware, this is
sort of, these machines are more second generation now, we have new
machines like Oxford Nanopore, which basically do very long reads,
PacBio to a certain extent, and that's sort of more a third generation
that we're entering right now.

Oh, poop. Oh, dear, so unfortunately, my pretty picture has been, a
video has been deleted, but what I want you to imagine is a big black
window with lots of colored points. And when I press play, on all of
these points, they're going to start changing colors, going from red to
green to yellow to blue, and this happening thousands of times at once
across this big black screen. And this is essentially the process of
sequencing.

\hypertarget{illumina-sequencing}{%
\section{Illumina sequencing}\label{illumina-sequencing}}

So what this black window that you should have been seeing would have
been is something called a flow cell. This is a glass slide, and on this
glass slide, it's embedded sort of bound to the base of this, is lots of
short DNA sequences, synthetic DNA sequences called oligos. And what you
do is essentially take your DNA, and you basically inject it into the
flow cell, and your DNA will spread across the flow cell, and start
binding to this lawn. So imagine, literally imagine like a grass lawn on
the base of this glass slide, and basically all of your DNA molecules
are attaching to these random synthetic nucleotides, oligos.

\hypertarget{adapters}{%
\subsection{Adapters}\label{adapters}}

But the question is, how is your DNA actually binding onto this lawn,
and not basically getting washed away as your solution flows through the
flow cell? What you do before injecting into the flow cell is convert
your DNA samples into something called a library. A library is an
adapters, and see these are basically the complement sequence of the
oligos, of these synthetic oligos, which allow you to bind to the lawn
flow cell. But in addition to the adapters, you add onto DNA sequence,
things called priming sites, so this is where the enzymes actually bind
on to start copying the DNA.

And also, when you're sequencing multiple samples at once on the same
sequencing run, you can add things called indices, or known as barcodes,
which are basically sample specific. So it allows you to mix multiple
samples at once into one sequencing run, sequencing all at the same
time, later on separate them out. So this is ultimately a slightly
simplified version of a Illuminate DNA construct. So the X, X, X, X, X
in the middle, this is your target. So this is actually your DNA
sequence from your sample.

And then at both ends, you essentially have a target primer. So this is
where your polymerase will bind onto to start, then actually going,
reading into your target, or your insert is another phrase that you can
call it. Then prior to the target primer, you also have an index. So
this is actually your sample specific barcode. This is actually added
typically onto the adapter, and so this happens in the, no, sorry. And
the library construction, sorry. And then also you have the adapter and
index primer right at the beginning. So this is both what binds onto
your flow cell, but also acts as a primer for actually reading the index
because you also need a sequence index to know which DNA molecule, to
which sample DNA molecule it's coming from.

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

So once you've done this, in Illuminate sequencing, you have the problem
that the fluorophores will be adding to your DNA molecule. The
eliterated is not enough for a camera to pick up. It's much, much, much
too small. You're dealing with these very, very small biomolecules. And
so what you have to do is once your DNA molecule is bound to your flow
cell, sort of some randomly, you start to make lots and lots and lots of
copies. So when you have lots of copies, that means that all the copies
will make the same nucleotide, sorry, the same color at once, and make
the emissions strong enough that a camera in the sequencer can actually
take a picture and record a lot base it is.

And the procedure making these copies is called clustering. So what this
consists of is you have your DNA molecule, which is bound to one of the
synthetic oligos on the lawn. And you basically do some of the bridge
amplification where you basically bend over the DNA molecule. Sorry,
sorry, I forgot. Your DNA molecule is single stranded at the moment. So
when you bind this over, bend this over so that the reverse complement
will bind to the other oligo, a primer can bind on and start reading
across the molecule to reconstruct your double stranded DNA. So you
basically get the complement or reverse strand of your DNA molecule to
double stranded DNA. Then you had a primer to basically cut at one end
or each end of the DNA molecule on the two different sort of forward and
reverse adapters to result in two single stranded again DNA molecules.
So basically the reverse complement of the DNA molecule, but you have
one, sorry, two strands. Then you do the same thing again, bend it over,
reconstruct the full double stranded molecule again, separate it out and
do this many, many, many, many, many different times. And basically you
have loads and loads and loads of copies of the same molecule in the
same tiny little cluster, which is why you could go clustering in the
same point in the flow cell.

Then prior to sequencing, you actually will cut all of one of these
types off. So let's say you'd only be left with the purple ones to make
sure you only have the same sequence. So not the reverse complements,
but just the same sequence in one spot on your flow cell. Sorry, one
second. Basically this onto the actual sequencing process.

\hypertarget{sequencing-by-synthesis}{%
\subsection{Sequencing-by-synthesis}\label{sequencing-by-synthesis}}

Like I mentioned before with the Sanger sequencing and replication, what
you do is you basically have your single stranded molecule, you'd had a
prime, the primer, the priming site, sorry, priming site at the enzyme
for the polymerase. And then you start to add free floating modified
nucleotides, which have these fluorophores which will basically emit
your light. So there is a slight difference in Sanger sequencing,
however. So once you start adding, your polymerase goes along, it starts
adding a free nucleotide and then you have the fluorophore. Now this
fluorophore does block the next nucleotide being added on. So this makes
sure that on all of the molecules in your cluster, and you're only
adding the correct nucleotide at one point. You then defy the laser, so
the light is emitted, you take a photo.

However, the difference here between Sanger sequencing and why actually
Illumina sequencing or the sequencing by synthesis is much more
resource-sufficient is you can actually cut off this fluorophore and
then basically repeat the process again. So rather than basically having
your DNA molecule having one use of single use, you can sort of recycle
the same DNA molecule to basically then add the next nucleotide. So this
case will be, let's say this blue one, and I'll attach that on. Fire a
laser, take a photo, remove the fluorophore, and then back again. And
basically you can reconstruct the entire sequence of the molecule
without having to basically throw away the DNA molecule once you've
taken the picture of just that single nucleotide. And this normally
happens at the Illumina sequencing somewhere betweein either categories
of 50, polyester and helicopter. Normally happens at 50, 75times. And
these are also known as cycles.

Okay, this is sort of what you should have been seeing like earlier in
this animation. You should see a big black square with all these colored
dots. So the black, black square is your flow cell and all these colored
dots are these different clusters, each representing a single DNA
molecule which we copy lots and lots and lots of times. And so what you
can imagine sort of like in like a video or a film where basically
you've got hundreds and hundreds of thousands of single shots just sped
up over time, you add the first, so you have your flow cell, your DNA
cluster is bound to the flow cell. You add the nucleotides, you fire the
laser. And when you fire the laser, you should see basically a color
being emitted. So in this case, green here and here, this is another DNA
molecule, which is yellow, blue. And you take the picture and then you
know that a green is only attached to teas. And so basically you see a
tea molecule being picked up. So you record a tea, you remove the
fluorophore, you add, wash those away, add the new fluorophores, the
next base pair, the nucleotide will be bound onto the molecule, you fire
the laser, it will be this time a blue. And this means you have a G.
Then you again, cut the fluorophore off, add the new ones, fire the
laser, emits a red. And so this is how you basically reconstruct all of
these molecules. And again, the flow cell has millions and millions of
these points, of these colored dots. And this is why you can see so many
different DNA molecules all at once. Now, again, you have in your head
these four colors before because you've seen the animation, also this
picture, but there is differences to this. So there's actually two
different, with the luminous sequencing, there's two different methods
of actually emitting light. One is called the four channel system, or
four color chemistry, where each nucleotide has a distinct color.

\hypertarget{colour-chemistry}{%
\subsection{Colour chemistry}\label{colour-chemistry}}

But there's also on particularly the next seek and no seek machines, a
slight different system, where they called, they tried to make it a bit
cheaper by only using two colors or two dyes. And the approach they take
there is that, again, a T is green, red is, C is a red. However, in this
case, an A is actually two colors mixed at once. So if you emit that
both, so if the machine picks up two colors, two wavelengths are being
emitted, that is an A. However, if there was no color being emitted at
all in that cluster, the machine reaches as a G, or a no detected dye.
So this is very important for some caveats or things you have to
consider when processing your data a bit later on.

\hypertarget{paired-end-sequencing}{%
\subsection{Paired-end sequencing}\label{paired-end-sequencing}}

So something you have to consider though is we're dealing with biology.
It is not like, I don't know, chemistry or even sort of physics, where
things are perfect and wonderful. Over time, areas start happening,
we're not perfect. And essentially the imaging reagents start getting
tired, the polymerases start adding mistakes, or don't bind on properly
and more areas will occur. So sometimes your nucleotides will not bind,
meaning you'll skip a base or you'll get multiple nucleotides being
added once and you go forward one. And essentially within your cluster,
the DNA molecules being sort of replicated and emitting light will get
desynced. So the color will get a bit blurry and less clear.

And so first the machine does calculate some sort of base quality. So it
captures the probability that it thinks it captured the right color, so
the right nucleotide of each photo. But to the point where if it has no
idea, it will call a dead base call, which will be reported as an N. And
this became more and more of a problem, particularly in the early days
of sequencing. And so people thought, how can we improve or correct such
sort of errors? And the idea that came up was paired end sequencing.

So what this means is you do one sequencing in one direction, then you
flip the entire read over and then read it from the other end. So
whereas you have to consider that when you're in the forward direction,
over time you're getting more and more errors. So the further you get
into the molecule, the less confident you are. And so the more errors
are gonna be. You can then turn it over and start the whole thing from
scratch with fresh reagents. By going from the reverse end forward, you
basically can correct the mistakes that were occurring at the end of the
forward sequencing, but get the high quality calls from the other end
from the beginning. So you can sort of, that's a bad explanation. But
you can sort of see here, you read it once in this direction. So you
read the DNA insert and the prime, the index. You turn it over and you
do the same thing, but from the beginning. So you basically sequence the
same DNA molecule twice. An added bonus of this is also you get more
cycles. So if your DNA molecule is a single molecule, it's actually
longer than the cycles. It's 50\% fair, five base pairs. By going from
the other end, you can capture any DNA nucleotides which you're missing
from the forward sequencing. This is not so necessarily relevant to DNA.
We will typically very short DNA molecules, but in some cases that will
apply.

\hypertarget{demultiplexing}{%
\subsection{Demultiplexing}\label{demultiplexing}}

Then it comes on to biological to computational sequence. How do we take
these sort of very raw nucleotide sequences and put it into a format
that a computer can read? This typically happens in a step of
demultiplexing. It's actually very rare that you yourself as students
will have to deal with this or do with this. Typically this is done by
sequencing centers or by your lab team. But what this essentially
consists of is normally or rather often nowadays, there is two steps in
which you're stuck together, which colloquially are no demultiplexing.

The first step is called base calling. This is where you basically take
your photo files of every single nucleotide and convert this into an
ACTNG. So taking image files, putting into a text file. But also in most
cases, we have multiple sequence, multiple samples at once. And all the
samples have these barcodes or indices. And we need to actually separate
these out. So you know that all of these DNA molecules come from this
sample or these DNA sample, sorry, DNA molecules from this sample. And
this happens in this demultiplexing step. So essentially a computer
program will read in each DNA molecule whether it finds such a barcode
across from one sample, let's say an example here, the red one that
corresponds to sample one and the reverse here. And then the machine
will basically sort or order the DNA molecules accordingly. So all of
the DNA molecules which have these combinations and indices will put
into sample one, sample two. So the blue and yellow will go here and so
on. So this is something you, again, rarely will have to do yourself,
but it's something just to keep in mind.

\hypertarget{fastq-file}{%
\subsection{FASTQ File}\label{fastq-file}}

And the output of this demultiplexing step is something called a fastq
file. So this is a text-based format for storing biological sequences
itimes of cases nucleotide sequences, but also with the base quality
scores. So these are the things where the computer tries to estimate the
probability that it thinks that the nucleotide call was correct. And
both are encoded basically with ASCII text characters. Doesn't really
matter what they are, if you're not familiar with that. So this is a
very, very small example. These files can be gigabytes in size, so huge,
huge text files typically compressed, but still a gigabyte start to
being compressed.

But what they, all they are made of, of basically a repeating set of
four different lines. You have the first line up here, and this is
called the ID line. This stores multiple information from the sequencer.
So the sequencing, or the sequencing machine ID, then a run number, a
flow cell ID, so each flow cell will have an ID from the manufacturer.
And then you have essentially a bunch of coordinates going on here,
which basically tells you from which cluster on their flow cell has the
DNA molecule come from. Then you can have a bunch of extra information.
This is often quite random depending on the sequencing and what they put
in here, but often people put things like barcodes, maybe sort of how
many errors were allowed during gene multiplexing, because you could do
a certain enough filtering during that step. On the second line, you
have the DNA sequence itself. So in this case, it starts with an N, this
is dead-based call. This is actually quite often common, because this is
when the camera is still calibrating itself. And so the first base is
often a bit rubbish, but then the rest of the molecule, as you can see,
sorry, the sequence here is A, C, T, and G, so it's usually a new
molecule. You'll then have a plus, which is a separator. And the fourth
line of this repeating set of four is then the base quality scores. So
these are random, sort of a random set of characters, which I'll explain
in a second. But basically this tells you the confidence of that, how
good we think that that nucleotide call was.

And then you can basically see the same thing here on the next line, and
it repeats as follows. So you can see, for example, this number is a bit
different, because it's from a slightly different coordinate cluster. So
these quality scores, they are not uniform, I have to say. So it depends
on both the age of the machine and what the manufacturer selected. But
typically they will look something like this, where there's a fixed
order in the ASCII characters. And each one will correspond to a
different probability of Fred's score. I won't explain exactly what that
is, it's a bit mazzy. But essentially what it means is that the higher
the character along this score, the more confident you are of the base
call, because the probability that it's incorrect is sort of low.

\hypertarget{sequencing-recap}{%
\section{Sequencing recap}\label{sequencing-recap}}

So to recap, DNA molecules are essentially made of nucleotides, A, C, T,
and Gs. We have two strands, which is complementary based pairings, C,
G, CGI, AT, Atatoka, Star Wars, it's great. Modern DNA is very long, but
AT DNA is very short. And this is very good for NGS sequencing, where we
do this massive multiplexing. So where rather than trying to sequence
few very long DNA molecules, we sequence lots and lots and lots of very
short ones at very high accuracy, which we can then reconstruct the long
sequences later on, if you've got a good DNA. So the main steps are
adding adapters to create something called a library, which allows your
DNA molecules to bind to the glass slide, the flow cell, or something
called a lawn. You then basically make a new strand, each cycle, you
basically add a fluorescent nucleotide, which you can fire a laser, AT,
which emits a colour, you can take a photo, and by basically recording,
the order of the colours being emitted in a single point in the flow
cell, you can reconstruct the DNA sequence. This de-syncing of clusters
results in lower based quality scores over time, so you can also improve
this by paired end sequencing, where you basically sequence from one
end, and then do turn it around, then you sequence from the other end
with fresh reagents.

\hypertarget{sequencing-and-considerations-for-ancient-metagenomics}{%
\section{Sequencing and considerations for ancient
metagenomics}\label{sequencing-and-considerations-for-ancient-metagenomics}}

So for the last section of this lecture, I want to give you a few ideas
of things you should consider when you're dealing with DNA for ancient
genomics. Some of those are applicable to modern genomics too, but it's
things I find that, through my career, people forget about in some
cases.

\hypertarget{low-dna-preservation}{%
\subsection{Low DNA preservation}\label{low-dna-preservation}}

So firstly is low DNA preservation. So when you're dealing with an
ancient DNA, your samples are very old, and only have very little DNA in
the sample, and during library preparation, you may have to do lots and
lots and lots of amplification, make lots and lots and lots of copies of
your DNA to make a sufficient amount to actually put sequencing. And
this is important, and it will be discussed later during the week, but
this is important because during library construction, you can actually
inflate your counts in terms of DNA molecules that come from a
particular taxon, micro-brothaxon, for example, and basically skew your
estimate of which species are in your sample or not, or were in your
sample because they're now dead. Also, by overamplifying your DNA
molecules, you reduce the number of sequencers you actually get out
there. By having these duplicate molecules, you're not actually
providing any more extra information about your DNA library, and your
sequencing flow cell only has a fixed number of sequencing slots, which
basically, if you've overamplified your library, will basically fill up
your slots and you will not sequence as many unique reads, and so the
amount of information you're getting out of your DNA molecule can be
problematic. So this is actually quite, I've jumped ahead quite a bit in
terms of detail here, but this is, I wanted to have the slides here for
you to go back to and recap later.

\hypertarget{index-hopping}{%
\subsection{Index hopping}\label{index-hopping}}

Another thing during sequencing, you have to consider is something
called index hopping. So this is a problem, particularly with Illumina
Sequ, or people are aware of with Illumina sequencing, and it's a
challenge when you're doing multiplex sequencing. When you're sequencing
lots and lots of samples at once, and you have to have these indexes or
barcodes, which allow you to identify this DNA molecule comes from this
sample. This seems to happen more often on a type of flow cell, which
you find on Illumina HiSeq X and NovaSeq machines, and is ultimately
caused by free floating index primers. And why that is a problem, is
that if you do not sufficiently clean up your primers, during the
clustering process, you can accidentally start adding on or switching
barcodes between DNA molecules. This does happen at a relatively low
rate, but it does happen. And so what it means is that essentially you
switch the barcodes and you may accidentally assign a DNA molecule from
one sample into another sample when you're doing demultiplexing. And so
let's say you're dealing with a microbiome sample, and you have lots of,
let's say, unless you have an oral microbiome sample and a gut
microbiome sample, you may start seeing, for example, oral species
popping up in your, or sort of oral species which are only found in the
mouth, ending up in your gut samples, which may be a bit weird. This can
also be a particular problem if you're doing microbial genomics, so
doing, let's say, pathogen reconstruction, if you're working on that.
And you mix capture results with your shotgun samples because then you
may start picking up the high amount of capture results in your shotgun
samples where you're doing screening. So you may start getting false
positives there. There's quite a few papers on this, also in the context
of ancient DNA, so van der Waag has got quite a good paper to understand
that and also had to correct such, or estimate the level of this in your
studies.

\hypertarget{sequencing-errors}{%
\subsection{Sequencing errors}\label{sequencing-errors}}

They go back to the sequencing, you have to consider your sequencing
errors. So if you don't sufficiently quality control and check for these
errors happening on DNA sequencing, you may actually start incorporating
errors into your analysis downstream. So for example, what may happen is
if you have a low base quality score, the machine may have picked up the
wrong base or the wrong nucleotide. And this means that your DNA
molecule or sequence, when you compare to reference genome databases or
reference genomes, may start going to the wrong place or match the wrong
reference genome because you have the wrong nucleotides, the wrong
sequence. Which can be a problem. Also, it can reduce your chance of
getting sufficient overlap during the assembly, which is where you
basically stick together all of the overlapping DNA molecules together
to try and reconstruct the entire molecule. This is something that Alex
will present on Thursday, like Subna. And also if you're doing variant
calling for phylogenomics and you have very low coverage, this may also
start increasing, increasing the add errors and you will do the wrong
SNP call, which means that basically your, the relationship between your
genomes in your tree, for example, will be skewed. And so it's always
very important to check for such errors and check that your sequencing
run was high quality.

\hypertarget{dirty-genomes}{%
\subsection{Dirty Genomes}\label{dirty-genomes}}

You also have to consider, so this is again sort of jumping ahead, but
there's a reason why I'm putting in this presentation, is dirty genomes.
So unfortunately, there are many reference genomes which are very dirty.
So dirty, I mean, for example, having a lot of adapters still in there.
And this means you have the problem where if you yourself have not
sufficiently cleaned up your DNA library to remove the adapters and
remove also pre-processing, you will start seeing weird results. For
example, I very, very highly expect that if any of you screen against
the NTPI-NT nucleotide database, you will start seeing carp everywhere.
And the reason why is because people somehow got onto the NCBI a whole
carp genome without removing any of their adapters. So there's adapters
sequences everywhere in the genome. And so whenever you have an adapter
in your library, this will basically align to the carp genome and then
you'll get carp, which particularly if you're trying to look at diet,
for example, or ancient diet in, like say microbiome studies, where you
look at some calculus, you may start seeing carp even if you're, I don't
know, from, got samples from, I don't know, Chile or somewhere where
carp is not expected to find. You also often will find this with
zebrafish. So often many, many, many, I think it's Darius Varini or
something like that, which makes no sense because all of these fish
comes from one lake in Africa, but you see it everywhere in your
metronome examples. And again, it's because of dirty genomes where they
think of adapters or vectors in the genome.

\hypertarget{low-sequence-diversity}{%
\subsection{Low Sequence Diversity}\label{low-sequence-diversity}}

Another thing is low sequence diversity. So what I mean by this is
mononucleotide reads, like GGGGG, or dinoucleotide repeats. This is not
so much of an error necessarily when you're doing metagenomics, but when
you come into genomics, this can be a problem. So the problem with such
DNA molecules, like this one here, GGGGG, is they're very unspecific.
They give you no information. That can come from any species everywhere
because they are very common across all genomes. So the problem here is
that firstly, it slows down your processing because basically you are
aligning against, or comparing of DNA sequence against many, many, many
different genomes to ultimately say, I don't know which one it comes
from, which is unnecessary. And also in some cases, it can inflate
counts at higher nodes when you're doing an LCA. This will be described
later on. But this can be very common. So if you remember this two-color
chemistry which I mentioned earlier, where you don't have one color per
base, but rather if there is no color emitted, the machine uses a G,
particularly with an ek-seq and no-seq data, you will have a lot of
these Gs, particularly as we're dealing with ancient DNA, which is very
short. When you have very short ancient DNA and you don't reach all of
your cycles, so let's say you're doinbase pair cycles, but your DNA
molecules are onlbase pairs, you will basically get to the end of your
DNA molecules and not add anything else. So you start getting these very
long tails of Gs at the end of your molecules. And if you don't remove
these, this will make it very difficult to correctly align your DNA
molecule to a reference genome or sequence. So be aware to look for
these and remove these. Also because it will speed up your processing.
So to recap the considerations, a lot of this will make more sense later
on in the other sessions, but consider your duplication rates. You check
for lots of copies of same DNA molecule in your library. It's a good
idea to check for index hopping, so making sure that the index
combinations that you have in your library are correct and you've sorted
your samples correctly. Always check for sequencing error and remove low
quality bases if possible. Check for adapters, so to make sure you don't
start finding CARP everywhere. And also it's a good idea to look for
low-seq and diversity reads. For example, particularly if you put
next-seq or no-seq data, because it just slows down your results and you
get lower quality text-long assignment.

\hypertarget{q-and-a}{%
\subsection{Q and A}\label{q-and-a}}

\hypertarget{how-to-design-barcodes}{%
\subsubsection{How to design barcodes}\label{how-to-design-barcodes}}

Okay. How to design the barcodes. That's a question from UD. That is
quite tricky because there's a lot of considerations you have to make
when making sure there's a balance and they're not too similar to each
other. Often manufacturers have tools which allow you to basically
generate this. I believe they also to sentence and have standard sets
which they can also send you that you request. So you yourself do not
have to necessarily design these. It depends on your lab. So often I'd
say speak to your sequencing center if you have them. Because they all
have advice. All check manufacturers, I think most like Agilent and
Illumina will also basically have such things for you then. Yeah,
sometimes they make it a little bit defined but you can't find them. And
if you read the Meyer and Chercher article for buildinnew libraries, it
would be provided in that set. Yeah. Could everyone hear Tina then? No?
Okay. So she said that often the manufacturer make it a bit hard to find
such functionality on their websites and stuff, but you can often do
that. But also if you read the sort of classic paper by Meyer and
Chercher 2011, 12, they actually have the set of barcodes that you can
use yourself. So I think that link, that paper is on the website
somewhere but we can also share with you.

\hypertarget{how-many-indicies-can-you-use}{%
\subsubsection{How many indicies can you
use}\label{how-many-indicies-can-you-use}}

How many indices can you use? So this is a good question. This depends
on your strategy and is slowly changing over time. So you can actually
choose one index if you want, which is at the beginning of your, this is
from Laura, which is the beginning of your molecule. What has been
recommended and what the Meyer and Chercher paper introduces double
indexing where you have two at the end. And these are attached to your
adapters. What people are commonly doing now is actually adding
additional barcodes called inline barcodes. So these are very short
sequences, about seven base pairs I think, which you actually attach
immediately to the DNA molecule before library preparation. So after
extraction before library preparation. And this actually helps you with,
sorry, with correcting for index hopping. So if you read the van der
Waal paper, which I mentioned earlier, we can send the link again later.
They also describe how they use these internal barcodes to separate out.
So for example, you can get from a manufacturer about 100, let's say
barcodes, but you will normally per library can have somewhere between
one to four separate identifiers.

\hypertarget{why-gs-called-more-often-in-two-colour-chemistry-sequencing}{%
\subsubsection{Why Gs called more often in two colour chemistry
sequencing}\label{why-gs-called-more-often-in-two-colour-chemistry-sequencing}}

So I have a question regarding the gene calling for Gs, right? You
mentioned that Gs are a characteristic of ancient DNA or sequencing
errors. Why is it specifically Gs that are called more often rather than
the other bases? The reason, okay, so that is because your DNA molecule
is very short. So let's say your DNA sequence is onlbase pairs long, but
your sequencing cycles, so the number of cycles of imaging your machine
is gonna do is let's sabase pairs. Once you've got through thbase pairs,
there is nothing to sequence anymore. Your lights are not going to emit
anything. So when you're on nobody-connected data, if no, sorry,
machines, if no light is emitted, it reads it as a G. And you have to
remember that the machine is not going to stop imaging once that one DNA
molecule is finished. The DNA, the sequencing machine will keep taking
photos until it's reached to the number of cycles you've set, whether in
this case, 75. So once you've exhausted your DNA molecule, there's
nothing to sequence, nothing to image anymore. So basically the machine
will just keep picking up G for every remaining cycle of the run. Does
that make sense? Yes. Yes, it does. But still I don't get why G and not
like AT. Why is it specifically the space? Because on nobody can make
sick data, whatever reason they've decided Gs means nothing. There's no
color. Once it runs out of the\ldots{}

Yeah, so basically the problem with ancient DNA is we often have very
short reads. So you might do say two bsequencing, which is very common,
but you might have a read that's onlbases long. And so once it kind of
runs out of DNA, it will just, it won't sequence anymore. So there'll be
no more fluorophores. And because the NovaSeq and the next you can
interpret that as a G, you'll just get these polyG tails, but actually
it just means no more data. And another thing to\ldots{} Could you make
it up for yourself? And I think this may also happen in modern data as
well. If you've fragmented your modern data too short, you'll also get
that. It's just that in the complex of ancient DNA, the reason why I
said that is because we are naturally already very, very short because
of the degradation. And I'll also say this is, if you're used to doing
modern DNA, this is where it's really different because let's say you're
sequencing a regular library. What you would normally do for modern DNA
is you have genomic DNA, which is huge. So for your microbial DNA, each
genome is something likmillion bases long, and that's way too big for an
aluminum machine. So what you would do is you would shear it either
enzymatically or by sonication, usually to an average size of aboubases.
And then you do your alumina sequencing usually two by 150. So you kind
of measure one side, then you measure the other side, and you get a
total obases sequenced out of thbase pair read. You never run out of
DNA. You never actually get to the end of the molecule when you're
sequencing. And so for most people that do modern DNA sequencing,
they've never dealt with this problem before because they never see it
because they're always sequencing a DNA molecule longer than what their
sequencing chemistry can actually do. For ancient DNA, it's very
different. We actually don't shear. We take advantage of the fact that
because our DNA is short, we don't have to shear. And because we don't
shear, it actually allows us to exclude some of the modern contamination
because any modern DNA that's in there will be so long that it won't
build a proper library and it won't be sequenced. And so it kind of
helps us clear out some of the modern DNA that might be present. And so
we will only sequence the short DNA sequences, which are more likely to
actually be ancient. But the problem there is we're dealing with the
real size of the ancient DNA, which might bbases, 30 basesbasesbases. We
don't have necessarily the kind of consistency you would have if you
were intentionally shearing modern DNA. So we do have some sequences
that are very short.

\hypertarget{what-is-full-genome-seuqencing}{%
\subsubsection{What is full genome
seuqencing}\label{what-is-full-genome-seuqencing}}

Okay, so Liasat asked, when we're talking about whole genome sequencing,
so WGS and full genome sequencing, FGS, is it the same? I've never heard
of FGS. So yes, I would say it probably is.

\hypertarget{tools-for-generating-indicies}{%
\subsubsection{Tools for generating
indicies}\label{tools-for-generating-indicies}}

And then Jaime asks, is, I hope I'm saying that right. Is this kind of
program publicly available? Is this to? Sorry? It was the index checker
to make sure your pool is not. It was the index checker, yes. So again,
lots of tools online, I think, basically, to make sure there's no
overlap. Normally the manufacturer will offer such thing.

\hypertarget{readings}{%
\section{Readings}\label{readings}}

\hypertarget{reviews}{%
\subsection{Reviews}\label{reviews}}

{[}@Schuster2008-qx{]}

{[}@Shendure2008-fh{]}

{[}@Slatko2018-hg{]}

{[}@Van\_Dijk2014-ep{]}

\hypertarget{sequencing-library-construction}{%
\subsection{Sequencing Library
Construction}\label{sequencing-library-construction}}

{[}@Kircher2012-fg{]}

{[}@Meyer2010-qc{]}

\hypertarget{errors-and-considerations}{%
\subsection{Errors and Considerations}\label{errors-and-considerations}}

{[}@Ma2019-lg{]}

{[}@Sinha2017-zo{]}

{[}@Van\_der\_Valk2019-to{]}

\hypertarget{questions-to-think-about}{%
\section{Questions to think about}\label{questions-to-think-about}}

\begin{itemize}
\tightlist
\item
  Why is Illumina sequencing technologies useful for aDNA?
\item
  What problems can the 2-colour chemistry technology of NextSeq and
  NovaSeqs cause in downstream analysis?
\item
  Why is `Index-Hopping' a problem?
\item
  What is good software to evaluate the quality of your sequencing runs?
\end{itemize}

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\hypertarget{introduction-to-ancient-dna-1}{%
\chapter{Introduction to Ancient
DNA}\label{introduction-to-ancient-dna-1}}

\hypertarget{lecture-1}{%
\section{Lecture}\label{lecture-1}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/2a-intro-to-adna/SPAAM\%20Summer\%20School\%202022\%20-\%202A\%20-\%20Intro\%20to\%20Ancient\%20DNA.pdf}{here}.

\hypertarget{questions-to-think-about-1}{%
\section{Questions to think about}\label{questions-to-think-about-1}}

\begin{itemize}
\tightlist
\item
  What is ancient DNA?
\item
  Where do we find ancient DNA from microbes?
\item
  How does DNA degrade?
\item
  How do I interpret a DNA damage plot?
\item
  How is DNA damage used to authenticate ancient genomes and samples?
\item
  What methods are available for managing DNA damage?
\item
  How does DNA damage matter for my analyses?
\end{itemize}

\hypertarget{introduction-to-metagenomics-1}{%
\chapter{Introduction to
Metagenomics}\label{introduction-to-metagenomics-1}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

\hypertarget{lecture-2}{%
\section{Lecture}\label{lecture-2}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/3a-intro-to-metagenomics/SPAAM\%20Summer\%20School\%202022\%20-\%203A\%20-\%20Intro\%20to\%20Metagenomics.pdf}{here}.

\hypertarget{questions-to-think-about-2}{%
\section{Questions to think about}\label{questions-to-think-about-2}}

\begin{itemize}
\tightlist
\item
  What is a metagenome? a microbiota? a microbiome?
\item
  What is ancient metagenomics?
\item
  What challenges do DNA degradation and sample decay pose for ancient
  metagenomics
\item
  How do you find out ``who's there'' in your samples?
\item
  How do alignment based and k-mer based taxonomic profilers differ?
  What are the advantages and disadvantages of each?
\item
  Why does database selection matter?
\item
  How do you estimate the preservation and integrity of your ancient
  metagenome?
\item
  What are tools you can use to identify poorly preserved samples and
  remove contaminant taxa?
\item
  What aspects of diversity are important in investigating microbial
  communities?
\item
  Which distance metrics are commonly used to compare the beta-diversity
  of microbial communities and why? What are some advantages and
  disadvantages to these different approaches?
\end{itemize}

\hypertarget{introduction-to-microbial-genomics-1}{%
\chapter{Introduction to Microbial
Genomics}\label{introduction-to-microbial-genomics-1}}

\hypertarget{lecture-3}{%
\section{Lecture}\label{lecture-3}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

Slides and videos are from previous version of summer school and may not
match text

\end{tcolorbox}

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/4a-intro-to-microbial-genomics/SPAAM\%20Summer\%20School\%202022\%20-\%204A\%20-\%20Intro\%20to\%20Microbial\%20Genomics.pdf}{here}.

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

Microbial genomics is the identification and study of microbial genomes,
their structure and composition. Microbial species come from diverse
groups of organisms but are generally defined as organisms which are not
visible to the naked eye. Most prominent amongst them are
\textbf{bacteria}, which are single-celled prokaryotes with either
circular and linear dsDNA genomes (up to \textasciitilde14 Mbp).
\textbf{Archaea} are mostly mutualistic or commensal single-celled
prokaryotes with circular dsDNA genomes (\textasciitilde0.5 to
\textasciitilde5.8 Mbp). The often references group of
\textbf{protozoa}, is a not phylogenetically defined grouping (often
used in databases). They are a wide variety of free-feeding
single-celled eukaryotes from the protists group with larger genomes
(\textasciitilde2.9 to \textasciitilde160 Mbp).

Finally, \textbf{viruses} are understood as microbial organisms, even
though they are not technically considered as ``organisms''. Viruses
lack the ability to live or replicate independently of host cells. They
are mostly defined as infectious agents and have smaller linear or
circular ssDNA, dsDNA \& RNA genomes (2 kb to over 1 Mb). Finally,
another category of viruses are \textbf{retroviruses}. These will
usually be RNA viruses which can integrate into the host genome by
converting to DNA. But there are also DNA viruses, which integrate into
the human genome. There the virus can either be latent and be later
triggered to activate, continuously produce virions or lose the ability
to produce virions and become part of the host genome. An integrated
genome is called a \emph{provirus} and vertically inherited proviral
sequences are called \emph{endogenous retroviruses} (ERVs).

There are several types of microbial organisms found within ancient DNA
samples of animal hosts.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  \textbf{Pathogens}: infectious microorganisms or agents which cause
  disease in the infected host. Usually specialized organisms with
  delimited ecological niches.
\item
  \textbf{Commensals}: non-infectious microorganisms or agents which
  live within a host/environment without causing harm and can be
  beneficial.
\item
  \textbf{Environmental microorganisms}: Environmental microbial
  organisms not endogenous to the host ante-mortem, which e.g.~stem from
  the depositional environment, the storage environment or the lab.
\end{enumerate}

However, it should be noted that not all cases are quite as clear-cut as
there are wide varieties of microbial lifestyles. Some organisms can be
considered pathogenic in one sampling location and commensal in the next
(\emph{pathobionts}). Some organisms can also be considered harmful,
when they are represented in very large amounts or in combination with
other organisms. The sampling location can therefore be important
information when studying the health of hosts.

\hypertarget{larger-genomic-elements}{%
\section{Larger Genomic Elements}\label{larger-genomic-elements}}

When using reference sequences from public depositories such as NCBI you
will often be confronted with multiple intervals which represent the
chromosome(s) and additional extrachromosomal sequences associated with
a species or strain. A \textbf{chromosome} is understood as the main
genetic element. It contains the core genome with all essential genetic
elements, which usually remain the same across a species.

\textbf{Plasmids}, on the other hand, are extrachromosomal DNA
replicons. They are replicating and acquiring/losing genetic material
independently of the chromosome and can be present in high copy numbers.
Plasmids can be vertically or horizontally transferred. The number of
plasmids of a bacterium varies a lot (0 to 30+), and can vary within a
species. They can carry virulence genes and genes which can give
selective advantages. Plasmids generally have much smaller circular
genomes (1 to over 400 Kbp). It should be noted that they are
under-represented in databases, meaning that they are often ignored
during the assembly/sequencing process and thus not present in all
reference sequences, even if they could otherwise have been there.

Besides plasmids, \textbf{bacteriophages} (or phages) are also important
extrachromosomal (and sometimes chromosomally integrated) genetic
material associated with bacterial genomes. Although they are not
considered as part of the genome, and therefore part of reference
sequences, if they are not integrated in the host cell genome.
Bacteriophages are dsDNA viruses of small size that infect bacteria. As
phages have mostly dsDNA genomes, they are also well suited for the
recovery using standard aDNA techniques. They can be found everywhere
and carry gene sets of diverse size and composition. They enter the
cytoplasm of bacteria, where they can then replicate. There are a huge
number of phages, and some are specific to certain bacterial genera or
species, which can be useful for identifying taxa or phylogenetic
clades. When bacteriophage genomes integrate into the host cell
chromosomal genome (e.g.~through horizontal gene transfer) or exist as a
plasmid within the bacterial cell, they are called \textbf{prophages}.
They can make up a large fraction of the pan-genome of plastic species.

\hypertarget{sampling-sequencing}{%
\section{Sampling \& Sequencing}\label{sampling-sequencing}}

For ancient DNA samples, screenings for organisms of interest is usually
performed on ``shotgun'' sequencing data, which can then be either
further gnomically analysed following species identification or the
library will be enriched for organisms identified during screening.
Prior to sequencing, laboratory workflows, and particularly library
building setups, can have a major impact on your final output (e.g.~if
you are interested in RNA/ssDNA viruses but have dsDNA libraries).

Each pathogen is subject to tissue tropism, meaning that each species
will have a range of cells or tissue types it will preferentially or
exclusively proliferate and replicate in. This phenomenon is called
\textbf{tissue tropism}. In some instances, the pathogen can also become
latent within said tissues, meaning it will remain within the host
tissue without causing disease. Organisms which cause
\textbf{bacteremia} or \textbf{viremia}, meaning they are present in the
bloodstream, will not be as restricted and are generally considered
easier to detect. Although this can be limited by the disease phenotype
or the stage of infection (e.g.~\emph{Haemophilus influenzae}).

Accordingly, what organism you can find is highly dependent on the
sample and where it was taken from. For example, if the organism of
interest enters the bloodstream, teeth, which are vascularized and are
excellent DNA archives, will probably be a good choice for sampling.
Some pathogens (e.g.~\emph{Mycobacterium leprae}) are present in higher
quantities within lesions caused by their infections, making those
lesions better suited for sampling. In the case of infections, which are
unlikely to be retrievable from hard tissue, calcified nodules can be an
interesting type of sample. Bone will generally be less well suited,
with some exceptions. Integrated viruses and retroviruses will be
likelier to be found in samples types with higher host DNA, such as the
petrous bone or the ossicles. And early childhood infection can also be
found within low remodelling cortical bone.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Some Myths to Dispel\ldots{}}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  Microbial content and pathogen content is NOT directly correlated to
  human DNA content. Human DNA content is not a measure for overall
  sample preservation. You can have really bad samples for human DNA but
  get a full microbial genome from the same sample!
\item
  You can also find microbial signatures in petrous bones etc. it will
  just be a different range of organisms. Everything is worth getting
  screened!
\end{itemize}

\end{tcolorbox}

The typical number of sequences recovered for an organism will depend on
a range of factors which can only rarely be predicted even in samples
for which osteological/historical record show a clear association with a
pathogen. Major factors are: overall sample preservation, depth of
sequencing, abundance of the organism peri-mortem, disease phenotype,
genome size \& composition, ``noise'' from other organism, etc.

Overall, it is considered a good result if your target organism makes
out around 0.1\% of shotgun datasets, but in many cases it will be way
less. However, depending on the organism, the amount of data you need to
confidently classify it will be very different, mostly due to sequence
conservation and genome size and complexity. While a bacterium is
generally easier to detect, their large genome size and sequence
conservation, makes them harder to verify. On the other hand, viruses,
having much smaller genomes, are harder to detect, especially at low
sequencing depth, but generally easier to verify within a margin of
uncertainty.

For some applications in microbial genomics, unselective/biased
sequencing can be key, and for this shotgun sequencing is a powerful
tool at our disposal since it allows for the indiscriminate sequencing
of all DNA found within the sample. Particularly in cases of organisms
with large pangenomes, where you might be interested in looking for a
large variety of intervals, which would be very costly to do using
target enrichment. Or in cases where novel insights and genomes become
relevant following the design of a target enrichment kit. Additionally,
it allows for the simultaneous analysis of both microbial and host DNA.
Understanding the composition of the microbial community of your sample
can also be highly relevant with regard to identifying co-infectants,
reconstructing disease histories and excluding gene intervals which
could also stem from commensals or contaminants.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shotgun sequencing for microbial genomics:}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Pro(s)}:

\begin{itemize}
\tightlist
\item
  Non-targeted taxa can be found.
\item
  Allows the analysis of DNA from both microbial organisms and host
  simultaneously.
\item
  Allows you to detect untargeted co-infections.
\item
  Allows you to understand the composition of the microbial community
  (can be relevant for genomic analysis).
\item
  No knowledge of the taxon genomic diversity is needed beforehand.
\item
  Enables the long time use of the data with ever growing databases.
\end{itemize}

\textbf{Con(s)}:

\begin{itemize}
\tightlist
\item
  Can be much more expensive (depending on the sample).
\item
  Overall less effective at generating adequate/high coverage data.
\end{itemize}

\end{tcolorbox}

However, microbial species make up only very small fractions of genomic
libraries. So small that it can either be impossible or very expensive
to try to assemble a genome using only shotgun data in most
circumstances. This is usually where target enrichment, or capture,
comes into play. Often, the baits required for capture are custom
designed for each study. The design of such kits necessities appropriate
knowledge of the genetic diversity to be expected during analysis. In
most cases, a mixture of shotgun and target enrichment can be most
effective, especially with regards to authentication if enrichment is
performed on UDG/Half-UDG genomic libraries.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Target enrichment for microbial genomics:}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Pro(s)}:

\begin{itemize}
\tightlist
\item
  Much cheaper per genome, especially in samples with bad preservation.
\item
  Effective at generating high coverage genomes.
\item
  Allows for the generation of large number of genomes effectively.
\item
  Great for core-genome reconstructions.
\end{itemize}

\textbf{Con(s)}:

\begin{itemize}
\tightlist
\item
  Necessitates knowledge of the genetic diversity relevant to your
  research questions during the design phase.
\item
  Any sequence not included, within a margin of variation, will not be
  captured.
\item
  Will not generate any data with regards to the host or the rest of the
  microbial community (in fact that is the point).
\item
  For some species, capturing the pangenome would be extremely expensive
  provided it doesn't fully exceed kit sizes.
\end{itemize}

\end{tcolorbox}

\hypertarget{validating-the-presence-of-organisms-of-interest}{%
\section{Validating the Presence of Organisms of
Interest}\label{validating-the-presence-of-organisms-of-interest}}

Following analysis of your data using appropriate databases and
taxonomic classifiers, it is time to validate your hit. The results of a
classifier should not be your end result. Properly validating the
presence of a species is key to any genomic analysis! An identification
by classifiers is not a sufficient validation on its own, as it can be
very tricky to work around false-positive hits and missing database
entries. Databases are biased towards pathogenic species, as they
represent the bulk of research. Closely non-pathogenic species will
either be represented less or not at all, which leads to high read
assignments to the LCA (Lowest Common Ancestor) and pathogenic taxa. You
should also consider whether it makes biological sense for the species
you found to be detected in the sample's tissue and based on your
laboratory workflow.

For ancient DNA, the validation will often consist of three steps:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Authentication of the data as aDNA using deamination signatures.
\item
  Comparative or competitive mappings to relevant reference sequences
  (target organism and closely related environmental/commensal species).
\item
  Identification of intervals specific to the target species or
  sub-species.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

It can also be a good idea in some cases to double check the modern data
you are using during your analysis. There are cases in which the
metadata doesn't match reality.

\end{tcolorbox}

Increased coverage in selected intervals in an alignment is often cause
by sequence conservation. Meaning that you could have the same number of
reads mapping to a genome but in the first case all reads are mapping to
5\% of the genome with high depth of coverage, whether in the other one
you will see low coverage across the whole sequence. The first one is
likely to be a false positive, and your detection is based on intervals
from either a commensal or an environmental organism which have very
high sequence similarity to the ones in your reference genome. This can
be further worsened by low complexity intervals. These peaks will
usually also reflect in the edit distance, as such tiling will also
result in more sequence mismatches. While this can be reduced with
higher mapping stringency, this in turn will probably decrease your
coverage significantly and in some cases this can cause a reference
bias. However, in most cases, these intervals will not pass the mapping
quality filter.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title={Definition}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Conserved Intervals:} Regions of genomes common to organisms
from the same taxonomic units (can affect all taxonomic ranks and child
taxa, and very distantly related organisms). They will show high
sequence similarity and cause noise/contamination within the analysis,
particularly for ancient DNA.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

In many cases, running a range comparative and/or competitive mappings
to closely related species is advised in order to exclude a
misidentification and the presence of multiple species with similar sets
of genes in your dataset (e.g.~\emph{Neisseria meningitidis}). It can
also be useful to use multiple reference sequences per species if it has
multiple phylogenetically strongly delimited clades (e.g.~\emph{H.
influenzae})

\end{tcolorbox}

Viruses are less affected, as they do not carry house-keeping genes, and
only have a very small usually highly specialized set of genes, which
makes them easier to validate. However, viral sequencing of
non-pathogenic species has only recently started to become more popular
due to metagenomic studies, i.e.~non-pathogenic taxa are even more
under-represented, and many viral genomes have low complexity repeats
over large portions of their sequence.

Finally, intervals or mutations specific to the target organism should
be identified and investigated. This can range from plasmids, genes,
phages to specific mutations. Often a combination of these factors is
required to confidently validate the presence of the organism. This step
should not be overlook as this will not always be clearly visible in a
phylogeny, depending on the composition of the alignment used.

\hypertarget{genomes}{%
\section{Genomes}\label{genomes}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Strains VS Genomes}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  A \textbf{strain} is a genetic variant or subtype of a species or
  sub-species. They usually exhibit significant genetic differences to
  other strains of the same taxonomic unit. The level of change required
  for significance can vary.
\item
  A \textbf{genome} can be the exact copy of an already sequenced
  strain.
\end{itemize}

\textbf{The case of aDNA:} Since our samples are so old, most of our
genome constitute new strains. However, caution is advised. Particularly
with clonal species (e.g.~Black Death genomes).

\end{tcolorbox}

There are many different microbial species, and they all have very
different evolutionary dynamics and genomes. This means that depending
on which species you are working on, you might have to approach genomic
analysis differently. Additionally, the research questions which could
be answered with your data will/can also be rather different. This
sections will summarise some of the core principles underlying many
possible research questions.

Mapping is an important tool in ancient DNA to investigate the presence
and absence of genomic intervals. However, mapping can be impeded by the
reference sequence itself. Gene duplication, low complexity sequences
and GC Skew (over- or under-abundance of GC in intervals) can impact the
mappability (and mapping evenness) of sequencing reads to the reference
sequence and their mapping quality score. Which in turn might be
problematic during analysis. This can be expressed in mappability
estimates, which estimate how likely it is for short reads to map to a
sequence interval based on its composition and uniqueness.

Applying a mapping quality filter by default after a bwa aln mapping,
will not only cause all bad quality reads to be removed from the
alignment, but also any read which could align to multiple sections of
your reference genome. This means that most likely every duplicated
interval/gene, specific or conserved, and low complexity regions will
also be removed! Mapping quality filters are important tools, but you
should make sure you understand how much of the genome is actually
covered (e.g.~terminal repeats in viral genomes) and when to apply them.
The same applies for competitive mappings using bwa aln!

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title={Definition}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Sequence Complexity:} Defined as the observed vocabulary usage
for word size. Meaning, how complex is the sequence of nucleotides
within an interval. Often given as values calculated using either
entropy or DUST algorithms. E.g.:

AAAAAAAAAAAAAAAAAAAA \textgreater\textgreater{} \emph{LOWEST COMPLEXITY}

ATGATGATGATGATGATGATG \textgreater\textgreater{} \emph{LOW COMPLEXITY}

ATGTTTCGAGGCATGATAACCGTATG \textgreater\textgreater{} \emph{COMPLEX
SEQUENCE}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title={Definition}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{GC Content:} Is usually given as a percentage of guanine and
cytosine bases within the sequence. Can impact DNA stability,
amplification, sequencing, and capture if the GC content is too high or
too low. Too high or too low GC-content will also lead to a loss in
sequence complexity. E.g.:

GCCCCCCCGGAATGGACCCGCGCCT \textgreater\textgreater{} \emph{80\% GC}

ATTTTGAACCTAATTTATATAGCAA \textgreater\textgreater{} \emph{20\% GC}

\end{tcolorbox}

\hypertarget{recombination}{%
\subsection{Recombination}\label{recombination}}

Bacteria and viruses are haploid and have small genomes, making some
things much easier, but\ldots{} They like to mix and match! Microbial
organisms will exchange genetic material using a range of biological
mechanisms (conjugation, transduction, and/or transformation) this leads
to increased genetic diversity via \textbf{horizontal gene transfer}.
Some parts of the genome will be more heavily affected by this exchange
than others. This constant exchange of genetic material can happen while
maintaining genome size, meaning that while genes are gained, others are
lost. How much \textbf{recombination} a genome will undergo, is highly
dependent on the species or sometimes the phylogenetic clade. These
dynamics cause recombination breakpoints in reference based alignments
where SNP counts will increase, which can impede phylogenetic analysis
(e.g.: by causing elongated branches).

However, some species do not recombine at all or do so only very rarely.
These species have \textbf{clonal genomes}, meaning they transfer their
entire genome vertically, most the of time, without significant changes
to their pangenome or the sequence itself. Actually, many of the species
which have been extensively studied using aDNA, fall within this group
(e.g.~\emph{Y. pestis}, \emph{M. leprae}).

Most microbial organisms change their genomes via recombination and gene
loss/gain, while maintaining genome size and retaining their core
genome. Virulence is then often defined by gaining or losing virulence
factors. However, some increase their virulence by reducing their genome
and becoming highly specialized (e.g.~\emph{Borrelia recurrentis}) in
what is called \textbf{reductive evolution}. In these cases, it is of
interest to include closely related phylogenetically ``basal'' species
(pathogenic or not), which might inform you on genes/plasmids the modern
genomes have lost, but the ancestral genomes might have still retained.

\hypertarget{pangenomes}{%
\subsection{Pangenomes}\label{pangenomes}}

As mentioned, microbial organisms can be very plastic and exchange
genomic material. This can lead to a wide variety of genes being
represented within a single species. Pangenomes represent the sum of all
genomic intervals represented within the genomes of one species. And
thus, characterising the genome of a new strain can be a lot more
complex than sticking to a single reference sequence.

Pangenomes are made up of three groups:

\begin{itemize}
\item
  \textbf{Core Genes:} a set of essential genes common to all strains of
  a species.
\item
  \textbf{Accessory/Shell Genes:} a set of genes common amongst some
  strains of a species which encode for supplementary or modified
  biochemical functions. In some cases these will also be virulence
  genes.
\item
  \textbf{Singleton/Unique/Cloud Genes:} genes, which are specific to
  single strains of a species. Unlikely to be recovered and identified
  using ancient DNA.
\end{itemize}

We also differentiate between open and closed pangenomes. In open
pangenomes, the gene set available to the bacterium constantly expands
by acquisition and loss of genes via horizontal gene transfer from its
environment, for the purpose of adaptation (e.g.~environmental
adaptation, metabolism, virulence and antibiotic resistance). All while
maintaining genome size and vertical transmission of the core genome.
Closed pangenome have limited exchanges of genetic material. This is
often the case in highly specialized organisms. So while they are much
more plastic than strictly clonal species, the available genetic pool
for exchange will be smaller.

\hypertarget{snp-effects}{%
\subsection{SNP Effects}\label{snp-effects}}

Single nucleotide polymorphisms (SNPs) can heavily impact the function
of genes and the phenotype of an organism when they happen to affect and
change the amino acid sequence of coding elements of a genome. This
effect is frequently used in ancient DNA to either predict the
presence/absence of significant mutation, which could have changed the
phenotype of a disease.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title={Definitions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Non-synonymous (missense) mutation:} SNPs that alters the amino
acid sequence of a protein by changing one base of a triplet group.
E.g.: GAT\textgreater GGT == Asp(D)\textgreater Gly(G)

\textbf{Frameshift:} Changes in the amino acid sequence caused by
insertions or deletions of nucleotides in coding sequences which are not
multiples of three.

\textbf{Start/Stop Codon:} Nucleotide triplet which signals the
beginning and end of translation.

\textbf{Stop/Start-gain mutation:} Mutation which leads to a change in
the amino acid sequence and leads to a premature stop/start of
translation.

\textbf{Start/Stop-loss mutation:} Mutation which leads to a change in
the amino acid sequence and leads to the loss of a start/stop codon.
Leads to the reduction or elimination of protein production.

\end{tcolorbox}

The impact of such changes is predicted by tracking changes in the
translation of coding sequences using a reference sequence and
SNP/MNP/Indel calls. The problem when using ancient DNA, is that genome
coverage is often uneven, and it isn't rare to lack full coverage of
intervals of interest or lack resolution to correctly call indels across
the reference. Larger genomic insertions and recombination are also not
accounted for when a genome is reconstructed solely using a reference
based approach. This means that while SNP effects can indeed be very
interesting and potentially highly significant, they should be
understood as prediction based on the available coverage. It is
therefore important to report coverage over the entire coding and
promoter intervals, when discussing SNP effect.

Important exception are known and heavily conserved mutations, which for
example are involved in \textbf{pseudogenization}. Pseudogenization is
the process through which genes lose their function due to mutations but
remain in the genome without being expressed or without having a
function. It is a mechanism underlying gene loss. They are significant
in aDNA research because we can capture ``active'' versions of
pseudogenes and potentially date their loss of function. It is an
critical part of the genome reduction process of highly specialised
bacteria.

\hypertarget{virulence-associated-intervals}{%
\subsection{Virulence Associated
Intervals}\label{virulence-associated-intervals}}

One of the question ancient DNA research is interested in is the
evolution of virulence in pathogenic species. The location and nature of
virulence factors can vary. An increase in virulence can be caused by
gene gain (on chromosomes or plasmids), plasmids, mutations, or changes
to complex gene mechanisms (e.g.~immune evasion). To understand the
underlying processes of virulence adaptation has been one of the
recurring questions in the field. Virulence intervals vary but can be
found within the literature and in some cases in curated databases.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  Are there loci associated with increased virulence known to the
  literature?
\item
  Are there phylogenetic groups associated with increased virulence?
\end{itemize}

\end{tcolorbox}

Not all species have evolutionary dynamics which allow us to detect a
temporal signal or recognise geographic structure within their
phylogenies. However, phylogenetic clades can also inform us on other
aspects of their evolution (virulence, ecological niche etc.). When
investigating virulence, you should be checking for the presence of
chromosomal Virulence factors (genes, mutations), currently this is
mostly done using either as a Presence/Absence matrix or a cluster
analysis. As well as investigating the presence/absence of plasmids and
plasmid mediated virulence factors and functional mechanisms, and
relevant SNPs.

Beyond virulence, some genes, and gene combinations can also inform us
on changes in disease phenotype and microbial adaptation to host or
ecological niche.

\hypertarget{coinfections-multi-strain-infections-etc.}{%
\section{Coinfections, Multi-strain Infections
etc.}\label{coinfections-multi-strain-infections-etc.}}

The detection and study of co-infection should also be considered. While
it may not always be possible to reconstruct full genomes for each
pathogen, the knowledge of the presence of multiple additional
infectious agents alone can be very informative. Additionally, the
detection of multi-strain infections (in high coverage genomes), should
also be considered, especially in the case of multi-focal infections for
which multiple samples are available.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reconstructing disease phenotype \& etiopathology:}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  Where was the genome isolated from?

  \begin{itemize}
  \tightlist
  \item
    How can this relate to the disease phenotype?
  \end{itemize}
\item
  Do we have knowledge of potential coinfection? Strain differences?
\item
  Do we have knowledge of the extent of affected tissues within the host
  body?
\item
  Can we conclude whether it is likely to be a chronic or an acute
  infection?
\item
  In what demographic cohort would the individual(s) belong to?

  \begin{itemize}
  \tightlist
  \item
    What could this tell you about the course of the disease or the
    likelihood of acute infections?
  \end{itemize}
\item
  Does the host genomes show any clinically relevant variants which
  could impact how the pathogen would affect them?
\end{itemize}

\textbf{FINALLY:} Integrate the data in a synthesis of all available
data types. E.g.: osteology, archaeology, isotopes etc.

\end{tcolorbox}

\hypertarget{resources}{%
\section{Resources}\label{resources}}

Some useful websites:

\begin{itemize}
\tightlist
\item
  GenBank NCBI Database: \url{https://www.ncbi.nlm.nih.gov/genbank/}
\item
  The European Nucleotide Archive (ENA):
  \url{https://www.ebi.ac.uk/ena/browser/home}
\item
  The virulence factor database (VFDB):
  \url{http://www.mgc.ac.cn/VFs/main.htm}
\item
  Viral Neighbour Genomes In The Assembly Resource (NCBI):
  \url{https://www.ncbi.nlm.nih.gov/genome/viruses/about/assemblies/}
\item
  NCBI Taxonomic Browser: \url{https://www.ncbi.nlm.nih.gov/taxonomy}
\item
  BacDiv: \url{https://bacdive.dsmz.de/}
\item
  Bacterial And Viral Bioinformatics Resource Center:
  \url{https://www.bv-brc.org/}
\end{itemize}

\hypertarget{questions-to-think-about-3}{%
\section{Questions to think about}\label{questions-to-think-about-3}}

Get to know your genome!

\begin{itemize}
\tightlist
\item
  Is the genome circular or linear?
\item
  Does the species carry any plasmids?
\item
  How genomically diverse are the genomes of the species?
\item
  Does the species share large portions of its genome with closely
  related environmental or commensal microbial organisms? If yes could
  they also be present in the data?
\end{itemize}

Get to know your species!

\begin{itemize}
\tightlist
\item
  Is the species clonal or heavily recombinant? Overall? Within clades?
\item
  Is the pangenome open or closed? How large is it?
\item
  Has the genome undergone a reductive evolution?
\item
  Is the genome plastic but maintains genome size?
\item
  Are there significant genomic/phenotypic differences across clades?
\item
  Is your samples grouping within modern diversity or basal too it?
\end{itemize}

Depending on the organism, available data and databases will be very
different\ldots{}

\begin{itemize}
\tightlist
\item
  What data is available?
\item
  What type of data is available?
\item
  What metadata is available?
\end{itemize}

Reconstructing disease phenotype \& etiopathology:

\begin{itemize}
\tightlist
\item
  Where was the genome isolated from?

  \begin{itemize}
  \tightlist
  \item
    How can this relate to the disease phenotype?
  \end{itemize}
\item
  Do we have knowledge of potential coinfection? Strain differences?
\item
  Do we have knowledge of the extent of affected tissues within the host
  body?
\item
  Can we conclude whether it is likely to be a chronic or an acute
  infection?
\item
  In what demographic cohort would the individual(s) belong to?

  \begin{itemize}
  \tightlist
  \item
    What could this tell you about the course of the disease or the
    likelihood of acute infections?
  \end{itemize}
\item
  Does the host genomes show any clinically relevant variants which
  could impact how the pathogen would affect them?
\end{itemize}

\hypertarget{introduction-to-evolutionary-biology-1}{%
\chapter{Introduction to Evolutionary
Biology}\label{introduction-to-evolutionary-biology-1}}

\hypertarget{lecture-4}{%
\subsection{Lecture}\label{lecture-4}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/5a-intro-to-evobio/SPAAM\%20Summer\%20School\%202022\%20-\%205A\%20-\%20Evolutionary\%20Biology\%20and\%20Phylogenetics.pdf}{here}.

\part{Useful Skills}

In this section, we will cover some useful computational skills that
will likely be important for executing the analysis phase of any ancient
DNA projects. With a focus on open- and reproducible science, we will
cover introducing the command line, common programming languages to help
automate your analyses, and also how to use Git(Hub) for sharing code.

\hypertarget{introduction-to-the-command-line-bare-bones-bash}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-the-command-line}{Introduction
to the Command Line (Bare Bones
Bash)}}{Introduction to the Command Line (Bare Bones Bash)}}\label{introduction-to-the-command-line-bare-bones-bash}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-the-command-line}{Introduction
to the Command Line (Bare Bones Bash)}}

\markright{Introduction to the Command Line (Bare Bones Bash)}

Computational work in metagenomics often involves connecting to remote
servers to run analyses via the use of command line tools. Bash is a
programming language that is used as the main command line interface of
most UNIX systems, and hence most remote servers a user will encounter.
By learning bash, users can work more efficiently and reproducibly on
these remote servers.

In this chapter we will introduce the basic concepts of bash and the
command line. Students will learn how to move around the filesystem and
interact with files, how to chain multiple commands together using
``pipes'', and how to use loops and regular expressions to simplify the
running of repetitive tasks.

Finally, readers will learn how to create a bash script of their own,
that can run a set of commands in sequence. This session requires no
prior knowledge of bash or the command line and is meant to serve as an
entry-level introduction to basic programming concepts that can be
applicable in other programming languages too.

\hypertarget{introduction-to-r}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-r-and-the-tidyverse}{Introduction
to R}}{Introduction to R}}\label{introduction-to-r}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-r-and-the-tidyverse}{Introduction
to R}}

\markright{Introduction to R}

R is an interpreted programming language with a particular focus on data
manipulation and analysis. It is very well established for scientific
computing and supported by an active community developing and
maintaining a huge ecosystem of software packages for both general and
highly derived applications.

In this chapter we will explore how to use R for a simple, standard data
science workflow. We will import, clean, and visualise context and
summary data for and from our ancient metagenomics analysis workflow. On
the way we will learn about the RStudio integrated development
environment, dip into the basic logic and syntax of R and finally write
some first useful code within the tidyverse framework for tidy, readable
and reproducible data analysis.

This chapter will be targeted at beginners without much previous
experience with R or programming and will kickstart your journey to
master this powerful tool.

\hypertarget{introduction-to-python}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-python-and-pandas}{Introduction
to Python}}{Introduction to Python}}\label{introduction-to-python}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-python-and-pandas}{Introduction
to Python}}

\markright{Introduction to Python}

While R has traditionally been the language of choice for statistical
programming for many years, Python has taken away some of the hegemony
thanks to its numerous available libraries for machine and deep
learning. With its ever increasing collection of libraries for
statistics and bioinformatics, Python has now become one the most used
language in the bioinformatics community.

In this tutorial, mirroring to the R session, we will learn how to use
the Python libraries Pandas for importing, cleaning, and manipulating
data tables, and producing simple plots with the Python sister library
of ggplot2, plotnine.

We will also get ourselves familiar with the Jupyter notebook
environment, often used by many high performance computing clusters as
an interactive scripting interface.

This chapter is meant for participants with a basic experience in
R/tidyverse, but assumes no prior knowledge of Python/Jupyter.

\hypertarget{introduction-to-git-and-github}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-github}{Introduction
to Git and
GitHub}}{Introduction to Git and GitHub}}\label{introduction-to-git-and-github}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-github}{Introduction
to Git and GitHub}}

\markright{Introduction to Git and GitHub}

As the size and complexity of metagenomic analyses continues to expand,
effectively organizing and tracking changes to scripts, code, and even
data, continues to be a critical part of ancient metagenomic analyses.
Furthermore, this complexity is leading to ever more collaborative
projects, with input from multiple researchers.

In this chapter, we will introduce `Git', an extremely popular version
control system used in bioinformatics and software development to store,
track changes, and collaborate on scripts and code. We will also
introduce, GitHub, a cloud-based service for Git repositories for
sharing data and code, and where many bioinformatic tools are stored. We
will learn how to access and navigate course materials stored on GitHub
through the web interface as well as the command line, and we will
create our own repositories to store and share the output of upcoming
sessions.

\hypertarget{introduction-to-the-command-line}{%
\chapter{Introduction to the Command
Line}\label{introduction-to-the-command-line}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/bare-bones-bash.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate bare{-}bones{-}bash}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-5}{%
\section{Lecture}\label{lecture-5}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

\hypertarget{session-1}{%
\subsection{Session 1}\label{session-1}}

This is the Brief version of a more Broad two-part session that we gave
in 2022, which can be found
\href{https://spaam-community.github.io/wss-summer-school/\#/2022/day-1?id=practical-1b-bare-bones-bash-1}{here}.

The teaching material for the Boundless Bare Bones Bash (Basic +
Boosted) can be found in the
\href{https://barebonesbash.github.io/\#/}{Bare Bones Bash website}.

\begin{figure}

{\centering \includegraphics[width=0.15\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/BareBonesBash_Icon.png}

}

\caption{BareBonesBash Logo}

\end{figure}

The original Bare Bones Bash material was created by Aida Andrades
Valtueña, James Fellows Yates, and Thiseas C. Lamnidis.

\includegraphics[width=0.15\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/bbb_aida.png}
\includegraphics[width=0.15\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/bbb_james.png}
\includegraphics[width=0.15\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/bbb_thiseas.png}

The author portraits were designed by Zandra Fagernäs.

\begin{figure}

{\centering \includegraphics{index_files/mediabag/88x31.png}

}

\end{figure}

All material is provided under a
\href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons
Attribution-ShareAlike 4.0 International License}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{TL;DR: uncollapse me!}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Below is a quick reference guide to the commands discussed in this
tutorial. To understand actually what each command does, carry on
reading below! For a complete run of all these commands AND MORE(!!),
consider following the full Bare Bones Bash walkthroughs
\href{https://barebonesbash.github.io/\#/}{here}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0669}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2641}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3345}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3345}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
command
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
common flags or arguments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
pwd & print working directory & pwd & \\
ls & list contents of directory & ls & -l (long info) \\
mkdir & make directory & mkdir pen & \\
cd & change directory & cd \textasciitilde/pen & \textasciitilde{} (home
dir), - (previous dir) \\
ssh & log into a remote server & ssh @.com & -Y (allows graphical
windows) \\
mv & move something to a new location (\& rename if needed) & mv pen
pineapple & \\
rmdir & remove a directory & rmdir pineapple & \\
wget & download something from an URL & wget
\href{https://media.giphy.com/media/10fVn57KfQ8Wkw/giphy.gif}{www.pineapple.com/pen.txt}
& -i (use input file) \\
cat & print contents of a file to screen & cat pen.txt & \\
gzip & a tool for dealing with gzip files & gzip pen.txt & -l (show
info) \\
zcat & print contents of a gzipped file to screen & zcat pen.txt.gz & \\
whatis & get a short description of a program & whatis zcat & \\
man & print the man(ual) page of a command & man zcat & \\
head & print first X number of lines of a file to screen & head -n 20
pineapple.txt & -n (number of lines to show) \\
\textbar{} & pipe, a way to pass output of one command to another & cat
pineapple.txt \textbar{} head & \\
tail & print last X number of lines of a file to screen & tail -n 20
pineapple.txt & -n (number of lines to show) \\
less & print file to screen, but allow scrolling & less pineapple.txt
& \\
wc & tool to count words, lines or bytes of a files & wc -l
pineapple.txt & -l (number of lines not words) \\
grep & print to screen lines in a file matching a pattern & grep
pineapple.txt \textbar{} grep pen & \\
ln & make a (sym)link between a file and a new location & ln -s
pineapple.txt pineapple\_pen.txt & -s (make \emph{symbolic} link) \\
nano & user-friendly terminal-based text editor & nano
pineapple\_pen.txt & \\
rm & more general `remove' command, including files & rm
pineapple\_pen.txt & -r (to remove directories) \\
\$VAR & Dollar sign + text indicates the name of a variable & \$PPAP
& \\
echo & prints string to screen & echo ``\$PPAP'' & \\
for & begins `for' loop, requires `in', `do' and `done' & for p in apple
pineapple; do echo ``\$p\$PPAP''; done applePen pineapplePen & \\
find & search for files or directories & find -name `pen' & -type f
(search only for files) -name '*JPG' (search for file names matching the
pattern) \\
``\$var'' & use double quotes to use contents of variable & pen=apple
\&\& echo ``\$pen'' & \\
\textless{}\textgreater{}2\textgreater{} & redirects the standard
input/output/error stream respectively into a file & cat
\textless file.txt \textgreater file\_copy.txt
2\textgreater cat\_file.err & \\
\end{longtable}

\end{tcolorbox}

\hypertarget{introduction-4}{%
\subsection{Introduction}\label{introduction-4}}

The aim of this tutorial is to make you familiar with using bash
everyday\ldots{} for the rest of your life! More specifically, we want
to do this in the context of bioinformatics. We will start with how to
navigate around a filesystem in the terminal, download sequencing files,
and then to manipulate these. Within these sections we will also show
you simple tips and tricks to make your life generally easier.

This tutorial is designed so you follow along on any machine with a UNIX
terminal (no warranty provided).

\hypertarget{the-5-commandments-of-bare-bones-bash}{%
\subsection{The 5 commandments of Bare Bones
Bash}\label{the-5-commandments-of-bare-bones-bash}}

The Bare Bones Bash philosophy of learning to code follows five simple
commandments:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5614}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1)} Be lazy! & Desire for shortcuts motivates you to explore
more! \\
\textbf{2)} \st{Google} The Hive-Mind knows everything! & 99\% of the
time, someone else has already had the same issue. \\
\textbf{3)} Document everything you do! & Make future you happy! \\
\textbf{4)} There will ALWAYS be a typo! & Don't get disheartened, even
best programmers make mistakes! \\
\textbf{5)} Don't be afraid of you freedom! & Explore! Try out
things! \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Pro Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Remember: No one writes code that works first time, or without looking
at StackOverflow sooner or later.

\end{tcolorbox}

\hypertarget{what-is-a-terminal}{%
\subsection{What is a terminal?}\label{what-is-a-terminal}}

A terminal is simply a fancy window that allows you to access the
command-line interface of a computer or server
(Figure~\ref{fig-Command_line_screenshot}).

The command-line itself is how you can work on the computer with just
text.

\texttt{bash} (\textbf{b}ourne \textbf{a}gain \textbf{sh}ell) is one of
the most popular languages used in the terminal.

\hypertarget{understanding-the-command-prompt}{%
\subsection{Understanding the command
prompt}\label{understanding-the-command-prompt}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/Command_line_screenshot.png}

}

\caption{\label{fig-Command_line_screenshot}An example command prompt}

\end{figure}

After opening the terminal what you will normally see is a blank screen
with a `command prompt', like the one shown above. This typically
consists of your username, the device name, a colon, a directory path
and ends with a dollar symbol. Like so:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\textless{}}\NormalTok{username}\OperatorTok{\textgreater{}}\NormalTok{@}\OperatorTok{\textless{}}\NormalTok{device\_name}\OperatorTok{\textgreater{}}\NormalTok{:\textasciitilde{}$}
\end{Highlighting}
\end{Shaded}

The command prompt is \textbf{never} involved in any command, it is just
there to ensure you know who and where you are. When copying a command
you should \textbf{NOT} copy the command prompt.

Often times, when looking for commands online, the commands ran will pe
prefaced with a \texttt{\$}. This is a stand-in for the command prompt.
When adding multi-line commands, it is also common to preface the
additional lines with a \texttt{\textgreater{}}. When copying such
commands it is therefore important to remove these characters from the
start of each line (if present).

Finally, in this tutorial, the symbols\texttt{\textless{}\textgreater{}}
are used to show things that will/should be replaced by another value.
For example, in Thiseas' command prompt
\texttt{\textless{}username\textgreater{}} will be replaced by
\texttt{lamnidis}, as that is his username.

Now back to your prompt: It tells you that you are in the directory
\texttt{\textasciitilde{}}. The directory \texttt{\textasciitilde{}},
stands for your \textbf{home} directory. Note that this shorthand will
point to a different place, depending on the machine and the user.

If you want to know what the shorthand means, (here comes your first
command!) you can type in \texttt{pwd}, which stands for
``\textbf{p}rint \textbf{w}orking \textbf{d}irectory''. Your ``working
directory'' is whichever directory you are currently in.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{pwd}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/home/ubuntu
\end{verbatim}

This prints the entire ``filepath'' of the directory i.e.~the route from
the ``root'' (the deepest directory of the machine), through every
subdirectory, leading to your particular working directory.

\hypertarget{absolute-vs-relative-paths}{%
\subsection{Absolute vs Relative
paths}\label{absolute-vs-relative-paths}}

Filepaths (a.k.a. ``paths''), come in two flavours. Let's talk a bit
about them!

\begin{itemize}
\tightlist
\item
  An \textbf{absolute} path will start with the deepest directory in the
  machine shown as a \texttt{/}. Paths starting with
  \texttt{\textasciitilde{}} are also absolute paths, since
  \texttt{\textasciitilde{}} translates to an absolute path of your
  specific home directory. That is the directory path you see in the
  output of the \texttt{pwd} command you just ran.
\item
  Alternatively a \textbf{relative} path always begins from your working
  directory (i.e.~your current directory). Often this type of path will
  begin with one (\texttt{./}) or two (\texttt{../}) dots followed by a
  forward slash, \textbf{but not always}. In the syntax of relative
  pathways \texttt{.} means ``the current directory'' and \texttt{..}
  means ``the parent directory'' (or the `one above').
\end{itemize}

\hypertarget{a-real-life-analogy-for-paths}{%
\subsubsection{A real life analogy for
paths}\label{a-real-life-analogy-for-paths}}

You have just arrived to Leipzig for a summer school that is taking
place at MPI-EVA. After some questionable navigation, you find yourself
at the Bayerische Bahnhof. Tired and disheartened, you decide to ask for
help.

You see a friendly-looking metalhead (Figure~\ref{fig-bbb_james}), and
decide to ask them for directions!

\begin{figure}

{\centering \includegraphics[width=0.2\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/bbb_james.png}

}

\caption{\label{fig-bbb_james}A friendly-looking metalhead.}

\end{figure}

\begin{quote}
I'm Happy to help, but I only give directions in \textbf{absolute
paths!}\\
From Leipzig Main Station, you should take Querstraße southward.\\
Continue straight and take Nürnberger Str. southward until you reach
Str. des 18 Oktober.\\
Finally take Str. des 18 Oktober. moving southeast until you reach
MPI-EVA!
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Absolute paths}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

The directions above are equivalent to an absolute path, because they
will ALWAYS take you to MPI-EVA, but you can only apply these directions
\textbf{if} you start from Leipzig Main Station!

Examples of absolute paths:\\
\texttt{/home/ubuntu}\strut \\
\texttt{/Leipzig\_Main\_Station/Querstraße/Nürnberger\_Str/Str\_18\_Oktober/Deutscher\_Platz/MPI-EVA}

\end{tcolorbox}

Not sure how to get back to Leipzig Main Station to apply those
directions, you decide to ask someone else for directions\ldots{}

Lucky for you, a friendly looking local is passing by
(Figure~\ref{fig-bbb_aida})!

\begin{figure}

{\centering \includegraphics[width=0.2\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/bbb_aida.png}

}

\caption{\label{fig-bbb_aida}A friendly-looking local.}

\end{figure}

\begin{quote}
You're currently on Str. des 18 Oktober. Walk straight that way, past
the tram tracks, and you will find Deutscher Platz. You will see MPI-EVA
to your right!
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Relative paths}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

These directions are equivalent to a relative path! They are easy to
follow, but only work when you happen to start at the position you were
in when you first got the directions!

Examples of relative paths:\\
\texttt{./my\_directory/my\_file.txt}\strut \\
\texttt{../Str\_18\_Oktober/Deutscher\_Platz/MPI-EVA}

\end{tcolorbox}

\hypertarget{basic-commands}{%
\subsection{Basic commands}\label{basic-commands}}

We will now explore some basic commands that you will use to explore
folders and interact with files:

\begin{itemize}
\tightlist
\item
  \textbf{l}i\textbf{s}t directory contents:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}
\end{Highlighting}
\end{Shaded}

Output should look like:

\begin{verbatim}
Desktop    Downloads  Pictures  Templates  bin    snap
Documents  Music      Public    Videos     cache  thinclient_drives
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

We will use this format to show you commands and their corresponding
output in the terminal (if any) for the rest of this chapter.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  \textbf{m}a\textbf{k}e a \textbf{dir}ectory:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ barebonesbash}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{m}o\textbf{v}e (or rename) files and directories
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mv}\NormalTok{ barebonesbash BareBonesBash}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{c}hange \textbf{d}irectories
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ BareBonesBash}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Download (\textbf{w}ww \textbf{get}) a remote file to your computer
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wget}\NormalTok{ git.io/Boosted{-}BBB{-}meta}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{c}o\textbf{p}y a file or directory to a new location
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cp}\NormalTok{ Boosted{-}BBB{-}meta Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{r}e\textbf{m}ove (delete) files
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{ Boosted{-}BBB{-}meta}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Con\textbf{cat}enate file contents to screen
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  See only the \textbf{first}/\textbf{last} 10 lines of a file
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head} \AttributeTok{{-}n}\NormalTok{ 10 Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail} \AttributeTok{{-}n}\NormalTok{ 10 Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

This is because the start of a cat is its head and the end of the cat is
its tail (The great humour of computer scientists)

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Look at the contents of a file interactively (\textbf{less} than the
  complete file, press \texttt{q} to quit)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{less}\NormalTok{ Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{w}ord \textbf{c}ount the number of \textbf{l}ines (-l) in a
  file
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wc} \AttributeTok{{-}l}\NormalTok{ Boosted{-}BBB{-}meta.tsv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
15 Boosted-BBB-meta.tsv
\end{verbatim}

\hypertarget{datastreams-piping-and-redirects}{%
\subsection{Datastreams, piping, and
redirects}\label{datastreams-piping-and-redirects}}

Each of the commands you learned above is a small program with a very
specialised functionality. Programs come in many forms and can be
written in various programming languages, but most of them share some
features. Specifically, most programs take some data in and spit some
data out! Here's how that works, conceptually:

\hypertarget{datastreams}{%
\subsubsection{Datastreams}\label{datastreams}}

Computer programs can take in and spit out data from different
\emph{streams} (Figure~\ref{fig-Datastreams}). By default there are 3
such data streams.

\begin{itemize}
\tightlist
\item
  \texttt{stdin} : the \textbf{st}an\textbf{d}ard \textbf{in}put
\item
  \texttt{stdout}: the \textbf{st}an\textbf{d}ard \textbf{out}put
\item
  \texttt{stderr}: the \textbf{st}an\textbf{d}ard \textbf{err}or
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/Datastreams.png}

}

\caption{\label{fig-Datastreams}Diagram showing stdin going into the
program, and two output streams from the program: stderr and stdout.}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Pro Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Each programme also has an `exit code', which can tell you if execution
completed with/without errors. You will rarely see these in the wild.

\end{tcolorbox}

Typically, the \texttt{stdin} is where the input data comes in.\\
The \texttt{stdout} is the actual output of the command. In some cases
this gets printed to the screen, but most often this is the information
that needs to be saved in an output file.\\
The \texttt{stderr} is the datastream where errors and warnings go. This
gets printed to your terminal to let you know when something is not
going according to plan (Figure~\ref{fig-Datastream_showcase})!

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/Datastream_showcase.png}

}

\caption{\label{fig-Datastream_showcase}This program (in the form of a
bash script executed in the terminal) takes no input, and prints one
line to the stdout and one line to the stderr.}

\end{figure}

\hypertarget{piping}{%
\subsubsection{Piping}\label{piping}}

A ``pipe'' (\texttt{\textbar{}}) is a really useful feature of
\texttt{bash} that lets you chain together multiple commands! When two
commands are chained together with a pipe, the \texttt{stdout} of the
first command becomes the \texttt{stdin} of the second
(Figure~\ref{fig-Piping})! The \texttt{stderr} is still printed on your
screen, so you can always know when things fail.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/Piping.png}

}

\caption{\label{fig-Piping}Diagram showing stdin going into the program,
and two output streams from the program: stderr and stdout, with the
stdout becoming the stdin of a second program.}

\end{figure}

Example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head} \AttributeTok{{-}n}\NormalTok{ 10 Boosted{-}BBB{-}meta.tsv }\KeywordTok{|} \FunctionTok{tail} \AttributeTok{{-}n1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
netsukeJapan    C       Artwork
\end{verbatim}

The above command will only show the 10th line of
\texttt{Boosted-BBB-meta.tsv}. The way it works is that \texttt{head}
will take the first 10 lines of the file. These lines are then passed on
to \texttt{tail} which will keep only the last of those lines.

\hypertarget{redirects}{%
\subsubsection{Redirects}\label{redirects}}

Much like streams of water in the real world, datastreams can be
redirected.

This way you can save the stdout of a program (or even the stderr) into
a file for later!

\begin{itemize}
\tightlist
\item
  \texttt{stdin} can be redirected with \texttt{\textless{}}.

  \begin{itemize}
  \tightlist
  \item
    An arrow pointing TO your program name!
  \end{itemize}
\item
  \texttt{stdout} can be redirected with \texttt{\textgreater{}}.

  \begin{itemize}
  \tightlist
  \item
    An arrow pointing AWAY your program name!
  \end{itemize}
\item
  \texttt{stderr} can be redirected with \texttt{2\textgreater{}}.

  \begin{itemize}
  \tightlist
  \item
    Because it is the secondary output stream.
  \end{itemize}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Pro Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

It is also possible to combine streams, but we won't get into that here.

\end{tcolorbox}

Example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head} \AttributeTok{{-}n}\NormalTok{ 10 Boosted{-}BBB{-}meta.tsv }\KeywordTok{|} \FunctionTok{tail} \AttributeTok{{-}n1} \OperatorTok{\textgreater{}}\NormalTok{ line10.txt}
\end{Highlighting}
\end{Shaded}

This will create a new file called \texttt{line10.txt} within your work
directory. Using \texttt{cat} on this file will BLOW YOUR MIND - (GONE
WRONG)!

\begin{quote}
(Don't forget to like and subscribe!)
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ line10.txt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
netsukeJapan    C       Artwork
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{assets/images/chapters/bare-bones-bash/Redirects_showcase.png}

}

\caption{\label{fig-Redirects_showcase}Here you can see an example of
redirecting the output of the \texttt{datastreams\_demo.sh} program from
before. Redirecting the stdout with \texttt{\textgreater{}} only prints
the stderr to the screen, and saves the stdout into \texttt{output.txt}.
Additionally, we can redirect the stderr with \texttt{2\textgreater{}}
into \texttt{runtime.log}, and then nothing is printed onto the screen.}

\end{figure}

\hypertarget{help-text}{%
\subsection{Help text}\label{help-text}}

You don't always have to google for documentation! Many programs come
with in-built help text, or access to online manuals right from your
terminal!

\begin{itemize}
\item
  You can get a \textbf{one sentence summary} of what a tool does with
  \texttt{whatis}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{whatis}\NormalTok{ cat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
cat(1)  - concatenate files and print on the standard 
        output
\end{verbatim}
\item
  While \texttt{man} gives you \textbf{access to online manuals} for
  each tool (exit with \texttt{q})

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{man}\NormalTok{ cat}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\hypertarget{variables}{%
\subsection{Variables}\label{variables}}

Variables are a central concept of all programming. In short, a variable
is a named container whose contents you can expand at will or change.

You can assign variables (tell the computer what do you want it to
contain) with \texttt{=} and pull their contents with \texttt{\$}

The easiest way to see the contents of a variable is using
\texttt{echo}!

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{echo} \StringTok{"This is my home directory: }\VariableTok{$HOME}\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
This is my home directory: /home/ubuntu
\end{verbatim}

\texttt{\$HOME} is a variable of the type called \textbf{environment
variables}, which are set the moment you open your terminal or log into
a server, they ensure the system works as intended and should not be
change unless you are \textbf{very} sure of why.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Environment Variables}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

Environment variables in bash are typically named in all capital
letters. It is a good idea to avoid using only capital letters for your
variable names, so you avoid accidentally overwriting any environment
variables.

\end{tcolorbox}

But as mentioned, you can store in a variable anything you want, so
let's see a few examples:

First let's try to store a number:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{GreekFood}\OperatorTok{=}\NormalTok{4            }\CommentTok{\#Here, \textquotesingle{}GreekFood\textquotesingle{} is a number.}
\BuiltInTok{echo} \StringTok{"Greek food is }\VariableTok{$GreekFood}\StringTok{ people who want to know what heaven tastes like."}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Greek food is 4 people who want to know what heaven tastes like.
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The \texttt{\#} is used to add comments to your code. Comments are
annotations that you write in your code to understand what it is doing
but that the computer does not run. Very useful for when your future
self or another person looks at your code

\end{tcolorbox}

Now let's store a word (``string''):

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{GreekFood}\OperatorTok{=}\NormalTok{delicious   }\CommentTok{\#We overwrite that number with a word (i.e. a \textquotesingle{}string\textquotesingle{}).}
\BuiltInTok{echo} \StringTok{"Everyone says that Greek food is }\VariableTok{$GreekFood}\StringTok{."}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Everyone says that Greek food is delicious.
\end{verbatim}

You can also store more than a single word (that is still a ``string''):

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{GreekFood}\OperatorTok{=}\StringTok{"Greek wine"} \CommentTok{\#We can overwrite \textquotesingle{}GreekFood\textquotesingle{} again, }
\CommentTok{\#\# but when there is a space in our string, we need quotations.}
\BuiltInTok{echo} \StringTok{"The only thing better than Greek food is }\VariableTok{$GreekFood}\StringTok{!"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The only thing better than Greek food is Greek wine!
\end{verbatim}

Since variables can be reset to whatever you want, you can also store a
number again:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{GreekFood}\OperatorTok{=}\NormalTok{7 }\CommentTok{\#And, of course, we can overwrite with a number again too.}
\BuiltInTok{echo} \StringTok{"I have been to Greece }\VariableTok{$GreekFood}\StringTok{ times already this year, for the food and wine!"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
I have been to Greece 7 times already this year, for the food and wine!
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Overwriting variables}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

In these examples you have seen how the same variable has been
overwritten, this means that you can only access the last content that
you stored in the variable. All the previous contents that a variable
may have had are inaccessible as soon as the same variable is given a
new value.

\end{tcolorbox}

\hypertarget{quotes-matter}{%
\subsection{Quotes matter!}\label{quotes-matter}}

In bash, there is a big difference between a single quote
\texttt{\textquotesingle{}} and a double quote \texttt{"}!

\begin{itemize}
\tightlist
\item
  The contents of single quotes, are passed on as they are.
\item
  Inside double quotes, contents are \emph{interpreted}!
\end{itemize}

In some cases the difference doesn't matter:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{echo} \StringTok{"I like Greek Food"}
\BuiltInTok{echo} \StringTok{\textquotesingle{}I like Greek Food\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
I like Greek Food
I like Greek Food
\end{verbatim}

In other cases it makes all the difference:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{Arr}\OperatorTok{=}\StringTok{"Banana"}
\BuiltInTok{echo} \StringTok{\textquotesingle{}Pirates say $Arr\textquotesingle{}}
\BuiltInTok{echo} \StringTok{"Minions say }\VariableTok{$Arr}\StringTok{"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pirates say $Arr
Minions say Banana
\end{verbatim}

\textbf{Why does it make a difference in the second example?}

This is because in the second example we are using a variable. We have
assigned \texttt{Banana} to the variable \texttt{\$Arr}. As mentioned
above, when single (\texttt{\textquotesingle{}}) quotes are used the
computer just prints what it receives without caring that \texttt{\$Arr}
is a variable.

In the \texttt{echo} with the double (\texttt{"}) quotes we are telling
the computer to extract the value from the variable \texttt{\$Arr} and
that is why we see the store value (\texttt{Banana}) in the printed
output in the terminal.

\hypertarget{find}{%
\subsection{Find}\label{find}}

You can also ask your computer where you have put your files, in case
you forgot. To do this you can use \texttt{find}! The \texttt{find}
command has the following syntax:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Don\textquotesingle{}t run! Fake example}
\FunctionTok{find}\NormalTok{ /your/folder/ }\AttributeTok{{-}type}\NormalTok{ f }\AttributeTok{{-}name} \StringTok{\textquotesingle{}your\_file.txt\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{First} part of the \texttt{find} command: \emph{the place to
  look from}

  \begin{itemize}
  \tightlist
  \item
    e.g.~\texttt{.} to indicate `here'
  \item
    Could also use \texttt{\textasciitilde{}/}
  \item
    Could use absolute path e.g.~\texttt{/home/james/}
  \end{itemize}
\item
  \textbf{Second} part of the \texttt{find} command: \emph{what type of
  things to look for?}

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{-type} to define the filetype:

    \begin{itemize}
    \tightlist
    \item
      \textbf{f}ile
    \item
      \textbf{d}irectory
    \end{itemize}
  \end{itemize}
\item
  \textbf{Third} part of the \texttt{find} command: \emph{what to look
  in?}

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{-name} to say `look in \textbf{names} of things'
  \end{itemize}
\item
  \textbf{Finally} after \texttt{-name} we give the the `strings' to
  search for

  \begin{itemize}
  \tightlist
  \item
    Use wildcards (\texttt{*}) for maximum laziness!
  \end{itemize}
\end{itemize}

Now let's put into practise what you have learnt about \texttt{find}.

For that you will download a messy folder from a collaborator, remember
to check you are in the BareBonesBash folder!:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wget}\NormalTok{ git.io/Boosted{-}BBB{-}images }\AttributeTok{{-}O}\NormalTok{ Boosted{-}BBB.zip}
\end{Highlighting}
\end{Shaded}

We realise that this is a compressed file, and more precisely is a zip
file (extension \texttt{.zip}). In order to access to its content we
will need to ``unzip'' it first. For that we can use the command
\textbf{unzip}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unzip}\NormalTok{ Boosted{-}BBB.zip}
\end{Highlighting}
\end{Shaded}

We know that our collaborator has shared with us some pictures from
animals that we need to use for our research, and according to your
collaborator they are marked with \texttt{JPG}. We first try to check
the contents of the directory to find them quickly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}\NormalTok{ Boosted{-}BBB}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
And        Digging       Friday     Leave      Only     Where    Young
Anybody    Everything    Getting    Looking    Ooh      With     Youre
Dancing    Feel          Having     Night      Watch    You
\end{verbatim}

Wow, what a mess! How would you retrieve all the files? Thanks to your
wonderful teachers you have learnt how to use find and can simply run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{find}\NormalTok{ Boosted{-}BBB }\AttributeTok{{-}type}\NormalTok{ f }\AttributeTok{{-}name} \StringTok{\textquotesingle{}*JPG*\textquotesingle{}} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Boosted-BBB/Having/the/time/of/your/life/bubobubo.JPG.MP3.TXT
Boosted-BBB/With/a/bit/of/rock/music/exhibitRoyal.JPG.MP3.TXT
Boosted-BBB/Friday/night/and/the/lights/are/low/fanta.JPG.MP3.TXT
Boosted-BBB/Everything/is/fine/nomnom.JPG.MP3.TXT
Boosted-BBB/Getting/in/the/swing/giacomo.JPG.MP3.TXT
Boosted-BBB/Youre/in/the/mood/for/a/dance/snore.JPG.MP3.TXT
Boosted-BBB/Digging/the/dancing/queen/excited.JPG.MP3.TXT
Boosted-BBB/Anybody/could/be/that/guy/alopochenaegyptiacaArnhem.JPG.MP3.TXT
Boosted-BBB/And/when/you/get/the/chance/stretch.JPG.MP3.TXT
Boosted-BBB/Looking/out/for/angry.JPG.MP3.TXT
Boosted-BBB/Feel/the/beat/from/the/tambourine/oh/yeah/netsukeJapan.JPG.MP3.TXT
Boosted-BBB/Watch/that/scene/licorne.JPG.MP3.TXT
Boosted-BBB/You/can/weimanarer.JPG.MP3.TXT
Boosted-BBB/Night/is/young/and/the/musics/high/bydgoszczForest.JPG.MP3.TXT
Boosted-BBB/Ooh/see/that/girl/pompeii.JPG.MP3.TXT
\end{verbatim}

After \texttt{-name} we have written
\texttt{\textquotesingle{}*JPG*\textquotesingle{}}, this tells to
\texttt{find} to search for any file that contains \texttt{JPG} in any
part of its name, indicated by the \texttt{*}. The \texttt{*} are what
are known as \textbf{wildcards}. To learn more on how to use them,
please refer to the more complete material for this
\href{https://barebonesbash.github.io/\#/}{tutorial}.

Now you have all the paths of the files that you will need!

\hypertarget{for-loops}{%
\subsection{For loops}\label{for-loops}}

Until now we have seen how to run single commands on a file. But, what
about when you need to \textbf{repeat} a command multiple times on a
list of things, for example a list of files?

To repeat an action (command) for a set of things (list, e.g.~files) one
needs to employ the concept of a loop. One of the most commonly used
loops, is the \textbf{for} loop.

A \textbf{for} loop allows us to go through a list of things and perform
some actions. Let's see an example:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{Variable}\OperatorTok{=}\NormalTok{Yes}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ Greece Spain Britain}\KeywordTok{;} \ControlFlowTok{do}
  \BuiltInTok{echo} \StringTok{"Does }\VariableTok{$i}\StringTok{ have lovely food? }\VariableTok{$Variable}\StringTok{"}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Does Greece have lovely food? Yes
Does Spain have lovely food? Yes
Does Britain have lovely food? Yes
\end{verbatim}

The for loop went through the list \texttt{Greece\ Spain\ Britain} and
printed a statement with each item in the list. What happens if we
change the order of the list to \texttt{Britain\ Greece\ Spain}?

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{Variable}\OperatorTok{=}\NormalTok{Yes}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ Britain Greece Spain}\KeywordTok{;} \ControlFlowTok{do}
  \BuiltInTok{echo} \StringTok{"Does }\VariableTok{$i}\StringTok{ have lovely food? }\VariableTok{$Variable}\StringTok{"}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Does Britain have lovely food? Yes
Does Greece have lovely food? Yes
Does Spain have lovely food? Yes
\end{verbatim}

We see that changing the order of the list will affect the output, this
is because the \textbf{for} loop will go through the list in a
sequential manner.

We can also add more elements to the list, and the \textbf{for} loop
will continue until it reaches the end of the list.

\hypertarget{how-to-google-like-a-pro}{%
\subsection{How to Google like a pro}\label{how-to-google-like-a-pro}}

One of the most important skills you develop when coding and/or using
the command line is \textbf{how to phrase your questions} so you can get
relevant answers out of your search engine.

As Deep Thought put it in the Hitchhiker's Guide to the Galaxy:

\begin{quote}
Only when you know the question will you know what the answer means.
\end{quote}

Here are some quick tips to get you started:

\begin{itemize}
\tightlist
\item
  ALWAYS include the name of the language in your query.

  \begin{itemize}
  \tightlist
  \item
    \textbf{BAD:} ``How to cat'' (Figure~\ref{fig-how-to-cat})
  \item
    \textbf{GOOD:} ``How to cat bash''
    (Figure~\ref{fig-how-to-cat-bash})
  \end{itemize}
\item
  BROADEN your question!

  \begin{itemize}
  \tightlist
  \item
    \textbf{BAD:} ``How to set \textbf{X} to \textbf{4} in bash?''
  \item
    \textbf{GOOD:} ``How to set a \textbf{variable} to an
    \textbf{integer} in bash?''
  \end{itemize}
\item
  When you are more familiar, use fancy programmer lingo to make google
  think you know what you are talking about.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\textbf{All the cool hackers say:}}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  \texttt{string} and not \texttt{text}.\\
\item
  \texttt{float} and not \texttt{decimal}.
\end{itemize}

\textbf{Note:} some of these terms can be language specific.

\end{tcolorbox}

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{assets/images/chapters/bare-bones-bash/How_to_cat.png}

}

\caption{\label{fig-how-to-cat}How to cat the wrong way (i.e., don't
include the language in your Google search and get loads of cat
pictures)}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{assets/images/chapters/bare-bones-bash/How_to_cat_bash.png}

}

\caption{\label{fig-how-to-cat-bash}How to cat the BASH way (i.e.,
include the the language in your Google search and you get lots of
terminal pictures! Much better, right? \ldots right?)}

}

\end{minipage}%

\end{figure}

\hypertarget{optional-clean-up}{%
\subsection{(optional) Clean-up}\label{optional-clean-up}}

It is extremely important to ALWAYS keep your directories clean from
random clutter. This lowers the chances you will get lost in your
directories, but also ensures you can stay lazy, since tab completion
will not keep finding similarly named files. So let's clean up your home
directory by removing all the clutter we downloaded and worked with
today. The command below will remove the
\texttt{\textasciitilde{}/BareBonesBash} directory \textbf{as well as
all of its contents}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}     }\CommentTok{\#\# We shouldn\textquotesingle{}t delete a directory while we are still in it. (It is possible though).}
\FunctionTok{rm} \AttributeTok{{-}r}\NormalTok{ \textasciitilde{}/BareBonesBash}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Pro Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Always be VERY careful when using \texttt{rm\ -r}. Check 3x that the
path you are specifying is exactly what you want to delete and nothing
more before pressing ENTER!

\end{tcolorbox}

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

You should now know the basics of working on the command line, like:

\begin{itemize}
\tightlist
\item
  What a command prompt is
\item
  How to navigate around the filesystem via the command line
\item
  How to view the contents of a file
\item
  How to remove files and directories
\item
  What a datastream is, and how they can be redirected
\item
  How to chain commands together
\item
  What a variable is, how to assign them and how to unpack them
\item
  How to construct a simple for loop
\item
  How to google more efficiently!
\end{itemize}

If you would like to know more about the magic of bash, you can find
more commands as well as and more advanced bash concepts in the
\href{https://barebonesbash.github.io/\#/}{BareBonesBash website}.

\hypertarget{introduction-to-r-and-the-tidyverse}{%
\chapter{Introduction to R and the
Tidyverse}\label{introduction-to-r-and-the-tidyverse}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

This session is typically ran held in parallel to the Introduction to
Python and Pandas. Participants of the summer schools chose which to
attend based on their prior experience. We recommend this session if you
have no experience with neither R nor Python.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/r-tidyverse.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate r{-}tidyverse}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-6}{%
\section{Lecture}\label{lecture-6}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}. Note that the 2023 version features a
different, more general dataset. The up-to-date, written tutorial below
is the recommended version.

PDF version of these slides can be downloaded from
\href{https://github.com/nevrome/spaam_r_tidyverse_intro_2h/raw/main/presentation.pdf}{here}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-working-environment}{%
\section{The working environment}\label{the-working-environment}}

\hypertarget{r-rstudio-the-tidyverse-and-penguins}{%
\subsection{R, RStudio, the tidyverse and
penguins}\label{r-rstudio-the-tidyverse-and-penguins}}

R {[}@RCoreTeam2023{]} is a fully featured programming language, but it
excels as an environment for (statistical) data analysis
(\url{https://www.r-project.org}) RStudio {[}@RstudioTeam{]} is an
integrated development environment (IDE) for R (and other languages)
(\url{https://www.rstudio.com/products/rstudio})

The tidyverse {[}@Wickham2019-ot{]} is a powerful collection of R
packages with well-designed and consistent interfaces for the main steps
of data analysis: loading, transforming and plotting data
(\url{https://www.tidyverse.org}). This tutorial works with tidyverse
\textasciitilde v2.0. We will learn about the packages \texttt{readr},
\texttt{tibble}, \texttt{ggplot2}, \texttt{dplyr}, \texttt{magrittr} and
\texttt{tidyr}. \texttt{forcats} will be briefly mentioned, but
\texttt{purrr} and \texttt{stringr} are left out.

The \texttt{palmerpenguins} package {[}@Horst2020{]} provides a neat
example dataset to learn data exploration and visualization in R
(\url{https://allisonhorst.github.io/palmerpenguins})

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Environment preparation code (self-guided)}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

When you are going through the textbook in your own time, to set up your
working environment for this tutorial, please carry out the following
steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Install R and RStudio for your operating system according to one of
  the many instructions online
  (e.g.~\href{https://www.r-bloggers.com/2022/01/how-to-install-and-update-r-and-rstudio}{here})
\item
  Download the .qmd file underlying this webpage
  \href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/blob/main/r-tidyverse.qmd}{here}
  and copy it to a new directory
\item
  Open RStudio and create a new project in this directory with
  \texttt{File} -\textgreater{} \texttt{New\ Project...} -\textgreater{}
  \texttt{Existing\ directory}
\item
  Open this very file from the RStudio project, so that you can easily
  follow along
\item
  Run the \texttt{Environment\ preparation\ code} code to install
  necessary R package dependencies and prepare the required data used
  below
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# installing necessary R packages}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"palmerpenguins"}\NormalTok{))}

\CommentTok{\# preparing data}
\CommentTok{\# (at the end of this tutorial}
\CommentTok{\# you will understand this code)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5678}\NormalTok{)}

\NormalTok{peng\_prepped }\OtherTok{\textless{}{-}}\NormalTok{ palmerpenguins}\SpecialCharTok{::}\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}
        \SpecialCharTok{!}\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{if\_any}\NormalTok{(}
            \AttributeTok{.cols =}\NormalTok{ tidyselect}\SpecialCharTok{::}\FunctionTok{everything}\NormalTok{(),}
            \AttributeTok{.fns =}\NormalTok{ is.na}
\NormalTok{        )}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{add\_column}\NormalTok{(., }\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(.), }\AttributeTok{.before =} \StringTok{"species"}\NormalTok{)}

\NormalTok{peng\_prepped }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{300}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{bill\_length\_mm, }\SpecialCharTok{{-}}\NormalTok{bill\_depth\_mm) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    readr}\SpecialCharTok{::}\FunctionTok{write\_csv}\NormalTok{(}\StringTok{"penguins.csv"}\NormalTok{)}

\NormalTok{peng\_prepped }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{300}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(id, bill\_length\_mm, bill\_depth\_mm) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    readr}\SpecialCharTok{::}\FunctionTok{write\_csv}\NormalTok{(}\StringTok{"penguin\_bills.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{loading-data-into-tibbles}{%
\section{Loading data into tibbles}\label{loading-data-into-tibbles}}

\hypertarget{reading-data-with-readr}{%
\subsection{Reading data with readr}\label{reading-data-with-readr}}

With R we usually operate on data in our computer's memory. The
tidyverse provides the package \texttt{readr} to read data from text
files into the memory. \texttt{readr} can read from our file system or
the internet. It provides functions to read data in almost any (text)
format:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{() }\CommentTok{\# .csv files {-}\textgreater{} see penguins.csv}
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_tsv}\NormalTok{() }\CommentTok{\# .tsv files}
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_delim}\NormalTok{() }\CommentTok{\# tabular files with an arbitrary separator}
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_fwf}\NormalTok{() }\CommentTok{\# fixed width files}
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_lines}\NormalTok{() }\CommentTok{\# read linewise to parse yourself}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-does-the-interface-of-read_csv-work}{%
\subsection{\texorpdfstring{How does the interface of \texttt{read\_csv}
work?}{How does the interface of read\_csv work?}}\label{how-does-the-interface-of-read_csv-work}}

We can learn more about an R function with the \texttt{?} operator.

To open a help file for a specific function run
\texttt{?\textless{}function\_name\textgreater{}}
(e.g.~\texttt{?readr::read\_csv}) in the R console.
\texttt{readr::read\_csv} has many options to specify how to read a text
file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{read\_csv}\NormalTok{(}
\NormalTok{  file,                      }\CommentTok{\# The path to the file we want to read}
  \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,          }\CommentTok{\# Are there column names?}
  \AttributeTok{col\_types =} \ConstantTok{NULL}\NormalTok{,          }\CommentTok{\# Which types do the columns have? NULL {-}\textgreater{} auto}
  \AttributeTok{locale =} \FunctionTok{default\_locale}\NormalTok{(), }\CommentTok{\# How is information encoded in this file?}
  \AttributeTok{na =} \FunctionTok{c}\NormalTok{(}\StringTok{""}\NormalTok{, }\StringTok{"NA"}\NormalTok{),          }\CommentTok{\# Which values mean "no data"}
  \AttributeTok{trim\_ws =} \ConstantTok{TRUE}\NormalTok{,            }\CommentTok{\# Should superfluous white{-}spaces be removed?}
  \AttributeTok{skip =} \DecValTok{0}\NormalTok{,                  }\CommentTok{\# Skip X lines at the beginning of the file}
  \AttributeTok{n\_max =} \ConstantTok{Inf}\NormalTok{,               }\CommentTok{\# Only read X lines}
  \AttributeTok{skip\_empty\_rows =} \ConstantTok{TRUE}\NormalTok{,    }\CommentTok{\# Should empty lines be ignored? }
  \AttributeTok{comment =} \StringTok{""}\NormalTok{,              }\CommentTok{\# Should comment lines be ignored?}
  \AttributeTok{name\_repair =} \StringTok{"unique"}\NormalTok{,    }\CommentTok{\# How should "broken" column names be fixed}
\NormalTok{  ...}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{what-does-readr-produce-the-tibble}{%
\subsection{\texorpdfstring{What does \texttt{readr} produce? The
\texttt{tibble}!}{What does readr produce? The tibble!}}\label{what-does-readr-produce-the-tibble}}

To read a .csv file (here \texttt{"penguins.csv"}) into a variable (here
\texttt{peng\_auto}) run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng\_auto }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"penguins.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 300 Columns: 7
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (3): species, island, sex
dbl (4): id, flipper_length_mm, body_mass_g, year

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\texttt{readr} first prints some information on the number and type of
rows and columns it discovered in the file.

It automatically detects column types -- but you can also define them
manually.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}
  \StringTok{"penguins.csv"}\NormalTok{,}
  \AttributeTok{col\_types =} \StringTok{"iccddcc"} \CommentTok{\# this string encodes the desired types for each column}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It then returns an in-memory representation of this data, a
\texttt{tibble}.

A \texttt{tibble} is a ``data frame'', a tabular data structure with
rows and columns. Unlike a simple array, each column can have another
data type.

\hypertarget{how-to-look-at-a-tibble}{%
\subsection{\texorpdfstring{How to look at a
\texttt{tibble}?}{How to look at a tibble?}}\label{how-to-look-at-a-tibble}}

Typing the name of any object into the R console will print an overview
to the console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
      id species island    flipper_length_mm body_mass_g sex    year 
   <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
 1     1 Adelie  Torgersen               181        3750 male   2007 
 2     2 Adelie  Torgersen               186        3800 female 2007 
 3     4 Adelie  Torgersen               193        3450 female 2007 
 4     5 Adelie  Torgersen               190        3650 male   2007 
 5     6 Adelie  Torgersen               181        3625 female 2007 
 6     7 Adelie  Torgersen               195        4675 male   2007 
 7     9 Adelie  Torgersen               191        3800 male   2007 
 8    10 Adelie  Torgersen               198        4400 male   2007 
 9    11 Adelie  Torgersen               185        3700 female 2007 
10    12 Adelie  Torgersen               195        3450 female 2007 
# i 290 more rows
\end{verbatim}

But there are various other ways to inspect the content of a
\texttt{tibble}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(peng) }\CommentTok{\# a structural overview of an R object}
\FunctionTok{summary}\NormalTok{(peng) }\CommentTok{\# a human{-}readable summary of an R object}
\FunctionTok{View}\NormalTok{(peng) }\CommentTok{\# open RStudio\textquotesingle{}s interactive data browser}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting-data-in-tibbles}{%
\section{\texorpdfstring{Plotting data in
\texttt{tibble}s}{Plotting data in tibbles}}\label{plotting-data-in-tibbles}}

\hypertarget{ggplot2-and-the-grammar-of-graphics}{%
\subsection{\texorpdfstring{\texttt{ggplot2} and the ``grammar of
graphics''}{ggplot2 and the ``grammar of graphics''}}\label{ggplot2-and-the-grammar-of-graphics}}

To understand and present data, we usually have to visualize it.

\texttt{ggplot2} is an R package that offers an unusual, but powerful
and logical interface for this task {[}@Wickham2016{]}. The following
example describes a stacked bar chart.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2) }\CommentTok{\# Loading a library to use its functions without ::}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\CommentTok{\# Every plot starts with a call to the ggplot() function}
    \AttributeTok{data =}\NormalTok{ peng }\CommentTok{\# This function can also take the input tibble}
\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# The plot consists of individual functions linked with "+"}
    \FunctionTok{geom\_bar}\NormalTok{( }\CommentTok{\# "geoms" define the plot layers we want to draw}
        \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{( }\CommentTok{\# The aes() function maps variables to visual properties}
            \AttributeTok{x =}\NormalTok{ island, }\CommentTok{\# publication\_year {-}\textgreater{} x{-}axis}
            \AttributeTok{fill =}\NormalTok{ species }\CommentTok{\# community\_type   {-}\textgreater{} fill color}
\NormalTok{        )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

A \texttt{geom\_*} combines data (here \texttt{peng}), a geometry (here
vertical, stacked bars) and a statistical transformation (here counting
the number of penguins per island and species). \texttt{ggplot2}
features many such geoms: A good overview is provided by this
cheatsheet:
\url{https://rstudio.github.io/cheatsheets/html/data-visualization.html}.

Beyond \texttt{geom}s, a ggplot plot can be further specified with
(among others) \texttt{scale}s, \texttt{facet}s and \texttt{theme}s.

\hypertarget{scales-control-the-behaviour-of-visual-elements}{%
\subsection{\texorpdfstring{\texttt{scale}s control the behaviour of
visual
elements}{scales control the behaviour of visual elements}}\label{scales-control-the-behaviour-of-visual-elements}}

Here is another plot to demonstrate this: Boxplots of penguin weight per
species.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Let's assume we had some extreme outliers in this dataset. To simulate
this, we replace some random weights with extreme values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{\# we set a seed for reproducible randomness}
\NormalTok{peng\_out }\OtherTok{\textless{}{-}}\NormalTok{ peng}
\NormalTok{peng\_out}\SpecialCharTok{$}\NormalTok{body\_mass\_g[}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(peng\_out), }\DecValTok{10}\NormalTok{)] }\OtherTok{\textless{}{-}} \DecValTok{50000} \SpecialCharTok{+} \DecValTok{50000} \SpecialCharTok{*} \FunctionTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we plot the dataset with these ``outliers''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng\_out) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

This is not well readable, because the extreme outliers dictate the
scale of the y-axis. A 50+kg penguin is a scary thought and we would
probably remove these outliers, but let's assume they are valid
observation we want to include in the plot.

To mitigate this issue we can change the \textbf{scale} of different
visual elements - e.g.~the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng\_out) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g)) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_log10}\NormalTok{() }\CommentTok{\# adding the log{-}scale improves readability}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

We will now go back to the normal dataset without the artificial
outliers.

\hypertarget{colour-scales}{%
\subsection{\texorpdfstring{Colour
\texttt{scale}s}{Colour scales}}\label{colour-scales}}

(Fill) colour is a visual element of a plot and its scaling can be
adjusted as well.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ species)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"C"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}

We use the \texttt{scale\_*} function to select one of the visually
appealing (and robust to colourblindness) viridis colour palettes
(\url{https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html}).

\hypertarget{more-variables-defining-plot-matrices-via-facets}{%
\subsection{\texorpdfstring{More variables! Defining plot matrices via
\texttt{facet}s}{More variables! Defining plot matrices via facets}}\label{more-variables-defining-plot-matrices-via-facets}}

In the previous example we didn't add additional information with the
fill colour, as the plot already distinguished by species on the x-axis.

We can instead use colour to encode more information, for example by
mapping the variable sex to it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ sex))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\end{figure}

Another way to visualize more variables in one plot is to split the plot
by categories into \textbf{facets}, so sub-plots per category.

Here we split by sex, which is already mapped to the fill colour.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-19-1.pdf}

}

\end{figure}

The fill colour is therefore free again to show yet another variable,
for example the year a given penguin was examined.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ year)) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

\hypertarget{setting-purely-aesthetic-settings-with-theme}{%
\subsection{\texorpdfstring{Setting purely aesthetic settings with
\texttt{theme}}{Setting purely aesthetic settings with theme}}\label{setting-purely-aesthetic-settings-with-theme}}

Aesthetic changes can be applied as part of the \texttt{theme}, which
allows for very detailed configuration (see \texttt{?theme}).

Here we rotate the x-axis labels by 45°, which often helps to resolve
overplotting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species, }\AttributeTok{y =}\NormalTok{ body\_mass\_g, }\AttributeTok{fill =}\NormalTok{ year)) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sex) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-21-1.pdf}

}

\end{figure}

\hypertarget{ordering-elements-in-a-plot-with-factors}{%
\subsection{\texorpdfstring{Ordering elements in a plot with
\texttt{factors}}{Ordering elements in a plot with factors}}\label{ordering-elements-in-a-plot-with-factors}}

R supports defining ordinal data with \texttt{factor}s. This can be used
to set the order of elements in a plot, e.g.~the order of bars in a bar
chart.

We do not cover \texttt{factor}s beyond the following example here,
although the tidyverse includes a package (\texttt{forcats})
specifically for that purpose.

Elements based on \texttt{character} columns are generally ordered
alphabetically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species)) }\CommentTok{\# bars are alphabetically ordered}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-22-1.pdf}

}

\end{figure}

With \texttt{forcats::fct\_reorder} we can transform an input vector to
a \texttt{factor}, ordered by a summary statistic (even based on another
vector).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng2 }\OtherTok{\textless{}{-}}\NormalTok{ peng}
\NormalTok{peng2}\SpecialCharTok{$}\NormalTok{species\_ordered }\OtherTok{\textless{}{-}}\NormalTok{ forcats}\SpecialCharTok{::}\FunctionTok{fct\_reorder}\NormalTok{(}
\NormalTok{  peng2}\SpecialCharTok{$}\NormalTok{species,}
\NormalTok{  peng2}\SpecialCharTok{$}\NormalTok{species, length}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With this change, the plot will be ordered according to the intrinsic
order defined for \texttt{species\_ordered}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(peng2) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ species\_ordered)) }\CommentTok{\# bars are ordered by size}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-24-1.pdf}

}

\end{figure}

\hypertarget{exercise}{%
\subsection{Exercise}\label{exercise}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Look at the \texttt{mtcars} dataset and read up on the meaning of its
  variables with the help operator \texttt{?}. \texttt{mtcars} is a test
  dataset integrated in R and can be accessed by typing \texttt{mtcars}
  in the console
\item
  Visualize the relationship between \emph{Gross horsepower} and
  \emph{1/4 mile time}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Integrate the \emph{Number of cylinders} into your plot as an
  additional variable
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Possible solutions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?mtcars}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ qsec))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-28-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(mtcars) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ qsec, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(cyl)))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{r-tidyverse_files/figure-pdf/unnamed-chunk-29-1.pdf}

}

\end{figure}

\end{tcolorbox}

\hypertarget{conditional-queries-on-tibbles}{%
\section{Conditional queries on
tibbles}\label{conditional-queries-on-tibbles}}

\hypertarget{selecting-columns-and-filtering-rows-with-select-and-filter}{%
\subsection{\texorpdfstring{Selecting columns and filtering rows with
\texttt{select} and
\texttt{filter}}{Selecting columns and filtering rows with select and filter}}\label{selecting-columns-and-filtering-rows-with-select-and-filter}}

The \texttt{dplyr} package includes powerful functions to subset data in
tibbles based on conditions. \texttt{dplyr::select} allows to select
columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(peng, id, flipper\_length\_mm) }\CommentTok{\# reduce tibble to two columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 2
     id flipper_length_mm
  <int>             <dbl>
1     1               181
2     2               186
3     4               193
4     5               190
5     6               181
# i 295 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(peng, }\SpecialCharTok{{-}}\NormalTok{island, }\SpecialCharTok{{-}}\NormalTok{flipper\_length\_mm) }\CommentTok{\# remove two columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 5
     id species body_mass_g sex    year 
  <int> <chr>         <dbl> <chr>  <chr>
1     1 Adelie         3750 male   2007 
2     2 Adelie         3800 female 2007 
3     4 Adelie         3450 female 2007 
4     5 Adelie         3650 male   2007 
5     6 Adelie         3625 female 2007 
# i 295 more rows
\end{verbatim}

\texttt{dplyr::filter} allows for conditional filtering of rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(peng, year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{) }\CommentTok{\# penguins examined in 2007}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 93 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     1 Adelie  Torgersen               181        3750 male   2007 
2     2 Adelie  Torgersen               186        3800 female 2007 
3     4 Adelie  Torgersen               193        3450 female 2007 
4     5 Adelie  Torgersen               190        3650 male   2007 
5     6 Adelie  Torgersen               181        3625 female 2007 
# i 88 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(peng, year }\SpecialCharTok{==} \DecValTok{2007} \SpecialCharTok{|}
\NormalTok{    year }\SpecialCharTok{==} \DecValTok{2009}\NormalTok{) }\CommentTok{\# penguins examined in 2007 OR 2009}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 198 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     1 Adelie  Torgersen               181        3750 male   2007 
2     2 Adelie  Torgersen               186        3800 female 2007 
3     4 Adelie  Torgersen               193        3450 female 2007 
4     5 Adelie  Torgersen               190        3650 male   2007 
5     6 Adelie  Torgersen               181        3625 female 2007 
# i 193 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}
\NormalTok{    peng, }\CommentTok{\# an alternative way to express}
\NormalTok{    year }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{2007}\NormalTok{, }\DecValTok{2009}\NormalTok{)}
\NormalTok{) }\CommentTok{\# OR with the match operator "\%in\%"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 198 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     1 Adelie  Torgersen               181        3750 male   2007 
2     2 Adelie  Torgersen               186        3800 female 2007 
3     4 Adelie  Torgersen               193        3450 female 2007 
4     5 Adelie  Torgersen               190        3650 male   2007 
5     6 Adelie  Torgersen               181        3625 female 2007 
# i 193 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(peng, species }\SpecialCharTok{==} \StringTok{"Adelie"} \SpecialCharTok{\&}
\NormalTok{    body\_mass\_g }\SpecialCharTok{\textgreater{}=} \DecValTok{4000}\NormalTok{) }\CommentTok{\# Adelie penguins heavier than 4kg}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 29 x 7
     id species island    flipper_length_mm body_mass_g sex   year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr> <chr>
1     7 Adelie  Torgersen               195        4675 male  2007 
2    10 Adelie  Torgersen               198        4400 male  2007 
3    13 Adelie  Torgersen               197        4500 male  2007 
4    31 Adelie  Dream                   196        4150 male  2007 
5    39 Adelie  Dream                   196        4400 male  2007 
# i 24 more rows
\end{verbatim}

\hypertarget{chaining-functions-together-with-the-pipe}{%
\subsection{\texorpdfstring{Chaining functions together with the pipe
\texttt{\%\textgreater{}\%}}{Chaining functions together with the pipe \%\textgreater\%}}\label{chaining-functions-together-with-the-pipe}}

A core feature of the tidyverse is the pipe \texttt{\%\textgreater{}\%}
in the \texttt{magrittr} package. This `infix' operator allows to chain
data and operations for concise and clear data analysis syntax.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 93 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     1 Adelie  Torgersen               181        3750 male   2007 
2     2 Adelie  Torgersen               186        3800 female 2007 
3     4 Adelie  Torgersen               193        3450 female 2007 
4     5 Adelie  Torgersen               190        3650 male   2007 
5     6 Adelie  Torgersen               181        3625 female 2007 
# i 88 more rows
\end{verbatim}

It forwards the LHS (left-hand side) as the first argument of the
function appearing on the RHS (right-hand side), which enables sequences
of functions (``tidyverse style'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(id, species, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(species }\SpecialCharTok{==} \StringTok{"Adelie"} \SpecialCharTok{\&}\NormalTok{ body\_mass\_g }\SpecialCharTok{\textgreater{}=} \DecValTok{4000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nrow}\NormalTok{() }\CommentTok{\# count the resulting rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 29
\end{verbatim}

\texttt{magrittr} also offers some more operators, among which the
extraction \texttt{\%\$\%} is particularly useful to easily extract
individual variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(island }\SpecialCharTok{==} \StringTok{"Biscoe"}\NormalTok{) }\SpecialCharTok{\%$\%}
\NormalTok{    species }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the species column as a vector}
    \FunctionTok{unique}\NormalTok{() }\CommentTok{\# get the unique elements of said vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Adelie" "Gentoo"
\end{verbatim}

Here we already use the base R summary function \texttt{unique}.

\hypertarget{summary-statistics-in-base-r}{%
\subsection{\texorpdfstring{Summary statistics in \texttt{base}
R}{Summary statistics in base R}}\label{summary-statistics-in-base-r}}

Summarising and counting data is indispensable and R offers all basic
operations you would expect in its \texttt{base} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chinstraps\_weights }\OtherTok{\textless{}{-}}\NormalTok{ peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(species }\SpecialCharTok{==} \StringTok{"Chinstrap"}\NormalTok{) }\SpecialCharTok{\%$\%}
\NormalTok{    body\_mass\_g}

\FunctionTok{length}\NormalTok{(chinstraps\_weights) }\CommentTok{\# length/size of a vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 63
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(chinstraps\_weights) }\CommentTok{\# unique elements of a vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 3500 3900 3650 3525 3725 3950 3250 3750 4150 3700 3800 3775 4050 3300 3450
[16] 4400 3400 2900 4550 3200 4300 3350 4100 3600 3850 4800 2700 4500 3550 3675
[31] 4450 3325 4000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{min}\NormalTok{(chinstraps\_weights) }\CommentTok{\# minimum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2700
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(chinstraps\_weights) }\CommentTok{\# maximum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4800
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(chinstraps\_weights) }\CommentTok{\# mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3751.19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(chinstraps\_weights) }\CommentTok{\# median}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3725
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(chinstraps\_weights) }\CommentTok{\# variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 153032.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(chinstraps\_weights) }\CommentTok{\# standard deviation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 391.1941
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(chinstraps\_weights, }\AttributeTok{probs =} \FloatTok{0.75}\NormalTok{) }\CommentTok{\# quantiles for the given probabilities}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 75% 
3975 
\end{verbatim}

Many of these functions can ignore missing values with an option
\texttt{na.rm\ =\ TRUE}.

\hypertarget{group-wise-summaries-with-group_by-and-summarise}{%
\subsection{\texorpdfstring{Group-wise summaries with \texttt{group\_by}
and
\texttt{summarise}}{Group-wise summaries with group\_by and summarise}}\label{group-wise-summaries-with-group_by-and-summarise}}

These summary statistics are particular useful when applied to
conditional subsets of a dataset.

\texttt{dplyr} allows such summary operations with a combination of the
functions \texttt{group\_by} and \texttt{summarise}, where the former
tags a \texttt{tibble} with categories based on its variables and the
latter reduces it to these groups while simultanously creating new
columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group the tibble by the material column}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}
        \AttributeTok{min\_weight =} \FunctionTok{min}\NormalTok{(body\_mass\_g), }\CommentTok{\# new col: min weight for each group}
        \AttributeTok{median\_weight =} \FunctionTok{median}\NormalTok{(body\_mass\_g), }\CommentTok{\# new col: median weight for each group}
        \AttributeTok{max\_weight =} \FunctionTok{max}\NormalTok{(body\_mass\_g) }\CommentTok{\# new col: max weight for each group}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  species   min_weight median_weight max_weight
  <chr>          <dbl>         <dbl>      <dbl>
1 Adelie          2850          3650       4775
2 Chinstrap       2700          3725       4800
3 Gentoo          3950          5050       6300
\end{verbatim}

Grouping can also be applied across multiple columns at once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(species, year) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group by species and year}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}
        \AttributeTok{n =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{n}\NormalTok{(), }\CommentTok{\# a new column: number of penguins for each group}
        \AttributeTok{.groups =} \StringTok{"drop"} \CommentTok{\# drop the grouping after this summary operation}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 9 x 3
  species   year      n
  <chr>     <chr> <int>
1 Adelie    2007     38
2 Adelie    2008     45
3 Adelie    2009     43
4 Chinstrap 2007     23
5 Chinstrap 2008     18
# i 4 more rows
\end{verbatim}

If we group by more than one variable, then \texttt{summarise} will not
entirely remove the group tagging when generating the result dataset. We
can force this with \texttt{.groups\ =\ "drop"} to avoid undesired
behaviour with this dataset later on.

\hypertarget{sorting-and-slicing-tibbles-with-arrange-and-slice}{%
\subsection{\texorpdfstring{Sorting and slicing tibbles with
\texttt{arrange} and
\texttt{slice}}{Sorting and slicing tibbles with arrange and slice}}\label{sorting-and-slicing-tibbles-with-arrange-and-slice}}

\texttt{dplyr} allows to \texttt{arrange} tibbles by one or multiple
columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(sex) }\CommentTok{\# sort by sex}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     2 Adelie  Torgersen               186        3800 female 2007 
2     4 Adelie  Torgersen               193        3450 female 2007 
3     6 Adelie  Torgersen               181        3625 female 2007 
4    11 Adelie  Torgersen               185        3700 female 2007 
5    12 Adelie  Torgersen               195        3450 female 2007 
# i 295 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(sex, body\_mass\_g) }\CommentTok{\# sort by sex and weight}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
     id species   island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>     <chr>                 <dbl>       <dbl> <chr>  <chr>
1   304 Chinstrap Dream                   192        2700 female 2008 
2    53 Adelie    Biscoe                  181        2850 female 2008 
3    59 Adelie    Biscoe                  184        2850 female 2008 
4    49 Adelie    Biscoe                  187        2900 female 2008 
5   111 Adelie    Torgersen               188        2900 female 2009 
# i 295 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{desc}\NormalTok{(body\_mass\_g)) }\CommentTok{\# sort descending}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
     id species island flipper_length_mm body_mass_g sex   year 
  <int> <chr>   <chr>              <dbl>       <dbl> <chr> <chr>
1   164 Gentoo  Biscoe               221        6300 male  2007 
2   179 Gentoo  Biscoe               230        6050 male  2007 
3   260 Gentoo  Biscoe               222        6000 male  2009 
4   224 Gentoo  Biscoe               223        5950 male  2008 
5   160 Gentoo  Biscoe               213        5850 male  2007 
# i 295 more rows
\end{verbatim}

Sorting also works within groups and can be paired with \texttt{slice}
to extract extreme values per group.

Here we extract the heaviest individuals per species.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group by species}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{desc}\NormalTok{(body\_mass\_g)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# sort by weight within (!) groups}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# keep the first three penguins per group}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# remove the still lingering grouping}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 9 x 7
     id species   island    flipper_length_mm body_mass_g sex   year 
  <int> <chr>     <chr>                 <dbl>       <dbl> <chr> <chr>
1   104 Adelie    Biscoe                  197        4775 male  2009 
2    96 Adelie    Biscoe                  203        4725 male  2009 
3    76 Adelie    Torgersen               196        4700 male  2008 
4   303 Chinstrap Dream                   210        4800 male  2008 
5   295 Chinstrap Dream                   205        4550 male  2008 
# i 4 more rows
\end{verbatim}

Slicing is also the relevant operation to take random samples from the
observations in a \texttt{tibble}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 7
     id species island flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>              <dbl>       <dbl> <chr>  <chr>
1    47 Adelie  Biscoe               190        3450 female 2008 
2   239 Gentoo  Biscoe               214        4850 female 2009 
3   221 Gentoo  Biscoe               209        4600 female 2008 
4   104 Adelie  Biscoe               197        4775 male   2009 
5   138 Adelie  Dream                190        3725 male   2009 
# i 5 more rows
\end{verbatim}

\hypertarget{exercise-1}{%
\subsection{Exercise}\label{exercise-1}}

For this exercise we once more go back to the \texttt{mtcars} dataset.
See \texttt{?mtcars} for details

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Determine the number of cars with four \emph{forward gears}
  (\texttt{gear}) in the \texttt{mtcars} dataset
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Determine the mean \emph{1/4 mile time} (\texttt{qsec}) per
  \emph{Number of cylinders} (\texttt{cyl}) group
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Identify the least efficient cars for both \emph{transmission types}
  (\texttt{am})
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Possible solutions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(gear }\SpecialCharTok{==} \DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(cyl) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}
        \AttributeTok{qsec\_mean =} \FunctionTok{mean}\NormalTok{(qsec)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
    cyl qsec_mean
  <dbl>     <dbl>
1     4      19.1
2     6      18.0
3     8      16.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars2 }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{rownames\_to\_column}\NormalTok{(mtcars, }\AttributeTok{var =} \StringTok{"car"}\NormalTok{)}
\NormalTok{mtcars2 }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(am) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(mpg) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{slice\_head}\NormalTok{() }\SpecialCharTok{\%$\%}
\NormalTok{    car}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Cadillac Fleetwood" "Maserati Bora"     
\end{verbatim}

\end{tcolorbox}

\hypertarget{transforming-and-manipulating-tibbles}{%
\section{Transforming and manipulating
tibbles}\label{transforming-and-manipulating-tibbles}}

\hypertarget{renaming-and-reordering-columns-with-rename-and-relocate}{%
\subsection{\texorpdfstring{Renaming and reordering columns with
\texttt{rename} and
\texttt{relocate}}{Renaming and reordering columns with rename and relocate}}\label{renaming-and-reordering-columns-with-rename-and-relocate}}

Columns in tibbles can be renamed with \texttt{dplyr::rename}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{penguin\_name =}\NormalTok{ id) }\CommentTok{\# rename a column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
  penguin_name species island    flipper_length_mm body_mass_g sex    year 
         <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1            1 Adelie  Torgersen               181        3750 male   2007 
2            2 Adelie  Torgersen               186        3800 female 2007 
3            4 Adelie  Torgersen               193        3450 female 2007 
4            5 Adelie  Torgersen               190        3650 male   2007 
5            6 Adelie  Torgersen               181        3625 female 2007 
# i 295 more rows
\end{verbatim}

And with \texttt{dplyr::relocate} they can be reordered.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{relocate}\NormalTok{(year, }\AttributeTok{.before =}\NormalTok{ species) }\CommentTok{\# reorder columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 7
     id year  species island    flipper_length_mm body_mass_g sex   
  <int> <chr> <chr>   <chr>                 <dbl>       <dbl> <chr> 
1     1 2007  Adelie  Torgersen               181        3750 male  
2     2 2007  Adelie  Torgersen               186        3800 female
3     4 2007  Adelie  Torgersen               193        3450 female
4     5 2007  Adelie  Torgersen               190        3650 male  
5     6 2007  Adelie  Torgersen               181        3625 female
# i 295 more rows
\end{verbatim}

Adding columns to tibbles with \texttt{mutate} and \texttt{transmute}.

A common application of data manipulation is adding new, derived
columns, that combine or modify the information in the already available
columns. \texttt{dplyr} offers this core feature with the
\texttt{mutate} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{( }\CommentTok{\# add a column that}
        \AttributeTok{kg =}\NormalTok{ body\_mass\_g }\SpecialCharTok{/} \DecValTok{1000} \CommentTok{\# manipulates an existing column}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 8
     id species island    flipper_length_mm body_mass_g sex    year     kg
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr> <dbl>
1     1 Adelie  Torgersen               181        3750 male   2007   3.75
2     2 Adelie  Torgersen               186        3800 female 2007   3.8 
3     4 Adelie  Torgersen               193        3450 female 2007   3.45
4     5 Adelie  Torgersen               190        3650 male   2007   3.65
5     6 Adelie  Torgersen               181        3625 female 2007   3.62
# i 295 more rows
\end{verbatim}

\texttt{dplyr::transmute} has the same interface as
\texttt{dplyr::mutate}, but it removes all columns except for the newly
created ones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{transmute}\NormalTok{(}
    \AttributeTok{id =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Penguin Nr."}\NormalTok{, id), }\CommentTok{\# overwrite this column}
\NormalTok{    flipper\_length\_mm               }\CommentTok{\# select this column}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 2
  id            flipper_length_mm
  <chr>                     <dbl>
1 Penguin Nr. 1               181
2 Penguin Nr. 2               186
3 Penguin Nr. 4               193
4 Penguin Nr. 5               190
5 Penguin Nr. 6               181
# i 295 more rows
\end{verbatim}

\texttt{tibble::add\_column} behaves as \texttt{dplyr::mutate}, but
gives more control over column position.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{add\_column}\NormalTok{(}
  \AttributeTok{flipper\_length\_cm =}\NormalTok{ .}\SpecialCharTok{$}\NormalTok{flipper\_length\_mm}\SpecialCharTok{/}\DecValTok{10}\NormalTok{, }\CommentTok{\# not the . representing the LHS of the pipe}
  \AttributeTok{.after =} \StringTok{"flipper\_length\_mm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 8
     id species island    flipper_length_mm flipper_length_cm body_mass_g sex   
  <int> <chr>   <chr>                 <dbl>             <dbl>       <dbl> <chr> 
1     1 Adelie  Torgersen               181              18.1        3750 male  
2     2 Adelie  Torgersen               186              18.6        3800 female
3     4 Adelie  Torgersen               193              19.3        3450 female
4     5 Adelie  Torgersen               190              19          3650 male  
5     6 Adelie  Torgersen               181              18.1        3625 female
# i 295 more rows
# i 1 more variable: year <chr>
\end{verbatim}

\texttt{dplyr::mutate} can also be combined with
\texttt{dplyr::group\_by} (instead of \texttt{dplyr::summarise}) to add
information on a group level. This is relevant, when a value for an
individual entity should be put into context of a group-wise summary
statistic.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(species, sex, year) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{mean\_weight =} \FunctionTok{mean}\NormalTok{(body\_mass\_g, }\AttributeTok{na.rm =}\NormalTok{ T),}
        \AttributeTok{relation\_to\_mean =}\NormalTok{ body\_mass\_g }\SpecialCharTok{/}\NormalTok{ mean\_weight}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(id, species, sex, year, relation\_to\_mean) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# mutate does not remove rows, unlike summarise, so we use select}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{desc}\NormalTok{(relation\_to\_mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 5
     id species   sex    year  relation_to_mean
  <int> <chr>     <chr>  <chr>            <dbl>
1   104 Adelie    male   2009              1.21
2    96 Adelie    male   2009              1.20
3   274 Chinstrap female 2007              1.17
4     7 Adelie    male   2007              1.17
5   303 Chinstrap male   2008              1.16
# i 295 more rows
\end{verbatim}

\hypertarget{conditional-operations-with-ifelse-case_when-and-case_match}{%
\subsection{\texorpdfstring{Conditional operations with \texttt{ifelse},
\texttt{case\_when} and
\texttt{case\_match}}{Conditional operations with ifelse, case\_when and case\_match}}\label{conditional-operations-with-ifelse-case_when-and-case_match}}

\texttt{ifelse} allows to implement conditional \texttt{mutate}
operations, that consider information from other columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
  \AttributeTok{weight =} \FunctionTok{ifelse}\NormalTok{(}
    \AttributeTok{test =}\NormalTok{ body\_mass\_g }\SpecialCharTok{\textgreater{}=} \DecValTok{4200}\NormalTok{, }\CommentTok{\# is weight below or above mean weight?}
    \AttributeTok{yes =} \StringTok{"above mean"}\NormalTok{,}
    \AttributeTok{no =} \StringTok{"below mean"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 8
     id species island    flipper_length_mm body_mass_g sex    year  weight    
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr> <chr>     
1     1 Adelie  Torgersen               181        3750 male   2007  below mean
2     2 Adelie  Torgersen               186        3800 female 2007  below mean
3     4 Adelie  Torgersen               193        3450 female 2007  below mean
4     5 Adelie  Torgersen               190        3650 male   2007  below mean
5     6 Adelie  Torgersen               181        3625 female 2007  below mean
# i 295 more rows
\end{verbatim}

\texttt{ifelse} gets cumbersome easily. \texttt{dplyr::case\_when} is
more readable and scales much better for this application.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
  \AttributeTok{weight =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    body\_mass\_g }\SpecialCharTok{\textgreater{}=} \DecValTok{4200} \SpecialCharTok{\textasciitilde{}} \StringTok{"above mean"}\NormalTok{, }\CommentTok{\# the number of conditions is arbitrary}
\NormalTok{    body\_mass\_g }\SpecialCharTok{\textless{}} \DecValTok{4200}  \SpecialCharTok{\textasciitilde{}} \StringTok{"below mean"}\NormalTok{,}
    \ConstantTok{TRUE}                \SpecialCharTok{\textasciitilde{}} \StringTok{"unknown"}     \CommentTok{\# TRUE catches all remaining cases}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 8
     id species island    flipper_length_mm body_mass_g sex    year  weight    
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr> <chr>     
1     1 Adelie  Torgersen               181        3750 male   2007  below mean
2     2 Adelie  Torgersen               186        3800 female 2007  below mean
3     4 Adelie  Torgersen               193        3450 female 2007  below mean
4     5 Adelie  Torgersen               190        3650 male   2007  below mean
5     6 Adelie  Torgersen               181        3625 female 2007  below mean
# i 295 more rows
\end{verbatim}

\texttt{dplyr::case\_match} is similar, but unlike
\texttt{dplyr::case\_when} it does not check logical expressions, but
matches by value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peng }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{island\_rating =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{case\_match}\NormalTok{(}
\NormalTok{            island,}
            \StringTok{"Torgersen"} \SpecialCharTok{\textasciitilde{}} \StringTok{"My favourite island"}\NormalTok{,}
            \StringTok{"Biscoe"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Overrated tourist trap"}\NormalTok{,}
            \StringTok{"Dream"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Lost my wallet there. 4/10"}
\NormalTok{        )}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# here we use group\_by+summarise only to show the result}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(island, island\_rating) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  island    island_rating             
  <chr>     <chr>                     
1 Biscoe    Overrated tourist trap    
2 Dream     Lost my wallet there. 4/10
3 Torgersen My favourite island       
\end{verbatim}

\hypertarget{switching-between-long-and-wide-data-with-pivot_longer-and-pivot_wider}{%
\subsection{\texorpdfstring{Switching between long and wide data with
\texttt{pivot\_longer} and
\texttt{pivot\_wider}}{Switching between long and wide data with pivot\_longer and pivot\_wider}}\label{switching-between-long-and-wide-data-with-pivot_longer-and-pivot_wider}}

To simplify certain analysis or plotting operations data often has to be
transformed from a \textbf{wide} to a \textbf{long} format or vice versa
(Figure~\ref{fig-rtidyverse-longtowide}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/pivot_longer_wider.png}

}

\caption{\label{fig-rtidyverse-longtowide}Graphical representation of
converting a table from a wide format (first column are categories,
e.g.~species, first row are also categories e.g., samples, and counts
are in cells after first row/column. Conversion from wide to long makes
the long-format so that the first columnn still has categories, but the
second column becomes the column-categories from the first row, and
third column has values. Converting long format back to wider shows
again first row and column as categories and cells as values).}

\end{figure}

\begin{itemize}
\tightlist
\item
  A table in \textbf{wide} format has N key columns and N value columns.
\item
  A table in \textbf{long} format has N key columns, one descriptor
  column and one value column.
\end{itemize}

Here is an example of a wide dataset. It features information about the
number of cars sold per year per brand at a dealership.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{carsales }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tribble}\NormalTok{(}
    \SpecialCharTok{\textasciitilde{}}\NormalTok{brand, }\SpecialCharTok{\textasciitilde{}}\StringTok{\textasciigrave{}}\AttributeTok{2014}\StringTok{\textasciigrave{}}\NormalTok{, }\SpecialCharTok{\textasciitilde{}}\StringTok{\textasciigrave{}}\AttributeTok{2015}\StringTok{\textasciigrave{}}\NormalTok{, }\SpecialCharTok{\textasciitilde{}}\StringTok{\textasciigrave{}}\AttributeTok{2016}\StringTok{\textasciigrave{}}\NormalTok{, }\SpecialCharTok{\textasciitilde{}}\StringTok{\textasciigrave{}}\AttributeTok{2017}\StringTok{\textasciigrave{}}\NormalTok{,}
    \StringTok{"BMW"}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{45}\NormalTok{,}
    \StringTok{"VW"}\NormalTok{, }\DecValTok{67}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{55}
\NormalTok{)}
\NormalTok{carsales}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 5
  brand `2014` `2015` `2016` `2017`
  <chr>  <dbl>  <dbl>  <dbl>  <dbl>
1 BMW       20     25     30     45
2 VW        67     40    120     55
\end{verbatim}

Wide format data becomes a problem, when the columns are semantically
identical. This dataset is in wide format and we can not easily plot it.
We generally prefer data in long format, although it is more verbose
with more duplication.

To transform this dataset to a long format, we can apply
\texttt{tidyr::pivot\_longer}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{carsales\_long }\OtherTok{\textless{}{-}}\NormalTok{ carsales }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =}\NormalTok{ tidyselect}\SpecialCharTok{::}\FunctionTok{num\_range}\NormalTok{(}
        \StringTok{""}\NormalTok{,}
        \AttributeTok{range =} \DecValTok{2014}\SpecialCharTok{:}\DecValTok{2017}
\NormalTok{    ), }\CommentTok{\# define a set of columns to transform}
    \AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{, }\CommentTok{\# the name of the descriptor column we want}
    \AttributeTok{names\_transform =}\NormalTok{ as.integer, }\CommentTok{\# a transformation function to apply to the names}
    \AttributeTok{values\_to =} \StringTok{"sales"} \CommentTok{\# the name of the value column we want}
\NormalTok{)}
\NormalTok{carsales\_long}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8 x 3
  brand  year sales
  <chr> <int> <dbl>
1 BMW    2014    20
2 BMW    2015    25
3 BMW    2016    30
4 BMW    2017    45
5 VW     2014    67
# i 3 more rows
\end{verbatim}

Wide datasets are not always the wrong choice, for example for adjacency
matrices to represent graphs, covariance matrices or other pairwise
statistics. When data gets big, then wide formats can be significantly
more efficient (e.g.~for spatial data).

So transform data from long to wide, we can use
\texttt{tidyr::pivot\_wider}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{carsales\_wide }\OtherTok{\textless{}{-}}\NormalTok{ carsales\_long }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{id\_cols =} \StringTok{"brand"}\NormalTok{, }\CommentTok{\# the set of id columns that should not be changed}
    \AttributeTok{names\_from =}\NormalTok{ year, }\CommentTok{\# the descriptor column with the names of the new columns}
    \AttributeTok{values\_from =}\NormalTok{ sales }\CommentTok{\# the value column from which the values should be extracted}
\NormalTok{)}
\NormalTok{carsales\_wide}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 5
  brand `2014` `2015` `2016` `2017`
  <chr>  <dbl>  <dbl>  <dbl>  <dbl>
1 BMW       20     25     30     45
2 VW        67     40    120     55
\end{verbatim}

\hypertarget{exercise-2}{%
\subsection{Exercise}\label{exercise-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Move the column \texttt{gear} to the first position of the mtcars
  dataset
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Make a new dataset \texttt{mtcars2} with the column \texttt{mpg} and
  an additional column \texttt{am\_v}, which encodes the
  \emph{transmission type} (\texttt{am}) as either \texttt{"manual"} or
  \texttt{"automatic"}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Count the number of cars per \emph{transmission type} (\texttt{am\_v})
  and \emph{number of gears} (\texttt{gear}). Then transform the result
  to a wide format, with one column per \emph{transmission type}.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Possible solutions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{relocate}\NormalTok{(gear, }\AttributeTok{.before =}\NormalTok{ mpg) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{as\_tibble}\NormalTok{() }\CommentTok{\# transforming the raw dataset for better printing}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 32 x 11
   gear   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  carb
  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1     4  21       6   160   110  3.9   2.62  16.5     0     1     4
2     4  21       6   160   110  3.9   2.88  17.0     0     1     4
3     4  22.8     4   108    93  3.85  2.32  18.6     1     1     1
4     3  21.4     6   258   110  3.08  3.22  19.4     1     0     1
5     3  18.7     8   360   175  3.15  3.44  17.0     0     0     2
# i 27 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars2 }\OtherTok{\textless{}{-}}\NormalTok{ mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
\NormalTok{        gear,}
        \AttributeTok{am\_v =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{case\_match}\NormalTok{(}
\NormalTok{            am,}
            \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{"automatic"}\NormalTok{,}
            \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"manual"}
\NormalTok{        )}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{as\_tibble}\NormalTok{()}
\NormalTok{mtcars2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 32 x 12
    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb am_v     
  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>    
1  21       6   160   110  3.9   2.62  16.5     0     1     4     4 manual   
2  21       6   160   110  3.9   2.88  17.0     0     1     4     4 manual   
3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1 manual   
4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1 automatic
5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2 automatic
# i 27 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mtcars2 }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(am\_v, gear) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{tally}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dplyr::tally() is identical to}
    \CommentTok{\# dplyr::summarise(n = dplyr::n())}
    \CommentTok{\# it counts the number of entities in a group}
\NormalTok{    tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}
        \AttributeTok{names\_from =}\NormalTok{ am\_v,}
        \AttributeTok{values\_from =}\NormalTok{ n}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
   gear automatic manual
  <dbl>     <int>  <int>
1     3        15     NA
2     4         4      8
3     5        NA      5
\end{verbatim}

\end{tcolorbox}

\hypertarget{combining-tibbles-with-join-operations}{%
\section{Combining tibbles with join
operations}\label{combining-tibbles-with-join-operations}}

\hypertarget{types-of-joins}{%
\subsection{Types of joins}\label{types-of-joins}}

Joins combine two datasets x and y based on overlapping key columns.

Mutating joins add columns from one dataset to the other:

\begin{itemize}
\tightlist
\item
  Left join: Take observations from x and add fitting information from
  y.
\item
  Right join: Take observations from y and add fitting information from
  x.
\item
  Inner join: Join the overlapping observations from x and y.
\item
  Full join: Join all observations from x and y, even if information is
  missing.
\end{itemize}

Filtering joins remove observations from x based on their presence in y.

Types of filtering consist of:

\begin{itemize}
\tightlist
\item
  Semi join: Keep every observation in x that is in y.
\item
  Anti join: Keep every observation in x that is not in y.
\end{itemize}

The following sections will introduce each join with an example.

To experiment with joins, we need a second dataset with complementary
information. This new dataset contains additional variables for a subset
of the penguins in our first dataset -- both datasets feature 300
penguins, but only with a partial overlap in individuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bills }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"penguin\_bills.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 3
     id bill_length_mm bill_depth_mm
  <dbl>          <dbl>         <dbl>
1     1           39.1          18.7
2     2           39.5          17.4
3     3           40.3          18  
4     4           36.7          19.3
5     5           39.3          20.6
# i 295 more rows
\end{verbatim}

\hypertarget{left-join-with-left_join}{%
\subsection{\texorpdfstring{Left join with
\texttt{left\_join}}{Left join with left\_join}}\label{left-join-with-left_join}}

Take observations from x and add fitting information from y
(Figure~\ref{fig-rtidyverse-leftjoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/left_join.png}

}

\caption{\label{fig-rtidyverse-leftjoin}Graphical representation of left
join operation. Two tables with a shared first column (A B C, and A B D
respectively) are merged together to have columns A B C D. As A and B
have a one to one match of values, this remains the same in the joined
table. The B column between the two have a different value on the third
row, and thus is lost from the second table, retaining row 3 of the
first table. Column D (from the second table) has an empty value on row
three, as this row was not in row 3 of the second table that column D
was derived from.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{left\_join}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ peng,  }\CommentTok{\# 300 observations}
  \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
  \AttributeTok{by =} \StringTok{"id"}  \CommentTok{\# the key column by which to join}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 9
     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm
  <dbl> <chr>   <chr>               <dbl>       <dbl> <chr> <chr>          <dbl>
1     1 Adelie  Torger~               181        3750 male  2007            39.1
2     2 Adelie  Torger~               186        3800 fema~ 2007            39.5
3     4 Adelie  Torger~               193        3450 fema~ 2007            36.7
4     5 Adelie  Torger~               190        3650 male  2007            39.3
5     6 Adelie  Torger~               181        3625 fema~ 2007            38.9
# i 295 more rows
# i 1 more variable: bill_depth_mm <dbl>
\end{verbatim}

Left joins are the most common join operation: Add information from y to
the main dataset x.

\hypertarget{right-join-with-right_join}{%
\subsection{\texorpdfstring{Right join with
\texttt{right\_join}}{Right join with right\_join}}\label{right-join-with-right_join}}

Take observations from y and add fitting information from x
(Figure~\ref{fig-rtidyverse-rightjoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/right_join.png}

}

\caption{\label{fig-rtidyverse-rightjoin}Graphical representation of
right join operation. Two tables with a shared first column (A B C, and
A B D respectively) are merged together to have columns A B C D. As A
and B have a one to one match of values, this remains the same in the
joined table. The B column between the two have a different value on the
third row, and thus is lost from the first table, retaining row 3 of the
second table. Column C (from the first table) has an empty value on row
three, as this row was not in row 3 of the first table that column C was
derived from.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{right\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ peng, }\CommentTok{\# 300 observations}
    \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
    \AttributeTok{by =} \StringTok{"id"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# we arrange by id to highlight the missing}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(id) }\CommentTok{\# observation in the peng dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 300 x 9
     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm
  <dbl> <chr>   <chr>               <dbl>       <dbl> <chr> <chr>          <dbl>
1     1 Adelie  Torger~               181        3750 male  2007            39.1
2     2 Adelie  Torger~               186        3800 fema~ 2007            39.5
3     3 <NA>    <NA>                   NA          NA <NA>  <NA>            40.3
4     4 Adelie  Torger~               193        3450 fema~ 2007            36.7
5     5 Adelie  Torger~               190        3650 male  2007            39.3
# i 295 more rows
# i 1 more variable: bill_depth_mm <dbl>
\end{verbatim}

Right joins are almost identical to left joins -- only x and y have
reversed roles.

\hypertarget{inner-join-with-inner_join}{%
\subsection{\texorpdfstring{Inner join with
\texttt{inner\_join}}{Inner join with inner\_join}}\label{inner-join-with-inner_join}}

Join the overlapping observations from x and y
(Figure~\ref{fig-rtidyverse-innerjoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/inner_join.png}

}

\caption{\label{fig-rtidyverse-innerjoin}Graphical representation of
inner join operation. Two tables with a shared first column (A B C, and
A B D respectively) are merged together to have columns A B C D. Only
rows from both table that have exact matches on columns A and B are
retained. The third rows from both tables that had a different value in
column B are lost.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{inner\_join}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ peng,  }\CommentTok{\# 300 observations}
  \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
  \AttributeTok{by =} \StringTok{"id"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 275 x 9
     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm
  <dbl> <chr>   <chr>               <dbl>       <dbl> <chr> <chr>          <dbl>
1     1 Adelie  Torger~               181        3750 male  2007            39.1
2     2 Adelie  Torger~               186        3800 fema~ 2007            39.5
3     4 Adelie  Torger~               193        3450 fema~ 2007            36.7
4     5 Adelie  Torger~               190        3650 male  2007            39.3
5     6 Adelie  Torger~               181        3625 fema~ 2007            38.9
# i 270 more rows
# i 1 more variable: bill_depth_mm <dbl>
\end{verbatim}

Inner joins are a fast and easy way to check, to which degree two
dataset overlap.

\hypertarget{full-join-with-full_join}{%
\subsection{\texorpdfstring{Full join with
\texttt{full\_join}}{Full join with full\_join}}\label{full-join-with-full_join}}

Join all observations from x and y, even if information is missing
(Figure~\ref{fig-rtidyverse-fulljoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/inner_join.png}

}

\caption{\label{fig-rtidyverse-fulljoin}Graphical representation of full
join operation. Two tables with a shared first column (A B C, and A B D
respectively) are merged together to have columns A B C D. All rows from
both tables are retained, even though they do not share the same value
on column B on both tables. The missing values for the two third rows
(i.e., column C from the second table, and column D from the first
table) are are filled with an empty cell indicated with \texttt{-}.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ peng, }\CommentTok{\# 300 observations}
    \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
    \AttributeTok{by =} \StringTok{"id"}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 325 x 9
     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm
  <dbl> <chr>   <chr>               <dbl>       <dbl> <chr> <chr>          <dbl>
1     1 Adelie  Torger~               181        3750 male  2007            39.1
2     2 Adelie  Torger~               186        3800 fema~ 2007            39.5
3     3 <NA>    <NA>                   NA          NA <NA>  <NA>            40.3
4     4 Adelie  Torger~               193        3450 fema~ 2007            36.7
5     5 Adelie  Torger~               190        3650 male  2007            39.3
# i 320 more rows
# i 1 more variable: bill_depth_mm <dbl>
\end{verbatim}

Full joins allow to preserve every bit of information.

\hypertarget{semi-join-with-semi_join}{%
\subsection{\texorpdfstring{Semi join with
\texttt{semi\_join}}{Semi join with semi\_join}}\label{semi-join-with-semi_join}}

Keep every observation in x that is in y
(Figure~\ref{fig-rtidyverse-semijoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/semi_join.png}

}

\caption{\label{fig-rtidyverse-semijoin}Graphical representation of semi
join operation. Two tables with a shared first column (A B C, and A B D
respectively) are merged together to have only columns A B C. Only
columns A B and C are retained in the joined table. Row three of both
tables are not included as the column values in columns A and B do not
match.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{semi\_join}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ peng,  }\CommentTok{\# 300 observations}
  \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
  \AttributeTok{by =} \StringTok{"id"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 275 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1     1 Adelie  Torgersen               181        3750 male   2007 
2     2 Adelie  Torgersen               186        3800 female 2007 
3     4 Adelie  Torgersen               193        3450 female 2007 
4     5 Adelie  Torgersen               190        3650 male   2007 
5     6 Adelie  Torgersen               181        3625 female 2007 
# i 270 more rows
\end{verbatim}

Semi joins are underused (!) operations to filter datasets.

\hypertarget{anti-join-with-anti_join}{%
\subsection{\texorpdfstring{Anti join with
\texttt{anti\_join}}{Anti join with anti\_join}}\label{anti-join-with-anti_join}}

Keep every observation in x that is not in y
(Figure~\ref{fig-rtidyverse-antijoin}).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.5625in]{assets/images/chapters/r-tidyverse/anti_join.png}

}

\caption{\label{fig-rtidyverse-antijoin}Graphical representation of anti
join operation. Two tables with a shared first column (A B C, and A B D
respectively) are merged together to have only columns A B C, and of
that only row 3 of the first table. Only row three is retained from the
first table as this is the only unique row present only in the first
table.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{anti\_join}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ peng,  }\CommentTok{\# 300 observations}
  \AttributeTok{y =}\NormalTok{ bills, }\CommentTok{\# 300 observations}
  \AttributeTok{by =} \StringTok{"id"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 7
     id species island    flipper_length_mm body_mass_g sex    year 
  <int> <chr>   <chr>                 <dbl>       <dbl> <chr>  <chr>
1    22 Adelie  Biscoe                  183        3550 male   2007 
2    34 Adelie  Dream                   181        3300 female 2007 
3    74 Adelie  Torgersen               195        4000 male   2008 
4    92 Adelie  Dream                   196        4350 male   2008 
5    99 Adelie  Biscoe                  193        2925 female 2009 
# i 20 more rows
\end{verbatim}

Anti joins allow to quickly determine what information is missing in a
dataset compared to an other one.

\hypertarget{exercise-3}{%
\subsection{Exercise}\label{exercise-3}}

Consider the following additional dataset with my opinions on cars with
a specific number of gears:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gear\_opinions }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{gear =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \AttributeTok{opinion =} \FunctionTok{c}\NormalTok{(}\StringTok{"boring"}\NormalTok{, }\StringTok{"wow"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add my opinions about gears to the \texttt{mtcars} dataset
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Remove all cars from the dataset for which I do not have an opinion
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Possible solutions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{left\_join}\NormalTok{(mtcars, gear\_opinions, }\AttributeTok{by =} \StringTok{"gear"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 32 x 12
    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb opinion
  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>  
1  21       6   160   110  3.9   2.62  16.5     0     1     4     4 <NA>   
2  21       6   160   110  3.9   2.88  17.0     0     1     4     4 <NA>   
3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1 <NA>   
4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1 boring 
5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2 boring 
# i 27 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{anti\_join}\NormalTok{(mtcars, gear\_opinions, }\AttributeTok{by =} \StringTok{"gear"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 11
    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1  21       6  160    110  3.9   2.62  16.5     0     1     4     4
2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1
4  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
5  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
# i 7 more rows
\end{verbatim}

\end{tcolorbox}

\hypertarget{references-1}{%
\section{References}\label{references-1}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\hypertarget{introduction-to-python-and-pandas}{%
\chapter{Introduction to Python and
Pandas}\label{introduction-to-python-and-pandas}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

This session is typically ran held in parallel to the Introduction to R
and Tidyverse. Participants of the summer schools chose which to attend
based on their prior experience. We recommend the
\protect\hyperlink{introduction-to-r-and-the-tidyverse}{introduction to
R session} if you have no experience with neither R nor Python.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/python-pandas.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate python{-}pandas}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-7}{%
\section{Lecture}\label{lecture-7}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/3b2-python-pandas/SPAAM\%20Summer\%20School\%202022\%20-\%203B2\%20-\%20Intro\%20to\%20Python\%20and\%20Pandas.pdf}{here}.

This session is run using a Jupyter notebook. This can be found
\href{https://github.com/maxibor/intro-to-pandas-plotnine}{here}.
However, it will already be installed on compute nodes during the summer
school.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

We highly recommend viewing this walkthrough via the Jupyter notebook
above! The output of commands on the website for this walkthrough are
displayed in their own code blocks - be wary of what you copy-paste!

\end{tcolorbox}

\hypertarget{introduction-to-data-manipulation-in-python-with-pandas-and-visualisation}{%
\section{Introduction to data manipulation in Python with Pandas and
visualisation}\label{introduction-to-data-manipulation-in-python-with-pandas-and-visualisation}}

Over the last few years, Python has gained popularity thanks to the
numerous libraries (packages with pre-written functions) in the field of
machine learning, statistical data analysis, and bioinformatics. While a
few years ago, it was often necessary to go to R for performing routine
data manipulation and analysis tasks, nowadays Python has a vast
ecosystem of libraries. Libraries exist for many different file formats
that you might encounter in metagenomics, such as fasta, fastq, sam,
bam, etc.

This tutorial/walkthrough will provide a short introduction to the most
popular libraries for data analysis
\href{https://pandas.pydata.org/}{pandas}. This library has functions
for reading and manipulating tabular data similar to the
\texttt{data.frame()} in R together with some basic data plotting codes.

The aim of this walkthrough is to first: get familiar with the Python
code syntax and use Jupiter Notebook for executing codes and secondly
get a kickstart to utilising the endless possibilities of data analysis
in Python that can be applied to your data.

\hypertarget{table-of-content}{%
\section{Table of content:}\label{table-of-content}}

\begin{itemize}
\tightlist
\item
  8.3 \protect\hyperlink{working-in-a-jupyter-environment}{Working in a
  jupyter environment}
\item
  8.4 \protect\hyperlink{pandas}{Pandas}
\item
  8.5 \protect\hyperlink{reading-data-with-pandas}{Reading data with
  Pandas}
\item
  8.6 \protect\hyperlink{data-exploration}{Data exploration}
\item
  8.7 \protect\hyperlink{describing-a-dataframe}{Describing a DataFrame}
\item
  8.8 \protect\hyperlink{dealing-with-missing-data}{Dealing with missing
  data}
\item
  8.9 \protect\hyperlink{combining-data}{Combining data}
\item
  8.10 \protect\hyperlink{data-visualization}{Data visualization}
\item
  8.11 \protect\hyperlink{plotnine}{Plotnine}
\item
  8.12 {[}Lecture from 2022{]}
\end{itemize}

\hypertarget{working-in-a-jupyter-environment}{%
\section{Working in a jupyter
environment}\label{working-in-a-jupyter-environment}}

This tutorial run-through is using a \href{https://jupyter.org}{Jupyter
Notebook} for writing \& executing Python code and for annotating.

Jupyter notebooks are convenient and have two types of cells:
\textbf{Markdown} and \textbf{Code}. The \textbf{markup cell} syntax is
very similar to \textbf{R markdown}. The markdown cells are used for
annotating, which is important for sharing code with collaborators,
reproducibility, and documentation.

A few examples are shown below. For a full list of possible syntax
\href{https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet}{click
here} for a Jupyter Notebook cheat-sheet.

list of \textbf{markdown cell} examples:

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

In many cases, there are multiple possible syntaxes which give the same
result. We present only one way in this run-through.

\end{tcolorbox}

Text

\begin{itemize}
\tightlist
\item
  \texttt{**bold**} : \textbf{bold}
\item
  \texttt{*italics*} : \emph{italics}
\end{itemize}

Code

\begin{itemize}
\tightlist
\item
  `inline code` : \texttt{inline\ code}
\end{itemize}

LaTeX maths

\begin{itemize}
\tightlist
\item
  \texttt{\$\ x\ =\ \textbackslash{}frac\{\textbackslash{}pi\}\{42\}\ \$}
  : \[ x = \frac{\pi}{42} \]
\end{itemize}

url links

\begin{itemize}
\tightlist
\item
  \texttt{{[}link{]}(https://www.python.org/)}\href{https://www.python.org/}{link}
\end{itemize}

Images

\begin{itemize}
\tightlist
\item
  \texttt{!{[}{]}(https://www.spaam-community.org/assets/media/SPAAM-Logo-Full-Colour\_ShortName.svg)}
  \includegraphics{index_files/mediabag/assets/images/chapters/python-pandas/SPAAM-Logo-Full-Colour.pdf}
\end{itemize}

\hfill\break
The \textbf{code cells} can interpret many different coding languages
including Python and Bash. The syntax of the code cells is the same as
the syntax of the coding languages, in our case python.

Below are some examples of Python \textbf{code cells} with some useful
basic python functions:

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Python function}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\texttt{print()} is a python function for printing lines in the terminal
\texttt{print()\ ==\ echo} in bash

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hello World from Python!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{out -} \texttt{Hello\ World\ from\ Python!}

But it can also, for example, run bash commands by adding a \emph{!} at
the start of the line.

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{ echo }\StringTok{"Hello World from bash!"}
\end{Highlighting}
\end{Shaded}

\emph{out -} \texttt{Hello\ World\ from\ bash!}

Stings or numbers can be stored as a variable by using the \emph{=} sign

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

Ones a variable is set in one \textbf{code cell} they are stored and can
be accessed in other downstream \textbf{code cells}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(i)}
\end{Highlighting}
\end{Shaded}

You can also print multiple things together in one \texttt{print}
statement such as a number and a string:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"The number is"}\NormalTok{, i, }\StringTok{"Wow!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{out -} \texttt{The\ number\ is\ 0\ Wow!}

\hypertarget{pandas}{%
\section{Pandas}\label{pandas}}

\hypertarget{getting-started}{%
\subsection{Getting started}\label{getting-started}}

Pandas is a Python library used for data manipulation and analysis.

We can import the library like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

We set ``pandas'' to the alias ``pd'' because we are lazy and don't want
to write the full word too many times.

\end{tcolorbox}

Now, we can print the current version:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.\_\_version\_\_}
\end{Highlighting}
\end{Shaded}

\emph{out -} \texttt{\textquotesingle{}2.0.1\textquotesingle{}}

\hypertarget{pandas-data-structures}{%
\subsection{Pandas data structures}\label{pandas-data-structures}}

The primary data structures in Pandas are \texttt{Series} and
\texttt{DataFrame}.

A \texttt{DataFrame} is a table with \textbf{columns} and \textbf{rows}.

Each \textbf{column} has a \emph{column name} and each \textbf{row} has
an \emph{index}.

\includegraphics{assets/images/chapters/python-pandas/01_table_dataframe.png}

A single row or column (1 dimensional data) is a \texttt{Series}.

\includegraphics{assets/images/chapters/python-pandas/01_table_series.png}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

For a more in detail pandas getting started tutorial
\href{https://pandas.pydata.org/docs/getting_started/index.html\#}{click
here}

\end{tcolorbox}

\hypertarget{reading-data-with-pandas}{%
\section{Reading data with Pandas}\label{reading-data-with-pandas}}

Pandas can read in \textbf{csv} (comma separated values) files, which
are tables in text format.

It's called \textbf{c}sv because each value is separated from the others
via a comma, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A,B}
\NormalTok{5,6}
\NormalTok{8,4}
\end{Highlighting}
\end{Shaded}

\emph{out -} \textbar{} \textbar{} A \textbar{} B \textbar{}
\textbar---\textbar---\textbar---\textbar{} \textbar{} 1 \textbar{} 2
\textbar{} 3 \textbar{} \textbar{} 2 \textbar{} 3 \textbar{} 4
\textbar{}

Another common tabular separator are \textbf{tsv}, where each value is
separated by a \textbf{tab} \texttt{\textbackslash{}t}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A\textbackslash{}tB}
\NormalTok{5\textbackslash{}t6}
\NormalTok{8\textbackslash{}t4}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Our dataset \texttt{"all\_data.tsv"} is tab separated, which Pandas can
handle using the \texttt{sep} argument.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Pandas function}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\texttt{pd.read\_csv()} is the pandas function to read in tabular
tables. The \texttt{sep=} can be specified argument, \texttt{sep=,} is
the default.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"all\_data.tsv"}\NormalTok{, sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0214}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0249}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0427}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0427}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0569}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0356}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0320}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0356}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0356}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0391}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0605}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0605}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0641}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0498}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0605}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0747}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0676}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0676}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0356}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0534}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0391}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 & 546
& 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 & 11 \\
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 & 11 \\
3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 & 20
& 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 & 3 & 11 \\
4 & 7446 & 1967 & Master & Together & 62513.0 & 0 & 1 & 520 & 42 & 98 &
0 & 42 & 14 & 6 & 4 & 10 & 6 & 0 & 3 & 11 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} \\
1749 & 9432 & 1977 & Graduation & Together & 666666.0 & 1 & 0 & 9 & 14 &
18 & 8 & 1 & 12 & 3 & 1 & 3 & 6 & 0 & 3 & 11 \\
1750 & 8372 & 1974 & Graduation & Married & 34421.0 & 1 & 0 & 3 & 3 & 7
& 6 & 2 & 9 & 1 & 0 & 2 & 7 & 0 & 3 & 11 \\
1751 & 10870 & 1967 & Graduation & Married & 61223.0 & 0 & 1 & 709 & 43
& 182 & 42 & 118 & 247 & 9 & 3 & 4 & 5 & 0 & 3 & 11 \\
1752 & 7270 & 1981 & Graduation & Divorced & 56981.0 & 0 & 0 & 908 & 48
& 217 & 32 & 12 & 24 & 2 & 3 & 13 & 6 & 0 & 3 & 11 \\
1753 & 8235 & 1956 & Master & Together & 69245.0 & 0 & 1 & 428 & 30 &
214 & 80 & 30 & 61 & 6 & 5 & 10 & 3 & 0 & 3 & 11 \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

When you are unsure what arguments a function can take, it is possible
to get a \emph{help documentation} using \texttt{help(pd.read\_csv)}

\end{tcolorbox}

\hypertarget{data-exploration}{%
\section{Data exploration}\label{data-exploration}}

The data is from a customer personality analysis of a company trying to
better understand how to modify their product catalogue. Here is the
\href{https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis}{link
to the original source} for more information.

\hypertarget{columns}{%
\subsection{Columns}\label{columns}}

The command below prints all the column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',
       'Teenhome', 'MntWines', 'MntFruits', 'MntMeatProducts',
       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',
       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',
       'NumWebVisitsMonth', 'Complain', 'Z_CostContact', 'Z_Revenue'],
      dtype='object')
\end{verbatim}

We can also list their respective data types.

\begin{itemize}
\tightlist
\item
  \texttt{int64} are integers
\item
  \texttt{float64} are floating point numbers, also called
  \texttt{double} in other languages
\item
  \texttt{object} are Python objects, which are strings in this case
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.dtypes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ID                       int64
Year_Birth               int64
Education               object
Marital_Status          object
Income                 float64
Kidhome                  int64
Teenhome                 int64
MntWines                 int64
MntFruits                int64
MntMeatProducts          int64
MntFishProducts          int64
MntSweetProducts         int64
MntGoldProds             int64
NumWebPurchases          int64
NumCatalogPurchases      int64
NumStorePurchases        int64
NumWebVisitsMonth        int64
Complain                 int64
Z_CostContact            int64
Z_Revenue                int64
dtype: object
\end{verbatim}

\hypertarget{inspecting-the-dataframe}{%
\subsection{Inspecting the DataFrame}\label{inspecting-the-dataframe}}

What is the size of our \texttt{DataFrame}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(1754, 20)
\end{verbatim}

It has \textbf{1754} rows and \textbf{20} columns.

Let's look at the first 5 rows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0109}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0217}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0580}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0652}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0507}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0761}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0543}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 & 546
& 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 & 11 \\
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 & 11 \\
3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 & 20
& 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 & 3 & 11 \\
4 & 7446 & 1967 & Master & Together & 62513.0 & 0 & 1 & 520 & 42 & 98 &
0 & 42 & 14 & 6 & 4 & 10 & 6 & 0 & 3 & 11 \\
\end{longtable}

What we can see it that, unlike \textbf{R}, \textbf{Python} and in
extension \textbf{Pandas} is 0-indexed instead of 1-indexed.

Question: Can you show how to do the same using bash?

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{ head all\_data.tsv}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0109}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0217}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0580}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0652}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0507}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0761}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0543}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 & 546
& 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 & 11 \\
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 & 11 \\
3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 & 20
& 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 & 3 & 11 \\
4 & 7446 & 1967 & Master & Together & 62513.0 & 0 & 1 & 520 & 42 & 98 &
0 & 42 & 14 & 6 & 4 & 10 & 6 & 0 & 3 & 11 \\
5 & 965 & 1971 & Graduation & Divorced & 55635.0 & 0 & 1 & 235 & 65 &
164 & 50 & 49 & 27 & 7 & 3 & 7 & 6 & 0 & 3 & 11 \\
6 & 1994 & 1983 & Graduation & Married & NaN & 1 & 0 & 5 & 5 & 6 & 0 & 2
& 1 & 1 & 0 & 2 & 7 & 0 & 3 & 11 \\
7 & 387 & 1976 & Basic & Married & 7500.0 & 0 & 0 & 6 & 16 & 11 & 11 & 1
& 16 & 2 & 0 & 3 & 8 & 0 & 3 & 11 \\
8 & 2125 & 1959 & Graduation & Divorced & 63033.0 & 0 & 0 & 194 & 61 &
480 & 225 & 112 & 30 & 3 & 4 & 8 & 2 & 0 & 3 & 11 \\
9 & 8180 & 1952 & Master & Divorced & 59354.0 & 1 & 1 & 233 & 2 & 53 & 3
& 5 & 14 & 6 & 1 & 5 & 6 & 0 & 3 & 11 \\
\end{longtable}

\hypertarget{accessing-rows-and-columns}{%
\subsection{Accessing rows and
columns}\label{accessing-rows-and-columns}}

We can access parts of the data in \texttt{DataFrames} in different
ways.

The first method is sub-setting the rows using the index.

This will take only the second row and all columns, producing a
\texttt{Series}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[}\DecValTok{1}\NormalTok{, :]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ID                           2174
Year_Birth                   1954
Education              Graduation
Marital_Status             Single
Income                    46344.0
Kidhome                         1
Teenhome                        1
MntWines                       11
MntFruits                       1
MntMeatProducts                 6
MntFishProducts                 2
MntSweetProducts                1
MntGoldProds                    6
NumWebPurchases                 1
NumCatalogPurchases             1
NumStorePurchases               2
NumWebVisitsMonth               5
Complain                        0
Z_CostContact                   3
Z_Revenue                      11
Name: 1, dtype: object
\end{verbatim}

And this will take the second and third row, producing another
\texttt{DataFrame}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.loc[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, :]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0109}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0217}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0435}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0580}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0652}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0507}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0761}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0362}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0543}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0399}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 & 11 \\
\end{longtable}

It's important to understand that almost all operations on
\texttt{DataFrames} are not in-place, meaning that we don't modify the
original object and would have to save the results to the same or a new
variable to keep the changes.

This, for example will create a new \texttt{DataFrame} of only the
``Education'' and ``Marital\_Status'' columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_df }\OperatorTok{=}\NormalTok{ df.loc[:, [}\StringTok{"Education"}\NormalTok{, }\StringTok{"Marital\_Status"}\NormalTok{]]}
\NormalTok{new\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Education & Marital\_Status \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Graduation & Single \\
1 & Graduation & Single \\
2 & Graduation & Together \\
3 & Graduation & Together \\
4 & Master & Together \\
\ldots{} & \ldots{} & \ldots{} \\
1749 & Graduation & Together \\
1750 & Graduation & Married \\
1751 & Graduation & Married \\
1752 & Graduation & Divorced \\
1753 & Master & Together \\
\end{longtable}

Selecting only one column by name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"Year\_Birth"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0       1957
1       1954
2       1965
3       1984
4       1967
        ... 
1749    1977
1750    1974
1751    1967
1752    1981
1753    1956
\end{verbatim}

We can also remove columns from the \texttt{DataFrame}.

In this case, we want to remove the columns \texttt{Z\_CostContact} and
\texttt{Z\_Revenue} and keep those changes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.drop(}\StringTok{"Z\_CostContact"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.drop(}\StringTok{"Z\_Revenue"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{conditional-subsetting}{%
\subsection{Conditional subsetting}\label{conditional-subsetting}}

We can more specifically look at subsets of the data we might be
interested in.

This subsetting is a bit weird in the syntax at first but hopefully
makes more sense when we go through it step by step.

We can, for example, test each string in the column \texttt{Education}
if it is equal to \texttt{PhD}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{education\_is\_grad }\OperatorTok{=}\NormalTok{ (df[}\StringTok{"Education"}\NormalTok{] }\OperatorTok{==} \StringTok{"Graduation"}\NormalTok{)}
\NormalTok{education\_is\_grad}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0        True
1        True
2        True
3        True
4       False
        ...  
1749     True
1750     True
1751     True
1752     True
1753    False
Name: Education, Length: 1754, dtype: bool
\end{verbatim}

We can also check for multiple conditions at once:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{two\_at\_once }\OperatorTok{=}\NormalTok{ (df[}\StringTok{"Education"}\NormalTok{] }\OperatorTok{==} \StringTok{"Graduation"}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{"Marital\_Status"}\NormalTok{] }\OperatorTok{==} \StringTok{"Single"}\NormalTok{)}
\NormalTok{two\_at\_once}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0        True
1        True
2       False
3       False
4       False
        ...  
1749    False
1750    False
1751    False
1752    False
1753    False
Length: 1754, dtype: bool
\end{verbatim}

This will create a \texttt{Series} of booleans, which can then be used
to subset the data to rows where the condition(s) are \textbf{True}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[two\_at\_once]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0214}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0571}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0357}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0357}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0393}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0607}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0607}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0643}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0607}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0679}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0679}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0357}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0536}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0393}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 & 546
& 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 & 11 \\
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
18 & 7892 & 1969 & Graduation & Single & 18589.0 & 0 & 0 & 6 & 4 & 25 &
15 & 12 & 13 & 2 & 1 & 3 & 7 & 0 & 3 & 11 \\
20 & 5255 & 1986 & Graduation & Single & NaN & 1 & 0 & 5 & 1 & 3 & 3 &
263 & 362 & 27 & 0 & 0 & 1 & 0 & 3 & 11 \\
33 & 1371 & 1976 & Graduation & Single & 79941.0 & 0 & 0 & 123 & 164 &
266 & 227 & 30 & 174 & 2 & 4 & 9 & 1 & 0 & 3 & 11 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} \\
1720 & 10968 & 1969 & Graduation & Single & 57731.0 & 0 & 1 & 266 & 21 &
300 & 65 & 8 & 44 & 8 & 8 & 6 & 6 & 0 & 3 & 11 \\
1723 & 5959 & 1968 & Graduation & Single & 35893.0 & 1 & 1 & 158 & 0 &
23 & 0 & 0 & 18 & 3 & 1 & 5 & 8 & 0 & 3 & 11 \\
1743 & 4201 & 1962 & Graduation & Single & 57967.0 & 0 & 1 & 229 & 7 &
137 & 4 & 0 & 91 & 4 & 2 & 8 & 5 & 0 & 3 & 11 \\
1746 & 7004 & 1984 & Graduation & Single & 11012.0 & 1 & 0 & 24 & 3 & 26
& 7 & 1 & 23 & 3 & 1 & 2 & 9 & 0 & 3 & 11 \\
1748 & 8080 & 1986 & Graduation & Single & 26816.0 & 0 & 0 & 5 & 1 & 6 &
3 & 4 & 3 & 0 & 0 & 3 & 4 & 0 & 3 & 11 \\
\end{longtable}

The syntax that seems more complicated and does it in one step without
the extra \texttt{Series} is this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[(df[}\StringTok{"Education"}\NormalTok{] }\OperatorTok{==} \StringTok{"Master"}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{"Marital\_Status"}\NormalTok{] }\OperatorTok{==} \StringTok{"Single"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0215}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0251}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0430}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0394}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0573}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0323}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0323}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0394}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0645}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0502}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0753}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0681}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0681}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0538}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
26 & 10738 & 1951 & Master & Single & 49389.0 & 1 & 1 & 40 & 0 & 19 & 2
& 1 & 3 & 2 & 0 & 3 & 7 & 0 & 3 & 11 \\
46 & 6853 & 1982 & Master & Single & 75777.0 & 0 & 0 & 712 & 26 & 538 &
69 & 13 & 80 & 3 & 6 & 11 & 1 & 0 & 3 & 11 \\
76 & 11178 & 1972 & Master & Single & 42394.0 & 1 & 0 & 15 & 2 & 10 & 0
& 1 & 4 & 1 & 0 & 3 & 7 & 0 & 3 & 11 \\
98 & 6205 & 1967 & Master & Single & 32557.0 & 1 & 0 & 34 & 3 & 29 & 0 &
4 & 10 & 2 & 1 & 3 & 5 & 0 & 3 & 11 \\
110 & 821 & 1992 & Master & Single & 92859.0 & 0 & 0 & 962 & 61 & 921 &
52 & 61 & 20 & 5 & 4 & 12 & 2 & 0 & 3 & 11 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} \\
1690 & 3520 & 1990 & Master & Single & 91172.0 & 0 & 0 & 162 & 28 & 818
& 0 & 28 & 56 & 4 & 3 & 7 & 3 & 0 & 3 & 11 \\
1709 & 4418 & 1983 & Master & Single & 89616.0 & 0 & 0 & 671 & 47 & 655
& 145 & 111 & 15 & 7 & 5 & 12 & 2 & 0 & 3 & 11 \\
1714 & 2980 & 1952 & Master & Single & 8820.0 & 1 & 1 & 12 & 0 & 13 & 4
& 2 & 4 & 3 & 0 & 3 & 8 & 0 & 3 & 11 \\
1738 & 7366 & 1982 & Master & Single & 75777.0 & 0 & 0 & 712 & 26 & 538
& 69 & 13 & 80 & 3 & 6 & 11 & 1 & 0 & 3 & 11 \\
1747 & 9817 & 1970 & Master & Single & 44802.0 & 0 & 0 & 853 & 10 & 143
& 13 & 10 & 20 & 9 & 4 & 12 & 8 & 0 & 3 & 11 \\
\end{longtable}

\hypertarget{describing-a-dataframe}{%
\section{Describing a DataFrame}\label{describing-a-dataframe}}

Pandas can easily create overview statistics for all numeric columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.describe()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0248}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0496}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0532}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0638}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0496}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0674}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0674}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0461}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0532}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0390}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
count & 1754.000000 & 1754.000000 & 1735.000000 & 1754.000000 &
1754.000000 & 1754.000000 & 1754.000000 & 1754.000000 & 1754.000000 &
1754.000000 & 1754.000000 & 1754.000000 & 1754.000000 & 1754.000000 &
1754.000000 & 1754.000000 & 1754.0 & 1754.0 \\
mean & 5584.696123 & 1969.571266 & 51166.578098 & 0.456100 & 0.480616 &
276.072406 & 28.034778 & 166.492018 & 40.517104 & 28.958381 & 47.266819
& 3.990878 & 2.576967 & 5.714937 & 5.332383 & 0.011403 & 3.0 & 11.0 \\
std & 3254.655979 & 11.876614 & 26200.419179 & 0.537854 & 0.536112 &
314.604735 & 41.348883 & 225.561694 & 57.412986 & 42.830660 & 53.885647
& 2.708278 & 2.848335 & 3.231465 & 2.380183 & 0.106202 & 0.0 & 0.0 \\
min & 0.000000 & 1893.000000 & 1730.000000 & 0.000000 & 0.000000 &
0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 &
0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 3.0 & 11.0 \\
25\% & 2802.500000 & 1960.000000 & 33574.500000 & 0.000000 & 0.000000 &
19.000000 & 2.000000 & 15.000000 & 3.000000 & 2.000000 & 10.000000 &
2.000000 & 0.000000 & 3.000000 & 3.000000 & 0.000000 & 3.0 & 11.0 \\
50\% & 5468.000000 & 1971.000000 & 49912.000000 & 0.000000 & 0.000000 &
160.500000 & 9.000000 & 66.000000 & 13.000000 & 9.000000 & 27.000000 &
3.000000 & 1.000000 & 5.000000 & 6.000000 & 0.000000 & 3.0 & 11.0 \\
75\% & 8441.250000 & 1978.000000 & 68130.000000 & 1.000000 & 1.000000 &
454.000000 & 35.000000 & 232.000000 & 53.500000 & 35.000000 & 63.000000
& 6.000000 & 4.000000 & 8.000000 & 7.000000 & 0.000000 & 3.0 & 11.0 \\
max & 11191.000000 & 1996.000000 & 666666.000000 & 2.000000 & 2.000000 &
1492.000000 & 199.000000 & 1725.000000 & 259.000000 & 263.000000 &
362.000000 & 27.000000 & 28.000000 & 13.000000 & 20.000000 & 1.000000 &
3.0 & 11.0 \\
\end{longtable}

You can also directly calculate the relevant statistics on columns you
are interested in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"MntWines"}\NormalTok{].}\BuiltInTok{max}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1492
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[[}\StringTok{"Kidhome"}\NormalTok{, }\StringTok{"Teenhome"}\NormalTok{]].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kidhome     0.456100
Teenhome    0.480616
dtype: float64
\end{verbatim}

For non-numeric columns, you can get the represented values or their
counts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"Education"}\NormalTok{].unique()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array(['Graduation', 'Master', 'Basic', '2n Cycle'], dtype=object)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"Marital\_Status"}\NormalTok{].value\_counts()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Marital_Status
Married     672
Together    463
Single      382
Divorced    180
Widow        53
Alone         2
Absurd        2
Name: count, dtype: int64
\end{verbatim}

\textbf{Task}

Subset the \texttt{DataFrame} in two different ways:

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Just like with the ``PhD'' string before, you can subset using integers
and \(<\), \(>\), \(<=\) and \(>=\).

\end{tcolorbox}

\begin{itemize}
\item
  One where everybody is born before 1970

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_before }\OperatorTok{=}\NormalTok{ df[df[}\StringTok{"Year\_Birth"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{1970}\NormalTok{]}
\end{Highlighting}
\end{Shaded}
\item
  One where everybody is born in or after 1970

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_before }\OperatorTok{=}\NormalTok{ df[df[}\StringTok{"Year\_Birth"}\NormalTok{] }\OperatorTok{\textgreater{}=} \DecValTok{1970}\NormalTok{]}
\end{Highlighting}
\end{Shaded}
\item
  How many people are in the two \texttt{DataFrames}?

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"n(before)   ="}\NormalTok{, df\_before.shape[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"n(after)   ="}\NormalTok{, df\_before.shape[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n(before)   = 804
n(after)   = 950
\end{verbatim}
\item
  Do the total number of people sum up to the original
  \texttt{DataFrame} total?

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ df\_before.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ df\_after.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{==}\NormalTok{ df.shape[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

  True

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"n(sum)      ="}\NormalTok{, df\_before.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ df\_after.shape[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"n(expected) ="}\NormalTok{, df.shape[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n(sum)      = 1754
n(expected) = 1754
\end{verbatim}
\end{itemize}

\begin{itemize}
\tightlist
\item
  How does the mean income of the two groups differ?

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"income(before) ="}\NormalTok{, df\_before[}\StringTok{"Income"}\NormalTok{].mean())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"income(after)  ="}\NormalTok{, df\_after[}\StringTok{"Income"}\NormalTok{].mean())}
\end{Highlighting}
\end{Shaded}

  income(before) = 55513.38113207547 income(after) = 47490.29255319149
\end{itemize}

\textbf{Extra task} - Can you find something else that differs a lot
between the two groups?

\hypertarget{dealing-with-missing-data}{%
\section{Dealing with missing data}\label{dealing-with-missing-data}}

We can check for missing data for each cell like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.isna()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0252}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0432}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0396}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0576}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0288}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0324}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0360}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0360}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0396}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0612}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0612}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0647}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0504}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0612}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0755}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0683}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0683}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0360}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0540}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0396}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_CostContact
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Z\_Revenue
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
1 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
2 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
3 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
4 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} \\
1749 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
1750 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
1751 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
1752 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
1753 & False & False & False & False & False & False & False & False &
False & False & False & False & False & False & False & False & False &
False & False & False \\
\end{longtable}

By summing over each row, we see how many missing values are in each
column.

\texttt{True} is treated as \texttt{1} and \texttt{False} as \texttt{0}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.isna().}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ID                      0
Year_Birth              0
Education               0
Marital_Status          0
Income                 19
Kidhome                 0
Teenhome                0
MntWines                0
MntFruits               0
MntMeatProducts         0
MntFishProducts         0
MntSweetProducts        0
MntGoldProds            0
NumWebPurchases         0
NumCatalogPurchases     0
NumStorePurchases       0
NumWebVisitsMonth       0
Complain                0
dtype: int64
\end{verbatim}

We don't really know what a missing value means so we are just going to
keep them in the data.

However, we could remove them using \texttt{df.dropna()}

\hypertarget{grouping-data}{%
\subsection{Grouping data}\label{grouping-data}}

We can group a \texttt{DataFrame} using a categorical column (for
example \texttt{Education} or \texttt{Marital\_Status}).

This allows us to do perform operations on each group individually.

For example, we could group by \texttt{Education} and calculate the mean
\texttt{Income}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.groupby(by}\OperatorTok{=}\StringTok{"Education"}\NormalTok{)[}\StringTok{"Income"}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Education
2n Cycle      47633.190000
Basic         20306.259259
Graduation    52720.373656
Master        52917.534247
Name: Income, dtype: float6
\end{verbatim}

\hypertarget{combining-data}{%
\section{Combining data}\label{combining-data}}

\hypertarget{concatenation}{%
\subsection{Concatenation}\label{concatenation}}

One way to combine multiple datasets is through \textbf{concatenation},
which either combines all columns or rows of multiple
\texttt{DataFrames}.

The command to combine two \texttt{DataFrames} by appending all rows is
\texttt{pd.concat({[}first\_dataframe,\ second\_dataframe{]})}

\textbf{Task}

\begin{itemize}
\tightlist
\item
  Read the \textbf{tsv} ``phd\_data.tsv'' as a new \texttt{DataFrame}
  and name the variable \texttt{df2}

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"phd\_data.tsv"}\NormalTok{, sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  Concatenate the ``old'' \texttt{DataFrame} \texttt{df} and the new
  \texttt{df2} and name the concatenated one \texttt{concat\_df}

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{concat\_df }\OperatorTok{=}\NormalTok{ pd.concat([df, df2])}
\NormalTok{concat\_df}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0179}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0251}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0430}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0430}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0573}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0323}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0323}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0394}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0645}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0502}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0609}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0753}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0681}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0681}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0358}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0538}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 40\tabcolsep) * \real{0.0394}}@{}}
  \toprule\noalign{}
  \begin{minipage}[b]{\linewidth}\raggedright
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  ID
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Year\_Birth
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Education
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Marital\_Status
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Income
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Kidhome
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Teenhome
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntWines
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntFruits
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntMeatProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntFishProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntSweetProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntGoldProds
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumWebPurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumCatalogPurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumStorePurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumWebVisitsMonth
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Complain
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Z\_CostContact
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Z\_Revenue
  \end{minipage} \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 &
  546 & 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 & 11 \\
  1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 &
  2 & 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 & 11 \\
  2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
  127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 & 11 \\
  3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 &
  20 & 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 & 3 & 11 \\
  4 & 7446 & 1967 & Master & Together & 62513.0 & 0 & 1 & 520 & 42 & 98
  & 0 & 42 & 14 & 6 & 4 & 10 & 6 & 0 & 3 & 11 \\
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} & \ldots{} & \ldots{} \\
  481 & 11133 & 1973 & PhD & YOLO & 48432.0 & 0 & 1 & 322 & 3 & 50 & 4 &
  3 & 42 & 7 & 1 & 6 & 8 & 0 & 3 & 11 \\
  482 & 9589 & 1948 & PhD & Widow & 82032.0 & 0 & 0 & 332 & 194 & 377 &
  149 & 125 & 57 & 4 & 6 & 7 & 1 & 0 & 3 & 11 \\
  483 & 4286 & 1970 & PhD & Single & 57642.0 & 0 & 1 & 580 & 6 & 58 & 8
  & 0 & 27 & 7 & 6 & 6 & 4 & 0 & 3 & 11 \\
  484 & 4001 & 1946 & PhD & Together & 64014.0 & 2 & 1 & 406 & 0 & 30 &
  0 & 0 & 8 & 8 & 2 & 5 & 7 & 0 & 3 & 11 \\
  485 & 9405 & 1954 & PhD & Married & 52869.0 & 1 & 1 & 84 & 3 & 61 & 2
  & 1 & 21 & 3 & 1 & 4 & 7 & 0 & 3 & 11 \\
  \end{longtable}
\end{itemize}

\begin{itemize}
\item
  Is there anything weird about the new \texttt{DataFrame} and can you
  fix that?

  Solution

  We previously removed the columns ``Z\_CostContact'' and
  ``Z\_Revenue'' but they are in the new data again.

  We can remove them like before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{concat\_df }\OperatorTok{=}\NormalTok{ concat\_df.drop(}\StringTok{"Z\_CostContact"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{concat\_df }\OperatorTok{=}\NormalTok{ concat\_df.drop(}\StringTok{"Z\_Revenue"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{concat\_df}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0198}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0277}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0474}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0474}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0632}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0356}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0356}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0395}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0395}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0435}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0672}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0672}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0711}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0553}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0672}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0830}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0751}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0751}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0395}}@{}}
  \toprule\noalign{}
  \begin{minipage}[b]{\linewidth}\raggedright
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  ID
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Year\_Birth
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Education
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Marital\_Status
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Income
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Kidhome
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Teenhome
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntWines
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntFruits
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntMeatProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntFishProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntSweetProducts
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  MntGoldProds
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumWebPurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumCatalogPurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumStorePurchases
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  NumWebVisitsMonth
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Complain
  \end{minipage} \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 &
  546 & 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 \\
  1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 &
  2 & 1 & 6 & 1 & 1 & 2 & 5 & 0 \\
  2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
  127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 \\
  3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 &
  20 & 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 \\
  4 & 7446 & 1967 & Master & Together & 62513.0 & 0 & 1 & 520 & 42 & 98
  & 0 & 42 & 14 & 6 & 4 & 10 & 6 & 0 \\
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
  \ldots{} \\
  481 & 11133 & 1973 & PhD & YOLO & 48432.0 & 0 & 1 & 322 & 3 & 50 & 4 &
  3 & 42 & 7 & 1 & 6 & 8 & 0 \\
  482 & 9589 & 1948 & PhD & Widow & 82032.0 & 0 & 0 & 332 & 194 & 377 &
  149 & 125 & 57 & 4 & 6 & 7 & 1 & 0 \\
  483 & 4286 & 1970 & PhD & Single & 57642.0 & 0 & 1 & 580 & 6 & 58 & 8
  & 0 & 27 & 7 & 6 & 6 & 4 & 0 \\
  484 & 4001 & 1946 & PhD & Together & 64014.0 & 2 & 1 & 406 & 0 & 30 &
  0 & 0 & 8 & 8 & 2 & 5 & 7 & 0 \\
  485 & 9405 & 1954 & PhD & Married & 52869.0 & 1 & 1 & 84 & 3 & 61 & 2
  & 1 & 21 & 3 & 1 & 4 & 7 & 0 \\
  \end{longtable}
\item
  Is there something interesting about the marital status of some people
  that have a PhD?

  Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{concat\_df[concat\_df[}\StringTok{"Education"}\NormalTok{]}\OperatorTok{==}\StringTok{"PhD"}\NormalTok{][}\StringTok{"Marital\_Status"}\NormalTok{].value\_counts()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Marital_Status
Married     192
Together    117
Single       98
Divorced     52
Widow        24
YOLO          2
Alone         1
Name: count, dtype: int64
\end{verbatim}

  There's two people that have ``YOLO'' as their Marital Status \ldots{}
\end{itemize}

\hypertarget{merging}{%
\subsection{Merging}\label{merging}}

Analysing numbers can be easier than analysing categorial values, like
``PhD'' and ``Master''.

To make our like easier, we might want to have a new column when the
Education level is replaced with a number that ``ranks'' the Education
levels by how long it takes.

This information could be stored in a Python \texttt{Dictionary} (Also
called Hash Map in other languages), which stores \textbf{key} and
\textbf{value} pairs.

We could store the Education information like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{education\_dictionary }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"Basic"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
    \StringTok{"2n Cycle"}\NormalTok{: }\DecValTok{2}\NormalTok{,}
    \StringTok{"Graduation"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
    \StringTok{"Master"}\NormalTok{: }\DecValTok{4}\NormalTok{,}
    \StringTok{"PhD"}\NormalTok{: }\DecValTok{5}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can now convert this \texttt{Dictionary} to a \texttt{DataFrame}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{education\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame.from\_dict(education\_dictionary, orient}\OperatorTok{=}\StringTok{"index"}\NormalTok{)}
\NormalTok{education\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
& 0 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Basic & 1 \\
2n Cycle & 2 \\
Graduation & 3 \\
Master & 4 \\
PhD & 5 \\
\end{longtable}

The resulting \texttt{DataFrame} has the Education level as index and
the column 0 has the level information.

We can rename the column to ``Level''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{education\_df }\OperatorTok{=}\NormalTok{ education\_df.rename(columns}\OperatorTok{=}\NormalTok{\{}\DecValTok{0}\NormalTok{: }\StringTok{"Level"}\NormalTok{\})}
\NormalTok{education\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
& Level \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Basic & 1 \\
2n Cycle & 2 \\
Graduation & 3 \\
Master & 4 \\
PhD & 5 \\
\end{longtable}

We can now \textbf{merge} this new \texttt{education\_df} with our
previous \texttt{concat\_df}.

The left \texttt{DataFrame} is \texttt{concat\_df} and we merge on
``Education'' because that's where the Eduction information is.

The right one is \texttt{education\_df} and the information is in the
index.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_df }\OperatorTok{=}\NormalTok{ pd.merge(left}\OperatorTok{=}\NormalTok{concat\_df, right}\OperatorTok{=}\NormalTok{education\_df, left\_on}\OperatorTok{=}\StringTok{"Education"}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{merged\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0192}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0269}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0462}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0462}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0615}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0346}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0346}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0385}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0385}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0423}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0654}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0654}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0538}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0654}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0808}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0731}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0731}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0385}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0269}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year\_Birth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marital\_Status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Income
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kidhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Teenhome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntWines
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFruits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntMeatProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntFishProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntSweetProducts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MntGoldProds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumCatalogPurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumStorePurchases
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NumWebVisitsMonth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Complain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Level
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 5524 & 1957 & Graduation & Single & 58138.0 & 0 & 0 & 635 & 88 & 546
& 172 & 88 & 88 & 8 & 10 & 4 & 7 & 0 & 3 \\
1 & 2174 & 1954 & Graduation & Single & 46344.0 & 1 & 1 & 11 & 1 & 6 & 2
& 1 & 6 & 1 & 1 & 2 & 5 & 0 & 3 \\
2 & 4141 & 1965 & Graduation & Together & 71613.0 & 0 & 0 & 426 & 49 &
127 & 111 & 21 & 42 & 8 & 2 & 10 & 4 & 0 & 3 \\
3 & 6182 & 1984 & Graduation & Together & 26646.0 & 1 & 0 & 11 & 4 & 20
& 10 & 3 & 5 & 2 & 0 & 4 & 6 & 0 & 3 \\
5 & 965 & 1971 & Graduation & Divorced & 55635.0 & 0 & 1 & 235 & 65 &
164 & 50 & 49 & 27 & 7 & 3 & 7 & 6 & 0 & 3 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} \\
481 & 11133 & 1973 & PhD & YOLO & 48432.0 & 0 & 1 & 322 & 3 & 50 & 4 & 3
& 42 & 7 & 1 & 6 & 8 & 0 & 5 \\
482 & 9589 & 1948 & PhD & Widow & 82032.0 & 0 & 0 & 332 & 194 & 377 &
149 & 125 & 57 & 4 & 6 & 7 & 1 & 0 & 5 \\
483 & 4286 & 1970 & PhD & Single & 57642.0 & 0 & 1 & 580 & 6 & 58 & 8 &
0 & 27 & 7 & 6 & 6 & 4 & 0 & 5 \\
484 & 4001 & 1946 & PhD & Together & 64014.0 & 2 & 1 & 406 & 0 & 30 & 0
& 0 & 8 & 8 & 2 & 5 & 7 & 0 & 5 \\
485 & 9405 & 1954 & PhD & Married & 52869.0 & 1 & 1 & 84 & 3 & 61 & 2 &
1 & 21 & 3 & 1 & 4 & 7 & 0 & 5 \\
\end{longtable}

\hypertarget{data-visualization}{%
\section{Data visualization}\label{data-visualization}}

We can easily create simple graphs using \texttt{DataFrame.plot()}.

This uses the package \textbf{matplotlib} in the background, which is a
very powerful and popular plotting library but is not the most user
friendly.

Using this \textbf{Pandas} method is very easy and can be a good way to
do some initial exploratory plots and later refine them using either
pure \textbf{matplobib} or another library.

\hypertarget{histogram}{%
\subsection{Histogram}\label{histogram}}

We can plot the data from a \texttt{DataFrame} like this:

\texttt{kind} specifies the kind of plot (for example \emph{hist} for
histogram, \emph{bar} for bar graph or \emph{scatter} for scatter plot).

We usually specify the columns from which the \texttt{x} and \texttt{y}
components should be taken, but for a histogram we only need to specify
one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ax }\OperatorTok{=}\NormalTok{ merged\_df.plot(kind}\OperatorTok{=}\StringTok{"hist"}\NormalTok{, y}\OperatorTok{=}\StringTok{"Income"}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Income"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Histogram of income"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Text(0.5, 1.0, `Histogram of income')

\begin{figure}

{\centering \includegraphics{index_files/mediabag/histogram.png}

}

\caption{png}

\end{figure}

\textbf{Task}

This doesn't look very good because the x-axis extends so much!

\begin{itemize}
\tightlist
\item
  Looking at the data, can you figure out what might cause this?

  Solution
\end{itemize}

When we look at the highest earners, we see that somebody put
\emph{666666} as their income.

We can assume that this was put as a joke or is an outlier.

In either way, we can redo the plot with that datapoint removed.

\begin{itemize}
\tightlist
\item
  Can you ``fix'' the plot?

  Solution
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ax }\OperatorTok{=}\NormalTok{ merged\_df[merged\_df[}\StringTok{"Income"}\NormalTok{] }\OperatorTok{!=} \DecValTok{666666}\NormalTok{].plot(kind}\OperatorTok{=}\StringTok{"hist"}\NormalTok{,y}\OperatorTok{=}\StringTok{"Income"}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Income"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Fixed Histogram of income"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Text(0.5, 1.0, `Fixed Histogram of income')

\begin{figure}

{\centering \includegraphics{index_files/mediabag/fixed_histogram.png}

}

\caption{png}

\end{figure}

\hypertarget{bar-plot}{%
\subsection{Bar plot}\label{bar-plot}}

Another visualization we could do is a bar plot.

Using the \texttt{groupby} and \texttt{mean} methods, we can calculate
the mean Income like we've learned before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_by\_education }\OperatorTok{=}\NormalTok{ merged\_df.groupby(by}\OperatorTok{=}\StringTok{"Education"}\NormalTok{)[}\StringTok{"Income"}\NormalTok{].mean()}
\NormalTok{grouped\_by\_education}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Education
2n Cycle      47633.190000
Basic         20306.259259
Graduation    52720.373656
Master        52917.534247
PhD           56145.313929
Name: Income, dtype: float64
\end{verbatim}

Now, this data can be shown:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ax }\OperatorTok{=}\NormalTok{ grouped\_by\_education.plot(kind}\OperatorTok{=}\StringTok{"bar"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Mean income"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Mean income for each education level"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Text(0.5, 1.0, `Mean income for each education level')

\begin{figure}

{\centering \includegraphics{index_files/mediabag/bar_plot.png}

}

\caption{png}

\end{figure}

\hypertarget{scatter-plot}{%
\subsection{Scatter plot}\label{scatter-plot}}

Another kind of plot is the scatter plot, which needs two columns for
the \textbf{x} and \textbf{y} axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ax }\OperatorTok{=}\NormalTok{ df.plot(kind}\OperatorTok{=}\StringTok{"scatter"}\NormalTok{, x}\OperatorTok{=}\StringTok{"MntWines"}\NormalTok{, y}\OperatorTok{=}\StringTok{"MntFruits"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Wine purchases and Fruit purchases"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Text(0.5, 1.0, `Wine purchases and Fruit purchases')

\begin{figure}

{\centering \includegraphics{index_files/mediabag/scatter_polt_01.png}

}

\caption{png}

\end{figure}

You can also specify whether the axes should be on the log scale or not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ax }\OperatorTok{=}\NormalTok{ df.plot(kind}\OperatorTok{=}\StringTok{"scatter"}\NormalTok{, x}\OperatorTok{=}\StringTok{"MntWines"}\NormalTok{, y}\OperatorTok{=}\StringTok{"MntFruits"}\NormalTok{, logy}\OperatorTok{=}\VariableTok{True}\NormalTok{, logx}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Wine purchases and Fruit purchases, on log scale"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Text(0.5, 1.0, `Wine purchases and Fruit purchases, on log scale')

\begin{figure}

{\centering \includegraphics{index_files/mediabag/scatter_polt_02.png}

}

\caption{png}

\end{figure}

\hypertarget{plotnine}{%
\section{Plotnine}\label{plotnine}}

Plotnine is the Python clone of ggplot2, which is very powerful and is
great if you are already familiar with the ggplot2 syntax!

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ plotnine }\ImportTok{import} \OperatorTok{*}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(ggplot(merged\_df, aes(}\StringTok{"Education"}\NormalTok{, }\StringTok{"MntWines"}\NormalTok{, fill}\OperatorTok{=}\StringTok{"Education"}\NormalTok{))}
 \OperatorTok{+}\NormalTok{ geom\_boxplot(alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{index_files/mediabag/boxplot.png}

}

\caption{png}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(ggplot(merged\_df[(merged\_df[}\StringTok{"Year\_Birth"}\NormalTok{]}\OperatorTok{\textgreater{}}\DecValTok{1900}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (merged\_df[}\StringTok{"Income"}\NormalTok{]}\OperatorTok{!=}\DecValTok{666666}\NormalTok{)],}
\NormalTok{        aes(}\StringTok{"Year\_Birth"}\NormalTok{, }\StringTok{"Income"}\NormalTok{, fill}\OperatorTok{=}\StringTok{"Education"}\NormalTok{))}
 \OperatorTok{+}\NormalTok{ geom\_point(alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, stroke}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
 \OperatorTok{+}\NormalTok{ facet\_wrap(}\StringTok{"Marital\_Status"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{index_files/mediabag/facet_scatter.png}

}

\caption{png}

\end{figure}

\textbf{task}

Now that you are familiar with python, pandas, and plotting. There are
two data.tables from \textbf{AncientMetagenomeDir} which contains
metadata from metagenomes. You should, by using the code in the tutorial
be able to explore the datasets and make some fancy plots.

\begin{verbatim}
file names:
sample_table_url
library_table_url
\end{verbatim}

\hypertarget{introduction-to-github}{%
\chapter{Introduction to Git(Hub)}\label{introduction-to-github}}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

In this walkthrough, we will introduce the version control system
\textbf{Git} as well as \textbf{Github}, a remote hosting service for
version controlled repositories. Git and Github are increasingly popular
tools for tracking data, collaborating on research projects, and sharing
data and code, and learning to use them will help in many aspects of
your own research. For more information on the benefits of using version
control systems, see the slides.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/git-github.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate git{-}github}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-8}{%
\section{Lecture}\label{lecture-8}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/raw/main/docs/assets/slides/2022/2b-intro-to-github/SPAAM\%20Summer\%20School\%202022\%20-\%202B\%20-\%20Introduction\%20to\%20Git(Hub).pdf}{here}.

\hypertarget{ssh-setup}{%
\section{SSH setup}\label{ssh-setup}}

To begin, you will set up an SSH key to facilitate easier authentication
when transferring data between local and remote repositories. In other
words, follow this section of the tutorial so that you never have to
type in your github password again! Begin by activating the conda
environment for this section (see \textbf{Introduction} above).

Next, generate your own ssh key, replacing the email below with your own
address.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ssh{-}keygen} \AttributeTok{{-}t}\NormalTok{ ed25519 }\AttributeTok{{-}C} \StringTok{"your\_email@example.com"}
\end{Highlighting}
\end{Shaded}

I recommend saving the file to the default location and skipping
passphrase setup. To do this, simply press enter without typing
anything.

You should now (hopefully!) have generated an ssh key. To check that it
worked, run the following commands to list the files containing your
public and private keys and check that the ssh program is running.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/.ssh/}
\FunctionTok{ls}\NormalTok{ id}\PreprocessorTok{*}
\BuiltInTok{eval} \StringTok{"}\VariableTok{$(}\FunctionTok{ssh{-}agent} \AttributeTok{{-}s}\VariableTok{)}\StringTok{"}
\end{Highlighting}
\end{Shaded}

Now you need to give ssh your key to record:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ssh{-}add}\NormalTok{ \textasciitilde{}/.ssh/id\_ed25519}
\end{Highlighting}
\end{Shaded}

Next, open your webbrowser and navigate to your github account. Go to
settings -\textgreater{} SSH \& GPG Keys -\textgreater{} New SSH Key.

Here, give you key a title, and then paste the \emph{public} key into
the main text box that you just generated on your local machine - i.e.,
the \emph{whole} string (starting \texttt{ssh-ed} and ending in your
email address) when you run the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ \textasciitilde{}/.ssh/id\_ed25519.pub}
\end{Highlighting}
\end{Shaded}

Finally, press Add SSH key. To check that it worked, run the following
command on your local machine. You should see a message telling you that
you've successfully authenticated.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ssh} \AttributeTok{{-}T}\NormalTok{ git@github.com}
\end{Highlighting}
\end{Shaded}

For more information about setting up the SSH key, including
instructions for different operating systems, check out github's
documentation:
\url{https://docs.github.com/es/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent}.

\hypertarget{the-only-6-commands-you-really-need-to-know}{%
\section{The only 6 commands you really need to
know}\label{the-only-6-commands-you-really-need-to-know}}

Now that you have set up your own SSH key, we can begin working on some
version controlled data! Navigate to your github homepage and create a
new repository. You can choose any name for your new repo (including the
default). Add a README file, then select Create Repository.

\includegraphics{assets/images/chapters/git-github/create_repo.png}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

For the remainder of the session, replace the name of my repository
(vigilant-octo-journey) with your own repo name.

\end{tcolorbox}

Change into the directory where you would like to work, and let's get
started! First, we will learn to \textbf{clone} a remote repository onto
your local machine. Navigate to your new repo, select the \emph{Code}
dropdown menu, select SSH, and copy the address as shown below.

\includegraphics{assets/images/chapters/git-github/git_clone.png}

Back at your command line, clone the repo as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ clone git@github.com:meganemichel/vigilant{-}octo{-}journey.git}
\end{Highlighting}
\end{Shaded}

Next, let's \textbf{add} a new or modified file to our `staging area' on
our local machine.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ vigilant{-}octo{-}journey}
\BuiltInTok{echo} \StringTok{"test\_file"} \OperatorTok{\textgreater{}}\NormalTok{ file\_A.txt}
\BuiltInTok{echo} \StringTok{"Just an example repo"} \OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ README.md}
\FunctionTok{git}\NormalTok{ add file\_A.txt}
\end{Highlighting}
\end{Shaded}

Now we can check what files have been locally changed, staged, etc. with
\textbf{status}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ status}
\end{Highlighting}
\end{Shaded}

You should see that \texttt{file\_A.txt} is staged to be committed, but
\texttt{README.md} is NOT. Try adding \texttt{README.md} and check the
status again.

Now we need to package or save the changes into a \textbf{commit} with a
message describing the changes we've made. Each commit comes with a
unique hash ID and will be stored forever in git history.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Add example file"}
\end{Highlighting}
\end{Shaded}

Finally, let's \textbf{push} our local commit back to our remote
repository.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ push}
\end{Highlighting}
\end{Shaded}

What if we want to download new commits from our remote to our local
repository?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ pull}
\end{Highlighting}
\end{Shaded}

You should see that your repository is already up-to-date, since we have
not made new changes to the remote repo. Let's try making a change to
the remote repository's README file (as below). Then, back on the
command line, pull the repository again.

\includegraphics{assets/images/chapters/git-github/git_pull.png}

\hypertarget{working-collaboratively}{%
\section{Working collaboratively}\label{working-collaboratively}}

Github facilitates simultaneous work by small teams through branching,
which generates a copy of the main repository within the repository.
This can be edited without breaking the `master' version. First, back on
github, make a new branch of your repository.

\includegraphics{assets/images/chapters/git-github/git_switch.png}

From the command line, you can create a new branch as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ switch }\AttributeTok{{-}c}\NormalTok{ new\_branch}
\end{Highlighting}
\end{Shaded}

To switch back to the main branch, use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ switch main}
\end{Highlighting}
\end{Shaded}

Note that you \textbf{must commit changes} for them to be saved to the
desired branch!

\hypertarget{pull-requests}{%
\section{Pull requests}\label{pull-requests}}

A \textbf{Pull request} (aka PR) is used to propose changes to a branch
from another branch. Others can comment and make suggestinos before your
changes are merged into the main branch. For more information on
creating a pull request, see github's documentation:
\url{https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request}.

\hypertarget{questions-to-think-about-4}{%
\section{Questions to think about}\label{questions-to-think-about-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why is using a version control software for tracking data and code
  important?
\item
  How can using Git(Hub) help me to collaborate on group projects?
\end{enumerate}

\part{Ancient Metagenomics}

The techniques in this section of the book can be used in a variety of
stages of any ancient metagenomics projects, for screening for pathogens
(what species should I target for downstream genomic mapping?), for
differential abundance analysis (does the community make of this sample
change between different cultural periods?), but also for reference-free
assembly of genomes (can I recover the genome architecture of a variety
of species in my sample?). It focuses on the concept of `many samples to
many genomes' using high-throughput techniques and algorithms and trying
to analyse data at whole `community' levels.

\hypertarget{taxonomic-profiling}{%
\section*{\texorpdfstring{\protect\hyperlink{taxonomic-profiling-otu-tables-and-visualisation}{Taxonomic
Profiling}}{Taxonomic Profiling}}\label{taxonomic-profiling}}
\addcontentsline{toc}{section}{\protect\hyperlink{taxonomic-profiling-otu-tables-and-visualisation}{Taxonomic
Profiling}}

\markright{Taxonomic Profiling}

\emph{TBC}

\hypertarget{functional-profiling}{%
\section*{\texorpdfstring{\protect\hyperlink{functional-profiling-1}{Functional
Profiling}}{Functional Profiling}}\label{functional-profiling}}
\addcontentsline{toc}{section}{\protect\hyperlink{functional-profiling-1}{Functional
Profiling}}

\markright{Functional Profiling}

The value of microbial taxonomy lies in the implied biochemical
properties of a given taxon. Historically taxonomy was determined by
growth characteristics and cell properties, and more recently through
genomic and genetic similarity.

The genomic content of microbial taxa, specifically the presence or
absence of genes, determine how those taxa interact with their
environment, including all the biochemical processes they participate
in, both internally and externally. Strains within any microbial species
may have different genetic content and therefore may behave strikingly
differently in the same environment, which cannot be determined through
taxonomic profiling. Functionally profiling a microbial community, or
determining all of the genes present independent of the species they are
derived from, reveals the biochemical reactions and metabolic products
the community may perform and produce, respectively.

This approach may provide insights to community activity and
environmental interactions that are hidden when using taxonomic
approaches alone. In this chapter we will perform functional profiling
of metagenomic communities to assess their genetic content and inferred
metabolic pathways.

\hypertarget{de-novo-assembly}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-de-novo-genome-assembly}{\emph{De
novo} Assembly}}{De novo Assembly}}\label{de-novo-assembly}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-de-novo-genome-assembly}{\emph{De
novo} Assembly}}

\markright{\emph{De novo} Assembly}

\emph{De novo} assembly of ancient metagenomic samples enables the
recovery of the genetic information of organisms without requiring any
prior knowledge about their genomes. Therefore, this approach is very
well suited to study the biological diversity of species that have not
been studied well or are simply not known yet.

In this chapter, we will show you how to prepare your sequencing data
and subsequently \emph{de novo} assemble them. Furthermore, we will then
learn how we can actually evaluate what organisms we might have
assembled and whether we obtained enough data to reconstruct a whole
metagenome-assembled genome. We will particularly focus on the quality
assessment of these reconstructed genomes and how we can ensure that we
obtained high-quality genomes.

\hypertarget{taxonomic-profiling-otu-tables-and-visualisation}{%
\chapter{Taxonomic Profiling, OTU Tables and
Visualisation}\label{taxonomic-profiling-otu-tables-and-visualisation}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/taxonomic-profiling.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate taxonomic{-}profiling}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{introduction-6}{%
\section{Introduction}\label{introduction-6}}

In this chapter, we're going to look at taxonomic profiling, or in other
words, how to get the microbial composition of a sample from the DNA
sequencing data.

Though there are many algorithms, and even more different tools
available to perform taxonomic profiling, the general idea remains the
same (Figure~\ref{fig-overview}).

After cleaning up the sequencing data, generally saved as FASTQ files, a
\textbf{taxonomic profiler} is used to compare the sequenced DNA to a
reference database of sequences from known organisms, in order to
generate a taxonomic profile of all organisms identified in a sample
(Figure~\ref{fig-overview})

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/how_to_analyze.png}

}

\caption{\label{fig-overview}General overview of a taxonomic profiling
analysis workflow}

\end{figure}

If you prefer text instead of pictograms, the workflow we're going to
cover today is outlined in the figure below, adapted from
@sharptonIntroductionAnalysisShotgun2014

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_workflow.png}

}

\caption{\label{fig-workflow}A typical metagenomics analysis workflow,
adapted from @sharptonIntroductionAnalysisShotgun2014}

\end{figure}

Because different organisms can possess the same DNA, especially when
looking at shorter sequences, taxonomic profilers need to have a way to
resolve the ambiguity in the taxonomic assignation
(Figure~\ref{fig-ambiguity}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/ambiguity.png}

}

\caption{\label{fig-ambiguity}Different species can share the same DNA
sequence}

\end{figure}

By leveraging an algorithm known as the
\href{https://en.wikipedia.org/wiki/Lowest_common_ancestor}{Lowest
Common Ancestor} (LCA), and the taxonomic tree of all known species,
ambiguities are going to be resolved by assigning a higher, less
precise, taxonomic rank to ambiguous matches(Figure~\ref{fig-lca}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/lca.png}

}

\caption{\label{fig-lca}A diagram of the LCA algorithm, a way to resolve
these ambiguities}

\end{figure}

\hypertarget{chapter-overview}{%
\section{Chapter Overview}\label{chapter-overview}}

Today, we're going to use the following tools:

\begin{itemize}
\tightlist
\item
  fastp {[}@chenFastpUltrafastAllinone2018{]} for sequencing data
  cleaning
\item
  MetaPhlAn {[}@segata2012metagenomic{]}, for taxonomic profiling
\item
  Krona {[}@ondovInteractiveMetagenomicVisualization2011{]} and Pavian
  {[}@breitwieserPavianInteractiveAnalysis2016{]} for the interactive
  exploration of the taxonomic profiles
\item
  curatedMetagenomicData {[}@pasolliAccessibleCuratedMetagenomic2017{]}
  for retrieving modern comparison data
\item
  Python, pandas {[}@rebackPandasdevPandasPandas2022{]},
  \href{https://plotnine.readthedocs.io/en/stable/}{plotnine} , and
  scikit-bio {[}@scikit-bioScikitbioBioinformaticsLibrary2022{]} to
  perform exploratory data analysis and a bit of microbial ecology
\end{itemize}

to explore a toy dataset that has already been prepared for you.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Preamble: what has been done to generate this toy dataset}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\hypertarget{download-and-subsample}{%
\subsection{Download and Subsample}\label{download-and-subsample}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ subprocess}
\ImportTok{import}\NormalTok{ glob}
\ImportTok{from}\NormalTok{ pathlib }\ImportTok{import}\NormalTok{ Path}
\end{Highlighting}
\end{Shaded}

For this tutorial, we will be using the ERR5766177 library from the
sample 2612 published by
\href{https://doi.org/10.1016/j.cub.2021.09.031}{Maixner et al.~2021}

\includegraphics{assets/images/chapters/taxonomic-profiling/1-s2.0-S0960982221012719-fx1.jpg}

\includegraphics{assets/images/chapters/taxonomic-profiling/1-s2.0-S0960982221012719-gr1.jpg}`

\hypertarget{subsampling-the-sequencing-files-to-make-the-analysis-quicker-for-this-tutorial}{%
\subsubsection{Subsampling the sequencing files to make the analysis
quicker for this
tutorial}\label{subsampling-the-sequencing-files-to-make-the-analysis-quicker-for-this-tutorial}}

This Python code defines a function called subsample that takes in a
FASTQ file name, an output directory, and a depth value (defaulting to
1000000). The function uses the seqtk command-line tool to subsample the
input FASTQ file to the desired depth and saves the output to a new file
in the specified output directory. The function prints the constructed
command string to the console for debugging purposes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ subsample(filename, outdir, depth}\OperatorTok{=}\DecValTok{1000000}\NormalTok{):}
\NormalTok{    basename }\OperatorTok{=}\NormalTok{ Path(filename).stem}
\NormalTok{    cmd }\OperatorTok{=} \SpecialStringTok{f"seqtk sample {-}s42 }\SpecialCharTok{\{}\NormalTok{filename}\SpecialCharTok{\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{depth}\SpecialCharTok{\}}\SpecialStringTok{ \textgreater{} }\SpecialCharTok{\{}\NormalTok{outdir}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{basename}\SpecialCharTok{\}}\SpecialStringTok{\_subsample\_}\SpecialCharTok{\{}\NormalTok{depth}\SpecialCharTok{\}}\SpecialStringTok{.fastq"}
    \BuiltInTok{print}\NormalTok{(cmd)}
\NormalTok{    subprocess.check\_output(cmd, shell}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This Python code uses a for loop to iterate over all the files in the
\texttt{../data/raw/} directory that match the pattern \texttt{*}, and
calls the \texttt{subsample} (defined above) function on each file in
the directory \texttt{../data/subsampled}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ glob.glob(}\StringTok{"../data/raw/*"}\NormalTok{):}
\NormalTok{    outdir }\OperatorTok{=} \StringTok{"../data/subsampled"}
\NormalTok{    subsample(f, outdir)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
seqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.fwd.fq.gz 1000000 >
../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq
seqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.rev.fq.gz 1000000 >
../data/subsampled/ERR5766177_PE.mapped.hostremoved.rev.fq_subsample_1000000.fastq
\end{verbatim}

\end{tcolorbox}

Finally, we compress all files to gzip format

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gzip} \AttributeTok{{-}f}\NormalTok{ ../data/subsampled/}\PreprocessorTok{*}\NormalTok{.fastq}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{data-pre-processing}{%
\section{Data pre-processing}\label{data-pre-processing}}

Before starting to analyze our data, we will need to pre-process them to
remove reads mapping to the host genome, here, \emph{Homo sapiens}.

To do so, I've used the first steps of the
\href{https://github.com/nf-core/eager}{nf-core/eager} pipeline, more
information of which can be found in the
\protect\hyperlink{ancient-metagenomic-pipelines-1}{Ancient Metagenomic
Pipelines} chapter.

I've already done some pre-processed the data, and the resulting cleaned
files are available in the \texttt{data/eager\_cleaned/} but the basic
eager command to do so is:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{nextflow}\NormalTok{ run nf{-}core/eager }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}r 2.4.7 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}profile }\OperatorTok{\textless{}}\NormalTok{docker/singularity/podman/conda/institute}\OperatorTok{\textgreater{}} \DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}input }\StringTok{\textquotesingle{}*\_R\{1,2\}.fastq.gz\textquotesingle{}} \DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}fasta }\StringTok{\textquotesingle{}human\_genome.fasta\textquotesingle{}} \DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}hostremoval\_input\_fastq}
\end{Highlighting}
\end{Shaded}

\hypertarget{adapter-sequence-trimming-and-low-quality-bases-trimming}{%
\section{Adapter sequence trimming and low-quality bases
trimming}\label{adapter-sequence-trimming-and-low-quality-bases-trimming}}

Sequencing adapters are small DNA sequences adding prior to DNA
sequencing to allow the DNA fragments to attach to the sequencing flow
cells (see
\protect\hyperlink{introduction-to-ngs-sequencing-1}{Introduction to NGS
Sequencing}). Because these adapters could interfere with downstream
analyses, we need to remove them before proceeding any further.
Furthermore, because the quality of the sequencing is not always
optimal, we need to remove bases of lower sequencing quality to might
lead to spurious results in downstream analyses.

To perform both of these tasks, we'll use the program
\href{https://github.com/OpenGene/fastp}{fastp}.

The following command gets you the help of fastp (the \texttt{-\/-help}
option is a common option in command-line tools that displays a list of
available options and their descriptions).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp} \AttributeTok{{-}h}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
option needs value: --html
usage: fastp [options] ...
options:
  -i, --in1                            read1 input file name (string [=])
  -o, --out1                           read1 output file name (string [=])
  -I, --in2                            read2 input file name (string [=])
  -O, --out2                           read2 output file name (string [=])
      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1.
                                       Default is to discard it. (string [=])
      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2.
                                       If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be
                                       written to this same file. (string [=])
      --overlapped_out                 for each read pair, output the overlapped region if it has no any mismatched
                                       base. (string [=])
      --failed_out                     specify the file to store reads that cannot pass the filters. (string [=])
  -m, --merge                          for paired-end input, merge each pair of reads into a single read if they are
                                       overlapped. The merged reads will be written
                                       to the file given by --merged_out, the unmerged reads will be written to the
                                       files specified by --out1 and --out2. The merging mode is disabled by default.
      --merged_out                     in the merging mode, specify the file name to store merged output, or specify
                                       --stdout to stream the merged output (string [=])
      --include_unmerged               in the merging mode, write the unmerged or unpaired reads to the file specified
                                       by --merge. Disabled by default.
  -6, --phred64                        indicate the input is using phred64 scoring (it'll be converted to phred33,
                                       so the output will still be phred33)
  -z, --compression                    compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 4. (int [=4])
      --stdin                          input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in.
      --stdout                         stream passing-filters reads to STDOUT. This option will result in interleaved
                                       FASTQ output for paired-end output. Disabled by default.
      --interleaved_in                 indicate that <in1> is an interleaved FASTQ which contains both read1 and read2.
                                       Disabled by default.
      --reads_to_process               specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0])
      --dont_overwrite                 don't overwrite existing files. Overwritting is allowed by default.
      --fix_mgi_id                     the MGI FASTQ ID format is not compatible with many BAM operation tools, enable this option to fix it.
  -V, --verbose                        output verbose log information (i.e. when every 1M reads are processed).
  -A, --disable_adapter_trimming       adapter trimming is enabled by default. If this option is specified, adapter trimming is disabled
  -a, --adapter_sequence               the adapter for read1. For SE data, if not specified, the adapter will be auto-detected.
                                       For PE data, this is used if R1/R2 are found not overlapped. (string [=auto])
      --adapter_sequence_r2            the adapter for read2 (PE data only). This is used if R1/R2 are found not overlapped.
                                       If not specified, it will be the same as <adapter_sequence> (string [=auto])
      --adapter_fasta                  specify a FASTA file to trim both read1 and read2 (if PE) by all the sequences in this FASTA file (string [=])
      --detect_adapter_for_pe          by default, the auto-detection for adapter is for SE data input only, turn on this
                                    option to enable it for PE data.
  -f, --trim_front1                    trimming how many bases in front for read1, default is 0 (int [=0])
  -t, --trim_tail1                     trimming how many bases in tail for read1, default is 0 (int [=0])
  -b, --max_len1                       if read1 is longer than max_len1, then trim read1 at its tail to make it as
                                       long as max_len1. Default 0 means no limitation (int [=0])
  -F, --trim_front2                    trimming how many bases in front for read2. If it's not specified, it will follow read1's settings (int [=0])
  -T, --trim_tail2                     trimming how many bases in tail for read2. If it's not specified, it will follow read1's settings (int [=0])
  -B, --max_len2                       if read2 is longer than max_len2, then trim read2 at its tail to make it as long as max_len2.
                                       Default 0 means no limitation. If it's not specified, it will follow read1's settings (int [=0])
  -D, --dedup                          enable deduplication to drop the duplicated reads/pairs
      --dup_calc_accuracy              accuracy level to calculate duplication (1~6), higher level uses more memory (1G, 2G, 4G, 8G, 16G, 24G).
                                       Default 1 for no-dedup mode, and 3 for dedup mode. (int [=0])
      --dont_eval_duplication          don't evaluate duplication rate to save time and use less memory.
  -g, --trim_poly_g                    force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data
      --poly_g_min_len                 the minimum length to detect polyG in the read tail. 10 by default. (int [=10])
  -G, --disable_trim_poly_g            disable polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data
  -x, --trim_poly_x                    enable polyX trimming in 3' ends.
      --poly_x_min_len                 the minimum length to detect polyX in the read tail. 10 by default. (int [=10])
  -5, --cut_front                      move a sliding window from front (5') to tail, drop the bases in the window if
                                       its mean quality < threshold, stop otherwise.
  -3, --cut_tail                       move a sliding window from tail (3') to front, drop the bases in the window if
                                       its mean quality < threshold, stop otherwise.
  -r, --cut_right                      move a sliding window from front to tail, if meet one window with mean quality
                                       < threshold, drop the bases in the window and the right part, and then stop.
  -W, --cut_window_size                the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])
  -M, --cut_mean_quality               the mean quality requirement option shared by cut_front, cut_tail or cut_sliding.
                                       Range: 1~36 default: 20 (Q20) (int [=20])
      --cut_front_window_size          the window size option of cut_front, default to cut_window_size if not specified (int [=4])
      --cut_front_mean_quality         the mean quality requirement option for cut_front, default to cut_mean_quality if not specified (int [=20])
      --cut_tail_window_size           the window size option of cut_tail, default to cut_window_size if not specified (int [=4])
      --cut_tail_mean_quality          the mean quality requirement option for cut_tail, default to cut_mean_quality if not specified (int [=20])
      --cut_right_window_size          the window size option of cut_right, default to cut_window_size if not specified (int [=4])
      --cut_right_mean_quality         the mean quality requirement option for cut_right, default to cut_mean_quality if not specified (int [=20])
  -Q, --disable_quality_filtering      quality filtering is enabled by default. If this option is specified, quality filtering is disabled
  -q, --qualified_quality_phred        the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified. (int [=15])
  -u, --unqualified_percent_limit      how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40% (int [=40])
  -n, --n_base_limit                   if one read's number of N base is >n_base_limit, then this read/pair is discarded. Default is 5 (int [=5])
  -e, --average_qual                   if one read's average quality score <avg_qual, then this read/pair is discarded.
                                       Default 0 means no requirement (int [=0])
  -L, --disable_length_filtering       length filtering is enabled by default. If this option is specified, length filtering is disabled
  -l, --length_required                reads shorter than length_required will be discarded, default is 15. (int [=15])
      --length_limit                   reads longer than length_limit will be discarded, default 0 means no limitation. (int [=0])
  -y, --low_complexity_filter          enable low complexity filter. The complexity is defined as the percentage of base
                                       that is different from its next base (base[i] != base[i+1]).
  -Y, --complexity_threshold           the threshold for low complexity filter (0~100). Default is 30, which means 30% complexity is required. (int [=30])
      --filter_by_index1               specify a file contains a list of barcodes of index1 to be filtered out, one barcode per line (string [=])
      --filter_by_index2               specify a file contains a list of barcodes of index2 to be filtered out, one barcode per line (string [=])
      --filter_by_index_threshold      the allowed difference of index barcode for index filtering, default 0 means completely identical. (int [=0])
  -c, --correction                     enable base correction in overlapped regions (only for PE data), default is disabled
      --overlap_len_require            the minimum length to detect overlapped region of PE reads. This will affect overlap analysis based PE merge,
                                       adapter trimming and correction. 30 by default. (int [=30])
      --overlap_diff_limit             the maximum number of mismatched bases to detect overlapped region of PE reads.
                                       This will affect overlap analysis based PE merge, adapter trimming and correction. 5 by default. (int [=5])
      --overlap_diff_percent_limit     the maximum percentage of mismatched bases to detect overlapped region of PE reads.
                                       This will affect overlap analysis based PE merge, adapter trimming and correction. Default 20 means 20%. (int [=20])
  -U, --umi                            enable unique molecular identifier (UMI) preprocessing
      --umi_loc                        specify the location of UMI, can be (index1/index2/read1/read2/per_index/per_read, default is none (string [=])
      --umi_len                        if the UMI is in read1/read2, its length should be provided (int [=0])
      --umi_prefix                     if specified, an underline will be used to connect prefix and UMI (i.e.
                                       prefix=UMI, UMI=AATTCG, final=UMI_AATTCG). No prefix by default (string [=])
      --umi_skip                       if the UMI is in read1/read2, fastp can skip several bases following UMI, default is 0 (int [=0])
  -p, --overrepresentation_analysis    enable overrepresented sequence analysis.
  -P, --overrepresentation_sampling    one in (--overrepresentation_sampling) reads will be computed for overrepresentation
                                       analysis (1~10000), smaller is slower, default is 20. (int [=20])
  -j, --json                           the json format report file name (string [=fastp.json])
  -h, --html                           the html format report file name (string [=fastp.html])
  -R, --report_title                   should be quoted with ' or ", default is "fastp report" (string [=fastp report])
  -w, --thread                         worker thread number, default is 3 (int [=3])
  -s, --split                          split output by limiting total split file number with this option (2~999), a sequential number prefix
                                       will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (int [=0])
  -S, --split_by_lines                 split output by limiting lines of each file with this option(>=1000), a sequential number prefix will be
                                       added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (long [=0])
  -d, --split_prefix_digits            the digits for the sequential number padding (1~10), default is 4, so the filename will be padded as
                                       0001.xxx, 0 to disable padding (int [=4])
      --cut_by_quality5                DEPRECATED, use --cut_front instead.
      --cut_by_quality3                DEPRECATED, use --cut_tail instead.
      --cut_by_quality_aggressive      DEPRECATED, use --cut_right instead.
      --discard_unmerged               DEPRECATED, no effect now, see the introduction for merging.
  -?, --help                           print this message
\end{verbatim}

\end{tcolorbox}

Here we use fastp to preprocess a pair of FASTQ files. The code
specifies the input files, merges the paired-end reads on their
overlaps, removes duplicate reads, and generates JSON and HTML reports.
The output files are saved in the \texttt{../results/fastp/\ directory}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp} \DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}in1}\NormalTok{ ../data/subsampled/ERR5766177\_PE.mapped.hostremoved.fwd.fq\_subsample\_1000000.fastq.gz }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}in2}\NormalTok{ ../data/subsampled/ERR5766177\_PE.mapped.hostremoved.fwd.fq\_subsample\_1000000.fastq.gz }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}merge} \DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}merged\_out}\NormalTok{ ../results/fastp/ERR5766177.merged.fastq.gz }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}include\_unmerged} \DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}dedup} \DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}json}\NormalTok{ ../results/fastp/ERR5766177.fastp.json }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}html}\NormalTok{ ../results/fastp/ERR5766177.fastp.html }\DataTypeTok{\textbackslash{}}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
Read1 before filtering:
total reads: 1000000
total bases: 101000000
Q20 bases: 99440729(98.4562%)
Q30 bases: 94683150(93.7457%)

Read2 before filtering:
total reads: 1000000
total bases: 101000000
Q20 bases: 99440729(98.4562%)
Q30 bases: 94683150(93.7457%)

Merged and filtered:
total reads: 1994070
total bases: 201397311
Q20 bases: 198330392(98.4772%)
Q30 bases: 188843169(93.7665%)

Filtering result:
reads passed filter: 1999252
reads failed due to low quality: 728
reads failed due to too many N: 20
reads failed due to too short: 0
reads with adapter trimmed: 282
bases trimmed due to adapters: 18654
reads corrected by overlap analysis: 0
bases corrected by overlap analysis: 0

Duplication rate: 0.2479%

Insert size peak (evaluated by paired-end reads): 31

Read pairs merged: 228
% of original read pairs: 0.0228%
% in reads after filtering: 0.0114339%


JSON report: ../results/fastp/ERR5766177.fastp.json
HTML report: ../results/fastp/ERR5766177.fastp.html

fastp --in1 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \
--in2 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz --merge \
--merged_out ../results/fastp/ERR5766177.merged.fastq.gz --include_unmerged --dedup \
--json ../results/fastp/ERR5766177.fastp.json --html ../results/fastp/ERR5766177.fastp.html
fastp v0.23.2, time used: 11 seconds
\end{verbatim}

\end{tcolorbox}

\hypertarget{taxonomic-profiling-with-metaphlan}{%
\section{Taxonomic profiling with
Metaphlan}\label{taxonomic-profiling-with-metaphlan}}

MetaPhlAn is a computational tool for profiling the composition of
microbial communities from metagenomic shotgun sequencing data.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{metaphlan}  \AttributeTok{{-}{-}help}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
usage: metaphlan --input_type {fastq,fasta,bowtie2out,sam} [--force]
                 [--bowtie2db METAPHLAN_BOWTIE2_DB] [-x INDEX]
                 [--bt2_ps BowTie2 presets] [--bowtie2_exe BOWTIE2_EXE]
                 [--bowtie2_build BOWTIE2_BUILD] [--bowtie2out FILE_NAME]
                 [--min_mapq_val MIN_MAPQ_VAL] [--no_map] [--tmp_dir]
                 [--tax_lev TAXONOMIC_LEVEL] [--min_cu_len]
                 [--min_alignment_len] [--add_viruses] [--ignore_eukaryotes]
                 [--ignore_bacteria] [--ignore_archaea] [--stat_q]
                 [--perc_nonzero] [--ignore_markers IGNORE_MARKERS]
                 [--avoid_disqm] [--stat] [-t ANALYSIS TYPE]
                 [--nreads NUMBER_OF_READS] [--pres_th PRESENCE_THRESHOLD]
                 [--clade] [--min_ab] [-o output file] [--sample_id_key name]
                 [--use_group_representative] [--sample_id value]
                 [-s sam_output_file] [--legacy-output] [--CAMI_format_output]
                 [--unknown_estimation] [--biom biom_output] [--mdelim mdelim]
                 [--nproc N] [--install] [--force_download]
                 [--read_min_len READ_MIN_LEN] [-v] [-h]
                 [INPUT_FILE] [OUTPUT_FILE]

DESCRIPTION
 MetaPhlAn version 3.1.0 (25 Jul 2022):
 METAgenomic PHyLogenetic ANalysis for metagenomic taxonomic profiling.

AUTHORS: Francesco Beghini (francesco.beghini@unitn.it),Nicola Segata (nicola.segata@unitn.it), Duy Tin Truong,
Francesco Asnicar (f.asnicar@unitn.it), Aitor Blanco Miguez (aitor.blancomiguez@unitn.it)

COMMON COMMANDS

 We assume here that MetaPhlAn is installed using the several options available (pip, conda, PyPi)
 Also BowTie2 should be in the system path with execution and read permissions, and Perl should be installed)

========== MetaPhlAn clade-abundance estimation =================

The basic usage of MetaPhlAn consists in the identification of the clades (from phyla to species )
present in the metagenome obtained from a microbiome sample and their
relative abundance. This correspond to the default analysis type (-t rel_ab).

*  Profiling a metagenome from raw reads:
$ metaphlan metagenome.fastq --input_type fastq -o profiled_metagenome.txt

*  You can take advantage of multiple CPUs and save the intermediate BowTie2 output for re-running
   MetaPhlAn extremely quickly:
$ metaphlan metagenome.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq -o profiled_metagenome.txt

*  If you already mapped your metagenome against the marker DB (using a previous MetaPhlAn run), you
   can obtain the results in few seconds by using the previously saved --bowtie2out file and
   specifying the input (--input_type bowtie2out):
$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out -o profiled_metagenome.txt

*  bowtie2out files generated with MetaPhlAn versions below 3 are not compatibile.
   Starting from MetaPhlAn 3.0, the BowTie2 ouput now includes the size of the profiled metagenome and the average read length.
   If you want to re-run MetaPhlAn using these file you should provide the metagenome size via --nreads:
$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out --nreads 520000 -o profiled_metagenome.txt

*  You can also provide an externally BowTie2-mapped SAM if you specify this format with
   --input_type. Two steps: first apply BowTie2 and then feed MetaPhlAn with the obtained sam:
$ bowtie2 --sam-no-hd --sam-no-sq --no-unal --very-sensitive -S metagenome.sam -x \
  ${mpa_dir}/metaphlan_databases/mpa_v30_CHOCOPhlAn_201901 -U metagenome.fastq
$ metaphlan metagenome.sam --input_type sam -o profiled_metagenome.txt

*  We can also natively handle paired-end metagenomes, and, more generally, metagenomes stored in
  multiple files (but you need to specify the --bowtie2out parameter):
$ metaphlan metagenome_1.fastq,metagenome_2.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq

-------------------------------------------------------------------


========== Marker level analysis ============================

MetaPhlAn introduces the capability of characterizing organisms at the strain level using non
aggregated marker information. Such capability comes with several slightly different flavours and
are a way to perform strain tracking and comparison across multiple samples.
Usually, MetaPhlAn is first ran with the default -t to profile the species present in
the community, and then a strain-level profiling can be performed to zoom-in into specific species
of interest. This operation can be performed quickly as it exploits the --bowtie2out intermediate
file saved during the execution of the default analysis type.

*  The following command will output the abundance of each marker with a RPK (reads per kilo-base)
   higher 0.0. (we are assuming that metagenome_outfmt.bz2 has been generated before as
   shown above).
$ metaphlan -t marker_ab_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt
   The obtained RPK can be optionally normalized by the total number of reads in the metagenome
   to guarantee fair comparisons of abundances across samples. The number of reads in the metagenome
   needs to be passed with the '--nreads' argument

*  The list of markers present in the sample can be obtained with '-t marker_pres_table'
$ metaphlan -t marker_pres_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt
   The --pres_th argument (default 1.0) set the minimum RPK value to consider a marker present

*  The list '-t clade_profiles' analysis type reports the same information of '-t marker_ab_table'
   but the markers are reported on a clade-by-clade basis.
$ metaphlan -t clade_profiles metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt

*  Finally, to obtain all markers present for a specific clade and all its subclades, the
   '-t clade_specific_strain_tracker' should be used. For example, the following command
   is reporting the presence/absence of the markers for the B. fragilis species and its strains
   the optional argument --min_ab specifies the minimum clade abundance for reporting the markers

$ metaphlan -t clade_specific_strain_tracker --clade s__Bacteroides_fragilis metagenome_outfmt.bz2 --input_typ
  bowtie2out -o marker_abundance_table.txt

-------------------------------------------------------------------

positional arguments:
  INPUT_FILE            the input file can be:
                        * a fastq file containing metagenomic reads
                        OR
                        * a BowTie2 produced SAM file.
                        OR
                        * an intermediary mapping file of the metagenome generated by a previous MetaPhlAn run
                        If the input file is missing, the script assumes that the input is provided using the standard
                        input, or named pipes.
                        IMPORTANT: the type of input needs to be specified with --input_type
  OUTPUT_FILE           the tab-separated output file of the predicted taxon relative abundances
                        [stdout if not present]

Required arguments:
  --input_type {fastq,fasta,bowtie2out,sam}
                        set whether the input is the FASTA file of metagenomic reads or
                        the SAM file of the mapping of the reads against the MetaPhlAn db.

Mapping arguments:
  --force               Force profiling of the input file by removing the bowtie2out file
  --bowtie2db METAPHLAN_BOWTIE2_DB
                        Folder containing the MetaPhlAn database. You can specify the location by exporting the
                        DEFAULT_DB_FOLDER variable in the shell.
                        [default /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/metaphlan/metaphlan_databases]
  -x INDEX, --index INDEX
                        Specify the id of the database version to use. If "latest", MetaPhlAn will get the latest version.
                        If an index name is provided, MetaPhlAn will try to use it, if available, and skip the online check.
                        If the database files are not found on the local MetaPhlAn installation they
                        will be automatically downloaded [default latest]
  --bt2_ps BowTie2 presets
                        Presets options for BowTie2 (applied only when a FASTA file is provided)
                        The choices enabled in MetaPhlAn are:
                         * sensitive
                         * very-sensitive
                         * sensitive-local
                         * very-sensitive-local
                        [default very-sensitive]
  --bowtie2_exe BOWTIE2_EXE
                        Full path and name of the BowTie2 executable. This option allowsMetaPhlAn to reach the
                        executable even when it is not in the system PATH or the system PATH is unreachable
  --bowtie2_build BOWTIE2_BUILD
                        Full path to the bowtie2-build command to use, deafult assumes that 'bowtie2-build is present in the system path
  --bowtie2out FILE_NAME
                        The file for saving the output of BowTie2
  --min_mapq_val MIN_MAPQ_VAL
                        Minimum mapping quality value (MAPQ) [default 5]
  --no_map              Avoid storing the --bowtie2out map file
  --tmp_dir             The folder used to store temporary files [default is the OS dependent tmp dir]

Post-mapping arguments:
  --tax_lev TAXONOMIC_LEVEL
                        The taxonomic level for the relative abundance output:
                        'a' : all taxonomic levels
                        'k' : kingdoms
                        'p' : phyla only
                        'c' : classes only
                        'o' : orders only
                        'f' : families only
                        'g' : genera only
                        's' : species only
                        [default 'a']
  --min_cu_len          minimum total nucleotide length for the markers in a clade for
                        estimating the abundance without considering sub-clade abundances
                        [default 2000]
  --min_alignment_len   The sam records for aligned reads with the longest subalignment
                        length smaller than this threshold will be discarded.
                        [default None]
  --add_viruses         Allow the profiling of viral organisms
  --ignore_eukaryotes   Do not profile eukaryotic organisms
  --ignore_bacteria     Do not profile bacterial organisms
  --ignore_archaea      Do not profile archeal organisms
  --stat_q              Quantile value for the robust average
                        [default 0.2]
  --perc_nonzero        Percentage of markers with a non zero relative abundance for misidentify a species
                        [default 0.33]
  --ignore_markers IGNORE_MARKERS
                        File containing a list of markers to ignore.
  --avoid_disqm         Deactivate the procedure of disambiguating the quasi-markers based on the
                        marker abundance pattern found in the sample. It is generally recommended
                        to keep the disambiguation procedure in order to minimize false positives
  --stat                Statistical approach for converting marker abundances into clade abundances
                        'avg_g'  : clade global (i.e. normalizing all markers together) average
                        'avg_l'  : average of length-normalized marker counts
                        'tavg_g' : truncated clade global average at --stat_q quantile
                        'tavg_l' : truncated average of length-normalized marker counts (at --stat_q)
                        'wavg_g' : winsorized clade global average (at --stat_q)
                        'wavg_l' : winsorized average of length-normalized marker counts (at --stat_q)
                        'med'    : median of length-normalized marker counts
                        [default tavg_g]

Additional analysis types and arguments:
  -t ANALYSIS TYPE      Type of analysis to perform:
                         * rel_ab: profiling a metagenomes in terms of relative abundances
                         * rel_ab_w_read_stats: profiling a metagenomes in terms of relative abundances and estimate
                                                the number of reads coming from each clade.
                         * reads_map: mapping from reads to clades (only reads hitting a marker)
                         * clade_profiles: normalized marker counts for clades with at least a non-null marker
                         * marker_ab_table: normalized marker counts (only when > 0.0 and normalized by metagenome size if --nreads is specified)
                         * marker_counts: non-normalized marker counts [use with extreme caution]
                         * marker_pres_table: list of markers present in the sample (threshold at 1.0 if not differently specified with --pres_th
                         * clade_specific_strain_tracker: list of markers present for a specific clade, specified with --clade, and all its subclades
                        [default 'rel_ab']
  --nreads NUMBER_OF_READS
                        The total number of reads in the original metagenome. It is used only when
                        -t marker_table is specified for normalizing the length-normalized counts
                        with the metagenome size as well. No normalization applied if --nreads is not
                        specified
  --pres_th PRESENCE_THRESHOLD
                        Threshold for calling a marker present by the -t marker_pres_table option
  --clade               The clade for clade_specific_strain_tracker analysis
  --min_ab              The minimum percentage abundance for the clade in the clade_specific_strain_tracker analysis

Output arguments:
  -o output file, --output_file output file
                        The output file (if not specified as positional argument)
  --sample_id_key name  Specify the sample ID key for this analysis. Defaults to 'SampleID'.
  --use_group_representative
                        Use a species as representative for species groups.
  --sample_id value     Specify the sample ID for this analysis. Defaults to 'Metaphlan_Analysis'.
  -s sam_output_file, --samout sam_output_file
                        The sam output file
  --legacy-output       Old MetaPhlAn2 two columns output
  --CAMI_format_output  Report the profiling using the CAMI output format
  --unknown_estimation  Scale relative abundances to the number of reads mapping to known clades in order to estimate unknowness
  --biom biom_output, --biom_output_file biom_output
                        If requesting biom file output: The name of the output file in biom format
  --mdelim mdelim, --metadata_delimiter_char mdelim
                        Delimiter for bug metadata: - defaults to pipe. e.g. the pipe in k__Bacteria|p__Proteobacteria

Other arguments:
  --nproc N             The number of CPUs to use for parallelizing the mapping [default 4]
  --install             Only checks if the MetaPhlAn DB is installed and installs it if not. All other parameters are ignored.
  --force_download      Force the re-download of the latest MetaPhlAn database.
  --read_min_len READ_MIN_LEN
                        Specify the minimum length of the reads to be considered when parsing the input file with
                        'read_fastx.py' script, default value is 70
  -v, --version         Prints the current MetaPhlAn version and exit
  -h, --help            show this help message and exit
\end{verbatim}

\end{tcolorbox}

The following command uses MetaPhlAn to profile the taxonomic
composition of the ERR5766177 metagenomic sample. The input file is
specified as a merged FASTQ file, and the output is saved as a text file
containing the taxonomic profile. The \texttt{-\/-bowtie2out} option is
used to specify the output file for the Bowtie2 alignment, and the
--nproc option is used to specify the number of CPUs to use for the
analysis.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{metaphlan}\NormalTok{ ../results/fastp/ERR5766177.merged.fastq.gz  }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}input\_type}\NormalTok{ fastq }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}bowtie2out}\NormalTok{ ../results/metaphlan/ERR5766177.bt2.out  }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}nproc}\NormalTok{ 4 }\DataTypeTok{\textbackslash{}}
    \OperatorTok{\textgreater{}}\NormalTok{ ../results/metaphlan/ERR5766177.metaphlan\_profile.txt}
\end{Highlighting}
\end{Shaded}

The main results files that we're interested in is located at
\texttt{../results/metaphlan/ERR5766177.metaphlan\_profile.txt}

It's a tab separated file, with taxons in rows, with their relative
abundance in the sample

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ ../results/metaphlan/ERR5766177.metaphlan\_profile.txt}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
#mpa_v30_CHOCOPhlAn_201901
#/home/maxime_borry/.conda/envs/maxime/envs/summer_school_microbiome/bin/metaphlan ../results/fastp/ERR5766177.merged.fastq.gz \
--input_type fastq --bowtie2out ../results/metaphlan/ERR5766177.bt2.out --nproc 8
#SampleID   Metaphlan_Analysis
#clade_name NCBI_tax_id relative_abundance  additional_species
k__Bacteria 2   82.23198
k__Archaea  2157    17.76802
k__Bacteria|p__Firmicutes   2|1239  33.47957
k__Bacteria|p__Bacteroidetes    2|976   28.4209
k__Bacteria|p__Actinobacteria   2|201174    20.33151
k__Archaea|p__Euryarchaeota 2157|28890  17.76802
\end{verbatim}

\end{tcolorbox}

\hypertarget{visualizing-the-taxonomic-profile}{%
\section{Visualizing the taxonomic
profile}\label{visualizing-the-taxonomic-profile}}

\hypertarget{visualizing-metaphlan-taxonomic-profile-with-pavian}{%
\subsection{Visualizing metaphlan taxonomic profile with
Pavian}\label{visualizing-metaphlan-taxonomic-profile-with-pavian}}

\href{https://github.com/fbreitwieser/pavian}{Pavian} is a web-based
tool for interactive visualization and analysis of metagenomics data. It
provides a user-friendly interface for exploring taxonomic and
functional profiles of microbial communities, and allows users to
generate interactive plots and tables that can be customized and shared
(Figure~\ref{fig-pavian-interface}).

You can open Pavian in your browser by visiting
\href{https://fbreitwieser.shinyapps.io/pavian/.}{fbreitwieser.shinyapps.io/pavian}.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/pavian.png}

}

\caption{\label{fig-pavian-interface}Screenshot of the pavian
metagenomics visualisation interface, with menus on the left, a select
sample and filter taxa search bar at the top, and a Sankey visualisation
of the example metagenome sample}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Expand for instructions for running yourself}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

There are different ways to run it:

\begin{itemize}
\item
  If you have \href{https://www.docker.com/}{docker} installed

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ pull }\StringTok{\textquotesingle{}florianbw/pavian\textquotesingle{}}
\ExtensionTok{docker}\NormalTok{ run }\AttributeTok{{-}{-}rm} \AttributeTok{{-}p}\NormalTok{ 5000:80 florianbw/pavian}
\end{Highlighting}
\end{Shaded}
\end{itemize}

Then open your browser and visit \url{localhost:5000}

\begin{itemize}
\item
  If you are familiar with \href{https://www.r-project.org/}{R}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(remotes)) \{ }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{) \}}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"fbreitwieser/pavian"}\NormalTok{)}

\NormalTok{pavian}\SpecialCharTok{::}\FunctionTok{runApp}\NormalTok{(}\AttributeTok{port=}\DecValTok{5000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{itemize}

Then open your browser and visit \url{localhost:5000}

\end{tcolorbox}

\hypertarget{visualizing-metaphlan-taxonomic-profile-with-krona}{%
\subsection{Visualizing metaphlan taxonomic profile with
Krona}\label{visualizing-metaphlan-taxonomic-profile-with-krona}}

\href{https://github.com/marbl/Krona/wiki}{Krona} is a software tool for
interactive visualization of hierarchical data, such as taxonomic
profiles generated by metagenomics tools like MetaPhlAn. Krona allows
users to explore the taxonomic composition of microbial communities in a
hierarchical manner, from the highest taxonomic level down to the
species level.

The \texttt{metaphlan2krona.py} script is used to convert the MetaPhlAn
taxonomic profile output to a format that can be visualized by Krona.
The output of the script is a text file that contains the taxonomic
profile in a hierarchical format that can be read by Krona. The
\texttt{ktImportText} command is then used to generate an interactive
HTML file that displays the taxonomic profile in a hierarchical manner
using Krona.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ ../scripts/metaphlan2krona.py }\AttributeTok{{-}p}\NormalTok{ ../results/metaphlan/ERR5766177.metaphlan\_profile.txt }\AttributeTok{{-}k}\NormalTok{ ../results/krona/ERR5766177\_krona.out}
\ExtensionTok{ktImportText} \AttributeTok{{-}o}\NormalTok{ ../results/krona/ERR5766177\_krona.html ../results/krona/ERR5766177\_krona.out}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
Writing ../results/krona/ERR5766177_krona.html...
\end{verbatim}

\end{tcolorbox}

\hypertarget{getting-modern-comparative-reference-data}{%
\section{Getting modern comparative reference
data}\label{getting-modern-comparative-reference-data}}

In order to compare our sample with modern reference samples, I used the
curatedMetagenomicsData package, which provides both curated metadata,
and pre-computed metaphlan taxonomic profiles for published modern human
samples.

The full R code to get these data is available in
\texttt{curatedMetagenomics/get\_sources.Rmd}.

I pre-selected 200 gut microbiome samples from non-westernized (100) and
westernized (100) from healthy, non-antibiotic users donors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required packages}
\FunctionTok{library}\NormalTok{(curatedMetagenomicData)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Filter samples based on specific criteria}
\NormalTok{sampleMetadata }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(body\_site }\SpecialCharTok{==} \StringTok{"stool"} \SpecialCharTok{\&}\NormalTok{ antibiotics\_current\_use }\SpecialCharTok{==} \StringTok{"no"} \SpecialCharTok{\&}\NormalTok{ disease }\SpecialCharTok{==} \StringTok{"healthy"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(non\_westernized) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\OtherTok{{-}\textgreater{}}\NormalTok{ selected\_samples}

\CommentTok{\# Extract relative abundance data for selected samples}
\NormalTok{selected\_samples }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{returnSamples}\NormalTok{(}\StringTok{"relative\_abundance"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ rel\_ab}

\CommentTok{\# Split relative abundance data by taxonomic rank and write to CSV files}
\NormalTok{data\_ranks }\OtherTok{\textless{}{-}} \FunctionTok{splitByRanks}\NormalTok{(rel\_ab)}

\ControlFlowTok{for}\NormalTok{ (r }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(data\_ranks)) \{}
    \CommentTok{\# Print taxonomic rank and output file name}
    \FunctionTok{print}\NormalTok{(r)}
\NormalTok{    output\_file }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"../../data/curated\_metagenomics/modern\_sources\_"}\NormalTok{, }\FunctionTok{tolower}\NormalTok{(r), }\StringTok{".csv"}\NormalTok{)}
    \FunctionTok{print}\NormalTok{(output\_file)}

    \CommentTok{\# Write relative abundance data to CSV file}
\NormalTok{    assay\_rank }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{assay}\NormalTok{(data\_ranks[[r]]))}
    \FunctionTok{write.csv}\NormalTok{(assay\_rank, output\_file)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The resulting pre-computed metaphlan taxonomic profiles (split by
  taxonomic ranks) are available in \texttt{data/curated\_metagenomics}
\item
  The associated metadata is available at
  \texttt{data/metadata/curated\_metagenomics\_modern\_sources.csv}
\end{itemize}

\hypertarget{loading-the-ancient-sample-taxonomic-profile}{%
\section{Loading the ancient sample taxonomic
profile}\label{loading-the-ancient-sample-taxonomic-profile}}

This is the moment where we will use the
\href{https://pandas.pydata.org}{Pandas} library
\href{https://www.python.org/}{Python} to perform some data
manipulation.\\
We will also use the \href{https://github.com/apcamargo/taxopy}{Taxopy}
library to work with taxonomic information.

In python we need to import necessary libraries, i.e.~pandas and taxopy,
and a couple of other utility libraries.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ taxopy}
\ImportTok{import}\NormalTok{ pickle}
\ImportTok{import}\NormalTok{ gzip}
\end{Highlighting}
\end{Shaded}

And we then create an instance of the taxopy taxonomy database. This
will take a few seconds/minutes, as it needs to download the entire NCBI
taxonomy before storing in a local database.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taxdb }\OperatorTok{=}\NormalTok{ taxopy.TaxDb()}
\end{Highlighting}
\end{Shaded}

Let's read the metaphlan profile table with pandas (a python package
with a similar concept to tidyverse dyplyr, tidyr packa). It's a tab
separated file, so we need to specify the delimiter as
\texttt{\textbackslash{}t}, and skip the comment lines of the files that
start with \texttt{\#}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../results/metaphlan/ERR5766177.metaphlan\_profile.txt"}\NormalTok{,}
\NormalTok{                            comment}\OperatorTok{=}\StringTok{"\#"}\NormalTok{,}
\NormalTok{                            delimiter}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{,}
\NormalTok{                            names}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}clade\_name\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}NCBI\_tax\_id\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}additional\_species\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

To look at the head of a dataframe (Table~\ref{tbl-mp3-raw}) with pandas

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-mp3-raw}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0341}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3636}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1477}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2273}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2273}}@{}}
\caption{\label{tbl-mp3-raw}Top few lines of a MetaPhlAn taxonomic
profile}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
clade\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NCBI\_tax\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
additional\_species
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
clade\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NCBI\_tax\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
additional\_species
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & k\_\_Bacteria & 2 & 82.23198 & NaN \\
1 & k\_\_Archaea & 2157 & 17.76802 & NaN \\
2 & k\_\_Bacteria\textbar p\_\_Firmicutes & 2\textbar1239 & 33.47957 &
NaN \\
3 & k\_\_Bacteria\textbar p\_\_Bacteroidetes & 2\textbar976 & 28.42090 &
NaN \\
4 & k\_\_Bacteria\textbar p\_\_Actinobacteria & 2\textbar201174 &
20.33151 & NaN \\
\end{longtable}

We can also specify more rows by randomly picking 10 rows to display
(Table~\ref{tbl-mp3-sampled}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data.sample(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-mp3-sampled}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3000}}@{}}
\caption{\label{tbl-mp3-sampled}Ten randomly selected lines of a
MetaPhlAn taxonomic profile}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
clade\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NCBI\_tax\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
additional\_species
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
clade\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NCBI\_tax\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
additional\_species
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & k\_\_Archaea & 2157 & 17.76802 & NaN \\
46 &
k\_\_Bacteria\textbar p\_\_Bacteroidetes\textbar c\_\emph{Bacteroidia\textbar o}\ldots{}
&
2\textbar976\textbar200643\textbar171549\textbar171552\textbar838\textbar165179
& 25.75544 &
k\_\_Bacteria\textbar p\_\_Bacteroidetes\textbar c\_\emph{Bacteroidia\textbar o}\ldots{} \\
55 &
k\_\_Bacteria\textbar p\_\_Firmicutes\textbar c\_\_Clostridia\textbar o\_\_Clo\ldots{}
&
2\textbar1239\textbar186801\textbar186802\textbar186803\textbar189330\textbar88431
& 0.91178 & NaN \\
18 &
k\_\_Archaea\textbar p\_\_Euryarchaeota\textbar c\_\emph{Halobacteria\textbar o}\ldots{}
& 2157\textbar28890\textbar183963\textbar2235 & 0.71177 & NaN \\
36 &
k\_\_Bacteria\textbar p\_\_Actinobacteria\textbar c\_\_Actinobacteri\ldots{}
& 2\textbar201174\textbar1760\textbar85004\textbar31953\textbar1678 &
9.39377 & NaN \\
65 &
k\_\_Bacteria\textbar p\_\_Actinobacteria\textbar c\_\_Actinobacteri\ldots{}
&
2\textbar201174\textbar1760\textbar85004\textbar31953\textbar1678\textbar216816
& 0.05447 &
k\_\_Bacteria\textbar p\_\_Actinobacteria\textbar c\_\_Actinobacteri\ldots{} \\
37 &
k\_\_Bacteria\textbar p\_\_Firmicutes\textbar c\_\_Clostridia\textbar o\_\_Clo\ldots{}
& 2\textbar1239\textbar186801\textbar186802\textbar186803\textbar{} &
2.16125 & NaN \\
38 &
k\_\_Bacteria\textbar p\_\_Firmicutes\textbar c\_\_Clostridia\textbar o\_\_Clo\ldots{}
& 2\textbar1239\textbar186801\textbar186802\textbar541000\textbar216851
& 1.24537 & NaN \\
26 &
k\_\_Bacteria\textbar p\_\_Actinobacteria\textbar c\_\_Actinobacteri\ldots{}
& 2\textbar201174\textbar1760\textbar85004\textbar31953 & 9.39377 &
NaN \\
48 &
k\_\_Bacteria\textbar p\_\_Firmicutes\textbar c\_\_Clostridia\textbar o\_\_Clo\ldots{}
&
2\textbar1239\textbar186801\textbar186802\textbar541000\textbar1263\textbar40518
& 14.96816 &
k\_\_Bacteria\textbar p\_\_Firmicutes\textbar c\_\_Clostridia\textbar o\_\_Clo\ldots{} \\
\end{longtable}

Because for this analysis, we're only going to look at the relative
abundance, we'll only use this column, and the
\href{https://www.ncbi.nlm.nih.gov/taxonomy}{Taxonomic ID (TAXID)}
information, so we can \texttt{drop} (get rid of) the unnecessary
columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    ancient\_data}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}NCBI\_tax\_id\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TAXID\textquotesingle{}}\NormalTok{\})}
\NormalTok{    .drop([}\StringTok{\textquotesingle{}clade\_name\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}additional\_species\textquotesingle{}}\NormalTok{], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-important-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-important-color!10!white, opacityback=0, opacitybacktitle=0.6]

Always investigate your data at first !

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data.relative\_abundance.}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
700.00007
\end{verbatim}

\textbf{Pause and think}: A relative abundance of 700\%, really ?

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Unfold to see what's happening}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Let's proceed further and try to understand what's happening
(Table~\ref{tbl-taxid-table}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-taxid-table}A two column table of TAXIDs and the
organisms corresponding relative abundance}\tabularnewline
\toprule\noalign{}
& TAXID & relative\_abundance \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
& TAXID & relative\_abundance \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 2 & 82.23198 \\
1 & 2157 & 17.76802 \\
2 & 2\textbar1239 & 33.47957 \\
3 & 2\textbar976 & 28.42090 \\
4 & 2\textbar201174 & 20.33151 \\
\end{longtable}

To make sense of the TAXID, we will use taxopy to get all the taxonomic
related informations such as (Table~\ref{tbl-taxid-table-with-path}):

\begin{itemize}
\tightlist
\item
  name of the taxon
\item
  rank of the taxon
\item
  lineage of the taxon
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\#\# This function is here to help us get the taxon information}
\CommentTok{\#\#\#\# from the metaphlan taxonomic ID lineage, of the following form}
\CommentTok{\#\#\#\# 2|976|200643|171549|171552|838|165179}

\KeywordTok{def}\NormalTok{ to\_taxopy(taxid\_entry, taxo\_db):}
    \CommentTok{"""Returns a taxopy taxon object}
\CommentTok{    Args:}
\CommentTok{        taxid\_entry(str): metaphlan TAXID taxonomic lineage}
\CommentTok{        taxo\_db(taxopy database)}
\CommentTok{    Returns:}
\CommentTok{        (bool): Returns a taxopy taxon object}
\CommentTok{    """}
\NormalTok{    taxid }\OperatorTok{=}\NormalTok{ taxid\_entry.split(}\StringTok{"|"}\NormalTok{)[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\CommentTok{\# get the last element}
    \ControlFlowTok{try}\NormalTok{:}
        \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(taxid) }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ taxopy.Taxon(}\BuiltInTok{int}\NormalTok{(taxid), taxo\_db) }\CommentTok{\# if it\textquotesingle{}s not empty, get the taxon corresponding to the taxid}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ taxopy.Taxon(}\DecValTok{12908}\NormalTok{, taxo\_db) }\CommentTok{\# otherwise, return the taxon associated with unclassified sequences}
    \ControlFlowTok{except}\NormalTok{ taxopy.exceptions.TaxidError }\ImportTok{as}\NormalTok{ e:}
        \ControlFlowTok{return}\NormalTok{ taxopy.Taxon(}\DecValTok{12908}\NormalTok{, taxo\_db)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data[}\StringTok{\textquotesingle{}taxopy\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ ancient\_data[}\StringTok{\textquotesingle{}TAXID\textquotesingle{}}\NormalTok{].}\BuiltInTok{apply}\NormalTok{(to\_taxopy, taxo\_db}\OperatorTok{=}\NormalTok{taxo\_db)}

\NormalTok{ancient\_data.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table-with-path}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6000}}@{}}
\caption{\label{tbl-taxid-table-with-path}A three column table of TAXIDs
and the organisms corresponding relative abundance, and the attached
taxonomic path associated with the TAXID}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 2 & 82.23198 & s\_\_Bacteria \\
1 & 2157 & 17.76802 & s\_\_Archaea \\
2 & 2\textbar1239 & 33.47957 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Firmicutes \\
3 & 2\textbar976 & 28.42090 & s\_\_Bacteria;c\_\_FCB
group;p\_\_Bacteroidetes \\
4 & 2\textbar201174 & 20.33151 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Actinoba\ldots{} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data }\OperatorTok{=}\NormalTok{ ancient\_data.assign(}
\NormalTok{    rank }\OperatorTok{=}\NormalTok{ ancient\_data.taxopy.}\BuiltInTok{apply}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ x: x.rank),}
\NormalTok{    name }\OperatorTok{=}\NormalTok{ ancient\_data.taxopy.}\BuiltInTok{apply}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ x: x.name),}
\NormalTok{    lineage }\OperatorTok{=}\NormalTok{ ancient\_data.taxopy.}\BuiltInTok{apply}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ x: x.name\_lineage),}
\NormalTok{)}

\NormalTok{ancient\_data}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table-with-path-sep}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0228}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2283}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0913}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0639}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1279}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2329}}@{}}
\caption{\label{tbl-taxid-table-with-path-sep}A five column table of
TAXIDs and the organisms corresponding relative abundance, and the
attached taxonomic path associated with the TAXID, but also the rank and
name of the particular taxonomic ID}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rank
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lineage
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rank
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lineage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 2 & 82.23198 & s\_\_Bacteria & superkingdom & Bacteria &
{[}Bacteria, cellular organisms, root{]} \\
1 & 2157 & 17.76802 & s\_\_Archaea & superkingdom & Archaea &
{[}Archaea, cellular organisms, root{]} \\
2 & 2\textbar1239 & 33.47957 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Firmicutes & phylum & Firmicutes & {[}Firmicutes,
Terrabacteria group, Bacteria, ce\ldots{} \\
3 & 2\textbar976 & 28.42090 & s\_\_Bacteria;c\_\_FCB
group;p\_\_Bacteroidetes & phylum & Bacteroidetes & {[}Bacteroidetes,
Bacteroidetes/Chlorobi group, \ldots{} \\
4 & 2\textbar201174 & 20.33151 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Actinoba\ldots{} & phylum & Actinobacteria &
{[}Actinobacteria, Terrabacteria group, Bacteria\ldots{} \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} \\
62 &
2\textbar1239\textbar186801\textbar186802\textbar186803\textbar572511\textbar33039
& 0.24910 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Firmicut\ldots{}
& species & {[}Ruminococcus{]} torques & {[}{[}Ruminococcus{]} torques,
Mediterraneibacter, L\ldots{} \\
63 &
2\textbar201174\textbar84998\textbar84999\textbar84107\textbar1472762\textbar1232426
& 0.17084 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Actinoba\ldots{}
& species & {[}Collinsella{]} massiliensis & {[}{[}Collinsella{]}
massiliensis, Enorma, Coriobact\ldots{} \\
64 &
2\textbar1239\textbar186801\textbar186802\textbar186803\textbar189330\textbar39486
& 0.07690 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Firmicut\ldots{}
& species & Dorea formicigenerans & {[}Dorea formicigenerans, Dorea,
Lachnospiraceae\ldots{} \\
65 &
2\textbar201174\textbar1760\textbar85004\textbar31953\textbar1678\textbar216816
& 0.05447 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Actinoba\ldots{}
& species & Bifidobacterium longum & {[}Bifidobacterium longum,
Bifidobacterium, Bifi\ldots{} \\
66 &
2\textbar1239\textbar186801\textbar186802\textbar541000\textbar1263\textbar1262959
& 0.01440 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Firmicut\ldots{}
& species & Ruminococcus sp. CAG:488 & {[}Ruminococcus sp. CAG:488,
environmental sampl\ldots{} \\
\end{longtable}

Because our modern data are split by ranks, we'll first split our
ancient sample by rank

Which of the entries are at the \texttt{species\ rank} level?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_species }\OperatorTok{=}\NormalTok{ ancient\_data.query(}\StringTok{"rank == \textquotesingle{}species\textquotesingle{}"}\NormalTok{)}

\NormalTok{ancient\_species.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table-with-path-species}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2238}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0952}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2429}}@{}}
\caption{\label{tbl-taxid-table-with-path-species}A five column table of
TAXIDs and the organisms corresponding relative abundance, and the
attached taxonomic path associated with the TAXID, but also the rank and
name of the particular taxonomic ID, filtered to only
species}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rank
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lineage
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TAXID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
relative\_abundance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
taxopy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rank
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lineage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
46 &
2\textbar976\textbar200643\textbar171549\textbar171552\textbar838\textbar165179
& 25.75544 & s\_\_Bacteria;c\_\_FCB
group;p\_\_Bacteroidetes;c\_\_B\ldots{} & species & Prevotella copri &
{[}Prevotella copri, Prevotella, Prevotellaceae,\ldots{} \\
47 &
2157\textbar28890\textbar183925\textbar2158\textbar2159\textbar2172\textbar2173
& 17.05626 & s\_\_Archaea;p\_\_Euryarchaeota;c\_\_Methanomada
gro\ldots{} & species & Methanobrevibacter smithii &
{[}Methanobrevibacter smithii, Methanobrevibacte\ldots{} \\
48 &
2\textbar1239\textbar186801\textbar186802\textbar541000\textbar1263\textbar40518
& 14.96816 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Firmicut\ldots{} & species & Ruminococcus bromii &
{[}Ruminococcus bromii, Ruminococcus, Oscillospi\ldots{} \\
49 &
2\textbar1239\textbar186801\textbar186802\textbar186803\textbar841\textbar301302
& 13.57908 & s\_\_Bacteria;c\_\_Terrabacteria
group;p\_\_Firmicut\ldots{} & species & Roseburia faecis & {[}Roseburia
faecis, Roseburia, Lachnospiraceae,\ldots{} \\
50 &
2\textbar201174\textbar84998\textbar84999\textbar84107\textbar102106\textbar74426
& 9.49165 & s\_\_Bacteria;c\_\_Terrabacteria group;p\_\_Actinoba\ldots{}
& species & Collinsella aerofaciens & {[}Collinsella aerofaciens,
Collinsella, Corioba\ldots{} \\
\end{longtable}

Let's do a bit of renaming to prepare for what's coming next

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_species }\OperatorTok{=}\NormalTok{ ancient\_species[[}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{]].set\_index(}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{).rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{\})}

\NormalTok{ancient\_species.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table-with-name}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-taxid-table-with-name}Reconstruction of the first
two column taxonomic profile but with species-level organism names
rather than TAXIDs}\tabularnewline
\toprule\noalign{}
name & ERR5766177 \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
name & ERR5766177 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Prevotella copri & 25.75544 \\
Methanobrevibacter smithii & 17.05626 \\
Ruminococcus bromii & 14.96816 \\
Roseburia faecis & 13.57908 \\
Collinsella aerofaciens & 9.49165 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_phylums }\OperatorTok{=}\NormalTok{ ancient\_data.query(}\StringTok{"rank == \textquotesingle{}phylum\textquotesingle{}"}\NormalTok{)}

\NormalTok{ancient\_phylums }\OperatorTok{=}\NormalTok{ ancient\_phylums[[}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{]].set\_index(}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{).rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{\})}

\NormalTok{ancient\_phylums}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-taxid-table-with-name}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-taxid-table-with-name}Reconstruction of the first
two column taxonomic profile but with phylum-level organism names rather
than TAXIDs}\tabularnewline
\toprule\noalign{}
name & ERR5766177 \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
name & ERR5766177 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Firmicutes & 33.47957 \\
Bacteroidetes & 28.42090 \\
Actinobacteria & 20.33151 \\
Euryarchaeota & 17.76802 \\
\end{longtable}

Now, let's go back to the 700\% relative abundance issue\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ancient\_data.groupby(}\StringTok{\textquotesingle{}rank\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{].}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
    rank
    class            99.72648
    family           83.49854
    genus            97.56524
    no rank          19.48331
    order            99.72648
    phylum          100.00000
    species         100.00002
    superkingdom    100.00000
    Name: relative_abundance, dtype: float64
\end{verbatim}

\end{tcolorbox}

Seems better, right ?

\textbf{Pause and think: why don't we get exactly 100\%} ?

\end{tcolorbox}

\hypertarget{bringing-together-ancient-and-modern-samples}{%
\section{Bringing together ancient and modern
samples}\label{bringing-together-ancient-and-modern-samples}}

Now let's load our modern reference samples

First at the phylum level (Table~\ref{tbl-modern-taxtable-phylum})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modern\_phylums }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../data/curated\_metagenomics/modern\_sources\_phylum.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{modern\_phylums.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-modern-taxtable-phylum}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0552}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1234}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0617}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1104}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0357}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0390}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0390}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0390}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0390}}@{}}
\caption{\label{tbl-modern-taxtable-phylum}Taxonomic profiles at phylum
level of multiple modern samples}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
de028ad4-7ae6-11e9-a106-68b59976a384
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_283
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Validation\_55
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
G80275
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_363
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045572
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045355
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HD-13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
EGAR00001420773\_9002000001423910
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SID5428-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A46\_02\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_87532
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A94\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LDK\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A48\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_81781
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A09\_01\_1FE
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
de028ad4-7ae6-11e9-a106-68b59976a384
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_283
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Validation\_55
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
G80275
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_363
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045572
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045355
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HD-13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
EGAR00001420773\_9002000001423910
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SID5428-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A46\_02\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_87532
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A94\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LDK\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A48\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_81781
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A09\_01\_1FE
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bacteroidetes & 0.00000 & 17.44332 & 82.86400 & 69.99087 & 31.93081 &
51.76204 & 53.32801 & 74.59667 & 8.81074 & 26.39694 & \ldots{} & 1.97760
& 1.49601 & 67.21410 & 4.29848 & 68.16890 & 38.59709 & 14.81828 &
10.13908 & 57.14031 & 11.61544 \\
Firmicutes & 95.24231 & 60.47031 & 16.53946 & 22.81977 & 65.23075 &
41.96928 & 45.77661 & 23.51065 & 54.35341 & 62.23094 & \ldots{} &
76.68499 & 78.13269 & 29.72394 & 33.51772 & 19.11149 & 46.87139 &
72.68136 & 35.43789 & 40.57101 & 24.72113 \\
Proteobacteria & 4.49959 & 0.77098 & 0.05697 & 4.07757 & 0.27316 &
3.33972 & 0.02001 & 1.72865 & 0.00000 & 1.81016 & \ldots{} & 16.57250 &
0.76159 & 2.35058 & 9.83772 & 5.32392 & 0.19699 & 3.64655 & 17.64151 &
0.30580 & 56.20177 \\
Actinobacteria & 0.25809 & 10.27631 & 0.45187 & 1.11902 & 2.31075 &
2.92715 & 0.77667 & 0.16403 & 36.55138 & 1.19951 & \ldots{} & 3.01814 &
19.20468 & 0.69913 & 46.99479 & 7.39093 & 14.26365 & 5.47750 & 36.77145
& 1.16426 & 7.40894 \\
Verrucomicrobia & 0.00000 & 0.00784 & 0.00000 & 1.99276 & 0.25451 &
0.00000 & 0.00000 & 0.00000 & 0.09940 & 3.29795 & \ldots{} & 0.05011 &
0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.00000 &
0.00000 & 0.00000 \\
\end{longtable}

Then at the species level

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modern\_species }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../data/curated\_metagenomics/modern\_sources\_species.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As usual, we always check if our data has been loaded correctly
(Table~\ref{tbl-modern-taxtable-species})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modern\_species.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-modern-taxtable-species}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0892}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1210}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0446}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0605}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0446}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0446}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0446}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0318}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0350}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0159}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0382}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0318}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0382}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0382}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0318}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0382}}@{}}
\caption{\label{tbl-modern-taxtable-species}Taxonomic profiles at
species level of multiple modern samples}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
de028ad4-7ae6-11e9-a106-68b59976a384
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_283
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Validation\_55
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
G80275
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_363
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045572
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045355
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HD-13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
EGAR00001420773\_9002000001423910
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SID5428-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A46\_02\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_87532
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A94\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LDK\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A48\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_81781
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A09\_01\_1FE
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
de028ad4-7ae6-11e9-a106-68b59976a384
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_283
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Validation\_55
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
G80275
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PNP\_Main\_363
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045572
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SAMEA7045355
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HD-13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
EGAR00001420773\_9002000001423910
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SID5428-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A46\_02\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_87532
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A94\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LDK\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A48\_01\_1FE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
KHG\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TZ\_81781
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A09\_01\_1FE
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bacteroides vulgatus & 0.0 & 0.60446 & 1.59911 & 4.39085 & 0.04494 &
4.66505 & 2.99431 & 29.30325 & 1.48560 & 0.98818 & \ldots{} & 0.20717 &
0.0 & 0.00309 & 0.48891 & 0.00000 & 0.02230 & 0.00000 & 0.15112 & 0.0 &
0.00836 \\
Bacteroides stercoris & 0.0 & 0.00546 & 0.00000 & 0.00000 & 2.50789 &
0.00000 & 20.57498 & 8.28443 & 1.23261 & 0.00000 & \ldots{} & 0.00000 &
0.0 & 0.00000 & 0.00693 & 0.00000 & 0.02603 & 0.00000 & 0.19318 & 0.0 &
0.00000 \\
Acidaminococcus intestini & 0.0 & 0.00000 & 0.00000 & 0.00000 & 0.00000
& 0.00000 & 0.00000 & 0.00000 & 0.32822 & 0.00000 & \ldots{} & 0.00000 &
0.0 & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 0.0 &
0.00000 \\
Eubacterium sp CAG 38 & 0.0 & 0.06712 & 0.81149 & 0.05247 & 0.26027 &
0.00000 & 0.00000 & 2.62415 & 0.46585 & 0.23372 & \ldots{} & 0.78140 &
0.0 & 0.00000 & 0.00499 & 0.00000 & 0.02446 & 0.00000 & 0.00000 & 0.0 &
0.00000 \\
Parabacteroides distasonis & 0.0 & 1.34931 & 2.00672 & 5.85067 & 0.59019
& 7.00027 & 1.28075 & 0.61758 & 0.07383 & 2.80355 & \ldots{} & 0.11423 &
0.0 & 0.01181 & 0.01386 & 0.03111 & 0.07463 & 0.15597 & 0.07541 & 0.0 &
0.01932 \\
\end{longtable}

\hypertarget{time-to-merge}{%
\subsection{Time to merge !}\label{time-to-merge}}

Now, let's merge our ancient sample with the modern data in one single
table.\\
For that, we'll use the pandas \texttt{merge} function which will merge
the two tables together, using the index as the merge key.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_species }\OperatorTok{=}\NormalTok{ ancient\_species.merge(modern\_species, left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, how}\OperatorTok{=}\StringTok{\textquotesingle{}outer\textquotesingle{}}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{all\_phylums }\OperatorTok{=}\NormalTok{ ancient\_phylums.merge(modern\_phylums, left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, how}\OperatorTok{=}\StringTok{\textquotesingle{}outer\textquotesingle{}}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, let's load the metadata, which contains the information about
the modern samples (Table~\ref{tbl-sample-metadata}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{metadata }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../data/metadata/curated\_metagenomics\_modern\_sources.csv"}\NormalTok{)}

\NormalTok{metadata.head()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-sample-metadata}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0092}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0459}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0581}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0336}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0520}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0275}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0153}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0367}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0428}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0153}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0336}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0398}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1009}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0306}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0489}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0520}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0734}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0581}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0153}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0183}}@{}}
\caption{\label{tbl-sample-metadata}Taxonomic profiles at species level
of multiple modern samples}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
study\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sample\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
subject\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
body\_site
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
antibiotics\_current\_use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
study\_condition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
disease
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
infant\_age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age\_category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
hla\_drb11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
birth\_order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age\_twins\_started\_to\_live\_apart
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
zigosity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
brinkman\_index
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol\_numeric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
breastfeeding\_duration
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
formula\_first\_day
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ALT
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
eGFR
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
study\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sample\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
subject\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
body\_site
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
antibiotics\_current\_use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
study\_condition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
disease
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
infant\_age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age\_category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
hla\_drb11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
birth\_order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age\_twins\_started\_to\_live\_apart
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
zigosity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
brinkman\_index
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol\_numeric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
breastfeeding\_duration
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
formula\_first\_day
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ALT
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
eGFR
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & ShaoY\_2019 & de028ad4-7ae6-11e9-a106-68b59976a384 & C01528\_ba &
stool & no & control & healthy & 0.0 & 4.0 & newborn & \ldots{} & NaN &
NaN & NaN & NaN & NaN & NaN & NaN & NaN & NaN & NaN \\
1 & ZeeviD\_2015 & PNP\_Main\_283 & PNP\_Main\_283 & stool & no &
control & healthy & NaN & NaN & adult & \ldots{} & NaN & NaN & NaN & NaN
& NaN & NaN & NaN & NaN & NaN & NaN \\
2 & ZeeviD\_2015 & PNP\_Validation\_55 & PNP\_Validation\_55 & stool &
no & control & healthy & NaN & NaN & adult & \ldots{} & NaN & NaN & NaN
& NaN & NaN & NaN & NaN & NaN & NaN & NaN \\
3 & VatanenT\_2016 & G80275 & T014806 & stool & no & control & healthy &
1.0 & NaN & child & \ldots{} & NaN & NaN & NaN & NaN & NaN & NaN & NaN &
NaN & NaN & NaN \\
4 & ZeeviD\_2015 & PNP\_Main\_363 & PNP\_Main\_363 & stool & no &
control & healthy & NaN & NaN & adult & \ldots{} & NaN & NaN & NaN & NaN
& NaN & NaN & NaN & NaN & NaN & NaN \\
\end{longtable}

\hypertarget{comparing-ancient-and-modern-samples}{%
\section{Comparing ancient and modern
samples}\label{comparing-ancient-and-modern-samples}}

\hypertarget{taxonomic-composition}{%
\subsection{Taxonomic composition}\label{taxonomic-composition}}

One common plot in microbiome papers in a stacked barplot, often at the
phylum or family level.

First, we'll do some renaming, to make the value of the metadata
variables a bit easier to understand (Table~\ref{tbl-sample-groups})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{group\_info }\OperatorTok{=}\NormalTok{ pd.concat(}
\NormalTok{    [}
\NormalTok{        (}
\NormalTok{        metadata[}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{]}
\NormalTok{        .}\BuiltInTok{map}\NormalTok{(\{}\StringTok{\textquotesingle{}no\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}yes\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{\}) }\CommentTok{\# for the non\_westernized in the modern sample metadata, rename the value levels}
\NormalTok{        .to\_frame(name}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{).set\_index(metadata[}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{]) }\CommentTok{\# rename the column to group}
\NormalTok{        .reset\_index()}
\NormalTok{        ),}
\NormalTok{        (}
\NormalTok{        pd.Series(\{}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}ancient\textquotesingle{}}\NormalTok{\}).to\_frame().transpose()}
\NormalTok{        )}
\NormalTok{    ],}
\NormalTok{    axis}\OperatorTok{=}\DecValTok{0}\NormalTok{, ignore\_index}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}
\NormalTok{group\_info}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
    /var/folders/1c/l1qb09f15jddsh65f6xv1n_r0000gp/T/ipykernel_40830/27419655.py:2:
    FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version.
    Use pandas.concat instead.
      metadata['non_westernized']
\end{verbatim}

\end{tcolorbox}

\hypertarget{tbl-sample-groups}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-sample-groups}Table of samples and their
group}\tabularnewline
\toprule\noalign{}
& sample\_id & group \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
& sample\_id & group \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & de028ad4-7ae6-11e9-a106-68b59976a384 & westernized \\
1 & PNP\_Main\_283 & westernized \\
2 & PNP\_Validation\_55 & westernized \\
3 & G80275 & westernized \\
4 & PNP\_Main\_363 & westernized \\
\ldots{} & \ldots{} & \ldots{} \\
196 & A48\_01\_1FE & non\_westernized \\
197 & KHG\_1 & non\_westernized \\
198 & TZ\_81781 & non\_westernized \\
199 & A09\_01\_1FE & non\_westernized \\
200 & ERR5766177 & ancient \\
\end{longtable}

We need transform our data in
\href{https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html}{tidy}
format to plot with
\href{https://plotnine.readthedocs.io/en/stable/}{plotnine}, a python
clone of \href{https://ggplot2.tidyverse.org/index.html}{ggplot}.

\hypertarget{tbl-raw-multi-sample-table}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0147}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0469}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0352}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0821}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0352}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0499}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0147}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0411}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0469}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0469}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0411}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0499}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0499}}@{}}
\caption{\label{tbl-raw-multi-sample-table}Table of the raw multi-sample
taxonomic table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Actinobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Apicomplexa
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ascomycota
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bacteroidetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Basidiomycota
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Candidatus Melainabacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Chlamydiae
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Chloroflexi
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cyanobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Deferribacteres
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fusobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lentisphaerae
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Planctomycetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Proteobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Spirochaetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Synergistetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tenericutes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verrucomicrobia
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sample\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Actinobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Apicomplexa
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ascomycota
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bacteroidetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Basidiomycota
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Candidatus Melainabacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Chlamydiae
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Chloroflexi
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cyanobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Deferribacteres
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fusobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lentisphaerae
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Planctomycetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Proteobacteria
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Spirochaetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Synergistetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tenericutes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verrucomicrobia
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sample\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
200 & 20.33151 & 0.0 & 0.0 & 28.42090 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
0.0 & \ldots{} & 0.0 & 0.00000 & 0.0 & 0.00000 & 0.00000 & 0.0 & 0.0 &
0.00000 & ERR5766177 & ancient \\
0 & 0.25809 & 0.0 & 0.0 & 0.00000 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
\ldots{} & 0.0 & 0.00000 & 0.0 & 4.49959 & 0.00000 & 0.0 & 0.0 & 0.00000
& de028ad4-7ae6-11e9-a106-68b59976a384 & westernized \\
1 & 10.27631 & 0.0 & 0.0 & 17.44332 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0
& \ldots{} & 0.0 & 0.01486 & 0.0 & 0.77098 & 0.00000 & 0.0 & 0.0 &
0.00784 & PNP\_Main\_283 & westernized \\
2 & 0.45187 & 0.0 & 0.0 & 82.86400 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
\ldots{} & 0.0 & 0.00000 & 0.0 & 0.05697 & 0.00000 & 0.0 & 0.0 & 0.00000
& PNP\_Validation\_55 & westernized \\
3 & 1.11902 & 0.0 & 0.0 & 69.99087 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
\ldots{} & 0.0 & 0.00000 & 0.0 & 4.07757 & 0.00000 & 0.0 & 0.0 & 1.99276
& G80275 & westernized \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
195 & 14.26365 & 0.0 & 0.0 & 38.59709 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
0.0 & \ldots{} & 0.0 & 0.00000 & 0.0 & 0.19699 & 0.00000 & 0.0 & 0.0 &
0.00000 & KHG\_9 & non\_westernized \\
196 & 5.47750 & 0.0 & 0.0 & 14.81828 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0
& \ldots{} & 0.0 & 0.00000 & 0.0 & 3.64655 & 0.09964 & 0.0 & 0.0 &
0.00000 & A48\_01\_1FE & non\_westernized \\
197 & 36.77145 & 0.0 & 0.0 & 10.13908 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
0.0 & \ldots{} & 0.0 & 0.00000 & 0.0 & 17.64151 & 0.00000 & 0.0 & 0.0 &
0.00000 & KHG\_1 & non\_westernized \\
198 & 1.16426 & 0.0 & 0.0 & 57.14031 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0
& \ldots{} & 0.0 & 0.00000 & 0.0 & 0.30580 & 0.70467 & 0.0 & 0.0 &
0.00000 & TZ\_81781 & non\_westernized \\
199 & 7.40894 & 0.0 & 0.0 & 11.61544 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0
& \ldots{} & 0.0 & 0.00000 & 0.0 & 56.20177 & 0.00000 & 0.0 & 0.0 &
0.00000 & A09\_01\_1FE & non\_westernized \\
\end{longtable}

Now, we need transform this (Table~\ref{tbl-raw-multi-sample-table}) in
the tidy format, with the \texttt{melt} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_phylums }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    all\_phylums}
\NormalTok{    .transpose()}
\NormalTok{    .merge(group\_info, left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{)}
\NormalTok{    .melt(id\_vars}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{], value\_name}\OperatorTok{=}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{, var\_name}\OperatorTok{=}\StringTok{\textquotesingle{}Phylum\textquotesingle{}}\NormalTok{, ignore\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we only want to keep the mean relative abundance for each
phylum. To do so, we will compute the mean relative abundance, for each
phylum, for each group (\texttt{ancient}, \texttt{westernized}, and
\texttt{non\_westernized}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_phylums }\OperatorTok{=}\NormalTok{ tidy\_phylums.groupby([}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Phylum\textquotesingle{}}\NormalTok{]).mean().reset\_index()}
\end{Highlighting}
\end{Shaded}

We then verify that the sum of the mean relative abundance is still
\textasciitilde100\%, as an extra sanity check.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_phylums.groupby(}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{].}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
group
ancient            100.000000
non_westernized     99.710255
westernized         99.905089
Name: relative_abundance, dtype: float64
\end{verbatim}

\hypertarget{lets-make-some-plots}{%
\section{Let's make some plots}\label{lets-make-some-plots}}

We first import plotnine

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ plotnine }\ImportTok{import} \OperatorTok{*}
\end{Highlighting}
\end{Shaded}

And then run plotnine to a barplot of the mean abundance per group
(Figure~\ref{fig-plotnine-phylum-abundance}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggplot(tidy\_phylums, aes(x}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}relative\_abundance\textquotesingle{}}\NormalTok{, fill}\OperatorTok{=}\StringTok{\textquotesingle{}Phylum\textquotesingle{}}\NormalTok{)) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ geom\_bar(position}\OperatorTok{=}\StringTok{\textquotesingle{}stack\textquotesingle{}}\NormalTok{, stat}\OperatorTok{=}\StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ ylab(}\StringTok{\textquotesingle{}mean abundance\textquotesingle{}}\NormalTok{) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ xlab(}\StringTok{""}\NormalTok{) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ theme\_classic()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_80_0.png}

}

\caption{\label{fig-plotnine-phylum-abundance}Stacked bar chart of
ancient, non-westernised, and westernised sample groups on the X axis
columns, and mean abundance percentage on the Y-axis. The legend and
stacks of the bar represent different phyla each with a different
colour}

\end{figure}

\hypertarget{ecological-diversity}{%
\section{Ecological diversity}\label{ecological-diversity}}

\hypertarget{alpha-diversity}{%
\subsection{Alpha diversity}\label{alpha-diversity}}

Alpha diversity is the measure of diversity withing each sample. It is
used to estimate how many species are present in a sample, and how
diverse they are.\\
We'll use the python library \href{http://scikit-bio.org/}{scikit-bio}
to compute it, and the \href{https://plotnine.readthedocs.io/}{plotnine}
library (a python port of
\href{https://ggplot2.tidyverse.org/reference/ggplot.html}{ggplot2} to
visualize the results).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ skbio}
\end{Highlighting}
\end{Shaded}

Let's compute the
\href{https://en.wikipedia.org/wiki/Species_richness}{species richness},
the
\href{https://www.biologydiscussion.com/biodiversity/types/2-types-of-diversity-indices-of-biodiversity/8388}{Shannon,
and Simpson index of diversity} index (Table~\ref{tbl-alpha-div})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shannon }\OperatorTok{=}\NormalTok{ skbio.diversity.alpha\_diversity(metric}\OperatorTok{=}\StringTok{\textquotesingle{}shannon\textquotesingle{}}\NormalTok{, counts}\OperatorTok{=}\NormalTok{all\_species.transpose(), ids}\OperatorTok{=}\NormalTok{all\_species.columns)}
\NormalTok{simpson }\OperatorTok{=}\NormalTok{ skbio.diversity.alpha\_diversity(metric}\OperatorTok{=}\StringTok{\textquotesingle{}simpson\textquotesingle{}}\NormalTok{, counts}\OperatorTok{=}\NormalTok{all\_species.transpose(), ids}\OperatorTok{=}\NormalTok{all\_species.columns)}
\NormalTok{richness }\OperatorTok{=}\NormalTok{ (all\_species }\OperatorTok{!=} \DecValTok{0}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{alpha\_diversity }\OperatorTok{=}\NormalTok{ (shannon.to\_frame(name}\OperatorTok{=}\StringTok{\textquotesingle{}shannon\textquotesingle{}}\NormalTok{)}
\NormalTok{                   .merge(simpson.to\_frame(name}\OperatorTok{=}\StringTok{\textquotesingle{}simpson\textquotesingle{}}\NormalTok{), left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{                   .merge(richness.to\_frame(name}\OperatorTok{=}\StringTok{\textquotesingle{}richness\textquotesingle{}}\NormalTok{), left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\NormalTok{alpha\_diversity}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-alpha-div}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5588}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1471}}@{}}
\caption{\label{tbl-alpha-div}Table of the shannon, simpson, and
richness alpha diversity indicies for a subset of
samples}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
shannon
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
simpson
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
richness
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
shannon
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
simpson
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
richness
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ERR5766177 & 3.032945 & 0.844769 & 21 \\
de028ad4-7ae6-11e9-a106-68b59976a384 & 0.798112 & 0.251280 & 11 \\
PNP\_Main\_283 & 5.092878 & 0.954159 & 118 \\
PNP\_Validation\_55 & 3.670162 & 0.812438 & 72 \\
G80275 & 3.831358 & 0.876712 & 66 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
KHG\_9 & 3.884285 & 0.861683 & 87 \\
A48\_01\_1FE & 4.377755 & 0.930024 & 53 \\
KHG\_1 & 3.733834 & 0.875335 & 108 \\
TZ\_81781 & 2.881856 & 0.719491 & 44 \\
A09\_01\_1FE & 2.982322 & 0.719962 & 75 \\
\end{longtable}

Let's load the group information from the metadata. To do so, we merge
the alpha diversity dataframe that we compute previously, with the
metadata dataframe, using the \texttt{sample\_id} as a merge key.
Finally, we do a bit of renaming to re-encode \texttt{yes}/\texttt{no}
as \texttt{non\_westernized}/\texttt{westernized}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha\_diversity }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alpha\_diversity}
\NormalTok{    .merge(metadata[[}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{]], left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, how}\OperatorTok{=}\StringTok{\textquotesingle{}outer\textquotesingle{}}\NormalTok{)}
\NormalTok{    .set\_index(}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{)}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{\})}
\NormalTok{)}
\NormalTok{alpha\_diversity[}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ alpha\_diversity[}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}yes\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}no\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{, pd.NA:}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{\})}

\NormalTok{alpha\_diversity}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-alpha-div-group}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\label{tbl-alpha-div-group}Table of the shannon, simpson, and
richness alpha diversity indicies for a subset of samples but with the
group metadata}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
shannon
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
simpson
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
richness
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
shannon
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
simpson
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
richness
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
sample\_id & & & & \\
ERR5766177 & 3.032945 & 0.844769 & 21 & ERR5766177 \\
de028ad4-7ae6-11e9-a106-68b59976a384 & 0.798112 & 0.251280 & 11 &
westernized \\
PNP\_Main\_283 & 5.092878 & 0.954159 & 118 & westernized \\
PNP\_Validation\_55 & 3.670162 & 0.812438 & 72 & westernized \\
G80275 & 3.831358 & 0.876712 & 66 & westernized \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
KHG\_9 & 3.884285 & 0.861683 & 87 & non\_westernized \\
A48\_01\_1FE & 4.377755 & 0.930024 & 53 & non\_westernized \\
KHG\_1 & 3.733834 & 0.875335 & 108 & non\_westernized \\
TZ\_81781 & 2.881856 & 0.719491 & 44 & non\_westernized \\
A09\_01\_1FE & 2.982322 & 0.719962 & 75 & non\_westernized \\
\end{longtable}

And as always, we need it in tidy format
(Table~\ref{tbl-alpha-div-group-long}) for plotnine.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha\_diversity }\OperatorTok{=}\NormalTok{ alpha\_diversity.melt(id\_vars}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{, value\_name}\OperatorTok{=}\StringTok{\textquotesingle{}alpha diversity\textquotesingle{}}\NormalTok{, var\_name}\OperatorTok{=}\StringTok{\textquotesingle{}diversity\_index\textquotesingle{}}\NormalTok{, ignore\_index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{alpha\_diversity}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-alpha-div-group-long}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4270}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1910}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1910}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1910}}@{}}
\caption{\label{tbl-alpha-div-group-long}Table of the shannon, simpson,
and richness alpha diversity indicies for a subset of samples but with
the group metadata but in long-form tidy format}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
diversity\_index
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alpha diversity
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
diversity\_index
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alpha diversity
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
sample\_id & & & \\
ERR5766177 & ERR5766177 & shannon & 3.032945 \\
de028ad4-7ae6-11e9-a106-68b59976a384 & westernized & shannon &
0.798112 \\
PNP\_Main\_283 & westernized & shannon & 5.092878 \\
PNP\_Validation\_55 & westernized & shannon & 3.670162 \\
G80275 & westernized & shannon & 3.831358 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
KHG\_9 & non\_westernized & richness & 87.000000 \\
A48\_01\_1FE & non\_westernized & richness & 53.000000 \\
KHG\_1 & non\_westernized & richness & 108.000000 \\
TZ\_81781 & non\_westernized & richness & 44.000000 \\
A09\_01\_1FE & non\_westernized & richness & 75.000000 \\
\end{longtable}

We now make a violin plot to compare the alpha diversity for each group,
faceted by the type of alpha diversity index
(Figure~\ref{fig-diversity-violin-plots}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OperatorTok{=}\NormalTok{ ggplot(alpha\_diversity, aes(x}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}alpha diversity\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{))}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ geom\_violin()}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ geom\_jitter()}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ theme\_classic()}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ facet\_wrap(}\StringTok{\textquotesingle{}\textasciitilde{}diversity\_index\textquotesingle{}}\NormalTok{, scales }\OperatorTok{=} \StringTok{\textquotesingle{}free\textquotesingle{}}\NormalTok{)}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ theme(axis\_text\_x}\OperatorTok{=}\NormalTok{element\_text(rotation}\OperatorTok{=}\DecValTok{45}\NormalTok{, hjust}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ scale\_color\_manual(\{}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#DB5F57\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#5F57DB\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#57DB5E\textquotesingle{}}\NormalTok{\})}
\NormalTok{g }\OperatorTok{+=}\NormalTok{ theme(subplots\_adjust}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}wspace\textquotesingle{}}\NormalTok{: }\FloatTok{0.15}\NormalTok{\})}
\NormalTok{g}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_91_1.png}

}

\caption{\label{fig-diversity-violin-plots}Three groups of violin plots
of an ancient sample, westernised samples and non-westernised samples
(x-axis) of the alpha diversity (y-axis) calculated for richness,
shannon and simpson alpha indicies}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Pause and think: Why do we observe a smaller species richness
and diversity in our sample ?}

\end{tcolorbox}

\hypertarget{beta-diversity}{%
\subsection{Beta diversity}\label{beta-diversity}}

The Beta diversity is the measure of diversity between a pair of
samples. It is used to compare the diversity between samples and see how
they relate.

We will compute the beta diversity using the
\href{https://en.wikipedia.org/wiki/Bray\%E2\%80\%93Curtis_dissimilarity}{bray-curtis}
dissimilarity

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_diversity }\OperatorTok{=}\NormalTok{ skbio.diversity.beta\_diversity(metric}\OperatorTok{=}\StringTok{\textquotesingle{}braycurtis\textquotesingle{}}\NormalTok{, counts}\OperatorTok{=}\NormalTok{all\_species.transpose(), ids}\OperatorTok{=}\NormalTok{all\_species.columns, validate}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We get a distance matrix

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(beta\_diversity)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
    201x201 distance matrix
    IDs:
    'ERR5766177', 'de028ad4-7ae6-11e9-a106-68b59976a384', 'PNP_Main_283', ...
    Data:
    [[0.         1.         0.81508134 ... 0.85716612 0.69790092 0.8303726 ]
     [1.         0.         0.99988327 ... 0.99853413 0.994116   0.99877258]
     [0.81508134 0.99988327 0.         ... 0.82311942 0.87202543 0.91363156]
     ...
     [0.85716612 0.99853413 0.82311942 ... 0.         0.84253376 0.76616679]
     [0.69790092 0.994116   0.87202543 ... 0.84253376 0.         0.82409272]
     [0.8303726  0.99877258 0.91363156 ... 0.76616679 0.82409272 0.        ]]
\end{verbatim}

\end{tcolorbox}

To visualize this distance matrix in a lower dimensional space, we'll
use a
\href{https://en.wikipedia.org/wiki/Multidimensional_scaling\#Types}{PCoA},
which is is a method very similar to a PCA, but taking a distance matrix
as input.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcoa }\OperatorTok{=}\NormalTok{ skbio.stats.ordination.pcoa(beta\_diversity)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see command output}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{verbatim}
    /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:143: RuntimeWarning:
    The result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues.
    If the negative ones are smaller, it's probably safe to ignore them, but if they are large in magnitude, the results won't be useful.
    See the Notes section for more details. The smallest eigenvalue is -0.25334842745723996 and the largest is 10.204440747987945.
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcoa.samples}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-alpha-div-group-long}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.1704}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0224}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0314}}@{}}
\caption{\label{tbl-alpha-div-group-long}Table principal coordinates
(columns) for each of the samples (rows)}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC192
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC193
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC194
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC195
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC196
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC197
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC198
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC199
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC200
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC201
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC192
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC193
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC194
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC195
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC196
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC197
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC198
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC199
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC200
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PC201
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ERR5766177 & 0.216901 & -0.039778 & 0.107412 & 0.273272 & 0.020540 &
0.114876 & -0.256332 & -0.151069 & 0.097451 & 0.060211 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
de028ad4-7ae6-11e9-a106-68b59976a384 & -0.099355 & 0.145224 & -0.191676
& 0.127626 & 0.119754 & -0.132209 & -0.097382 & 0.036728 & 0.081294 &
-0.056686 & \ldots{} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 &
0.0 & 0.0 \\
PNP\_Main\_283 & -0.214108 & -0.147466 & 0.116027 & 0.090059 & 0.076644
& 0.111536 & 0.092115 & 0.026477 & -0.006460 & -0.018592 & \ldots{} &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
PNP\_Validation\_55 & 0.244827 & -0.173996 & -0.311197 & -0.012836 &
0.031759 & 0.117548 & 0.148715 & -0.135641 & 0.034730 & -0.009395 &
\ldots{} & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
G80275 & -0.261358 & -0.077147 & -0.254374 & -0.065932 & 0.088538 &
0.165970 & -0.005260 & -0.028739 & -0.002016 & 0.015719 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} &
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
KHG\_9 & 0.296057 & -0.150300 & 0.013941 & 0.032649 & -0.147692 &
0.019663 & -0.063120 & -0.034453 & -0.073514 & 0.070085 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
A48\_01\_1FE & 0.110621 & 0.030971 & 0.154231 & -0.185961 & -0.008512 &
-0.103420 & 0.028169 & -0.044530 & 0.041902 & 0.068597 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
KHG\_1 & -0.100009 & 0.167885 & 0.009915 & 0.076842 & -0.405582 &
-0.039111 & -0.006421 & -0.009774 & -0.072252 & 0.150000 & \ldots{} &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
TZ\_81781 & 0.405716 & -0.139297 & -0.075026 & -0.079716 & -0.053264 &
-0.119271 & 0.068261 & -0.018821 & 0.198152 & -0.012792 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
A09\_01\_1FE & 0.089101 & 0.471135 & 0.069629 & -0.125644 & -0.036793 &
0.115151 & 0.060507 & -0.000912 & -0.027239 & -0.138436 & \ldots{} & 0.0
& 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\end{longtable}

Let's look at the variance explained by the first axes by using a scree
plot (Figure~\ref{fig-scree-plot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_explained }\OperatorTok{=}\NormalTok{ pcoa.proportion\_explained[:}\DecValTok{9}\NormalTok{].to\_frame(name}\OperatorTok{=}\StringTok{\textquotesingle{}variance explained\textquotesingle{}}\NormalTok{).reset\_index().rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}index\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}PC\textquotesingle{}}\NormalTok{\})}

\NormalTok{ggplot(var\_explained, aes(x}\OperatorTok{=}\StringTok{\textquotesingle{}PC\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}variance explained\textquotesingle{}}\NormalTok{, group}\OperatorTok{=}\DecValTok{1}\NormalTok{)) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ geom\_point() }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ geom\_line() }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ theme\_classic()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_102_0.png}

}

\caption{\label{fig-scree-plot}Scree plot describing the variance
explained (Y-axis), for each Principal Componanent (X-axis), with a
curved line from PC1 having highest variance to lowest on PC9.}

\end{figure}

In this scree plot, we're looking for the ``elbow'', where there is a
drop in the slope. Here, it seems that most of the variance is captures
by the 3 first principal components

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcoa\_embed }\OperatorTok{=}\NormalTok{ pcoa.samples[[}\StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{]].rename\_axis(}\StringTok{\textquotesingle{}sample\textquotesingle{}}\NormalTok{).reset\_index()}

\NormalTok{pcoa\_embed }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    pcoa\_embed}
\NormalTok{    .merge(metadata[[}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{]], left\_on}\OperatorTok{=}\StringTok{\textquotesingle{}sample\textquotesingle{}}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, how}\OperatorTok{=}\StringTok{\textquotesingle{}outer\textquotesingle{}}\NormalTok{)}
\NormalTok{    .drop(}\StringTok{\textquotesingle{}sample\_id\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{\})}
\NormalTok{)}

\NormalTok{pcoa\_embed[}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ pcoa\_embed[}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}yes\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}no\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{, pd.NA:}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Let's first look at these components with 2D plots
(Figure~\ref{fig-poca-1}, Figure~\ref{fig-poca-2})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggplot(pcoa\_embed, aes(x}\OperatorTok{=}\StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{)) }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ geom\_point() }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ theme\_classic() }\OperatorTok{\textbackslash{}}
\OperatorTok{+}\NormalTok{ scale\_color\_manual(\{}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#DB5F57\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#5F57DB\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#57DB5E\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_107_0.png}

}

\caption{\label{fig-poca-1}Principal Coordinate Analysis plot of PC1
(X-axis) and PC2 (Y-axis), with three groups of points in the scatter
plot - blue circles of westernised data points in the bottom left,
overlapping with green circles of non-westernised datapoints in the top
right, and the single ancient sample as a red circle falling in between
the two on the right of the overlap}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggplot(pcoa\_embed, aes(x}\OperatorTok{=}\StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{)) }\OperatorTok{+}
\NormalTok{geom\_point() }\OperatorTok{+}
\NormalTok{theme\_classic() }\OperatorTok{+}
\NormalTok{scale\_color\_manual(\{}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#DB5F57\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#5F57DB\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#57DB5E\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_108_0.png}

}

\caption{\label{fig-poca-2}Principal Coordinate Analysis plot of PC1
(X-axis) and PC3 (Y-axis), a similar overlap between
westernised/non-westernised individuals and position of the ancient
sample as in the PC1-PC2 PCoA, however this time in a horseshoe shape
from bottom left for the westernised data points, curving up to the top
of PC3 at a peak, and then falling again at the top of PC1}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Expand to see additional visualisations}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

You can also plot the data above as a with a 3d plot if you were to run
the following command

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.express }\ImportTok{as}\NormalTok{ px}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ px.scatter\_3d(pcoa\_embed, x}\OperatorTok{=}\StringTok{"PC1"}\NormalTok{, y}\OperatorTok{=}\StringTok{"PC2"}\NormalTok{, z}\OperatorTok{=}\StringTok{"PC3"}\NormalTok{,}
\NormalTok{                  color }\OperatorTok{=} \StringTok{"group"}\NormalTok{,}
\NormalTok{                  color\_discrete\_map}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#DB5F57\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#5F57DB\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#57DB5E\textquotesingle{}}\NormalTok{\},}
\NormalTok{                  hover\_name}\OperatorTok{=}\StringTok{"sample"}\NormalTok{)}
\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Pause and think: How do you think this embedding represents how
our sample relates to modern reference samples ?}

\end{tcolorbox}

Finally, we can also visualize this distance matrix using a clustered
heatmap, where pairs of sample with a small beta diversity are clustered
together (Figure~\ref{fig-heatmap}).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ scipy.spatial }\ImportTok{as}\NormalTok{ sp, scipy.cluster.hierarchy }\ImportTok{as}\NormalTok{ hc}
\end{Highlighting}
\end{Shaded}

We set the color in seaborn to match the color palette we've used so
far.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcoa\_embed[}\StringTok{\textquotesingle{}colour\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ pcoa\_embed[}\StringTok{\textquotesingle{}group\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(\{}\StringTok{\textquotesingle{}ERR5766177\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#DB5F57\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#5F57DB\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}non\_westernized\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}\#57DB5E\textquotesingle{}}\NormalTok{\})}

\NormalTok{linkage }\OperatorTok{=}\NormalTok{ hc.linkage(sp.distance.squareform(beta\_diversity.to\_data\_frame()), method}\OperatorTok{=}\StringTok{\textquotesingle{}average\textquotesingle{}}\NormalTok{)}

\NormalTok{sns.clustermap(}
\NormalTok{    beta\_diversity.to\_data\_frame(),}
\NormalTok{    row\_linkage}\OperatorTok{=}\NormalTok{linkage,}
\NormalTok{    col\_linkage}\OperatorTok{=}\NormalTok{linkage,}
\NormalTok{    row\_colors }\OperatorTok{=}\NormalTok{ pcoa\_embed[}\StringTok{\textquotesingle{}colour\textquotesingle{}}\NormalTok{].to\_list()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/taxonomic-profiling/analysis_115_1.png}

}

\caption{\label{fig-heatmap}Sample-by-sample clustered heatmap, with
tree representation of the clustering on the left and top of the
heatmap}

\end{figure}

\hypertarget{references-2}{%
\section{References}\label{references-2}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\hypertarget{functional-profiling-1}{%
\chapter{Functional Profiling}\label{functional-profiling-1}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/functional-profiling.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate functional{-}profiling}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-9}{%
\section{Lecture}\label{lecture-9}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/5c-intro-to-functional-analysis/SPAAM\%20Summer\%20School\%202022\%20-\%205C\%20-\%20Function\%20Analysis.pdf}{here}.

\hypertarget{preparation}{%
\section{Preparation}\label{preparation}}

The data and conda environment \texttt{.yaml} file for this practical
session can be downloaded from here:
\url{https://doi.org/10.5281/zenodo.6983188}. See instructions on page.

Change into the session directory

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/functional{-}genomics/}
\end{Highlighting}
\end{Shaded}

Load the conda environment.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate phylogenomics{-}functional}
\end{Highlighting}
\end{Shaded}

Open R Studio from within the conda environment, and we can load the
required libraries for this walkthrough.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mixOmics) }\DocumentationTok{\#\# For PCA generation}

\DocumentationTok{\#\# Utility packages (pretty stuff)}
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(gplots)}
\FunctionTok{library}\NormalTok{(ggrepel)}
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(patchwork)}
\end{Highlighting}
\end{Shaded}

\hypertarget{humann3-pathways}{%
\section{HUMAnN3 Pathways}\label{humann3-pathways}}

First, we need to run HUMMAn3 to align reads against gene databases and
convert to gene family names counts.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

We will not run HUMANn3 here as it requires very large databases and
takes a long time to run, so we will give you the commands you normally
would run but we provide with you pre-made results files before you (see
below).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# DO NOT RUN!}

\CommentTok{\# run humann3}
\ExtensionTok{humann3} \AttributeTok{{-}{-}input}\NormalTok{ file.fastq }\AttributeTok{{-}{-}output}\NormalTok{ output }\AttributeTok{{-}{-}threads} \OperatorTok{\textless{}}\NormalTok{threads}\OperatorTok{\textgreater{}}

\CommentTok{\# join all output tables (can do for both gene and pathways)}
\ExtensionTok{humann\_join\_tables} \AttributeTok{{-}i}\NormalTok{ output/ }\AttributeTok{{-}o}\NormalTok{ genefamilies\_joined.tsv }\AttributeTok{{-}{-}file\_name}\NormalTok{ unmapped\_genefamilies}

\CommentTok{\# normalize the output (here by tss {-} total sum scaling, can do for both gene and pathways)}
\ExtensionTok{humann\_renorm\_table} \AttributeTok{{-}{-}input}\NormalTok{ genefamilies\_joined.tsv }\AttributeTok{{-}{-}output}\NormalTok{ genefamilies\_joined\_cpm.tsv }\AttributeTok{{-}{-}units}\NormalTok{ tss}

\CommentTok{\# regroup the table to combine gene families (standardise gene family IDs across taxa)}
\ExtensionTok{humann\_regroup\_table} \AttributeTok{{-}{-}input}\NormalTok{ genefamilies\_joined\_cpm.tsv }\AttributeTok{{-}{-}output}\NormalTok{ genefamilies\_joined\_cpm\_ur90rxn.tsv }\AttributeTok{{-}{-}groups}\NormalTok{ uniref90\_rxn}

\CommentTok{\# give the gene families names}
\ExtensionTok{humann\_rename\_table} \AttributeTok{{-}{-}input}\NormalTok{ genefamilies\_joined\_cpm\_ur90rxn.tsv }\AttributeTok{{-}{-}output}\NormalTok{ genefamilies\_joined\_cpm\_ur90rxn\_names.tsv }\AttributeTok{{-}n}\NormalTok{ metacyc{-}rxn}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{humann3-tables}{%
\section{humann3 tables}\label{humann3-tables}}

First lets load a pre-made pathway abundance file

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# load the species and genus tables generated with humann3}
\NormalTok{humann3\_path\_full }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"./pathabundance\_joined\_cpm.tsv"}\NormalTok{)}
\NormalTok{humann3\_path\_full }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(humann3\_path\_full)}

\CommentTok{\# clean the file names}
\NormalTok{humann3\_path\_full }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(humann3\_path\_full, }\AttributeTok{Pathway =} \StringTok{\textasciigrave{}}\AttributeTok{\# Pathway}\StringTok{\textasciigrave{}}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(humann3\_path\_full) }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{".unmapped\_Abundance"}\NormalTok{,}\StringTok{""}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(humann3\_path\_full))}
\FunctionTok{colnames}\NormalTok{(humann3\_path\_full) }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{".SG1"}\NormalTok{,}\StringTok{""}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(humann3\_path\_full))}

\CommentTok{\# remove unmapped and ungrouped reads}
\NormalTok{humann3\_path }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_full }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(Pathway, }\StringTok{"UNMAPPED|UNINTEGRATED"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Then lets load associated sample metadata to help make it easier for
comparative analysis and make actual informative inferences.

The data being used in this session, is from
\href{https://doi.org/10.1093/pnasnexus/pgac148}{Velsko et al.~2022
(PNAS Nexus)}, where we tried to find associations between dental
pathologies and taxonomic and genome content. We had a large skeletal
collection from a single site in the Netherlands, with a lot of
osteological metadata. The study aimed to see if there were any links
between the oral microbiome and groups of dental pathologies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the metadata file}
\NormalTok{full\_metadata }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"full\_combined\_metadata.tsv"}\NormalTok{)}


\DocumentationTok{\#\# Example of metadata}
\FunctionTok{tibble}\NormalTok{(full\_metadata }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(Site\_code }\SpecialCharTok{==} \StringTok{"MID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(Site, Time\_period, Library\_ID, Sequencing\_instrument, Pipenotch, Max\_Perio\_Score, }\StringTok{\textasciigrave{}}\AttributeTok{\%teeth\_with\_caries}\StringTok{\textasciigrave{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

First step: we can pre-define various functions for generate PCAs we
will use downstream - you don't have to worry about these too much they
are just custom functions to quickly plot PCAs from a \texttt{mixOmics}
PCA output object with ggplot, but we leave the code here for if you're
curious.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot PCA with colored dots and the title including the \# of species or genera}
\NormalTok{plot\_pca }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, pc1, pc2, color\_group, shape\_group, ncomps) \{}
\NormalTok{    metadata\_group\_colors }\OtherTok{\textless{}{-}} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(color\_group, }\StringTok{"\_colors"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{))}
\NormalTok{    metadata\_group\_shapes }\OtherTok{\textless{}{-}} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(shape\_group, }\StringTok{"\_shapes"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{))}

\NormalTok{    pca.list }\OtherTok{\textless{}{-}}\NormalTok{ mixOmics}\SpecialCharTok{::}\FunctionTok{pca}\NormalTok{(df, }\AttributeTok{ncomp =}\NormalTok{ ncomps, }\AttributeTok{logratio =} \StringTok{\textquotesingle{}CLR\textquotesingle{}}\NormalTok{)}

    \DocumentationTok{\#\# Pull out loadings}
\NormalTok{    exp\_var }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{round}\NormalTok{(pca.list}\SpecialCharTok{$}\NormalTok{explained\_variance }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{    df\_X }\OtherTok{\textless{}{-}}\NormalTok{ pca.list}\SpecialCharTok{$}\NormalTok{variates}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{inner\_join}\NormalTok{(full\_metadata, }\AttributeTok{by =} \StringTok{"Library\_ID"}\NormalTok{)}

\NormalTok{    color\_group }\OtherTok{=}\NormalTok{ df\_X[[color\_group]]}
\NormalTok{    shape\_group }\OtherTok{=}\NormalTok{ df\_X[[shape\_group]]}

    \DocumentationTok{\#\# Selecting which PCs to plot}
    \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{    \}  }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{    \}}

    \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{    \}  }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{    \}}

    \DocumentationTok{\#\# Generate figure}
\NormalTok{    pca\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df\_X, }\FunctionTok{aes}\NormalTok{(pc1, pc2)) }\SpecialCharTok{+}
     \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ color\_group, }\AttributeTok{shape =}\NormalTok{ shape\_group), }\AttributeTok{size =} \FloatTok{4.5}\NormalTok{, }\AttributeTok{stroke =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ metadata\_group\_colors) }\SpecialCharTok{+}
     \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ metadata\_group\_shapes) }\SpecialCharTok{+}
     \CommentTok{\# stat\_ellipse() +}
     \FunctionTok{xlab}\NormalTok{(}\FunctionTok{paste}\NormalTok{(xaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc1)) }\SpecialCharTok{+}
     \FunctionTok{ylab}\NormalTok{(}\FunctionTok{paste}\NormalTok{(yaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc2)) }\SpecialCharTok{+}
     \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{16}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(),}
           \AttributeTok{legend.key.size =} \FunctionTok{unit}\NormalTok{(}\DecValTok{2}\NormalTok{,}\StringTok{"mm"}\NormalTok{),}
           \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"top"}\NormalTok{)}

    \FunctionTok{return}\NormalTok{(pca\_plot)}
\NormalTok{\}}

\CommentTok{\# for continuous data}
\NormalTok{plot\_pca\_cont }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, pc1, pc2, color\_group, shape\_group, ncomps, title\_text) \{}

\NormalTok{    pca.list }\OtherTok{\textless{}{-}}\NormalTok{ mixOmics}\SpecialCharTok{::}\FunctionTok{pca}\NormalTok{(df, }\AttributeTok{ncomp =}\NormalTok{ ncomps, }\AttributeTok{logratio =} \StringTok{\textquotesingle{}CLR\textquotesingle{}}\NormalTok{)}

\NormalTok{    exp\_var }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{round}\NormalTok{(pca.list}\SpecialCharTok{$}\NormalTok{explained\_variance }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{    df\_X }\OtherTok{\textless{}{-}}\NormalTok{ pca.list}\SpecialCharTok{$}\NormalTok{variates}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{inner\_join}\NormalTok{(full\_metadata, }\AttributeTok{by =} \StringTok{"Library\_ID"}\NormalTok{)}

\NormalTok{    color\_group }\OtherTok{=}\NormalTok{ df\_X[[color\_group]]}
\NormalTok{    shape\_group }\OtherTok{=}\NormalTok{ df\_X[[shape\_group]]}

    \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{    \}  }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{    \}}

    \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{    \}  }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{    \}}

\NormalTok{    pca\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df\_X, }\FunctionTok{aes}\NormalTok{(pc1, pc2, }\AttributeTok{fill =}\NormalTok{ color\_group, }\AttributeTok{shape =}\NormalTok{ shape\_group)) }\SpecialCharTok{+}
     \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{scale\_fill\_viridis\_c}\NormalTok{(}\AttributeTok{option =} \StringTok{"C"}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{24}\NormalTok{,}\DecValTok{21}\NormalTok{)) }\SpecialCharTok{+}
     \CommentTok{\# stat\_ellipse() +}
     \FunctionTok{xlab}\NormalTok{(}\FunctionTok{paste}\NormalTok{(xaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc1)) }\SpecialCharTok{+}
     \FunctionTok{ylab}\NormalTok{(}\FunctionTok{paste}\NormalTok{(yaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc2)) }\SpecialCharTok{+}
     \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{16}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(),}
           \AttributeTok{legend.key.size =} \FunctionTok{unit}\NormalTok{(}\DecValTok{2}\NormalTok{,}\StringTok{"mm"}\NormalTok{),}
           \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"top"}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{ggtitle}\NormalTok{(title\_text) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{))}

    \FunctionTok{return}\NormalTok{(pca\_plot)}
\NormalTok{\}}

\NormalTok{plot\_pca\_bi }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, pc1, pc2, metadata\_group, columntitle) \{}
\NormalTok{    metadata\_group\_colors }\OtherTok{\textless{}{-}} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(metadata\_group, }\StringTok{"\_colors"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{))}
\NormalTok{    metadata\_group\_shapes }\OtherTok{\textless{}{-}} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(metadata\_group, }\StringTok{"\_shapes"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{))}

\NormalTok{    arrow\_pc }\OtherTok{\textless{}{-}} \FunctionTok{enquo}\NormalTok{(columntitle)}

\NormalTok{    exp\_var }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{round}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{explained\_variance }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{) }\CommentTok{\# explained variance for x{-} and y{-}labels}

    \CommentTok{\# select only the PCs from the PCA and add metadata}
\NormalTok{    df\_X }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{variates}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{inner\_join}\NormalTok{(full\_metadata, }\AttributeTok{by =} \StringTok{"Library\_ID"}\NormalTok{)}

\NormalTok{    metadata\_group }\OtherTok{=}\NormalTok{ df\_X[[metadata\_group]]}

\NormalTok{    corr\_lam }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{sdev[}\FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{, }\StringTok{"PC2"}\NormalTok{, }\StringTok{"PC3"}\NormalTok{)] }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df\_X))}

\NormalTok{    df\_X }\OtherTok{\textless{}{-}}\NormalTok{ df\_X }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{PC1 =}\NormalTok{ PC1 }\SpecialCharTok{/}\NormalTok{ corr\_lam[}\DecValTok{1}\NormalTok{],}
             \AttributeTok{PC2 =}\NormalTok{ PC2 }\SpecialCharTok{/}\NormalTok{ corr\_lam[}\DecValTok{2}\NormalTok{],}
             \AttributeTok{PC3 =}\NormalTok{ PC3 }\SpecialCharTok{/}\NormalTok{ corr\_lam[}\DecValTok{3}\NormalTok{])}

    \CommentTok{\# select the correct PC column and explained variance for PC1}
    \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        Pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        Pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc1 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        Pc1 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{        exp\_var\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{        xaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{   \}}

    \CommentTok{\# select the correct PC column and explained variance for PC2}
    \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC1\textquotesingle{}}\NormalTok{) \{}
\NormalTok{        Pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC1}
\NormalTok{        exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{1}\NormalTok{]}
\NormalTok{        yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{)}
\NormalTok{ \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC2\textquotesingle{}}\NormalTok{) \{}
\NormalTok{       Pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC2}
\NormalTok{       exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{2}\NormalTok{]}
\NormalTok{       yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (pc2 }\SpecialCharTok{==} \StringTok{\textquotesingle{}PC3\textquotesingle{}}\NormalTok{) \{}
\NormalTok{       Pc2 }\OtherTok{\textless{}{-}}\NormalTok{ df\_X}\SpecialCharTok{$}\NormalTok{PC3}
\NormalTok{       exp\_var\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ exp\_var[}\DecValTok{3}\NormalTok{]}
\NormalTok{       yaxis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"PC3"}\NormalTok{)}
\NormalTok{   \}}

    \CommentTok{\# Identify the 10 pathways that have highest positive and negative loadings in the selected PC}
\NormalTok{    pws\_10 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{loadings}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{as.data.frame}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"Pathway"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{separate}\NormalTok{(Pathway, }\AttributeTok{into =} \StringTok{"Pathway"}\NormalTok{, }\AttributeTok{sep =} \StringTok{":"}\NormalTok{, }\AttributeTok{extra =} \StringTok{"drop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, }\SpecialCharTok{!!}\NormalTok{arrow\_pc)}

\NormalTok{    neg\_10 }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{loadings}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{as.data.frame}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"Pathway"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{separate}\NormalTok{(Pathway, }\AttributeTok{into =} \StringTok{"Pathway"}\NormalTok{, }\AttributeTok{sep =} \StringTok{":"}\NormalTok{, }\AttributeTok{extra =} \StringTok{"drop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{top\_n}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\SpecialCharTok{!!}\NormalTok{arrow\_pc)}


\NormalTok{    pca\_plot\_bi }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df\_X, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Pc1, }\AttributeTok{y =}\NormalTok{ Pc2)) }\SpecialCharTok{+}
      \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \FloatTok{3.5}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{shape =}\NormalTok{ metadata\_group, }\AttributeTok{fill =}\NormalTok{ metadata\_group))}\SpecialCharTok{+}
      \FunctionTok{geom\_segment}\NormalTok{(}\AttributeTok{data =}\NormalTok{ pws\_10,}
                   \FunctionTok{aes}\NormalTok{(}\AttributeTok{xend =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc1)), }\AttributeTok{yend =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc2))),}
                   \AttributeTok{x =} \DecValTok{0}\NormalTok{, }\AttributeTok{y =} \DecValTok{0}\NormalTok{, }\AttributeTok{colour =} \StringTok{"black"}\NormalTok{,}
                   \AttributeTok{size =} \FloatTok{0.5}\NormalTok{,}
                   \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.03}\NormalTok{, }\StringTok{"npc"}\NormalTok{))) }\SpecialCharTok{+}
      \FunctionTok{geom\_label\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ pws\_10,}
                   \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc1)), }\AttributeTok{y =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc2)), }\AttributeTok{label =}\NormalTok{ Pathway),}
                   \AttributeTok{size =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{colour =} \StringTok{"grey20"}\NormalTok{, }\AttributeTok{label.padding =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{force =} \DecValTok{5}\NormalTok{, }\AttributeTok{max.overlaps =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{geom\_segment}\NormalTok{(}\AttributeTok{data =}\NormalTok{ neg\_10,}
                   \FunctionTok{aes}\NormalTok{(}\AttributeTok{xend =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc1)), }\AttributeTok{yend =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc2))),}
                   \AttributeTok{x =} \DecValTok{0}\NormalTok{, }\AttributeTok{y =} \DecValTok{0}\NormalTok{, }\AttributeTok{colour =} \StringTok{"grey50"}\NormalTok{,}
                   \AttributeTok{size =} \FloatTok{0.5}\NormalTok{,}
                   \AttributeTok{arrow =} \FunctionTok{arrow}\NormalTok{(}\AttributeTok{length =} \FunctionTok{unit}\NormalTok{(}\FloatTok{0.03}\NormalTok{, }\StringTok{"npc"}\NormalTok{))) }\SpecialCharTok{+}
      \FunctionTok{geom\_label\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ neg\_10,}
                   \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc1)), }\AttributeTok{y =} \FunctionTok{get}\NormalTok{(}\FunctionTok{paste}\NormalTok{(pc2)), }\AttributeTok{label =}\NormalTok{ Pathway),}
                   \AttributeTok{size =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{colour =} \StringTok{"grey20"}\NormalTok{, }\AttributeTok{label.padding =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{max.overlaps =} \DecValTok{12}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \FunctionTok{paste}\NormalTok{(xaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc1),}
           \AttributeTok{y =} \FunctionTok{paste}\NormalTok{(yaxis, }\StringTok{" {-} "}\NormalTok{, exp\_var\_pc2)) }\SpecialCharTok{+}
      \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ metadata\_group\_colors) }\SpecialCharTok{+}
      \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ metadata\_group\_shapes) }\SpecialCharTok{+}
      \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{16}\NormalTok{)) }\SpecialCharTok{+}
      \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{16}\NormalTok{)) }\SpecialCharTok{+}
      \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"top"}\NormalTok{)}

    \FunctionTok{return}\NormalTok{(pca\_plot\_bi)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As we are dealing with aDNA, and we often have bad samples, its
sometimes interesting to see differences between well/badly preserved
samples at all stages of analysis.

Therefore we may generate results for all samples. However for actual
analysis where we want to intepret biological differences, should
exclude outliers (in this case highly contaminated samples - as
identified by the \texttt{decontam} package - see
\href{https://doi.org/10.1093/pnasnexus/pgac148}{Velsko et al.~2022
\_PNAS Nexus} for more details).

We can make a list the outliers from the previous authentication
analyses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outliers\_mpa3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"EXB059.A2101"}\NormalTok{,}\StringTok{"EXB059.A2501"}\NormalTok{,}\StringTok{"EXB015.A3301"}\NormalTok{,}\StringTok{"EXB034.A2701"}\NormalTok{,}
                   \StringTok{"EXB059.A2201"}\NormalTok{,}\StringTok{"EXB059.A2301"}\NormalTok{,}\StringTok{"EXB059.A2401"}\NormalTok{,}\StringTok{"LIB058.A0103"}\NormalTok{,}\StringTok{"LIB058.A0106"}\NormalTok{,}\StringTok{"LIB058.A0104"}\NormalTok{)}
\NormalTok{poor\_samples\_mpa3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"CS28"}\NormalTok{,}\StringTok{"CS38"}\NormalTok{,}\StringTok{"CSN"}\NormalTok{,}\StringTok{"ELR003.A0101"}\NormalTok{,}\StringTok{"ELR010.A0101"}\NormalTok{,}
                       \StringTok{"KT09calc"}\NormalTok{,}\StringTok{"MID024.A0101"}\NormalTok{,}\StringTok{"MID063.A0101"}\NormalTok{,}\StringTok{"MID092.A0101"}\NormalTok{)}

\NormalTok{outliersF }\OtherTok{\textless{}{-}} \FunctionTok{str\_c}\NormalTok{(outliers\_mpa3, }\AttributeTok{collapse =} \StringTok{"|"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sample-clustering-with-pca}{%
\section{Sample Clustering with PCA}\label{sample-clustering-with-pca}}

\hypertarget{pathway-abundance-analyses}{%
\subsection{Pathway abundance
analyses}\label{pathway-abundance-analyses}}

Once we've removed outlier samples, our first simple question is - what
is the functional relationships of the groups?

Can we already see distinctive patterns between the different groups in
our dataset?

To do this lets clean up the data a bit (cleaning names, removing
samples with no metadata etc.), normalise (via a `centered-log-ratio'
transform ), and run a PCA.

Once we've done this we should always check our PCA's Scree plot first.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{humann3\_path\_l1 }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(Pathway, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# no full\_metadata, remove these}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"MID025.A0101"}\NormalTok{,}\StringTok{"MID033.A0101"}\NormalTok{,}\StringTok{"MID052.A0101"}\NormalTok{,}\StringTok{"MID056.A0101"}\NormalTok{,}
            \StringTok{"MID065.A0101"}\NormalTok{,}\StringTok{"MID068.A0101"}\NormalTok{,}\StringTok{"MID076.A0101"}\NormalTok{,}\StringTok{"MID078.A0101"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# remove poorly preserved saples}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"MID024.A0101"}\NormalTok{,}\StringTok{"MID063.A0101"}\NormalTok{,}\StringTok{"MID092.A0101"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{matches}\NormalTok{(}\StringTok{"EXB|LIB"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# inner\_join(., humann3\_path.decontam\_noblanks\_presence\_more\_30, by = "Pathway") \%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{,}\StringTok{"Counts"}\NormalTok{,}\DecValTok{2}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Counts =}\NormalTok{ Counts }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{spread}\NormalTok{(Pathway,Counts) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{column\_to\_rownames}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{)}

\CommentTok{\# prepare to run a PCA}
\CommentTok{\# check the number of components to retain by tuning the PCA}
\NormalTok{mixOmics}\SpecialCharTok{::}\FunctionTok{tune.pca}\NormalTok{(humann3\_path\_l1, }\AttributeTok{logratio =} \StringTok{\textquotesingle{}CLR\textquotesingle{}}\NormalTok{)}


\NormalTok{humann3\_all\_otu.pca }\OtherTok{\textless{}{-}}\NormalTok{ mixOmics}\SpecialCharTok{::}\FunctionTok{pca}\NormalTok{(humann3\_path\_l1, }\AttributeTok{ncomp =} \DecValTok{3}\NormalTok{, }\AttributeTok{logratio =} \StringTok{\textquotesingle{}CLR\textquotesingle{}}\NormalTok{)}
\NormalTok{humann3\_all\_pca\_values }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_all\_otu.pca}\SpecialCharTok{$}\NormalTok{variates}\SpecialCharTok{$}\NormalTok{X }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Library\_ID"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(., full\_metadata, }\AttributeTok{by =} \StringTok{"Library\_ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can see the first couple of PCs in the scree plot account for a good
chunk of the variation of our dataset, so lets visualise the PCA itself.

We visualise the PCA with one of our custom functions defined above, and
colour by the Pipe notch metadata column.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pipenotch colors/shapes}
\NormalTok{Pipenotch\_colors }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"\#311068"}\NormalTok{,}\StringTok{"\#C83E73"}\NormalTok{)}
\NormalTok{Pipenotch\_shapes }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{)}

\CommentTok{\# by minimum number of pipenotches}
\NormalTok{pipenotch }\OtherTok{\textless{}{-}} \FunctionTok{plot\_pca\_cont}\NormalTok{(humann3\_path\_l1, }\StringTok{"PC1"}\NormalTok{, }\StringTok{"PC2"}\NormalTok{,}\StringTok{"Min\_no\_Pipe\_notches"}\NormalTok{,}\StringTok{"Pipenotch"}\NormalTok{, }\DecValTok{3}\NormalTok{,}\StringTok{"Min. No. Pipe Notches"}\NormalTok{)}
\NormalTok{pipenotch}
\end{Highlighting}
\end{Shaded}

We can see there is a slight separation between the groups, but how do
we find out which pathways are maybe driving this pattern?

For this we can generate a PCA bi-plot which show what loadings are
driving the spread of the samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pipenotch\_colors }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"\#311068"}\NormalTok{,}\StringTok{"\#C83E73"}\NormalTok{)}
\NormalTok{Pipenotch\_shapes }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{24}\NormalTok{,}\DecValTok{21}\NormalTok{)}

\NormalTok{biplot }\OtherTok{\textless{}{-}} \FunctionTok{plot\_pca\_bi}\NormalTok{(humann3\_all\_otu.pca, }\StringTok{"PC1"}\NormalTok{, }\StringTok{"PC2"}\NormalTok{, }\StringTok{"Pipenotch"}\NormalTok{, PC1)}
\NormalTok{biplot}
\end{Highlighting}
\end{Shaded}

From the biplot we can see which pathways are differentiating along PC1.

We can pull these IDs out to find out what pathways there are from the
biplot object itself.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make a table of the pathways to save, to use again later in another R notebook}
\NormalTok{humann3\_pathway\_biplot\_list }\OtherTok{\textless{}{-}}\NormalTok{ biplot}\SpecialCharTok{$}\NormalTok{plot\_env}\SpecialCharTok{$}\NormalTok{pws\_10 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(PC1)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Pathway, PC1, PC2) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Direction =} \StringTok{"PC1+"}\NormalTok{)}
\NormalTok{humann3\_pathway\_biplot\_list }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(biplot}\SpecialCharTok{$}\NormalTok{plot\_env}\SpecialCharTok{$}\NormalTok{neg\_10 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(PC1)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Pathway, PC1, PC2)}\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Direction =} \StringTok{"PC1{-}"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{species-contributions-to-pathways}{%
\subsection{Species contributions to
pathways}\label{species-contributions-to-pathways}}

However, this ID numbers aren't very informative to us. At this point we
have to do a bit of literature review/database scraping to pull the
human-readable names/descriptions of the IDs - which we have already
done for you.

We can load these back into our environment

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# PC biplot loading top 10s}
\NormalTok{humann3\_pathway\_biplot\_list }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"./humann3\_pathway\_biplot\_list.tsv"}\NormalTok{)}
\NormalTok{humann3\_pathway\_biplot\_list }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Pathway =}\NormalTok{ pathway) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Path =} \FunctionTok{sapply}\NormalTok{(Pathway, }\ControlFlowTok{function}\NormalTok{(f) \{}
                                  \FunctionTok{unlist}\NormalTok{(}\FunctionTok{str\_split}\NormalTok{(f, }\StringTok{":"}\NormalTok{))[}\DecValTok{1}\NormalTok{]}
\NormalTok{                                  \})) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Pathway, Path, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# remove 3 of the 4 ubiquinol pathways w/identical loadings}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(Pathway, }\StringTok{"5856|5857|6708"}\NormalTok{))}

\FunctionTok{tibble}\NormalTok{(humann3\_pathway\_biplot\_list)}
\end{Highlighting}
\end{Shaded}

We now have the pathway ID, and a pathway description for each of the
loadings of the PCA.

\hypertarget{pc1}{%
\paragraph{PC1}\label{pc1}}

While we have the pathways, we don't \emph{who} contributed these.

For this, we can join our pathway table back onto the original output
from HUMANn3 we loaded at the beginning, which includes the taxa
information.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# list the 10 orthologs with strongest loading in PC1 + values}
\NormalTok{humann3\_path\_biplot\_pc }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Path) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{str\_c}\NormalTok{(., }\AttributeTok{collapse =} \StringTok{"|"}\NormalTok{) }\CommentTok{\# need this format for filtering in the next step}


\CommentTok{\# select only those 10 pathways from the list, and split the column with names into 3 (Pathway, Genus, Species)}
\NormalTok{humann3\_path\_pc1pws }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Pathway, humann3\_path\_biplot\_pc)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Pathway, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(}\StringTok{"SampleID"}\NormalTok{, }\StringTok{"CPM"}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Pathway =} \FunctionTok{str\_replace\_all}\NormalTok{(Pathway, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.s\_\_"}\NormalTok{, }\StringTok{"|s\_\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(., Pathway, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Pathway"}\NormalTok{, }\StringTok{"Genus"}\NormalTok{, }\StringTok{"Species"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Species =} \FunctionTok{replace\_na}\NormalTok{(Species, }\StringTok{"unclassified"}\NormalTok{),}
         \AttributeTok{Genus =} \FunctionTok{str\_replace\_all}\NormalTok{(Genus, }\StringTok{"g\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \AttributeTok{Species =} \FunctionTok{str\_replace\_all}\NormalTok{(Species, }\StringTok{"s\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(., humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(Pathway, Path) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{distinct}\NormalTok{(.), }\AttributeTok{by =} \StringTok{"Pathway"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(.,  humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{select}\NormalTok{(Path), }\AttributeTok{by =} \StringTok{"Path"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# select({-}Pathway) \%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Path, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Path)}

\FunctionTok{tibble}\NormalTok{(humann3\_path\_pc1pws)}
\end{Highlighting}
\end{Shaded}

We can now see who contributed which pathway, and also the abundance
information (CPM)!

Given many taxa may contribute to the same pathway, we may want to see
which taxa are more `dominantly' contributing to this.

For this we can calculate of all copies of a given pathway what fraction
comes from which taxa (you can imagine this like `depth' coverage in
genomic analysis), based on the percentage of the total copies per
million for that pathway.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the \% for each pathway contributed by each genus}
\NormalTok{humann3\_path\_pc1pws\_stats }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pc1pws }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Path, Genus) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Sum =} \FunctionTok{sum}\NormalTok{(CPM)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Percent =}\NormalTok{ Sum}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(Sum)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{(.)}

\CommentTok{\# create the list of 10 orthologs again, but don\textquotesingle{}t collapse the list as above}
\NormalTok{humann3\_path\_biplot\_pc }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Path) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Path)}

\CommentTok{\# calculate the total \% of all genera that contribute \textless{} X\% to each ortholog}
\NormalTok{humann3\_path\_pc1pws\_stats\_extra }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(humann3\_path\_biplot\_pc, }\ControlFlowTok{function}\NormalTok{(eclass) \{}
\NormalTok{ high\_percent }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pc1pws\_stats }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{filter}\NormalTok{(Path }\SpecialCharTok{==}\NormalTok{ eclass) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{filter}\NormalTok{(Percent }\SpecialCharTok{\textless{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Remaining =} \FunctionTok{sum}\NormalTok{(Percent)) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Path =}\NormalTok{ eclass,}
          \AttributeTok{Genus =} \StringTok{"Other"}\NormalTok{)}
\NormalTok{\}) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{bind\_rows}\NormalTok{(.)}

\CommentTok{\# add this additional \% to the main table}
\NormalTok{humann3\_path\_pcbi\_bar\_df }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pc1pws\_stats\_extra }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Percent =}\NormalTok{ Remaining) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(., humann3\_path\_pc1pws\_stats }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Sum)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Path, Genus, Percent) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Direction =} \StringTok{"PC1+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

And we can visualize the contributors to the top 10 pathways) driving
the main variation along PC1 (with the assumption these maybe the most
biological significant, and to reduce the numbers of pathways we have to
research.

For the loadings falling in the positive direction of the PC1:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the values in a bar chart}
\NormalTok{paths\_sp\_pc1 }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pcbi\_bar\_df }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# filter(Direction == "PC1+", Genus != "Other") \%\textgreater{}\% \# removing Other plots all species/unassigned {-} no need to filter the pathways}
  \FunctionTok{filter}\NormalTok{(Percent }\SpecialCharTok{\textgreater{}=} \DecValTok{5} \SpecialCharTok{|}\NormalTok{ (Percent }\SpecialCharTok{\textless{}=} \DecValTok{5} \SpecialCharTok{\&}\NormalTok{ Genus }\SpecialCharTok{==} \StringTok{"Other"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# filter out the genera with \% \textless{} 5, but keep Other \textless{} 5}
  \CommentTok{\# filter(Percent \textgreater{}= 5) \%\textgreater{}\% \# filter out the genera with \% \textless{} 5, but keep Other \textless{} 5}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Genus =} \FunctionTok{fct\_relevel}\NormalTok{(Genus, }\StringTok{"Other"}\NormalTok{,}\StringTok{"unclassified"}\NormalTok{,}\StringTok{"Aggregatibacter"}\NormalTok{,}\StringTok{"Capnocytophaga"}\NormalTok{,}\StringTok{"Cardiobacterium"}\NormalTok{,}
                        \StringTok{"Eikenella"}\NormalTok{,}\StringTok{"Haemophilus"}\NormalTok{,}\StringTok{"Kingella"}\NormalTok{,}\StringTok{"Lautropia"}\NormalTok{,}\StringTok{"Neisseria"}\NormalTok{,}\StringTok{"Ottowia"}\NormalTok{,}\StringTok{"Streptococcus"}\NormalTok{),}
         \AttributeTok{Path =} \FunctionTok{fct\_relevel}\NormalTok{(Path, humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
                              \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
                              \FunctionTok{pull}\NormalTok{(Path))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Path, }\AttributeTok{y=}\NormalTok{Percent, }\AttributeTok{fill =}\NormalTok{ Genus)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#0D0887FF"}\NormalTok{,}\StringTok{"\#969696"}\NormalTok{,}\StringTok{"\#5D01A6FF"}\NormalTok{,}\StringTok{"\#7E03A8FF"}\NormalTok{,}
                                 \StringTok{"\#9C179EFF"}\NormalTok{,}\StringTok{"\#B52F8CFF"}\NormalTok{,}\StringTok{"\#CC4678FF"}\NormalTok{,}\StringTok{"\#DE5F65FF"}\NormalTok{,}
                                 \StringTok{"\#ED7953FF"}\NormalTok{,}\StringTok{"\#F89441FF"}\NormalTok{,}\StringTok{"\#FDB32FFF"}\NormalTok{,}\StringTok{"\#FBD424FF"}\NormalTok{,}\StringTok{"\#F0F921FF"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{18}\NormalTok{),}
          \AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Percent"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Metacyc pathways {-} PC1 positive"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{10}\NormalTok{))}

\CommentTok{\# viridis\_pal(option = "B")(13)}
\NormalTok{paths\_sp\_pc1}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

PWY-5345 has no species assignment to that pathway.

\end{tcolorbox}

And the negative loadings:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# list the 10 orthologs with strongest loading in PC1 + values}
\NormalTok{humann3\_path\_biplot\_pc }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Path) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Path) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{str\_c}\NormalTok{(., }\AttributeTok{collapse =} \StringTok{"|"}\NormalTok{) }\CommentTok{\# need this format for filtering in the next step}

\CommentTok{\# select only those 10 orthologs from the list, and split the column with names into 3 (Ortholog, Genus, Species)}
\NormalTok{humann3\_path\_pc1neg }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Pathway, humann3\_path\_biplot\_pc)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Pathway, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(}\StringTok{"SampleID"}\NormalTok{, }\StringTok{"CPM"}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Pathway =} \FunctionTok{str\_replace\_all}\NormalTok{(Pathway, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.s\_\_"}\NormalTok{, }\StringTok{"|s\_\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(., Pathway, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Pathway"}\NormalTok{, }\StringTok{"Genus"}\NormalTok{, }\StringTok{"Species"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Species =} \FunctionTok{replace\_na}\NormalTok{(Species, }\StringTok{"unclassified"}\NormalTok{),}
         \AttributeTok{Genus =} \FunctionTok{str\_replace\_all}\NormalTok{(Genus, }\StringTok{"g\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{),}
         \AttributeTok{Species =} \FunctionTok{str\_replace\_all}\NormalTok{(Species, }\StringTok{"s\_\_"}\NormalTok{, }\StringTok{""}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(., humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(Pathway, Path) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{distinct}\NormalTok{(.), }\AttributeTok{by =} \StringTok{"Pathway"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(.,  humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{select}\NormalTok{(Path), }\AttributeTok{by =} \StringTok{"Path"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Pathway) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Path, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Path)}

\CommentTok{\# calculate the \% for each ortholog contributed by each genus}
\NormalTok{humann3\_path\_pc1neg\_stats }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pc1neg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Path, Genus) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{Sum =} \FunctionTok{sum}\NormalTok{(CPM)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Percent =}\NormalTok{ Sum}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(Sum)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{(.)}

\CommentTok{\# create the list of 10 orthologs again, but don\textquotesingle{}t collapse the list as above}
\NormalTok{humann3\_path\_biplot\_pc }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Path) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(Path)}

\CommentTok{\# calculate the total \% of all genera that contribute \textless{} X\% to each ortholog}
\NormalTok{humann3\_path\_pc1neg\_stats\_extra }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(humann3\_path\_biplot\_pc, }\ControlFlowTok{function}\NormalTok{(eclass) \{}
\NormalTok{ high\_percent }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pc1neg\_stats }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{filter}\NormalTok{(Path }\SpecialCharTok{==}\NormalTok{ eclass) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{filter}\NormalTok{(Percent }\SpecialCharTok{\textless{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Remaining =} \FunctionTok{sum}\NormalTok{(Percent)) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Path =}\NormalTok{ eclass,}
          \AttributeTok{Genus =} \StringTok{"Other"}\NormalTok{)}
\NormalTok{\}) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{bind\_rows}\NormalTok{(.)}

\CommentTok{\# add this additional \% to the main table}
\NormalTok{humann3\_path\_pcbi\_bar\_df }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pcbi\_bar\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(humann3\_path\_pc1neg\_stats\_extra }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{rename}\NormalTok{(}\AttributeTok{Percent =}\NormalTok{ Remaining) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{bind\_rows}\NormalTok{(., humann3\_path\_pc1neg\_stats }\SpecialCharTok{\%\textgreater{}\%}
                      \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Sum)) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{select}\NormalTok{(Path, Genus, Percent) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Direction =} \StringTok{"PC1{-}"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{()}

\CommentTok{\# plot the values in a bar chart}
\NormalTok{paths\_sp\_pc2 }\OtherTok{\textless{}{-}}\NormalTok{ humann3\_path\_pcbi\_bar\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# removing Other plots all species/unassigned {-} no need to filter the pathways}
  \FunctionTok{filter}\NormalTok{(Percent }\SpecialCharTok{\textgreater{}=} \DecValTok{5} \SpecialCharTok{|}\NormalTok{ (Percent }\SpecialCharTok{\textless{}=} \DecValTok{5} \SpecialCharTok{\&}\NormalTok{ Genus }\SpecialCharTok{==} \StringTok{"Other"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# filter out the genera with \% \textless{} 5, but keep Other \textless{} 5}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Genus =} \FunctionTok{fct\_relevel}\NormalTok{(Genus, }\StringTok{"Other"}\NormalTok{,}\StringTok{"unclassified"}\NormalTok{,}\StringTok{"Desulfobulbus"}\NormalTok{,}\StringTok{"Desulfomicrobium"}\NormalTok{,}\StringTok{"Methanobrevibacter"}\NormalTok{),}
         \AttributeTok{Path =} \FunctionTok{fct\_relevel}\NormalTok{(Path, humann3\_pathway\_biplot\_list }\SpecialCharTok{\%\textgreater{}\%}
                              \FunctionTok{filter}\NormalTok{(Direction }\SpecialCharTok{==} \StringTok{"PC1{-}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
                              \FunctionTok{pull}\NormalTok{(Path))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Path, }\AttributeTok{y=}\NormalTok{Percent, }\AttributeTok{fill =}\NormalTok{ Genus)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#0D0887FF"}\NormalTok{,}\StringTok{"\#969696"}\NormalTok{,}\StringTok{"\#B52F8CFF"}\NormalTok{,}\StringTok{"\#ED7953FF"}\NormalTok{,}\StringTok{"\#FCFFA4FF"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{18}\NormalTok{),}
          \AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
    \CommentTok{\# facet\_wrap(\textasciitilde{}pathrtholog, nrow=2) +}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Percent"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Metacyc pathways {-} PC1 negative"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{12}\NormalTok{))}

\NormalTok{paths\_sp\_pc2}
\end{Highlighting}
\end{Shaded}

\hypertarget{final-visualisation}{%
\subsection{Final Visualisation}\label{final-visualisation}}

Finally, we can stick the biplot and the taxon contribution plots
together!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h3biplots }\OtherTok{\textless{}{-}}\NormalTok{ biplot }\SpecialCharTok{+}\NormalTok{ paths\_sp\_pc2 }\SpecialCharTok{+}\NormalTok{ paths\_sp\_pc1 }\SpecialCharTok{+}
  \FunctionTok{plot\_layout}\NormalTok{(}\AttributeTok{widths =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\FunctionTok{ggsave}\NormalTok{(}\StringTok{"./h3\_paths\_biplots.pdf"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ h3biplots,}
        \AttributeTok{device =} \StringTok{"pdf"}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{, }\AttributeTok{width =} \DecValTok{20}\NormalTok{, }\AttributeTok{height =} \FloatTok{9.25}\NormalTok{, }\AttributeTok{units =} \FunctionTok{c}\NormalTok{(}\StringTok{"in"}\NormalTok{), }\AttributeTok{dpi =} \DecValTok{300}\NormalTok{)}

\FunctionTok{system}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}firefox "h3\_paths\_biplots.pdf"\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This allows us to evaluate all the information together.

From this point onwards, we would have to do manual research/literature
reviews into each of the pathways, see if they make `sense' to the
sample type and associated groups of samples, and evaluate whether they
are interesting or not..

\hypertarget{introduction-to-de-novo-genome-assembly}{%
\chapter{\texorpdfstring{Introduction to \emph{de novo} Genome
Assembly}{Introduction to de novo Genome Assembly}}\label{introduction-to-de-novo-genome-assembly}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/denovo-assembly.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate denovo{-}assembly}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-10}{%
\subsection{Lecture}\label{lecture-10}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/4c-intro-to-denovoassembly/SPAAM\%20Summer\%20School\%202022\%20-\%204C\%20-\%20Genome\%20Assembly.pdf}{here}.

\hypertarget{introduction-7}{%
\section{Introduction}\label{introduction-7}}

First of all, what is a \textbf{metagenomic} sample? Metagenomic sample
is a sample that consists of DNA from more than one source. The number
and the type of sources might vary widely between different samples.
Typical sources for ancient remains are e.g.~the host organism and the
microbial species. The important part is that we generally do not know
the origin of a DNA molecule prior to analysing the sequencing data
generated from the DNA library of this sample. In the example presented
in (Figure~\ref{fig-denovoassembly-overview}), our metagenomic sample
has DNA from three different sources, here coloured in blue, red, and
yellow.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/metagenome_assembly_scheme.png}

}

\caption{\label{fig-denovoassembly-overview}Overview of the ways how to
analyse metagenomic sequencing data.}

\end{figure}

How can we determine the sources of the DNA that we have in our
metagenomic sample? There are three main options whose \emph{pros} and
\emph{cons} are summarised in (Table~\ref{tbl-denovoassembly-proscons}).

\hypertarget{tbl-denovoassembly-proscons}{}
\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3077}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3407}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3516}}@{}}
\caption{\label{tbl-denovoassembly-proscons}Pros and cons of the major
methods for determining the sources of metagenomic DNA.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
pros
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
cons
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
pros
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
cons
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
reference-based alignment & highly sensitive, applicable to aDNA samples
& requires all sources to be represented in database \\
single-genome assembly & high-quality genomes from cultivated bacteria &
not available for ancient DNA samples \\
metagenome assembly & able to recover unknown diversity present in
sample & highly dependent on preservation of ancient DNA \\
\end{longtable}

Until recently, the only option for ancient DNA samples was to take the
short-read sequencing data and align them against some known reference
genomes. However, this approach is heavily relying on whether all
sources of our samples are represented in the available reference
genomes. If a source is missing in the reference database - in our toy
example, this is the case for the yellow source
(Figure~\ref{fig-denovoassembly-overview}) -, then we won't be able to
detect it using this reference database.

While there is a potential workaround for modern metagenomic samples,
\emph{single-genome assembly} relies on being able to cultivate a
microbial species to obtain an isolate. This is unfeasible for ancient
metagenomic samples because there are no more viable microbial cells
available that could be cultivated.

Around 2015, a technical revolution started when the first programs,
e.g.~MEGAHIT {[}@LiMegahit2015{]} and metaSPAdes {[}@Nurk2017{]}, were
published that could successfully perform \emph{de novo} assembly from
metagenomic data. Since then, tens of thousands metagenomic samples have
been assembled and it was revealed that even well studied environments,
such as the human gut microbiome, have a lot of additional microbial
diversity that has not been observed previously via culturing and
isolation {[}@Almeida2021{]}.

The technical advancement of being able to perform \emph{de novo}
assembly on metagenomic samples led to an explosion of studies that
analysed samples that were considered almost impossible to study
beforehand. For researchers that are exposed to ancient DNA, the
imminent question arises: can we apply the same methods to ancient DNA
data? In this practical course, we will walk through all required steps
that are necessary to successfully perform \emph{de novo} assembly from
ancient DNA metagenomic sequencing data and show you what you can do
once you have obtained the data.

\hypertarget{practical-course}{%
\section{Practical course}\label{practical-course}}

\hypertarget{sample-overview}{%
\subsection{Sample overview}\label{sample-overview}}

For this practical course, I selected a palaeofaeces sample from the
study by @Maixner2021, who generated deep metagenomic sequencing data
for four palaeofaeces samples that were excavated from an Austrian salt
mine in Hallstatt and were associated with the Celtic Iron Age. We will
focus on the youngest sample, \textbf{2612}, which was dated to be just
a few hundred years old (Figure~\ref{fig-denovoassembly-maixner}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/Maixner2021_GraphicalAbstract.jpg}

}

\caption{\label{fig-denovoassembly-maixner}The graphical abstract of
Maixner et al.~(2021).}

\end{figure}

However, because the sample was very deeply sequenced for more than 250
million paired-end reads and we do not want to wait for days for the
analysis to finish, we will not use all data but just a sub-sample of
them.

You can access the sub-sampled data by changing to the folder

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ /vol/volume/denovo{-}assembly/2612\_R1.fastq.gz 2612\_R1.fastq.gz}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ /vol/volume/denovo{-}assembly/2612\_R2.fastq.gz 2612\_R2.fastq.gz}
\end{Highlighting}
\end{Shaded}

on the cluster. If you would like to repeat the practical on your own
computational infrastructure, you will find information on how to access
the files in the following boxes with the title ``Self-guided: data
preparation'':

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self-guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

For everyone, who runs this practical on their own infrastructure, you
can download the sequencing data from here:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wget}\NormalTok{ https://share.eva.mpg.de/index.php/s/CtLq2R9iqEcAFyg/download/2612\_R1.fastq.gz}
\FunctionTok{wget}\NormalTok{ https://share.eva.mpg.de/index.php/s/mc5JrpDWdL4rC24/download/2612\_R2.fastq.gz}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, left=2mm, colback=white, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Question}\vspace{2mm}

\textbf{How many sequences are in each FastQ file?}

Hint: You can run either
\texttt{seqtk\ size\ \textless{}FastQ\ file\textgreater{}} or
\texttt{bioawk\ -c\ fastx\ \textquotesingle{}END\{print\ NR\}\textquotesingle{}\ \textless{}FastQ\ file\textgreater{}}
to find this out.

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

There are about 3.25 million paired-end sequences in these files.

\end{tcolorbox}

\hypertarget{preparing-the-sequencing-data-for-de-novo-assembly}{%
\subsection{\texorpdfstring{Preparing the sequencing data for \emph{de
novo}
assembly}{Preparing the sequencing data for de novo assembly}}\label{preparing-the-sequencing-data-for-de-novo-assembly}}

Before running the actual assembly, we need to pre-process our
sequencing data. Typical pre-processing steps include the trimming of
adapter sequences and barcodes from the sequencing data and the removal
of host or contaminant sequences, such as the bacteriophage PhiX, which
is commonly sequenced as a quality control.

Many assembly pipelines, such as
\href{https://nf-co.re/mag}{nf-core/mag}, have these steps automatically
included, however, these steps can be performed prior to it, too. For
this practical course, I have performed these steps for us and we could
directly continue with the \emph{de novo} assembly.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-important-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-important-color!10!white, opacityback=0, opacitybacktitle=0.6]

The characteristic of ancient DNA samples that pre-determines the
success of the \emph{de novo} assembly is the \textbf{distribution of
the DNA molecule length}. Determine this distribution prior to running
the \emph{de novo} assembly to be able to predict the results of the
\emph{de novo} assembly.

\end{tcolorbox}

However, since the average length of the insert size of sequencing data
(Figure~\ref{fig-denovoassembly-illumina}) is highly correlated with the
success of the assembly, we want to first evaluate it. For this we can
make use of the program fastp {[}@Chen2018{]}.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/insert_size_scheme.png}

}

\caption{\label{fig-denovoassembly-illumina}Scheme of a read pair of
Illumina sequencing data.}

\end{figure}

Fastp will try to overlap the two mates of a read pair and, if this is
possible, return the length of the merged sequence, which is identical
to insert size or the DNA molecule length. If the two mates cannot be
overlapped, we will not be able to know the exact DNA molecule length
but only know that it is longer than 290 bp (each read has a length of
150 bp and FastP requires a 11 bp overlap between the reads).

The final histogram of the insert sizes that is returned by FastP can
tell us how well preserved the DNA of an ancient sample is
(Figure~\ref{fig-denovoassembly-lendist}). The more the distribution is
skewed to the right, i.e.~the longer the DNA molecules are, the more
likely we are to obtain long contiguous DNA sequences from the \emph{de
novo} assembly. A distribution that is skewed to the left means that the
DNA molecules are more highly degraded and this lowers our chances for
obtaining long continuous sequences.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/Orlando2013_FigS4.5.png}

}

\caption{\label{fig-denovoassembly-lendist}Example of a DNA molecule
length distribution of a well-preserved ancient DNA sample. This
histogram belongs to a 700,000-year-old horse that was preserved in
permafrost, as reported in @Orlando2013, Fig. S4.}

\end{figure}

To infer the distribution of the DNA molecules, we can run the command

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{fastp} \AttributeTok{{-}{-}in1}\NormalTok{ 2612\_R1.fastq.gz }\DataTypeTok{\textbackslash{}}
      \AttributeTok{{-}{-}in2}\NormalTok{ 2612\_R2.fastq.gz }\DataTypeTok{\textbackslash{}}
      \AttributeTok{{-}{-}stdout} \AttributeTok{{-}{-}merge} \AttributeTok{{-}A} \AttributeTok{{-}G} \AttributeTok{{-}Q} \AttributeTok{{-}L} \AttributeTok{{-}{-}json}\NormalTok{ /dev/null }\AttributeTok{{-}{-}html}\NormalTok{ overlaps.html }\DataTypeTok{\textbackslash{}}
\OperatorTok{\textgreater{}}\NormalTok{ /dev/null}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, left=2mm, colback=white, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Question}\vspace{2mm}

\textbf{Infer the distribution of the DNA molecule length of the
sequencing data. Is this sample well-preserved?}

Hint: You can easily inspect the distribution by opening the HTML report
\texttt{overlaps.html}.

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Here is the histogram of the insert sizes determined by fastp
(Figure~\ref{fig-denovoassembly-insertsize}). By default, fastp will
only keep reads that are longer than 30 bp and requires an overlap
between the read mates of 30 bp. The maximum read length is 150 bp,
therefore, the histogram only spreads from 31 to 271 bp in total.

\begin{figure}[H]

{\centering \includegraphics{assets/images/chapters/denovo-assembly/2612_insertsize_hist.png}

}

\caption{\label{fig-denovoassembly-insertsize}Histogram of the insert
sizes determined by fastp}

\end{figure}

The sequencing data for the sample \textbf{2612} were generated across
eight different sequencing runs, which differed in their nominal length.
Some sequencing runs were 2x 100 bp, while others were 2x 150 bp. This
is the reason why we observe two peaks just short of 100 and 150 bp. The
difference to the nominal length is caused by the quality trimming of
the data.

Overall, we have almost no short DNA molecules (\textless{} 50 bp) but
most DNA molecules are longer than 80 bp. Additionally, there were
\textgreater{} 200,000 read pairs that could not be overlapped.
Therefore, we can conclude that the sample \textbf{2612} is moderately
degraded ancient DNA sample and has many long DNA molecules.

\end{tcolorbox}

\hypertarget{de-novo-assembly-1}{%
\subsection{\texorpdfstring{\emph{De novo}
assembly}{De novo assembly}}\label{de-novo-assembly-1}}

Now, we will actual perform the \emph{de novo} assembly on the
sequencing data. For this, we will use the program MEGAHIT
{[}@LiMegahit2015{]}, a \emph{de Bruijn}-graph assembler.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/kmer_counting.png}

}

\caption{\label{fig-denovoassembly-kmers}Overview of inferring
\emph{k}-mers from a DNA sequence. Credit:
\url{https://medium.com/swlh/bioinformatics-1-k-mer-counting-8c1283a07e29}}

\end{figure}

\emph{De Bruijn} graph assemblers are together with overlap assemblers
the predominant group of assemblers. They use \emph{k}-mers (see
Figure~\ref{fig-denovoassembly-kmers} for an example of \emph{4}-mers)
extracted from the short-read sequencing data to build the graph. For
this, they connect each \emph{k}-mer to adjacent \emph{k}-mer using a
directional edge. By walking along the edges and visiting each
\emph{k}-mer or node once, longer continuous sequences are
reconstructed. This is a very rough explanation and I would advise you
to watch this excerpt of a
\href{https://www.youtube.com/watch?v=OY9Q_rUCGDw}{lecture} by Rob
Edwards from San Diego State University and a
\href{https://youtu.be/TNYZZKrjCSk?t=112}{Coursera lecture} by Ben
Langmead from Johns Hopkins University, if you would like to learn more
about it.

All \emph{de Bruijn} graph assemblers work in a similar way so the
question is why do we use MEGAHIT and not other programs, such as
metaSPAdes?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Pros and cons of MEGAHIT}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Pros}:

\begin{itemize}
\tightlist
\item
  low-memory footprint: can be run on computational infrastructure that
  does not have large memory resources
\item
  can assembly both single-end and paired-end sequencing data
\item
  the assembly algorithm can cope with the presence of high amounts of
  ancient DNA damage
\end{itemize}

\textbf{Cons}:

\begin{itemize}
\tightlist
\item
  lower assembly quality on modern comparative data, particularly a
  higher rate of mis-assemblies (CAMI II challenge)
\end{itemize}

\end{tcolorbox}

In the \href{https://christinawarinner.com/about-us/}{Warinner group},
we realised after some tests that MEGAHIT has a clear advantage when
ancient DNA damage is present at higher quantities. While it produced a
higher number of mis-assemblies compared to metaSPAdes when being
evaluated on simulated modern metagenomic data {[}Critical Assessment of
Metagenome Interpretation II challenge, @Meyer2022{]}, it produces more
long contigs when ancient DNA damage is present.

To \emph{de novo} assemble the short-read sequencing data of the sample
\textbf{2612} using MEGAHIT, we can run the command

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{megahit} \AttributeTok{{-}1}\NormalTok{ 2612\_R1.fastq.gz }\DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}2}\NormalTok{ 2612\_R2.fastq.gz }\DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}t}\NormalTok{ 14 }\AttributeTok{{-}{-}min{-}contig{-}len}\NormalTok{ 500 }\DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}{-}out{-}dir}\NormalTok{ megahit}
\end{Highlighting}
\end{Shaded}

This will use the paired-end sequencing data as input and return all
contigs that are at least 500 bp long.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

While MEGAHIT is able to use merged sequencing data, it is advised to
use the unmerged paired-end data as input. In tests using simulated data
I have observed that MEGAHIT performed slightly better when using the
unmerged data and it likely has something to do with its internal
algorithm to infer insert sizes from the paired-end data.

\end{tcolorbox}

While we are waiting for MEGAHIT to finish, here is a question:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, left=2mm, colback=white, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Question}\vspace{2mm}

\textbf{Which \emph{k}-mer lengths did MEGAHIT select for the \emph{de
novo} assembly?}

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Based on the maximum read length, MEGAHIT decided to use the
\emph{k}-mer lengths of 21, 29, 39, 59, 79, 99, 119, and 141.

\end{tcolorbox}

Now, as MEGAHIT has finished, we want to evaluate the quality of the
assembly results. MEGAHIT has written the contiguous sequences (contigs)
into a single FastA file stored in the folder \texttt{megahit}. We will
process this FastA file with a small script, calN50, which will count
the number of contigs and give us an overview of their length
distribution.

To download the script and run it, we can execute the following
commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wget}\NormalTok{ https://raw.githubusercontent.com/lh3/calN50/master/calN50.js}
\ExtensionTok{k8}\NormalTok{ ./calN50.js megahit/final.contigs.fa}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, left=2mm, colback=white, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, bottomrule=.15mm, leftrule=.75mm, toprule=.15mm, opacityback=0]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Question}\vspace{2mm}

\textbf{How many contigs were assembled? What is the sum of the lengths
of all contigs? What is the maximum, the median, and the minimum contig
length?}

Hint: The maximum contig length is indicated by the label ``N0'', the
median by the label ``N50'', and the minimum by the label ``N100''?

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

MEGAHIT assembled 3,606 contigs and their lengths sum up to 11.4 Mb. The
maximum contig length was 448 kb, the median length was 15.6 kb, and the
minimum length was 500 bp.

\end{tcolorbox}

There is a final caveat when assembling ancient metagenomic data with
MEGAHIT: while it is able to assemble sequencing data with a high
percentage of C-to-T substitutions, it tends to introduce these changes
into the contig sequences, too.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/Leggett2013_Fig1a.png}

}

\caption{\label{fig-denovoassembly-debruijnbubble}\emph{De Bruijn} graph
with a bubble caused by a second allele. Adapted from {[}@Leggett2013,
Figure 1a{]}}

\end{figure}

These C-to-T substitutions are similar to biological single-nucleotide
polymorphisms in the sequencing data. Both lead the introduction of
bubbles in the \emph{de Bruijn} graph when two alleles are present in
the k-mer sequences (Figure~\ref{fig-denovoassembly-debruijnbubble}) and
the assembler decides during its pruning steps which allele to keep in
the contig sequence.

While it does not really matter which allele is kept for biological
polymorphisms, it does matter for technical artefacts that are
introduced by the presence of ancient DNA damage. In our group we
realised that the gene sequences that were annotated on the contigs of
MEGAHIT tended to have a higher number of nonsense mutations compared to
the known variants of the genes. After manual inspection, we observed
that many of these mutations appeared because MEGAHIT chose the
damage-derived T allele over the C allele or the damage-derived A allele
over a G allele {[}see @Klapper2023, Figure S1{]}.

To overcome this issue, my colleagues Maxime Borry, James Fellows Yates
and I developed a strategy to replace these damage-derived alleles in
the contig sequences. This approach consists of aligning the short-read
data against the contigs, performing genotyping along them, and finally
replacing all alleles for which we have strong support for an allele
that is different from the one selected by MEGAHIT.

We standardised this approach and added it to the Nextflow pipeline
nf-core/mag {[}@Krakau2022{]}. It can simply be activated by providing
the parameter \texttt{-\/-ancient\_dna}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

While MEGAHIT is able to assemble ancient metagenomic sequencing data
with high amounts of ancient DNA damage, it tends to introduce
damage-derived T and A alleles into the contig sequences instead of the
true C and G alleles. This can lead to a higher number of nonsense
mutations in coding sequences. We strongly advise you to correct such
mutations, e.g.~by using the ancient DNA workflow of the Nextflow
pipeline \href{https://nf-co.re/mag}{nf-core/mag}.

\end{tcolorbox}

\hypertarget{aligning-the-short-read-data-against-the-contigs}{%
\subsection{Aligning the short-read data against the
contigs}\label{aligning-the-short-read-data-against-the-contigs}}

After the assembly, the next detrimental step that is required for many
subsequent analyses is the alignment of the short-read sequencing data
back to assembled contigs.

Analyses that require these alignment information are for example:

\begin{itemize}
\tightlist
\item
  the correction of the contig sequences to remove damage-derived
  alleles
\item
  the non-reference binning of contigs into MAGs for inferring the
  coverage along the contigs
\item
  the quantification of the presence of ancient DNA damage
\end{itemize}

Aligning the short-read data to the contigs requires multiple steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build an index from the contigs for the alignment program BowTie2
\item
  Align the short-read data against the contigs using this index with
  BowTie2
\item
  Calculate the mismatches and deletions of the short-read data against
  the contig sequences
\item
  Sort the aligned reads by the contig name and the coordinate they were
  aligned to
\item
  Index the resulting alignment file for faster access
\end{enumerate}

To execute the alignment step we can run the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ alignment}
\ExtensionTok{bowtie2{-}build} \AttributeTok{{-}f}\NormalTok{ megahit/final.contigs.fa alignment/2612}
\ExtensionTok{bowtie2} \AttributeTok{{-}p}\NormalTok{ 14 –{-}very{-}sensitive }\AttributeTok{{-}N}\NormalTok{ 1 }\AttributeTok{{-}x}\NormalTok{ alignment/2612 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}1}\NormalTok{ 2612\_R1.fastq.gz }\AttributeTok{{-}2}\NormalTok{ 2612\_R2.fastq.gz }\KeywordTok{|} \DataTypeTok{\textbackslash{}}
\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}Sb} \AttributeTok{{-}} \KeywordTok{|} \DataTypeTok{\textbackslash{}}
\ExtensionTok{samtools}\NormalTok{ calmd }\AttributeTok{{-}u}\NormalTok{ /dev/stdin megahit/final.contigs.fa }\KeywordTok{|} \DataTypeTok{\textbackslash{}}
\ExtensionTok{samtools}\NormalTok{ sort }\AttributeTok{{-}o}\NormalTok{ alignment/2612.sorted.calmd.bam }\AttributeTok{{-}}
\ExtensionTok{samtools}\NormalTok{ index alignment/2612.sorted.calmd.bam}
\end{Highlighting}
\end{Shaded}

However, these steps are rather time-consuming, even when we just have
so little sequencing data as we do for our course example. The alignment
is rather slow because we allow a single mismatch in the seeds that are
used by the aligner BowTie2 to quickly determine the position of a read
along the contig sequences (parameter \texttt{-N\ 1}). This is necessary
because otherwise we might not be able to align reads with ancient DNA
damage present on them. Secondly, the larger the resulting alignment
file is the longer it takes to sort it by coordinate.

To save us some time and continue with the more interesting analyses, I
prepared the resulting files for us. For this, I also corrected
damage-derived alleles in the contig sequences. You can access these
files on the cluster by running the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ alignment}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ /vol/volume/denovo{-}assembly/2612.sorted.calmd.bam alignment/}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ /vol/volume/denovo{-}assembly/2612.sorted.calmd.bam.bai alignment/}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ /vol/volume/denovo{-}assembly/2612.fa alignment/}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self-guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

For everyone, who runs this practical on their own infrastructure, you
can download the files:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ alignment}
\FunctionTok{wget} \AttributeTok{{-}O}\NormalTok{ alignment/2612.sorted.calmd.bam }\DataTypeTok{\textbackslash{}}
\NormalTok{    https://share.eva.mpg.de/index.php/s/bDKgFLj9GpRFdPg/download/2612.sorted.calmd.bam}
\FunctionTok{wget} \AttributeTok{{-}O}\NormalTok{ alignment/2612.sorted.calmd.bam.bai }\DataTypeTok{\textbackslash{}}
\NormalTok{    https://share.eva.mpg.de/index.php/s/HWqg6fJj6ZEEBAL/download/2612.sorted.calmd.bam.bai}
\FunctionTok{wget} \AttributeTok{{-}O}\NormalTok{ alignment/2612.fa }\DataTypeTok{\textbackslash{}}
\NormalTok{    https://share.eva.mpg.de/index.php/s/z6ZAai42RPribX5/download/final.contigs.fa}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{reconstructing-metagenome-assembled-genomes}{%
\subsection{Reconstructing metagenome-assembled
genomes}\label{reconstructing-metagenome-assembled-genomes}}

There are typically two major approaches on how to study biological
diversity of samples using the results obtained from the \emph{de novo}
assembly. The first one is to reconstruct metagenome-assembled genomes
(MAGs) and to study the species diversity.

``Metagenome-assembled genome'' is a convoluted term that means that we
reconstructed a genome from metagenomic data via \emph{de novo}
assembly. While these reconstructed genomes, particularly the
high-quality ones, are most likely a good representation of the genomes
of the organisms present in the sample, the scientific community
refrains from calling them a species or a strain. The reason is that for
calling a genome a species or a strain additional analyses would be
necessary, of which many would include the cultivation of the organism.
For many samples, this is not feasible and therefore the community stuck
to the term MAG instead.

The most commonly applied method to obtain MAGs is the so-called
``non-reference binning''. Non-reference binning means that we do not
try to identify contigs by aligning them against known reference
genomes, but only use the characteristics of the contigs themselves to
cluster them (Figure~\ref{fig-denovoassembly-binning}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/mag_binning_scheme.png}

}

\caption{\label{fig-denovoassembly-binning}Scheme of binning \emph{de
novo} assembled contigs into metagenome-assembled genomes. During the
binning contigs are grouped into clusters based on their
characteristics, such as tetra-nucleotide frequency and the coverage
along the contigs. Clusters of contigs that fulfil a minimum of quality
criteria are then considered as metagenome-assembled genomes. However,
depending on the sample, a number of contigs will remain unbinned.}

\end{figure}

The two most commonly used characteristics are:

\begin{itemize}
\tightlist
\item
  the tetra-nucleotide frequency: the frequency of all 4-mers
  (e.g.~AAAA, AAAC, AAAG etc.) in the contig sequence
\item
  the coverage along the contig
\end{itemize}

The idea here is that two contigs that are derived from the same
bacterial genome will likely have a similar nucleotide composition and
coverage.

This approach works very well when the contigs are longer and they
strongly differ in the nucleotide composition and the coverage from each
other. However, if this is not the case, e.g.~there is more than one
strain of the same species in the sample, these approaches will likely
not be able to assign some contigs to clusters: these contigs remain
unbinned. In case there is a high number of unbinned contigs, one can
also employ the more sensitive reference-based binning strategies but we
will not cover this in this practical course.

There are a number of different binning tools out there that can be used
for this task. Since this number is constantly growing, there have been
attempts to standardise the test data sets that these tools are run on
so that their performances can be easily compared. The most well-known
attempt is the Critical Assessment of Metagenome Interpretation
{[}Critical Assessment of Metagenome Interpretation II challenge,
@Meyer2022{]}, which released the latest comparison of commonly used
tools for assembly, binning, and bin refinement in 2022. I recommend
that you first check the performance of a new tool against the CAMI
datasets before testing it to see whether it is worth using it.

Three commonly used binners are:

\begin{itemize}
\tightlist
\item
  metaBAT2 {[}@Kang2019, more than 1,300 citations{]}
\item
  MaxBin2 {[}@WuMaxbin2016, more than 1,200 citations{]}
\item
  CONCOCT {[}@Alneberg2014, more than 1,000 citation{]}
\end{itemize}

Each of these three binners employs a slightly different strategy. While
metaBAT2 simply uses the two previously mentioned metrics, the
tetra-nucleotide frequency and the coverage along the contigs, MaxBin2
additionally uses single-copy marker genes to infer the taxonomic origin
of contigs. In contrast, CONCOCT also just uses the two aforementioned
metrics but first performs a Principal Component Analysis (PCA) on these
metrics and uses the PCA results for the clustering.

The easiest way to run all these three programs is the program metaWRAP
{[}@Uritskiy2018{]}. metaWRAP is in fact a pipeline that allows you to
assemble your contigs, bin them, and subsequently refine the resulting
MAGs. However, the pipeline is not very well written and does not
contain any strategies to deal with ancient metagenomic sequencing data.
Therefore, I prefer to use different pipelines, such as nf-core/mag for
the assembly, and only use metaWRAP for binning and bin refinement.

To skip the first steps of metaWRAP and start straight with the binning,
we need to create the folder structure and files that metaWRAP expects:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ metawrap/INITIAL\_BINNING/2612/work\_files}
\FunctionTok{ln} \AttributeTok{{-}s} \VariableTok{$PWD}\NormalTok{/alignment/2612.sorted.calmd.bam }\DataTypeTok{\textbackslash{}}
\NormalTok{    metawrap/INITIAL\_BINNING/2612/work\_files/2612.bam}
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ metawrap/faux\_reads}
\BuiltInTok{echo} \StringTok{"@"} \OperatorTok{\textgreater{}}\NormalTok{ metawrap/faux\_reads/2612\_1.fastq}
\BuiltInTok{echo} \StringTok{"@"} \OperatorTok{\textgreater{}}\NormalTok{ metawrap/faux\_reads/2612\_2.fastq}
\end{Highlighting}
\end{Shaded}

Now, we can start to run the binning. In this practical course, we will
focus on metaBAT2 and MaxBin2. To bin the contigs with these binners, we
execute:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate metawrap{-}env}
\ExtensionTok{metawrap}\NormalTok{ binning }\AttributeTok{{-}o}\NormalTok{ metawrap/INITIAL\_BINNING/2612 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}t}\NormalTok{ 14 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}a}\NormalTok{ alignment/2612.fa }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}metabat2} \AttributeTok{{-}{-}maxbin2} \AttributeTok{{-}{-}universal} \DataTypeTok{\textbackslash{}}
\NormalTok{    metawrap/faux\_reads/2612\_1.fastq metawrap/faux\_reads/2612\_2.fastq}
\ExtensionTok{conda}\NormalTok{ deactivate}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

MetaWRAP is not a well-written Python software and has not been update
for more than three years. It still relies on the deprecated Python
v2.7. This is in conflict with many other tools and therefore it
requires its own conda environment, \texttt{metawrap-env}. Do not forget
to deactivate this environment afterwards again!

\end{tcolorbox}

MetaWRAP will run metaBAT2 and MaxBin2 for us and store their respective
output into sub-folders in the folder
\texttt{metawrap/INITIAL\_BINNING/2612}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{How many bins did metaBAT2 and MaxBin2 reconstruct,
respectively? Is there a difference in the genome sizes of these
reconstructed bins?}

Hint: You can use the previously introduced script
\texttt{k8\ ./calN50.js} to analyse the genome size of the individual
bins.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

metaBAT2 reconstructed seven bins, while MaxBin2 reconstructed only five
bins.

When comparing the genome sizes of these bins, we can see that despite
having reconstructed fewer bins, MaxBin2's bins have on average larger
genome size and all of them are at least 1.5 Mb. In contrast, five out
of seven metaBAT2 bins are shorter than 1.5 Mb.

\end{tcolorbox}

While we could have run these two binning softwares manually ourselves,
there is another reason why we should use metaWRAP: it has a powerful
bin refinement algorithm.

As we just observed, binning tools come to different results when
performing non-reference binning on the same contigs. So how do we know
which binning tool delivered the better or even correct results?

A standard approach is to identify single-copy marker genes that are
specific for certain taxonomic lineages, e.g.~to all members of the
family \emph{Prevotellaceae} or to all members of the kingdom archaea.
If we find lineage-specific marker genes from more than one lineage in
our bin, something likely went wrong. While in certain cases horizontal
gene transfer could explain such a finding, it is much more common that
a binning tool clustered two contigs from two different taxons.

During its bin refinement, metaWRAP first combines the results of all
binning tools in all combinations. So it would merge the results of
metaBAT2 and MaxBin2, metaBAT2 and CONCOCT, MaxBin2 and CONCOCT, and all
three together. Afterwards, it evaluates the presence of
lineage-specific marker genes on the contigs of the bins for every
combination and the individual binning tools themselves. In the case
that it would find marker genes of more than one lineage in a bin, it
would split the bin into two. After having evaluated everything,
metaWRAP selects the refined bins that have the highest quality score
across the whole set of bins.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/MetaWrap_Fig4.png}

}

\caption{\label{fig-denovoassembly-metawrap}Performance of metaWRAP's
bin refinement algorithm compared to other tools. Adapted from
@Uritskiy2018, Fig. 4}

\end{figure}

Using this approach, the authors of metaWRAP could show that they can
outperform the individual binning tools and other bin refinement
algorithms both regarding the \textbf{completeness} and
\textbf{contamination} that was estimated for the MAGs
(Figure~\ref{fig-denovoassembly-metawrap}).

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

Running metaWRAP's bin refinement module requires about 72 GB of memory
because it has to load a larger reference database containing the
lineage-specific marker genes of checkM.

If your computer infrastructure cannot provide so much memory, I have
prepared the results of metaWRAP's bin refinement algorithm,
\texttt{metawrap\_50\_10\_bins.stats}, which can be found in the folder
\texttt{/vol/volume/denovo-assembly}.

\end{tcolorbox}

To apply metaWRAP's bin refinement to the bins that we obtained from
metaBAT2 and MaxBin2, we first need to install the software checkM
{[}@Parks2015{]} that will provide the lineage-specific marker gene
catalogue:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ checkM}
\FunctionTok{wget} \AttributeTok{{-}O}\NormalTok{ checkM/checkm\_data\_2015\_01\_16.tar.gz }\DataTypeTok{\textbackslash{}}
\NormalTok{    https://data.ace.uq.edu.au/public/CheckM\_databases/checkm\_data\_2015\_01\_16.tar.gz}
\FunctionTok{tar}\NormalTok{ xvf checkM/checkm\_data\_2015\_01\_16.tar.gz }\AttributeTok{{-}C}\NormalTok{ checkM}

\BuiltInTok{echo}\NormalTok{ checkM }\KeywordTok{|} \ExtensionTok{checkm}\NormalTok{ data setRoot checkM}
\end{Highlighting}
\end{Shaded}

Afterwards, we can execute metaWRAP's bin refinement module:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ metawrap/BIN\_REFINEMENT/2612}
\ExtensionTok{metawrap}\NormalTok{ bin\_refinement }\AttributeTok{{-}o}\NormalTok{ metawrap/BIN\_REFINEMENT/2612 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}t}\NormalTok{ 14 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}c}\NormalTok{ 50 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}x}\NormalTok{ 10 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}A}\NormalTok{ metawrap/INITIAL\_BINNING/2612/maxbin2\_bins }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}B}\NormalTok{ metawrap/INITIAL\_BINNING/2612/metabat2\_bins}
\end{Highlighting}
\end{Shaded}

The latter step will produce a summary file,
\texttt{metawrap\_50\_10\_bins.stats}, that lists all retained bins and
some key characteristics, such as the genome size, the completeness
estimate, and the contamination estimate. The latter two can be used to
assign a quality score according to the Minimum Information for MAG
(MIMAG; see info box).

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The Minimum Information for MAG (MIMAG)}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The two most common metrics to evaluate the quality of MAGs are:

\begin{itemize}
\tightlist
\item
  the \textbf{completeness}: how many of the expected lineage-specific
  single-copy marker genes were present in the MAG?
\item
  the \textbf{contamination}: how many of the expected lineage-specific
  single-copy marker genes were present more than once in the MAG?
\end{itemize}

These metric is usually calculated using the marker-gene catalogue of
checkM {[}@Parks2015{]}, also if there are other estimates from other
tools such as BUSCO {[}@Manni2021{]}, GUNC {[}@Orakov2021{]} or checkM2
{[}@Chklovski2022{]}.

Depending on the estimates on completeness and contamination plus the
presence of RNA genes, MAGs are assigned to the quality category
following the Minimum Information for MAG criteria {[}@Bowers2017{]} You
can find the overview
\href{https://www.nature.com/articles/nbt.3893/tables/1}{here}.

\end{tcolorbox}

As these two steps will run rather long and need a large amount of
memory and disk space, I have provided the results of metaWRAP's bin
refinement. You can find the file here:
\texttt{/vol/volume/denovo-assembly/metawrap\_50\_10\_bins.stats}. Be
aware that these results are based on the bin refinement of the results
of three binning tools and include CONCOCT.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{How many bins were retained after the refinement with metaWRAP?
How many high-quality and medium-quality MAGs did the refinement yield
following the MIMAG criteria?}

Hint: You can more easily visualise tables on the terminal using the
Python program \texttt{visidata}. After installing it with
\texttt{pip\ install\ visidata}, you can open a table using
\texttt{vd\ -f\ tsv\ /vol/volume/denovo-assembly/metawrap\_50\_10\_bins.stats}.
Next to separating the columns nicely, it allows you to perform a lot of
operations like sorting conveniently. Check the cheat sheet
\href{https://jsvine.github.io/visidata-cheat-sheet/en/}{here}.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

In total, metaWRAP retained five bins, similarly to MaxBin2. Of these
five bins, the bins \texttt{bin.3} and \texttt{bin.4} had completeness
and contamination estimates that would qualify them for being
high-quality MAGs. However, we would need to check the presence of rRNA
and tRNA genes. The other three bins are medium-quality bins because
their completeness estimate was \textless{} 90\%.

\end{tcolorbox}

\hypertarget{taxonomic-assignment-of-contigs}{%
\subsection{Taxonomic assignment of
contigs}\label{taxonomic-assignment-of-contigs}}

What should we do when we simply want to know to which taxon a certain
contig most likely belongs to?

Reconstructing metagenome-assembled genomes requires multiple steps and
might not even provide the answer in the case that the contig of
interest is not binned into a MAG. Instead, it is sufficient to perform
a sequence search against a reference database.

There are plenty of tools available for this task, such as:

\begin{itemize}
\tightlist
\item
  BLAST/DIAMOND
\item
  Kraken2
\item
  Centrifuge
\item
  MMSeqs2
\end{itemize}

For each tool, we can either use pre-computed reference databases or
compute our own one. The two taxonomic classification systems that are
most commonly used are:

\begin{itemize}
\tightlist
\item
  NCBI Taxonomy
\item
  GTDB
\end{itemize}

As for any task that involves the alignment of sequences against a
reference database, the chosen reference database should fit the
sequences you are searching for. If your reference database does not
capture the diversity of your samples, you will not be able to assign a
subset of the contigs. There is also a trade-off between a large
reference database that contains all sequences and its memory
requirement. @Wright2023 elaborated on this quite extensively when
comparing Kraken2 against MetaPhlAn.

While all of these tools can do the job, I typically prefer to use the
program MMSeqs2 {[}@Steinegger2017{]} because it comes along with a very
fast algorithm based on aminoacid sequence alignment and implements a
lowest common ancestor (LCA) algorithm
(Figure~\ref{fig-denovoassembly-mmseqs2}). Recently, they implemented a
\emph{taxonomy} workflow {[}@Mirdita2021{]} that allows to efficiently
assign contigs to taxons. Luckily, it comes with multiple pre-computed
reference databases, such as the GTDB v207 reference database
{[}@Parks2020{]}, and therefore it is even more accessible for users.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/MMSeqs2_classify_Fig1.jpeg}

}

\caption{\label{fig-denovoassembly-mmseqs2}Scheme of the \emph{taxonomy}
workflow implemented into MMSeqs2. Adapted from @Mirdita2021, Fig. 1.}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

The latest version of the pre-computed GTDB reference database (r207)
requires about 105 GB of harddisk space and 700 GB of memory for
running.

As our computer infrastructure does not provide so much memory for every
user, I pre-computed the results. You can find the results
\texttt{2612.mmseqs2\_gtdb.tsv} in the folder
\texttt{/vol/volume/denovo-assembly}.

An alternative for users with a less powerful infrastructure is the
program \href{https://github.com/fbreitwieser/krakenuniq}{KrakenUniq}.

\end{tcolorbox}

Before running MMSeqs2's \emph{taxonomy} workflow against the GTDB
reference database, we need to install it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ refdbs/mmseqs2/gtdb}
\ExtensionTok{mmseqs}\NormalTok{ databases GTDB }\DataTypeTok{\textbackslash{}}
\NormalTok{    refdbs/mmseqs2/gtdb /tmp }\AttributeTok{{-}{-}threads}\NormalTok{ 14}
\end{Highlighting}
\end{Shaded}

Subsequently, we can align all the contigs of the sample 2612 against
the GTDB r207 with MMSeqs2:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ mmseqs2}
\ExtensionTok{mmseqs}\NormalTok{ createdb alignment/2612.fa mmseqs2/2612.contigs}
\ExtensionTok{mmseqs}\NormalTok{ taxonomy  mmseqs2/2612.contigs }\DataTypeTok{\textbackslash{}}
\NormalTok{    refdbs/mmseqs2/gtdb/mmseqs2\_gtdb }\DataTypeTok{\textbackslash{}}
\NormalTok{    mmseqs2/2612.mmseqs2\_gtdb }\DataTypeTok{\textbackslash{}}
\NormalTok{    /tmp }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}a} \AttributeTok{{-}{-}tax{-}lineage}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}lca{-}ranks}\NormalTok{ kingdom,phylum,class,order,family,genus,species }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}orf{-}filter}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}remove{-}tmp{-}files}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}threads}\NormalTok{ 14}
\ExtensionTok{mmseqs}\NormalTok{ createtsv mmseqs2/2612.contigs }\DataTypeTok{\textbackslash{}}
\NormalTok{    mmseqs2/2612.mmseqs2\_gtdb }\DataTypeTok{\textbackslash{}}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{What is the proportion of contigs that could be assigned to the
different taxonomic ranks, such as species or genus? What are the
dominant taxa?}

Hint: You can access this information easily by opening the file using
visidata: \texttt{vd\ 2612.mmseqs2\_gtdb.tsv}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

From the 3,606 contigs, MMSeqs2's \emph{taxonomy} workflow assigned
3,523 contigs to any taxonomy. For the rest, there was not enough
information and they were discarded.

From the 3,523 assigned contigs, 2,013 were assigned to the rank
``species'', while 1,137 could only be assigned to the rank ``genus''.

The most contigs were assigned the archael species \emph{Halococcus
morrhuae} (n=386), followed by the bacterial species \emph{Olsenella E
sp003150175} (n=298) and \emph{Collinsella sp900768795} (n=186).

\end{tcolorbox}

\hypertarget{taxonomic-assignment-of-mags}{%
\subsection{Taxonomic assignment of
MAGs}\label{taxonomic-assignment-of-mags}}

MMSeqs2's \emph{taxonomy} workflow is very useful to classify all
contigs taxonomically. However, how would we determine which species we
reconstructed by binning our contigs?

The simplest approach would be that we could summarise MMSeqs2's
taxonomic assignments of all contigs of a MAG and then determine which
lineage is the most frequent one. Although this would work in general,
there is another approach that is more sophisticated: GTDB toolkit
{[}GTDBTK, @Chaumeil2020{]}.

GTDBTK performs three steps to assign a MAG to a taxonomic lineage:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Lineage identification} based on single-copy marker genes
  using Hidden Markov Models (HMMs)
\item
  \textbf{Multi-sequence alignment} of the identified marker genes
\item
  Placement of the MAG genome into a \textbf{fixed reference tree} at
  class level
\end{enumerate}

The last step is particularly clever. Based on the known diversity of a
lineage present in the GTDB, it will construct a reference tree with all
known taxa of this lineage. Afterwards, the tree structure is fixed and
an algorithm attempts to create a new branch in the tree for placing the
unknown MAG based on both the tree structure and the multi-sequence
alignment.

In most cases, both the simple approach of taking the majority of the
MMSeqs2's \emph{taxonomy} results and the GTDBTK approach lead to very
similar results. However, GTDBTK performs better when determining
whether a new MAG potentially belongs to a new genus or even a new
family.

To infer which taxa our five reconstructed MAGs represent, we can run
the GTDBTK.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

The latest version of the database used by GTDBTK (r207) requires about
70 GB of harddisk space and 80 GB of memory for running.

As our computer infrastructure does not provide so much memory for every
user, I pre-computed the results. You can find the results
\texttt{2612.gtdbtk\_archaea.tsv} and \texttt{2612.gtdbtk\_bacteria.tsv}
in the folder \texttt{/vol/volume/denovo-assembly}.

\end{tcolorbox}

First, we need to install the GTDB database:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ refdbs/gtdbtk}
\FunctionTok{wget} \AttributeTok{{-}O}\NormalTok{ refdbs/gtdbtk/gtdbtk\_v2\_data.tar.gz }\DataTypeTok{\textbackslash{}}
\NormalTok{    https://data.gtdb.ecogenomic.org/releases/latest/auxillary\_files/gtdbtk\_v2\_data.tar.gz}
\FunctionTok{tar}\NormalTok{ xvf refdbs/gtdbtk/gtdbtk\_v2\_data.tar.gz }\AttributeTok{{-}C}\NormalTok{ refdbs/gtdbtk}
\end{Highlighting}
\end{Shaded}

Afterwards, we can run GTDBTK's \emph{classify} workflow:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ gtdbtk}
\VariableTok{GTDBTK\_DATA\_PATH}\OperatorTok{=}\StringTok{"}\VariableTok{$PWD}\StringTok{/refdbs/gtdbtk/gtdbtk\_r207\_v2"} \DataTypeTok{\textbackslash{}}
\ExtensionTok{gtdbtk}\NormalTok{ classify\_wf }\AttributeTok{{-}{-}cpu}\NormalTok{ 14 }\AttributeTok{{-}{-}extension}\NormalTok{ fa }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}genome\_dir}\NormalTok{ metawrap/BIN\_REFINEMENT/2612/metawrap\_50\_10\_bins }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}out\_dir}\NormalTok{ gtdbtk}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Do the classifications obtained by GTDBTK match the
classifications that were assigned to the contigs using MMSeqs2? Would
you expect these taxa given the archaeological context of the samples?}

Hint: You can access the classification results of GTDBTK easily by
opening the file using visidata: \texttt{vd\ 2612.gtdbtk\_archaea.tsv}
and \texttt{vd\ 2612.gtdbtk\_bacteria.tsv}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The five MAGs reconstructed from the sample 2612 were assigned to the
taxa:

\begin{itemize}
\tightlist
\item
  \texttt{bin.1}: \emph{Agathobacter rectalis}
\item
  \texttt{bin.2}: \emph{Halococcus morrhuae}
\item
  \texttt{bin.3}: \emph{Methanobrevibacter smithii}
\item
  \texttt{bin.4}: \emph{Ruminococcus bromii}
\item
  \texttt{bin.5}: \emph{Bifidobacterium longum}
\end{itemize}

All of these species were among the most frequent lineages that were
identified by MMSeqs2's \emph{taxonomy} workflow highlighting the large
overlap between the methods.

We would expect all five species to be present in our sample. All MAGs
but \texttt{bin.2} were assigned to human gut microbiome commensal
species that are typically found in healthy humans. The MAG
\texttt{bin.2} was assigned to a halophilic archaeal species, which is
typically found in salt mines.

\end{tcolorbox}

\hypertarget{evaluating-the-amount-of-ancient-dna-damage}{%
\subsection{Evaluating the amount of ancient DNA
damage}\label{evaluating-the-amount-of-ancient-dna-damage}}

One of the common questions that remain at this point of our analysis is
whether the contigs that we assembled show evidence for the presence of
ancient DNA damage. If yes, we could argue that these microbes are
indeed ancient, particularly when their DNA fragment length distribution
is rather short, too.

MAGs typically consist of either tens or hundreds of contigs. For this
case, many of the commonly used tools for quantifying the amount of
ancient DNA damage, such as damageprofiler {[}@Neukamm2021{]} or
mapDamage2 {[}@Jonsson2013{]}, are not very well suited because they
would require us to manually check each of their generated ``smiley
plots'', which visualise the amount of C-to-T substitutions at the end
of reads.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/denovo-assembly/Pydamage_NZ_JHCB02000011.1.png}

}

\caption{\label{fig-denovoassembly-pydamage}Overview of the model
comparison performed by pyDamage. The green line represents the null
model, i.e.~the absence of ancient DNA damage, while the orange line
represents the alternative model, i.e.~the presence of ancient DNA
damage.}

\end{figure}

Instead, we will use the program pyDamage {[}@Borry2021{]} that was
written with the particular use-case of metagenome-assembled contigs in
mind. Although pyDamage can visualise the amount of C-to-T substitutions
at the 5' end of reads, it goes a step further and fits two models upon
the substitution frequency (Figure~\ref{fig-denovoassembly-pydamage}).
The null hypothesis is that the observed distribution of C-to-T
substitutions at the 5' end of reads reflects a flat line, i.e.~a case
when no ancient DNA damage is present. The alternative model assumes
that the distribution resembles an exponential decay, i.e.~a case when
ancient DNA damage is present. By comparing the fit of these two models
to the observed data for each contig, pyDamage can quickly identify
contigs that are likely of ancient origin without requiring the user to
inspect the plots visually.

We can run pyDamage directly on the alignment data in BAM format:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pydamage}\NormalTok{ analyze }\AttributeTok{{-}w}\NormalTok{ 30 }\AttributeTok{{-}p}\NormalTok{ 14 alignment/2612.sorted.calmd.bam}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Evaluate the pyDamage results with respect to the amount of
C-to-T substitutions observed on the contigs! How many contigs does
pyDamage consider to show evidence for ancient DNA damage? How much
power (prediction accuracy) does it have for this decision? Which MAGs
are strongly ``ancient'' or ``modern''?}

Hint: You can access pyDamage's results easily by opening the file using
visidata: \texttt{vd\ pydamage\_results/pydamage\_results.tsv}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

From the 3,606 contigs, pyDamage inferred a q-value, i.e.~a p-value
corrected for multiple testing, of \textless{} 0.5 to 26 contigs. This
is partially due to a high fraction of the contigs with no sufficient
information to discriminate between the models (predicted accuracy of
\textless{} 0.5). However, the majority of the contigs with a prediction
accuracy of \textgreater{} 0.5 still had q-values of 0.05 and higher.
This suggests that overall the sample did not show large evidence of
ancient DNA damage.

This reflects also on the MAGs. Although four of the five MAGs were
human gut microbiome taxa, they did not show strong evidence of ancient
DNA damage. This suggests that the sample is too young and is well
preserved.

\end{tcolorbox}

\hypertarget{annotating-genomes-for-function}{%
\subsection{Annotating genomes for
function}\label{annotating-genomes-for-function}}

The second approach on how to study biological diversity of samples
using the assembly results is to compare the reconstructed genes and
their functions with each other.

While it is very interesting to reconstruct the metagenome-assembled
genomes from contigs and speculate about the evolution of a certain
taxon, this does not always help when studying the ecology of a
microbiome. Studying the ecology of a microbiome means to understand
which taxa are present in an environment, what type of community do they
form, what kind of natural products do they produce etc.?

With the lack of any data from culture isolates, it is rather difficult
to discriminate from which species a reconstructed gene sequence is
coming from, particularly when the contigs are short. Many microbial
species exchange genes among each other via horizontal gene transfer
which leads to multiple copies of a gene to be present in our metagenome
and increases the level of difficulty further.

Because of this, many researchers tend to annotate all genes of a MAGs
and compare the presence and absence of these genes across other genomes
that were taxonomically assigned to the same taxon. This analysis,
called pan-genome analysis, allows us to estimate the diversity of a
microbial species with respect to their repertoire of protein-coding
genes.

One of the most commonly used annotation tools for MAGs is Prokka
{[}@Seemann2014{]}, although it has recently been challenged by Bakta
{[}@Schwengers2021{]}. The latter provides the same functionality as
Prokka but incorporates more up-to-date reference databases for the
annotation. Therefore, the scientific community is slowly shifting to
Bakta.

Next to returning information on the protein-coding genes, Prokka also
returns the annotation of RNA genes (tRNAs and rRNAs), which will help
us to evaluate the quality of MAGs regarding the MIMAG criteria.

For this practical course, we will use Prokka and we will focus on
annotating the MAG \texttt{bin.3.fa} that we reconstructed from the
sample 2612.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{prokka} \AttributeTok{{-}{-}outdir}\NormalTok{ prokka }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}prefix}\NormalTok{ 2612\_003 }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}compliant} \AttributeTok{{-}{-}metagenome} \AttributeTok{{-}{-}cpus}\NormalTok{ 14 }\DataTypeTok{\textbackslash{}}
\NormalTok{    metawrap\_50\_10\_bins/bin.3.fa}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Question}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textbf{Prokka has annotated our MAG. What type of files does Prokka
return? How many genes/tRNAs/rRNAs were detected?}

Hint: Check the file \texttt{prokka/2612\_003.txt} for the number of
annotated elements.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Prokka returns the following files:

\begin{itemize}
\tightlist
\item
  \texttt{.faa}: the amino acid sequences of all identified coding
  sequences
\item
  \texttt{.ffn}: the nucleotide sequences of all identified coding
  sequences
\item
  \texttt{.fna}: all contigs in nucleotide sequence format renamed
  following Prokka's naming scheme
\item
  \texttt{.gbk}: all annotations and sequences in GenBank format
\item
  \texttt{.gff}: all annotations and sequences in GFF format
\item
  \texttt{.tsv}: the tabular overview of all annotations
\item
  \texttt{.txt}: the short summary of the annotation results
\end{itemize}

Prokka found 1,797 coding sequences, 32 tRNAs, but no rRNAs. Finding no
rRNAs is a common issues when trying to assemble MAGs without long-read
sequencing data and is not just characteristic for ancient DNA samples.
However, this means that we cannot technically call this MAG a
high-quality MAG due to the lack of the rRNA genes.

\end{tcolorbox}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

In this practical course you have gone through all the important steps
that are necessary for \emph{de novo} assembling ancient metagenomic
sequencing data to obtain contiguous DNA sequences with little error.
Furthermore, you have learned how to cluster these sequences into bins
without using any references and how to refine them based on
lineage-specific marker genes. For these refined bins, you have
evaluated their quality regarding common standards set by the scientific
community and assigned the MAGs to its most likely taxon. Finally, we
learned how to infer the presence of ancient DNA damage and annotate
them for RNA genes and protein-coding sequences.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Cautionary note - sequencing depth}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, opacitybacktitle=0.6]

Be aware of the sequencing depth when you assemble your sample. This
sample used in this practical course was not obtained by randomly
subsampling but I subsampled the sample so that we are able to
reconstruct MAGs.

The original sample had almost 200 million reads, however, I subsampled
it to less than 5 million reads. You usually need a lot of sequencing
data for \emph{de novo} assembly and definitely more data than for
reference-alignment based profiling. However, it also heavily depends on
the complexity of the sample. So the best advice is: \textbf{just give
it a try}!

\end{tcolorbox}

\hypertarget{authentication-and-decontamination}{%
\chapter{Authentication and
Decontamination}\label{authentication-and-decontamination}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/authentication-decontamination.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate authentication{-}decontamination}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{introduction-8}{%
\chapter{Introduction}\label{introduction-8}}

In ancient metagenomics we typically try to answer two questions: ``Who
is there?'' and ``How ancient?'', meaning we would like to detect an
organism and investigate whether this organism is ancient. There are
three typical ways to identify the presence of an organism in a
metagenomic sample:

\begin{itemize}
\tightlist
\item
  alignment of DNA fragments to a reference genome
  (\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322381/}{Bowtie},
  \href{https://pubmed.ncbi.nlm.nih.gov/19451168/}{BWA},
  \href{https://www.biorxiv.org/content/10.1101/050559v1}{Malt} etc.)
\item
  taxonomic (kmer-based) classification of DNA fragments
  (\href{https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0}{Kraken},
  \href{https://www.nature.com/articles/s41587-023-01688-w}{MetaPhlan},
  \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5131823/}{Centrifuge}
  etc.)
\item
  \emph{de-novo} genome assembly
  (\href{https://academic.oup.com/bioinformatics/article/31/10/1674/177884}{Megahit},
  \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5411777/}{metaSPAdes}
  etc.)
\end{itemize}

The first two are reference-based, i.e.~they assume a similarity of a
query ancient DNA fragment to a modern reference genome in a database.
This is a strong assumption, which might not be true for very old or
very diverged ancient organisms. This is the case when the
reference-free \emph{de-novo} assembly approach becomes powerful.
However, \emph{de-novo} assembly has its own computational challenges
for low-coverage ancient metagenomic samples that typically contain very
short DNA fragments.

\includegraphics{assets/images/chapters/authentication-decontamination/metagenomic_approaches.png}

While all the three types of metagenomic analysis are suitable for
exploring composition of metagenomic samples, they do not directly
validate the findings or provide information about ancient or endogenous
status of the detected orsganism. It can happen that the detected
organism

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  was mis-identified (the DNA belongs to another organism than initially
  thought),
\item
  has a modern origin (for example, lab or sequencing contaminant)
\item
  is of exogenous origin (for example, an ancient microbe that entered
  the host \emph{post-mortem}).
\end{enumerate}

Therefore, additional analysis is needed to follow-up each hit and
demonstrate its ancient origin. Below, we describe a few steps that can
help ancient metagenomic researchers to verify their findings and put
them into biological context.

In this chapter, we will cover:

\begin{itemize}
\tightlist
\item
  how to recognize that a detected organism was mis-identified based on
  breadth / evenness of coverage
\item
  how to validate findings by breadth of coverage filters via k-mer
  based taxonomic classification with KrakenUniq
\item
  how to validate findings using alignments and assess mapping quality,
  edit distance and evenness of coverage profile
\item
  how to detect modern contaminants via deamination profile, DNA
  fragmentation and post-mortem damage (PMD) scores
\item
  how negative (blank) controls can help disentangle ancient organisms
  from modern contaminants
\item
  how microbial source tracking can facilitate separating endogenous and
  exogenous microbial communities
\end{itemize}

The chapter has the following outline:

\begin{itemize}
\tightlist
\item
  Introduction
\item
  Simulated ancient metagenomic data
\item
  Genomic hit confirmation (how we see a true-positive hit)

  \begin{itemize}
  \tightlist
  \item
    Modern validation criteria

    \begin{itemize}
    \tightlist
    \item
      evenness and breadth of coverage
    \item
      alignment quality (edit distance, mapq)
    \item
      affinity to reference (percent identity, multi-allelic SNPs)
    \end{itemize}
  \item
    Ancient-specific validation

    \begin{itemize}
    \tightlist
    \item
      deamination profile (PMD scores)
    \item
      DNA fragmentation
    \end{itemize}
  \end{itemize}
\item
  Microbiome contamination correction

  \begin{itemize}
  \tightlist
  \item
    Decontamination via negative controls (blanks)
  \item
    Similarity to expected microbiome source (microbial source tracking)
  \end{itemize}
\end{itemize}

\hypertarget{simulated-ancient-metagenomic-data}{%
\chapter{Simulated ancient metagenomic
data}\label{simulated-ancient-metagenomic-data}}

You can begin the tutorial by changing into

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination}
\end{Highlighting}
\end{Shaded}

In this chapter, we will use 10 simulated with
\href{https://academic.oup.com/bioinformatics/article/33/4/577/2608651}{gargammel}
ancient metagenomic samples from @Pochon2022-hj.\\

\begin{figure}

{\centering \includegraphics{assets/images/chapters/authentication-decontamination/aMeta.png}

}

\caption{\label{fig-authenticationdecontamination-ameta}Screenshot of
preprint of aMeta by Pochon et al.~2022}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The raw simulated data can be accessed via
\url{https://doi.org/10.17044/scilifelab.21261405}

To download the simulated ancient metagenomic data please use the
following command lines:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ ameta/ }\KeywordTok{\&\&} \BuiltInTok{cd}\NormalTok{ ameta/}
\FunctionTok{wget}\NormalTok{ https://figshare.scilifelab.se/ndownloader/articles/21261405/versions/1 }\DataTypeTok{\textbackslash{}}
\KeywordTok{\&\&} \FunctionTok{unzip}\NormalTok{ 1 }\KeywordTok{\&\&} \FunctionTok{rm}\NormalTok{ 1}
\end{Highlighting}
\end{Shaded}

The DNA reads were simulated with damage, sequencing errors and Illumina
adapters, therefore one will have to trim the adapters first:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \VariableTok{$(}\FunctionTok{ls} \PreprocessorTok{*}\NormalTok{.fastq.gz}\VariableTok{)}
\ControlFlowTok{do}
\VariableTok{sample\_name}\OperatorTok{=}\VariableTok{$(}\FunctionTok{basename} \VariableTok{$i}\NormalTok{ .fastq.gz}\VariableTok{)}
\ExtensionTok{cutadapt} \AttributeTok{{-}a}\NormalTok{ AGATCGGAAGAG }\AttributeTok{{-}{-}minimum{-}length}\NormalTok{ 30 }\AttributeTok{{-}o} \VariableTok{$\{sample\_name\}}\NormalTok{.trimmed.fastq.gz }\VariableTok{$\{sample\_name\}}\NormalTok{.fastq.gz }\AttributeTok{{-}j}\NormalTok{ 4}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

Now, after the basic data pre-processing has been done, we can proceed
with validation, authentication and decontamination analyses.

\end{tcolorbox}

In here you will see a range of directories, each representing different
parts of this tutorial. One set of trimmed `simulated' reads from
@Pochon2022-hj in \texttt{rawdata/}.

\hypertarget{genomic-hit-confirmation}{%
\chapter{Genomic hit confirmation}\label{genomic-hit-confirmation}}

Once an organism has been detected in a sample (via alignment,
classification or \emph{de-novo} assembly), one needs to take a closer
look at multiple quality metrics in order to reliably confirm that the
organism is not a false-positive detection and is of ancient origin. The
methods used for this purpose can be divided into modern validation and
ancient-specific validation criteria. Below, we will cover both of them.

\hypertarget{modern-validation-criteria}{%
\section{Modern validation criteria}\label{modern-validation-criteria}}

The modern validation methods aim at confirming organism presence
regardless of its ancient status. The main approaches include evenness /
breadth of coverage computation, assessing alignment quality, and
monitoring affinity of the DNA reads to the reference genome of the
potential host.

\hypertarget{depth-vs-breadth-and-evenness-of-coverage}{%
\subsection{Depth vs breadth and evenness of
coverage}\label{depth-vs-breadth-and-evenness-of-coverage}}

Concluding organism presence by relying solely on the numbers of
assigned sequenced reads (aka depth of coverage metric) turns out to be
not optimal and too permissive, which may result in a large amount of
false-positive discoveries. For example, when using alignment to a
reference genome, the mapped reads may demonstrate non-uniform coverage
as visualized in the
\href{https://software.broadinstitute.org/software/igv/}{Integrative
Genomics Viewer (IGV)} below.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/authentication-decontamination/IGV_uneven_coverage_Y.pestis.png}

}

\caption{\label{fig-authenticationdecontamination-igv}Screenshot of the
IGV software. The reference genome bar is shown at the top, and in the
main panel tall isolated `towers' of reads with lots of coloured bases
representing read `stacking' i.e., un-uniform distribution of reads
across the whole genome (as expected of the correct reference genome),
but accumulation of all reads in the same isolated places on the
reference genome, with the many variants on the reads suggesting they
are from different species and aligning to conserved regions.}

\end{figure}

In this case, DNA reads originating from another microbe were
(mis-)aligned to \emph{Yersina pestis} reference genome. It can be
observed that a large number the reads align only to a few conserved
genomic loci. Therefore, even if many thousands of DNA reads are capable
of aligning to the reference genome, the overall uneven alignment
pattern suggests no presence of \emph{Yersina pestis} in the metagenomic
sample. Thus, not only the number of assigned reads (proportional to
depth of coverage metric) but also the \textbf{breadth and evenness of
coverage} metrics become of particular importance for verification of
metagenomic findings, i.e.~hits with DNA reads uniformly aligned across
the reference genome are more likely to be true-positive detections
(Figure~\ref{fig-authenticationdecontamination-igv}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/authentication-decontamination/depth_vs_breadth_of_coverage.png}

}

\caption{\label{fig-authenticationdecontamination-igv}Schematic diagram
of the differences between a) read stacking (all reads aligned at one
position fo the genome), indicating you've not correctly identified the
organism, vs b) reads distributed across all the genome. A formula at
the bottom of the image shows how both A and B have the same depth of
coverage even though they have very different actual patterns on the
genome.}

\end{figure}

In the next sections, we will show how to practically compute the
breadth and evenness of coverage via KrakenUniq and Samtools.

\hypertarget{breadth-of-coverage-via-krakenuniq}{%
\subsection{Breadth of coverage via
KrakenUniq}\label{breadth-of-coverage-via-krakenuniq}}

Here we are going to demonstrate that one can assess breadth of coverage
information already at the taxonomic profiling step. Although taxonomic
classifiers do not perform alignment, some of them, such as
\href{https://genomebiology.biomedcentral.com/articles/10.1186/s13059-018-1568-0}{KrakenUniq}
and
\href{https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0}{Kraken2}
provide a way to infer breadth of coverage in addition to the number of
assigned reads to a taxon. This allows for immediate filtering out a lot
of false positive hits. Since Kraken-family classifiers are typically
faster and
\href{https://www.biorxiv.org/content/10.1101/2022.06.01.494344v1.full.pdf}{less
memory-demanding}, i.e.~can work with very large reference databases,
compared to genome aligners, they provide a robust and fairly unbiased
initial taxonomic profiling, which can still later be followed-up with
proper alignment and computing evenness of coverage as described above.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

⚠️ This step will require large amounts of memory and CPUs!, so if
running yourself please note this step is better suited for a server,
HPC cluster, or the cloud rather than on a laptop!

To profile the data with KrakenUniq one needs a database, a pre-built
complete microbial NCBI RefSeq database can be accessed via
\url{https://doi.org/10.17044/scilifelab.21299541}.

Please use the following command line to download the database:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ krakenuniq/ }\CommentTok{\#\# If you\textquotesingle{}ve left it...}

\FunctionTok{wget}\NormalTok{ https://figshare.scilifelab.se/ndownloader/articles/21299541/versions/1 }\DataTypeTok{\textbackslash{}}
\KeywordTok{\&\&} \FunctionTok{unzip}\NormalTok{ 1 }\KeywordTok{\&\&} \FunctionTok{rm}\NormalTok{ 1}
\end{Highlighting}
\end{Shaded}

The following example command is how you would execute KrakenUniq.

\begin{Shaded}
\begin{Highlighting}[]

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \VariableTok{$(}\FunctionTok{ls} \PreprocessorTok{*}\NormalTok{.trimmed.fastq.gz}\VariableTok{)}
\ControlFlowTok{do}
\ExtensionTok{krakenuniq} \AttributeTok{{-}{-}db}\NormalTok{ KRAKENUNIQ\_DB }\AttributeTok{{-}{-}fastq{-}input} \VariableTok{$i} \AttributeTok{{-}{-}threads}\NormalTok{ 20 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}classified{-}out }\VariableTok{$\{i\}}\NormalTok{.classified\_sequences.krakenuniq }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}unclassified{-}out }\VariableTok{$\{i\}}\NormalTok{.unclassified\_sequences.krakenuniq }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}output }\VariableTok{$\{i\}}\NormalTok{.sequences.krakenuniq }\AttributeTok{{-}{-}report{-}file} \VariableTok{$\{i\}}\NormalTok{.krakenuniq.output}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Taxonomic k-mer-based classification of the ancient metagenomic reads
can be done via KrakenUniq. However as this requires a very large
database file, the results from running KrakenUniq on the 10 simulated
genomes can be found in.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ krakenuniq/}
\end{Highlighting}
\end{Shaded}

KrakenUniq by default delivers a proxy metric for breadth of coverage
called the \textbf{number of unique kmers} (in the 4th column of its
output table) assigned to a taxon. KrakenUniq output can be easily
filtered with respect to both depth and breadth of coverage, which
substantially reduces the number of false-positive hits.

\includegraphics{assets/images/chapters/authentication-decontamination/krakenuniq_filter.png}

We can filter the KrakenUniq output with respect to both depth
(\emph{taxReads}) and breadth (\emph{kmers}) of coverage with the
following custom Python script, which selects only species with at east
200 assigned reads and 1000 unique k-mers. After the filtering, we can
see a \emph{Yersinia pestis} hit in the \emph{sample 10} that possess
the filtering thresholds with respect to both depth and breadth of
coverage.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \VariableTok{$(}\FunctionTok{ls} \PreprocessorTok{*}\NormalTok{.krakenuniq.output}\VariableTok{)}
\ControlFlowTok{do}
\ExtensionTok{../scripts/filter\_krakenuniq.py} \VariableTok{$i}\NormalTok{ 1000 200 ../scripts/pathogensfound.very\_inclusive.tab}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/filtered_krakenuniq_output.png}

We can also easily produce a KrakenUniq taxonomic abundance table
\emph{krakenuniq\_abundance\_matrix.txt} using the custom R script
below, which takes as argument the contents of the \texttt{krakenuniq/}
folder containing the KrakenUniq output files.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Rscript}\NormalTok{ ../scripts/krakenuniq\_abundance\_matrix.R . krakenuniq\_abundance\_matrix/ 1000 200}
\end{Highlighting}
\end{Shaded}

From the \emph{krakenuniq\_abundance\_matrix.txt} table inside the
resulting directory, it becomes clear that \emph{Yersinia pestis} seems
to be present in a few other samples in addition to sample 10.

\includegraphics{assets/images/chapters/authentication-decontamination/krakenuniq_abundance_matrix.png}

While KrakenUniq delivers information about breadth of coverage by
default, you can also get this information from Kraken2.

For this one has to use a special flag \emph{--report-minimizer-data}
when running Kraken2 in order to get the breadth of coverage proxy which
is called the \textbf{number of distinct minimizers} for the case of
Kraken2. Below, we provide an example Kraken2 command line containing
the distinct minimizer flag:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example command {-} do not run!}
\VariableTok{DBNAME}\OperatorTok{=}\NormalTok{Kraken2\_DB\_directory}
\VariableTok{KRAKEN\_INPUT}\OperatorTok{=}\NormalTok{sample.fastq.gz}
\VariableTok{KRAKEN\_OUTPUT}\OperatorTok{=}\NormalTok{Kraken2\_output\_directory}
\ExtensionTok{kraken2} \AttributeTok{{-}{-}db} \VariableTok{$DBNAME} \AttributeTok{{-}{-}fastq{-}input} \VariableTok{$KRAKEN\_INPUT} \AttributeTok{{-}{-}threads}\NormalTok{ 20 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}classified{-}out }\VariableTok{$KRAKEN\_OUTPUT}\NormalTok{/classified\_sequences.kraken2 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}unclassified{-}out }\VariableTok{$KRAKEN\_OUTPUT}\NormalTok{/unclassified\_sequences.kraken2 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}output }\VariableTok{$KRAKEN\_OUTPUT}\NormalTok{/sequences.kraken2 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}report }\VariableTok{$KRAKEN\_OUTPUT}\NormalTok{/kraken2.output }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}use{-}names }\AttributeTok{{-}{-}report{-}minimizer{-}data}
\end{Highlighting}
\end{Shaded}

Then the filtering of Kraken2 output with respect to breadth and depth
of coverage can be done by analogy with filtering KrakenUniq output
table. In case of \emph{de-novo} assembly, the original DNA reads are
typically aligned back to the assembled contigs, and the evenness /
breadth of coverage can be computed from these alignments.

\hypertarget{evenness-of-coverage-via-samtools}{%
\subsection{Evenness of coverage via
Samtools}\label{evenness-of-coverage-via-samtools}}

Now, after we have detected an interesting \emph{Y. pestis} hit, we
would like to follow it up, and compute multiple quality metrics
(including proper breadth and evenness of coverage) from alignments
(Bowtie2 aligner will be used in our case) of the DNA reads to the
\emph{Y. pestis} reference genome. Below, we download \emph{Yersinia
pestis} reference genome from NCBI, build its Bowtie2 index, and align
trimmed reads against \emph{Yersinia pestis} reference genome with
Bowtie2. Do not forget to sort and index the alignments as it will be
important for computing the evenness of coverage. It is also recommended
to remove multi-mapping reads, i.e.~the ones that have MAPQ = 0, at
least for Bowtie and BWA aligners that are commonly used in ancient
metagenomics. Samtools with \emph{-q} flag can be used to extract reads
with MAPQ \textgreater{} = 1.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/bowtie2}

\CommentTok{\#\# Download reference genome}
\VariableTok{NCBI}\OperatorTok{=}\NormalTok{https://ftp.ncbi.nlm.nih.gov}\KeywordTok{;} \VariableTok{ID}\OperatorTok{=}\NormalTok{GCF\_000222975.1\_ASM22297v1}
\FunctionTok{wget} \VariableTok{$NCBI}\NormalTok{/genomes/all/GCF/000/222/975/}\VariableTok{$\{ID\}}\NormalTok{/}\VariableTok{$\{ID\}}\NormalTok{\_genomic.fna.gz}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/bowtie2}

\CommentTok{\#\# Prepare reference genome and build Bowtie2 index}
\FunctionTok{gunzip} \VariableTok{$\{ID\}}\NormalTok{\_genomic.fna.gz}\KeywordTok{;} \BuiltInTok{echo}\NormalTok{ NC\_017168.1 }\OperatorTok{\textgreater{}}\NormalTok{ region.bed}
\ExtensionTok{seqtk}\NormalTok{ subseq }\VariableTok{$\{ID\}}\NormalTok{\_genomic.fna region.bed }\OperatorTok{\textgreater{}}\NormalTok{ NC\_017168.1.fasta}
\ExtensionTok{bowtie2{-}build} \AttributeTok{{-}{-}large{-}index}\NormalTok{ NC\_017168.1.fasta NC\_017168.1.fasta }\AttributeTok{{-}{-}threads}\NormalTok{ 10}

\CommentTok{\#\# Run alignment of raw reads against FASTQ}
\ExtensionTok{bowtie2} \AttributeTok{{-}{-}large{-}index} \AttributeTok{{-}x}\NormalTok{ NC\_017168.1.fasta }\AttributeTok{{-}{-}end{-}to{-}end} \AttributeTok{{-}{-}threads}\NormalTok{ 10 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}very{-}sensitive }\AttributeTok{{-}U}\NormalTok{ ../rawdata/sample10.trimmed.fastq.gz }\KeywordTok{|} \ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}bS} \AttributeTok{{-}h} \AttributeTok{{-}q}\NormalTok{ 1 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}@ 20 }\AttributeTok{{-}} \OperatorTok{\textgreater{}}\NormalTok{ Y.pestis\_sample10.bam}

\CommentTok{\#\# Sort and index BAM files for rapid access in downstream commands}
\ExtensionTok{samtools}\NormalTok{ sort Y.pestis\_sample10.bam }\AttributeTok{{-}@}\NormalTok{ 10 }\OperatorTok{\textgreater{}}\NormalTok{ Y.pestis\_sample10.sorted.bam}
\ExtensionTok{samtools}\NormalTok{ index Y.pestis\_sample10.sorted.bam}
\end{Highlighting}
\end{Shaded}

Next, the breadth / evenness of coverage can be computed from the
BAM-alignments via \emph{samtools depth} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ depth }\AttributeTok{{-}a}\NormalTok{ Y.pestis\_sample10.sorted.bam }\OperatorTok{\textgreater{}}\NormalTok{ Y.pestis\_sample10.sorted.boc}
\end{Highlighting}
\end{Shaded}

and visualized using for example the following R code snippet
(alternatively
\href{https://github.com/MeriamGuellil/aDNA-BAMPlotter}{aDNA-BAMPlotter}
can be used):

Load R by typing \texttt{R} into your terminal. Note the following may
take a minute or so to run.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read output of samtools depth commans}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"Y.pestis\_sample10.sorted.boc"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Ref"}\NormalTok{, }\StringTok{"Pos"}\NormalTok{, }\StringTok{"N\_reads"}\NormalTok{)}

\CommentTok{\# Split reference genome in tiles, compute breadth of coverage for each tile}
\NormalTok{N\_tiles }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{step }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{max}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Pos) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Pos)) }\SpecialCharTok{/}\NormalTok{ N\_tiles}
\NormalTok{tiles }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\NormalTok{N\_tiles) }\SpecialCharTok{*}\NormalTok{ step; boc }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(tiles))}
\NormalTok{\{}
\NormalTok{  df\_loc }\OtherTok{\textless{}{-}}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{Pos }\SpecialCharTok{\textgreater{}=}\NormalTok{ tiles[i] }\SpecialCharTok{\&}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{Pos }\SpecialCharTok{\textless{}}\NormalTok{ tiles[i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{], ]}
\NormalTok{  boc }\OtherTok{\textless{}{-}} \FunctionTok{append}\NormalTok{(boc, }\FunctionTok{rep}\NormalTok{(}\FunctionTok{sum}\NormalTok{(df\_loc}\SpecialCharTok{$}\NormalTok{N\_reads }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(df\_loc}\SpecialCharTok{$}\NormalTok{N\_reads),}
  \FunctionTok{dim}\NormalTok{(df\_loc)[}\DecValTok{1}\NormalTok{]))}
\NormalTok{\}}

\NormalTok{boc[}\FunctionTok{is.na}\NormalTok{(boc)]}\OtherTok{\textless{}{-}}\DecValTok{0}\NormalTok{; df}\SpecialCharTok{$}\NormalTok{boc }\OtherTok{\textless{}{-}}\NormalTok{ boc}
\FunctionTok{plot}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{boc }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{Pos, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Genome position"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Coverage"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{mtext}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\FunctionTok{round}\NormalTok{((}\FunctionTok{sum}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{N\_reads }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{N\_reads)) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }
\StringTok{"\% of genome covered"}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/Evenness_of_coverage.png}

In the R script above, we simply split the reference genome into
\emph{N\_tiles} tiles and compute the breadth of coverage (number of
reference nucleotides covered by at least one read normalized by the
total length) locally in each tile. By visualizing how the local breadth
of coverage changes from tile to tile, we can monitor the distribution
of the reads across the reference genome. In the evenness of coverage
figure above, the reads seem to cover all parts of the reference genome
uniformly, which is a good evidence of true-positive detection, even
though the total mean breadth of coverage is low due to the low total
number of reads.

\hypertarget{alignment-quality}{%
\subsection{Alignment quality}\label{alignment-quality}}

In addition to evenness and breadth of coverage, it is very informative
to monitor how well the metagenomic reads map to a reference genome.
Here one can control for \textbf{mapping quality}
(\href{https://samtools.github.io/hts-specs/SAMv1.pdf}{MAPQ} field in
the BAM-alignments) and the number of mismatches for each read,
i.e.~\textbf{edit distance}.

Mapping quality (MAPQ) can be extracted from the 5th column of
BAM-alignments using Samtools and \emph{cut} command in bash.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ view Y.pestis\_sample10.sorted.bam }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}f5} \OperatorTok{\textgreater{}}\NormalTok{ mapq.txt}
\end{Highlighting}
\end{Shaded}

Then the 5th column of the filtered BAM-alignment can be visualized via
a simple histogram in R as below for two random metagenomic samples.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{readLines}\NormalTok{(}\StringTok{"mapq.txt"}\NormalTok{)), }\AttributeTok{col =} \StringTok{"darkred"}\NormalTok{, }\AttributeTok{breaks =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/MAPQ.png}

Note that MAPQ scores are computed slightly differently for Bowtie and
BWA, so they are not directly comparable, however, for both MAPQ
\textasciitilde{} 10-30, as in the histograms below, indicates good
affinity of the DNA reads to the reference genome. here we provide some
examples of how typical MAPQ histograms for Bowtie2 and BWA alignments
can look like:

\includegraphics{assets/images/chapters/authentication-decontamination/mapq.png}

Edit distance can be extracted by gathering information in the NM-tag
inside BAM-alignments, which reports the number of mismatches for each
aligned read. This can be done either in bash / awk, or using handy
functions from \emph{Rsamtools} R package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"Rsamtools"}\NormalTok{)}
\NormalTok{param }\OtherTok{\textless{}{-}} \FunctionTok{ScanBamParam}\NormalTok{(}\AttributeTok{tag =} \StringTok{"NM"}\NormalTok{)}
\NormalTok{bam }\OtherTok{\textless{}{-}} \FunctionTok{scanBam}\NormalTok{(}\StringTok{"Y.pestis\_sample10.sorted.bam"}\NormalTok{, }\AttributeTok{param =}\NormalTok{ param)}
\FunctionTok{barplot}\NormalTok{(}\FunctionTok{table}\NormalTok{(bam[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{tag}\SpecialCharTok{$}\NormalTok{NM), }\AttributeTok{ylab=}\StringTok{"Number of reads"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Number of mismatches"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/edit_distance.png}

In the barplot above we can see that the majority of reads align either
without or with very few mismatches, which is an evidence of high
affinity of the aligned reads with respect to the reference genome. For
a true-positive finding, the edit distance barplot typically has a
decreasing profile. However, for a very degraded DNA, it can have a mode
around 1 or 2, which can also be reasonable. A false-positive hit would
have a mode of the edit distance barplot shifted toward higher numbers
of mismatches.

\hypertarget{affinity-to-reference}{%
\subsection{Affinity to reference}\label{affinity-to-reference}}

Very related to edit distance is another alignment validation metric
which is called \textbf{percent identity}. It represents a barplot
demonstrating the numbers of reads that are 100\% identical to the
reference genome (i.e.~map without a single mismatch), 99\% identical,
98\% identical etc. Misaligned reads originating from another related
organism have typically most reads with percent identity of 93-96\%. In
the figure below, the panels (c--e) demonstrate different percent
identity distributions. In panel c, most reads show a high similarity to
the reference, which indicates a correct assignment of the reads to the
reference. In panel d, most reads are highly dissimilar to the
reference, which suggests that they originate from different related
species. In some cases, as in panel e, a mixture of correctly assigned
and misassigned reads can be observed.

\includegraphics{assets/images/chapters/authentication-decontamination/multialleleicSNPs.png}

Another important way to detect reads that cross-map between related
species is \textbf{haploidy} or checking the amount of
\textbf{multi-allelic SNPs}. Because bacteria are haploid organisms,
only one allele is expected for each genomic position. Only a small
number of multi-allelic sites are expected, which can result from a few
mis-assigned or incorrectly aligned reads. In the figure above, panels
(f--i) demonstrate histograms of SNP allele frequency distributions.
Panel f demonstrates the situation when we have only a few multi-allelic
sites originating from a misaligned reads. This is a preferable case
scenario corresponding to correct assignment of the reads to the
reference. Please also check the corresponding ``Good alignments'' IGV
visualization to the right in the figure above.

In contrast, a large number of multi-allelic sites indicates that the
assigned reads originate from more than one species or strain, which can
result in symmetric allele frequency distributions (e.g., if two species
or strains are present in equal abundance) (panel g) or asymmetric
distributions (e.g., if two species or strains are present in unequal
abundance) (panel h). A large number of mis-assigned reads from closely
related species can result in a large number of multi-allelic sites with
low frequencies of the derived allele (panel i). The situations (g-i)
correspond to incorrect assignment of the reads to the reference. Please
also check the corresponding ``Bad alignments'' IGV visualization to the
right in the figure above.

\hypertarget{ancient-specific-validation-criteria}{%
\section{Ancient-specific validation
criteria}\label{ancient-specific-validation-criteria}}

In contrast to modern genomic hit validation criteria, the
ancient-specific validation methods concentrate on DNA degradation and
damage pattern as ultimate signs of ancient DNA. Below, we will discuss
deamination profile, read length distribution and post mortem damage
(PMD) scores metrics that provide good confirmation of ancient origin of
the detected organism.

\hypertarget{ancient-status}{%
\subsection{Ancient status}\label{ancient-status}}

Checking evenness of coverage and alignment quality can help us to make
sure that the organism we are thinking about is really present in the
metagenomic sample. However, we still need to address the question ``How
ancinet?''. For this purpose we need to compute \textbf{deamination
profile} and \textbf{read length distribution} of the aligned reads in
order to prove that they demonstrate damage pattern and are sufficiently
fragmented, which would be a good evidence of ancient origin of the
detected organisms.

Deamination profile of a damaged DNA demonstrate an enrichment of C / T
polymorphisms at the ends of the reads compared to all other single
nucleotide substitutions. There are several tools for computing
deamination profile, but perhaps the most popular is
\href{https://academic.oup.com/bioinformatics/article/29/13/1682/184965}{mapDamage}.
The tool can be run using the following command line, still in the
\texttt{authentication-decontamination/bowtie2/} directory:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{mapDamage} \AttributeTok{{-}i}\NormalTok{ Y.pestis\_sample10.sorted.bam }\AttributeTok{{-}r}\NormalTok{ NC\_017168.1.fasta }\AttributeTok{{-}d}\NormalTok{ mapDamage\_results/ }\AttributeTok{{-}{-}merge{-}reference{-}sequences} \AttributeTok{{-}{-}no{-}stats}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/deamination.png}

maDamage delivers a bunch of useful statistics, among other read length
distribution can be checked. A typical mode of DNA reads should be
within a range 30-70 base-pairs in order to be a good evidence of DNA
fragmentation. Reads longer tha 100 base-pairs are more likely to
originate from modern contamination.

\includegraphics{assets/images/chapters/authentication-decontamination/read_length.png}

Another useful tool that can be applied to assess how DNA is damaged is
\href{https://github.com/pontussk/PMDtools}{PMDtools} which is a
maximum-likelihood probabilistic model that calculates an ancient score,
\textbf{PMD score}, for each read. The ability of PMDtools to infer
ancient status with a single read resolution is quite unique and
different from mapDamage that can only assess deamination based on a
number of reads. PMD scores can be computed using the following command
line, please note that Python2 is needed for this purpose.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}h}\NormalTok{ Y.pestis\_sample10.bam }\KeywordTok{|} \ExtensionTok{pmdtools} \AttributeTok{{-}{-}printDS} \OperatorTok{\textgreater{}}\NormalTok{ PMDscores.txt}
\end{Highlighting}
\end{Shaded}

The distribution of PMD scores can be visualized via a histogram in R as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pmd\_scores }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"PMDscores.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(pmd\_scores}\SpecialCharTok{$}\NormalTok{V4, }\AttributeTok{breaks =} \DecValTok{1000}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"PMDscores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/pmd_scores.png}

Typically, reads with PMD scores greater than 3 are considered to be
reliably ancient, i.e.~damaged, and can be extracted for taking a closer
look. Therefore PMDtools is great for separating ancient reads from
modern contaminant reads.

As mapDamage, PMDtools can also compute deamination profile. However,
the advantage of PMDtools that it can compute deamination profile for
UDG / USER treated samples (with the flag \emph{--CpG}). For this
purpose, PMDtools uses only CpG sites which escape the treatment, so
deamination is not gone completely and there is a chance to authenticate
treated samples. Computing deamination pattern with PMDtools can be
achieved with the following command line (please note that the scripts
\emph{pmdtools.0.60.py} and \emph{plotPMD.v2.R} can be downloaded from
the github repository here https://github.com/pontussk/PMDtools):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ view Y.pestis\_sample10.bam }\KeywordTok{|} \ExtensionTok{pmdtools} \AttributeTok{{-}{-}platypus} \OperatorTok{\textgreater{}}\NormalTok{ PMDtemp.txt}

\ExtensionTok{R}\NormalTok{ CMD BATCH plotPMD}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/PMD_Skoglund_et_al_2015_Current_Biology.png}

When performing ancient status analysis on \textbf{de-novo} assembled
contigs, it can be computationally challenging and time consuming to run
mapDamage or PMDtools on all of them as there can be hundreds of
thousands contigs. In addition, the outputs from mapDamage and PMDtools
lacking a clear numeric quantity or a statistical test that could
provide an ``ancient vs.~non-ancient'' decision for each
\textbf{de-novo} assembled contig. To address these limitations,
\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323603/pdf/peerj-09-11845.pdf}{pyDamage}
tool was recently developed. PyDamage evaluates the amount of aDNA
damage and tests the hypothesis whether a model assuming presence of
aDNA damage better explains the data than a null model.

\includegraphics{assets/images/chapters/authentication-decontamination/pyDamage.png}

pyDamage can be run on a sorted BAM-alignments of the microbial reads to
the \textbf{de-novo} assembled contigs using the following command line:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Do not run!}
\ExtensionTok{pydamage}\NormalTok{ analyze }\AttributeTok{{-}w}\NormalTok{ 30 }\AttributeTok{{-}p}\NormalTok{ 14 filtered.sorted.bam}
\end{Highlighting}
\end{Shaded}

\hypertarget{microbiome-contamination-correction}{%
\chapter{Microbiome contamination
correction}\label{microbiome-contamination-correction}}

Modern contamination can severely bias ancient metagenomic analysis.
Also, ancient contamination, i.e.~entered \emph{post-mortem}, can
potentially lead to false biological interpretations. Therefore, a lot
of efforts in the ancient metagenomics field are directed on
establishing methodology for identification of contaminants. Among them,
the use of negative (blank) control samples is perhaps the most reliable
and straightforward method. Additionally, one often performs microbial
source tracking for predicting environment (including contamination
environment) of origin for ancient metagenomic samples.

\hypertarget{decontamination}{%
\section{Decontamination}\label{decontamination}}

Modern contamination is one of the major problems in ancient
metagenomics analysis. Large fractions of modern bacterial, animal or
human DNA in metagenomic samples can lead to false biological and
historical conclusions. A lot of
\href{https://onlinelibrary.wiley.com/doi/10.1002/bies.202000081}{scientific
literature} is dedicated to this topic, and comprehensive tables and
sources of potential contamination (e.g.~animal and bacterial DNA
present in PCR reagents) are available.

\includegraphics{assets/images/chapters/authentication-decontamination/contaminants_literature.png}

A good practice to discriminate between endogenous and contaminant
organisms is to sequence negative controls, so-called \textbf{blanks}.
Organisms detected on blanks, like the microbial genera reported in the
table below, can substantially facilitate making more informed decision
about true metagenomic profile of a sample. Nevertheless, the table
below may seem rather conservative since in addition to well-known
environmental contaminants as \emph{Burkholderia} and \emph{Pseudomonas}
it includes also human oral genera as \emph{Streptococcus}, which are
probably less likely to be of environmental origin.

\includegraphics{assets/images/chapters/authentication-decontamination/contaminants_list.png}

It is typically assumed that an organism found on a blank has a lower
confidence to be endogenous to the studied metagenomic sample, and
sometimes it is even excluded from the downstream analysis as an
unreliable hit. Despite there are attempts to automate filtering out
modern contaminants (we will discuss them below), decontamination
process still remains to be a tedious manual work where each candidate
should be carefully investigated from different contexts in order to
prove its ancient and endogenous origin.

If negative control samples (blanks) are available, contaminating
organisms can be detected by comparing their abundances in the negative
controls with true samples. In this case, contaminant organisms stand
out by their high prevalence in both types of samples if one simply
plots mean across samples abundance of each detected organism in true
samples and negative controls against each other as in the figure below.

First move back into the KrakenUniq folder and load R.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/krakenuniq}
\ExtensionTok{R}
\end{Highlighting}
\end{Shaded}

Then in here we can compare between true samples and negative controls

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples}\OtherTok{\textless{}{-}}\FunctionTok{read.delim}\NormalTok{(}\StringTok{"krakenuniq\_abundance\_matrix/krakenuniq\_abundance\_matrix.txt"}\NormalTok{,}\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{,}
\AttributeTok{row.names =} \DecValTok{1}\NormalTok{, }\AttributeTok{check.names =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\DocumentationTok{\#\# The blanks abundance matrix has already been made for you}
\NormalTok{controls}\OtherTok{\textless{}{-}}\FunctionTok{read.delim}\NormalTok{(}\StringTok{"blank\_krakenuniq\_abundance\_matrix.txt"}\NormalTok{,}\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{,}
\AttributeTok{row.names =} \DecValTok{1}\NormalTok{, }\AttributeTok{check.names =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(samples, controls, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{by =} \StringTok{"row.names"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(df)}\OtherTok{\textless{}{-}}\NormalTok{df}\SpecialCharTok{$}\NormalTok{Row.names; df}\SpecialCharTok{$}\NormalTok{Row.names }\OtherTok{\textless{}{-}} \ConstantTok{NULL}\NormalTok{; df[}\FunctionTok{is.na}\NormalTok{(df)] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{true\_sample }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df,}\AttributeTok{select=}\FunctionTok{colnames}\NormalTok{(df)[}\SpecialCharTok{!}\FunctionTok{grepl}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\FunctionTok{colnames}\NormalTok{(df))])}
\NormalTok{negative\_control }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df,}\AttributeTok{select=}\FunctionTok{colnames}\NormalTok{(df)[}\FunctionTok{grepl}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\FunctionTok{colnames}\NormalTok{(df))])}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),}
\AttributeTok{xlab =} \StringTok{"Log10 ( Negative controls )"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Log10 ( True samples )"}\NormalTok{,}
\AttributeTok{main =} \StringTok{"Organism abundance in true samples vs. negative controls"}\NormalTok{,}
\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}

\FunctionTok{points}\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{)] }\SpecialCharTok{\textasciitilde{}} 
\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{)], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\FunctionTok{text}\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{)] }\SpecialCharTok{\textasciitilde{}} 
\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)  }\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{)],}
\AttributeTok{labels =} \FunctionTok{rownames}\NormalTok{(true\_sample)[(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(true\_sample)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{(}\FunctionTok{log10}\NormalTok{(}\FunctionTok{rowMeans}\NormalTok{(negative\_control)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{)], }\AttributeTok{pos =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/blank_decontam.png}

In the figure above, one point indicates an organism detected in a group
of metagenomic samples. The points highlighted by red have high
abundance in negative control samples, and therefore they are likely
contamiannts.

In addition to PCR reagents and lab contaminants, reference databses can
also be contaminanted by various, often microbial, organisms. A typical
example that when screening environmental or sedimentary ancient DNA
samples, a fish \emph{Cyprinos carpio} can pop up if adapter trimming
procedure was not successful for some reason.

\includegraphics{assets/images/chapters/authentication-decontamination/carpio.png}

It was noticed that the \emph{Cyprinos carpio} reference genome
available at NCBI contains large fraction of Illumina sequncing
adapters. Therefore, appearence of this organism in your analysis may
falsely lead your conclusion toward potential lake or river present in
the excavation site.

Let us now discuss a few available computational approaches to
decontaminate metagenomic samples. One of them is
\href{https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0605-2}{decontam}
R package that offers a simple statistical test for whether a detected
organism is likely contaminant. This approach is useful when DNA
quantitation data recording the concentration of DNA in each sample
(e.g.~PicoGreen fluorescent intensity measures) is available. The idea
of the \emph{decontam} is that contaminant DNA is expected to be present
in approximately equal and low concentrations across samples, while
sample DNA concentrations can vary widely. As a result, the expected
frequency of contaminant DNA varies inversely with total sample DNA
concentration (red line in the figure below), while the expected
frequency of non-contaminant DNA does not (blue line).

\includegraphics{assets/images/chapters/authentication-decontamination/decontam.png}

Another popular tool for detecting contaminating microorganisms is
\href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006967}{Recentrifuge}.
It works as a classifier that is trained to recognize contaminant
microbial organisms. In case of Recentrifuge, one has to use blanks or
other negative controls and provide microbial names and abundances on
the blanks in order to train Recentrifuge to recognize endogenous
vs.~contaminant sources.

If one wants to assess the degree of contamination for each sample,
there is a handy tool
\href{https://github.com/jfy133/cuperdec}{cuperdec}, which is an R
package that allows a quick comparison of microbial profiles in a query
metagenomic sample against a database. The idea of \emph{cuperdec} is to
rank organisms in each sample by their abundance and then using an
``expanding window'' approach to compute their enrichment in a reference
database that contains a comprehensive list of microbial organisms which
are specific to a tissue / environment in question. The tool produces
so-called \emph{Cumulative Percent Decay} curves that aim to represent
the level of endogenous content of microbiome samples, such as ancient
dental calculus, to help to identify samples with low levels of
preservation that should be discarded for downstream analysis.

First change into the cuperdec directory, and load R

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/cuperdec}
\end{Highlighting}
\end{Shaded}

Then we can load the files, generate the decay curves, set a
preservation threshold cut-off, and plot the result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"cuperdec"}\NormalTok{); }\FunctionTok{library}\NormalTok{(}\StringTok{"magrittr"}\NormalTok{); }\FunctionTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}

\CommentTok{\# Load database (in this case an internal \textquotesingle{}test\textquotesingle{} oral database from the package)}
\FunctionTok{data}\NormalTok{(cuperdec\_database\_ex)}
\NormalTok{database }\OtherTok{\textless{}{-}} \FunctionTok{load\_database}\NormalTok{(cuperdec\_database\_ex, }\AttributeTok{target =} \StringTok{"oral"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}

\CommentTok{\# Load abundance matrix and metadata}
\NormalTok{raw\_table }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"../krakenuniq/krakenuniq\_abundance\_matrix/krakenuniq\_abundance\_matrix.txt"}\NormalTok{, }\AttributeTok{row.names=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames=}\StringTok{\textquotesingle{}Taxon\textquotesingle{}}\NormalTok{)}
\NormalTok{taxatable }\OtherTok{\textless{}{-}} \FunctionTok{load\_taxa\_table}\NormalTok{(raw\_table)  }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\NormalTok{metadata }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Sample =} \FunctionTok{unique}\NormalTok{(taxatable}\SpecialCharTok{$}\NormalTok{Sample), }
\AttributeTok{Sample\_Source =} \StringTok{"Oral"}\NormalTok{))}

\CommentTok{\# Compute cumulative percent decay curves, filter and plot results}
\NormalTok{curves }\OtherTok{\textless{}{-}} \FunctionTok{calculate\_curve}\NormalTok{(taxatable, }\AttributeTok{database =}\NormalTok{ database) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\NormalTok{filter\_result }\OtherTok{\textless{}{-}} \FunctionTok{simple\_filter}\NormalTok{(curves, }\AttributeTok{percent\_threshold =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\FunctionTok{plot\_cuperdec}\NormalTok{(curves, }\AttributeTok{metadata =}\NormalTok{ metadata, filter\_result)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/cuperdec2.png}

In the figure above, one curve represents one sample, and the red curves
have a very high amount of contamination and very low amount of
endogenous DNA. These samples might be considered to be dropped from the
downstream analysis.

\hypertarget{microbial-source-tracking}{%
\section{Microbial source tracking}\label{microbial-source-tracking}}

For the case of ancient microbiome profiling, in addition to traditional
inspection of the list of detected organisms and comparing it with the
ones detected on blanks, we can use tools that make a prediction on what
environment the detected organisms most likely come from.

The most popular and widely used tool is called
\href{https://www.nature.com/articles/nmeth.1650\#citeas}{\textbf{SourceTracker}}.
SourceTracker is a Bayesian version of the Gaussian Mixture Model (GMM)
clustering algorithm that is trained on a user-supplied reference data
called \textbf{Sources}, i.e.~different classes such as Soil or Human
Oral or Human Gut microbial communities etc., and then it can estimate
proportion / contribution of each of these sources the users actual
samples called \textbf{Sinks}.

\includegraphics{assets/images/chapters/authentication-decontamination/SourceTracker.png}

Originally, SourceTracker was developed for 16S data, i.e.~using only
16S ribosomal RNA genes, but it can be easily trained using also shotgun
metagenomics data, which was demonstrated in its metagenomic extension
called
\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7100590/}{mSourceTracker}
and its faster and more scalable version
\href{https://www.nature.com/articles/s41592-019-0431-x}{FEAST}. The
input data for SourceTracker are metadata, i.e.~each sample has to have
``source'' or ``sink'' annotation as well as environmental label
(e.g.~Oral, Gut, Soil etc.), and microbial abundances (OTU abundances)
quantified in some way, for example through
\href{https://qiime2.org/}{QIIME} pipeline, MetaPhlan or Kraken. The
SourceTracker R script can be downloaded from
\url{https://github.com/danknights/sourcetracker}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

In addition to the input files (already prepared for you), you will also
need to clone the Sourcetracker 1 R code repository (unfortunately it's
not available as a package)

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/sourcetracker/}
\FunctionTok{git}\NormalTok{ clone https://github.com/danknights/sourcetracker.git}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Patching sourcetracker for newer R versions}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

Unfortunately Sourcetracker's original R code was done a very long time
ago, and has resulted in some incompatiblities with more recent versions
of R.

If using a more recent version of R (such as in this chapter's conda
environment), you will need to perform the following `patching' of the
code, to prevent an error.

Run the following command to
\href{https://github.com/danknights/sourcetracker/issues/15}{comment out
the offending lines} (538, 539), where R's behaviour in later versions
of a fundamental validation function has changed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sed} \AttributeTok{{-}i} \StringTok{\textquotesingle{}538,539s/\^{}/\#/\textquotesingle{}}\NormalTok{ sourcetracker/src/SourceTracker.r}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Sourcetracker expects two input data frames: metadata with at least
sample name, environment and source / sink labels, and abundance matrix.
Note that source and sink metadata and abundances have to be merged
together prior to using SourceTracker. Here we are going to use data
from the \href{https://hmpdacc.org/}{Human Microbiome Project (HMP)} as
sources, and we are going to merge the HMP data with the sink samples
into single OTU table and meta-data table.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/sourcetracker/}

\CommentTok{\#\# Load R}
\ExtensionTok{R}
\end{Highlighting}
\end{Shaded}

Then in R we can load the HMP source files, and our `sink samples' from
the KrakenUniq matrix we made earlier, and clean them up to make them
compatible for Sourcetracker.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Load files}
\NormalTok{otus\_hmp }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"otus\_hmp.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\NormalTok{meta\_hmp }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"meta\_hmp.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\NormalTok{otus\_sink}\OtherTok{\textless{}{-}}\FunctionTok{read.delim}\NormalTok{(}\StringTok{"../krakenuniq/krakenuniq\_abundance\_matrix/krakenuniq\_abundance\_matrix.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{row.names=}\DecValTok{1}\NormalTok{,}\AttributeTok{sep=}\StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}

\DocumentationTok{\#\# Join source and sink tables into one}
\NormalTok{otus }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(otus\_hmp, otus\_sink, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{by =} \StringTok{"row.names"}\NormalTok{)}

\DocumentationTok{\#\# Put OTU IDs as row names and replace NAs with 0}
\FunctionTok{rownames}\NormalTok{(otus) }\OtherTok{\textless{}{-}}\NormalTok{ otus}\SpecialCharTok{$}\NormalTok{Row.names; otus}\SpecialCharTok{$}\NormalTok{Row.names }\OtherTok{\textless{}{-}} \ConstantTok{NULL}\NormalTok{; otus[}\FunctionTok{is.na}\NormalTok{(otus)] }\OtherTok{\textless{}{-}} \DecValTok{0}

\DocumentationTok{\#\# Set column and row names for meta}
\NormalTok{meta\_sink }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{colnames}\NormalTok{(otus\_sink), }\AttributeTok{Env =} \StringTok{"Unknown"}\NormalTok{, }\AttributeTok{SourceSink =} \StringTok{"sink"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(meta\_sink) }\OtherTok{\textless{}{-}}\NormalTok{ meta\_sink}\SpecialCharTok{$}\NormalTok{ID; meta\_sink}\SpecialCharTok{$}\NormalTok{ID}\OtherTok{\textless{}{-}}\ConstantTok{NULL}
\NormalTok{metadata }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(meta\_hmp, meta\_sink)}

\DocumentationTok{\#\# Further value cleanups and sample filtering to remove not useful HMP samples}
\NormalTok{otus }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(otus)))}
\NormalTok{otus[otus }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}\NormalTok{; otus }\OtherTok{\textless{}{-}}\NormalTok{ otus[}\FunctionTok{rowSums}\NormalTok{(otus)}\SpecialCharTok{!=}\DecValTok{0}\NormalTok{,]}
\NormalTok{metadata}\OtherTok{\textless{}{-}}\NormalTok{metadata[}\FunctionTok{as.character}\NormalTok{(metadata}\SpecialCharTok{$}\NormalTok{Env)}\SpecialCharTok{!=}\StringTok{"Vaginal"}\NormalTok{,]; envs }\OtherTok{\textless{}{-}}\NormalTok{ metadata}\SpecialCharTok{$}\NormalTok{Env}
\NormalTok{common.sample.ids }\OtherTok{\textless{}{-}} \FunctionTok{intersect}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(metadata), }\FunctionTok{rownames}\NormalTok{(otus))}
\NormalTok{otus }\OtherTok{\textless{}{-}}\NormalTok{ otus[common.sample.ids,]; metadata }\OtherTok{\textless{}{-}}\NormalTok{ metadata[common.sample.ids,]}
\end{Highlighting}
\end{Shaded}

Next, training SourceTracker on source samples and running predictions
on sink samples can be done using following commands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Train SourceTracker on sources (HMP) and run predictions on sinks}
\FunctionTok{source}\NormalTok{(}\StringTok{\textquotesingle{}sourcetracker/src/SourceTracker.r\textquotesingle{}}\NormalTok{)}
\NormalTok{train.ix }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(metadata}\SpecialCharTok{$}\NormalTok{SourceSink}\SpecialCharTok{==}\StringTok{\textquotesingle{}source\textquotesingle{}}\NormalTok{)}
\NormalTok{test.ix }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(metadata}\SpecialCharTok{$}\NormalTok{SourceSink}\SpecialCharTok{==}\StringTok{\textquotesingle{}sink\textquotesingle{}}\NormalTok{)}
\NormalTok{st }\OtherTok{\textless{}{-}} \FunctionTok{sourcetracker}\NormalTok{(otus[train.ix,], envs[train.ix])}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(st, otus[test.ix,], }\AttributeTok{alpha1 =} \FloatTok{0.001}\NormalTok{, }\AttributeTok{alpha2 =} \FloatTok{0.001}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we can plot SourceTracker environment inference in the form of
barcharts as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort SourceTracker proportions for plotting}
\NormalTok{props }\OtherTok{\textless{}{-}}\NormalTok{ results}\SpecialCharTok{$}\NormalTok{proportions}
\NormalTok{props }\OtherTok{\textless{}{-}}\NormalTok{ props[}\FunctionTok{order}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{props[,}\StringTok{"Oral"}\NormalTok{]),]}
\NormalTok{results}\SpecialCharTok{$}\NormalTok{proportions }\OtherTok{\textless{}{-}}\NormalTok{ props}

\CommentTok{\# Prepare SourceTracker output for plotting}
\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{proportions), }\AttributeTok{each =} \DecValTok{4}\NormalTok{)}
\NormalTok{value }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{t}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{proportions))}
\NormalTok{labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Gut"}\NormalTok{,}\StringTok{"Oral"}\NormalTok{,}\StringTok{"Skin"}\NormalTok{,}\StringTok{"Unknown"}\NormalTok{); condition}\OtherTok{\textless{}{-}}\FunctionTok{rep}\NormalTok{(labels, }\FunctionTok{length}\NormalTok{(test.ix))}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(name, condition, value)}

\CommentTok{\# Plot SourceTracker inference as a barplot}
\FunctionTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{condition, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(name, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(name))))) }\SpecialCharTok{+} 
\FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{, }\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
\FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }\AttributeTok{size=}\DecValTok{5}\NormalTok{, }\AttributeTok{hjust=}\DecValTok{1}\NormalTok{, }\AttributeTok{vjust=}\FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+} 
\FunctionTok{xlab}\NormalTok{(}\StringTok{"Sample"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{"Fraction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{assets/images/chapters/authentication-decontamination/source_tracker1.png}

}

\caption{\label{fig-authenticationdecontamination-sourcetrackeroutput}Stacked
barchart of each sample (sinks) with the estimated fraction of each
source `sample group' that each sink is made up of. Note that the X-axis
sorting of samples may change between each person, thus your plot may
not match exactly the same as the one here.}

\end{figure}

In the figure above
(Figure~\ref{fig-authenticationdecontamination-sourcetrackeroutput}) the
SourceTracker was trained on Human Microbiome Project (HMP) data, and
was capable of predicting the fractions of oral, gut, skin or other
microbial composition on the query sink samples. In a similar way,
environmental soil or marine microbes can be used as Sources. In this
way, environmental percentage of contamination can be detected per
sample.

A drawback of SourceTracker, mSourceTracker and FEAST is that they
require a microbial abundance table after a taxonomic classification
with e.g.~QIIME or Kraken has been performed. Such taxonomic
classification can be biased since it is computed against a reference
database with known taxonomic annotation. In contrast, a novel microbial
source tracking tool
\href{https://www.biorxiv.org/content/10.1101/2023.01.26.525439v1.full}{decOM}
aims at moving away from database-dependent methods and using
unsupervised approaches exploiting read-level sequence composition.

\includegraphics{assets/images/chapters/authentication-decontamination/deCOM.png}

\emph{decOM} uses
\href{https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac029/6576015}{kmtricks}
to compute a matrix of k-mer counts on raw reads (FASTQ files) from
source samples, and then uses the source k-mer abundance matrix for
looking up k-mer composition of sink samples. This allows \emph{decOM}
to calculate microbial contributions / fractions from the sources. For
example, for estimating contributions from ancient Oral (aOral), modern
Oral (mOral), Skin and Sediment / Soil environments one can use an
already computed source matrix from here
\url{https://github.com/CamilaDuitama/decOM/} and provide it as a
\emph{-p\_sources} parameter.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: data preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

If doing a self-guided tutorial, you will need to download a rather
large pre-built database.

\begin{Shaded}
\begin{Highlighting}[]

\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/decom}
\FunctionTok{git}\NormalTok{ clone https://github.com/CamilaDuitama/decOM.git  }
\BuiltInTok{cd}\NormalTok{ decOM  }
\ExtensionTok{conda}\NormalTok{ env create }\AttributeTok{{-}n}\NormalTok{ decOM }\AttributeTok{{-}{-}file}\NormalTok{ environment.yml  }
\ExtensionTok{conda}\NormalTok{ deactivate}
\ExtensionTok{conda}\NormalTok{ activate decOM  }

\BuiltInTok{export} \VariableTok{PATH}\OperatorTok{=}\NormalTok{/absolute/path/to/decOM:}\VariableTok{$\{PATH\}}

\CommentTok{\#\# In the directory }
\CommentTok{\#\# Note that decOM does not like dots in names so make symlinks}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ ../rawdata/}\PreprocessorTok{*}\NormalTok{trimmed.fastq.gz}\KeywordTok{;} \ControlFlowTok{do} 
  \FunctionTok{ln} \AttributeTok{{-}s} \StringTok{"}\VariableTok{$i}\StringTok{"} \StringTok{"}\VariableTok{$\{i}\OperatorTok{/}\SpecialStringTok{.trimmed}\OperatorTok{/}\NormalTok{\_trimmed}\VariableTok{\}}\StringTok{"}
\ControlFlowTok{done}

\CommentTok{\# Prepare input fof{-}files that have a key {-} value format}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \DataTypeTok{\{}\DecValTok{1}\DataTypeTok{..}\DecValTok{10}\DataTypeTok{\}}\KeywordTok{;} \ControlFlowTok{do} 
  \BuiltInTok{echo} \StringTok{"sample}\VariableTok{$\{i\}}\StringTok{\_trimmed : sample}\VariableTok{$\{i\}}\StringTok{\_trimmed.fastq.gz"} \OperatorTok{\textgreater{}}\NormalTok{ sample}\VariableTok{$\{i\}}\NormalTok{\_trimmed.fof}
  \BuiltInTok{echo}\NormalTok{ sample}\VariableTok{$\{i\}}\NormalTok{\_trimmed }\OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ FASTQ\_NAMES\_LIST.txt}\KeywordTok{;} 
\ControlFlowTok{done}

\CommentTok{\# Download pre{-}built kmer{-}matrix of sources (aOral, mOral, Sediment/Soil, Skin)}
\FunctionTok{wget}\NormalTok{ https://zenodo.org/record/6513520/files/decOM\_sources.tar.gz  }
\FunctionTok{tar} \AttributeTok{{-}xf}\NormalTok{ decOM\_sources.tar.gz}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

The following example command is how you would execute deCOM. However as
this requires a very large database file and thus large memory
requirements, therefore we have already made available for you the
output files from this step.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# DO NOT RUN!: Run decOM predictions}
\ExtensionTok{decOM} \AttributeTok{{-}p\_sources}\NormalTok{ decOM\_sources/ }\AttributeTok{{-}p\_sinks}\NormalTok{ FASTQ\_NAMES\_LIST.txt }\AttributeTok{{-}p\_keys}\NormalTok{ decOM/FASTQ }\AttributeTok{{-}mem}\NormalTok{ 900GB }\AttributeTok{{-}t}\NormalTok{ 15}
\end{Highlighting}
\end{Shaded}

You can find the relevant output files in the following directory (the
output of deCOM with our simulated data from the command above was 29GB
worth of files!).

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/authentication{-}decontamination/decom}
\end{Highlighting}
\end{Shaded}

In the command line used to generate, the \emph{-p\_sinks} parameter
provides a list of sink samples, for example \emph{SRR13355807}.

The sink fastq-files are placed together with \emph{keys} fof-files
containing the mapping between fastq file names and locations of the
fastq-files, for example \texttt{SRR13355807\ :\ SRR13355807.fastq.gz}.
The contributions from the sources to the sink samples, which are
recorded in the \texttt{decOM\_output.csv} output file (which is in the
\texttt{authentication-decontamination/decom} directory already for
you), can then be processed and plotted as follows:

Load R

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{R}
\end{Highlighting}
\end{Shaded}

Then we can make a plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"decOM\_output.csv"}\NormalTok{, }\AttributeTok{check.names=}\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df, }\AttributeTok{select =} \FunctionTok{c}\NormalTok{(}\StringTok{"Sink"}\NormalTok{, }\StringTok{"Sediment/Soil"}\NormalTok{, }\StringTok{"Skin"}\NormalTok{, }\StringTok{"aOral"}\NormalTok{, }
\StringTok{"mOral"}\NormalTok{, }\StringTok{"Unknown"}\NormalTok{))}
\FunctionTok{rownames}\NormalTok{(result) }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{Sink; result}\SpecialCharTok{$}\NormalTok{Sink }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{result }\OtherTok{\textless{}{-}}\NormalTok{ result }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(result)}
\NormalTok{result}\OtherTok{\textless{}{-}}\NormalTok{result[}\FunctionTok{order}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{result}\SpecialCharTok{$}\NormalTok{aOral),]}

\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(result), }\AttributeTok{each =} \DecValTok{5}\NormalTok{); value }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{t}\NormalTok{(result))}
\NormalTok{condition }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Sediment/Soil"}\NormalTok{,}\StringTok{"Skin"}\NormalTok{,}\StringTok{"aOral"}\NormalTok{,}\StringTok{"mOral"}\NormalTok{,}\StringTok{"Unknown"}\NormalTok{), }
\FunctionTok{dim}\NormalTok{(result)[}\DecValTok{1}\NormalTok{])}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(name, condition, value)}

\FunctionTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{); }\FunctionTok{library}\NormalTok{(}\StringTok{"viridis"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{condition, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(name,}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(name))))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{, }\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle=}\DecValTok{90}\NormalTok{, }\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{, }\AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Sample"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{"Fraction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once finished examining the plot you can quit R

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Press \textquotesingle{}n\textquotesingle{} when asked if you want to save your workspace image.}
\FunctionTok{quit()}
\end{Highlighting}
\end{Shaded}

\includegraphics{assets/images/chapters/authentication-decontamination/decOM1.png}

\emph{decOM} has certain advantages compared to \emph{SourceTracker} as
its is a taxonomic classification / database free approach. However, it
also appears to be very sensitive to the particular training / source
data set. In the example above it can be seen that the microbial source
tracking of sink samples is very much dominated by the Oral community,
which was the training / source data set.

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

In this chapter we have learned that:

\begin{itemize}
\tightlist
\item
  Evenness of coverage is an important metric for validation of findings
\item
  Deamination profile, DNA fragmentation, mapping quality, edit distance
  and PMD scores are other authentication / validation metrics to
  consider
\item
  Negative controls are important for disentangling ancient / endogenous
  from modern / exogenous contamination
\item
  Microbial source tracking is another layer of evidence that can
  facilitate interpretation of ancient metagenomic findings
\end{itemize}

\hypertarget{questions-to-think-about-5}{%
\section{Questions to think about}\label{questions-to-think-about-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is a false-positive microbial finding and how can we recognize
  it?
\item
  What is the diffeerence between depth, breadth and evenness of
  coverage?
\item
  What is contamination and how can it bias ancient metagenomic
  analysis?
\item
  How can we separate ancient from modern DNA sequence?
\item
  What is a negative (blank) control sample and why is it useful to
  have?
\item
  What is microbial source tracking and how can it help with
  decontamination?
\end{enumerate}

\hypertarget{readings-1}{%
\section{Readings}\label{readings-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Clio Der Sarkissian, Irina M. Velsko, Anna K. Fotakis, Åshild J.
  Vågene, Alexander Hübner, and James A. Fellows Yates, Ancient
  Metagenomic Studies: Considerations for the Wider Scientific
  Community, mSystems 2021 Volume 6 Issue 6 e01315-21.
\item
  Warinner C, Herbig A, Mann A, Fellows Yates JA, Weiß CL, Burbano HA,
  Orlando L, Krause J. A Robust Framework for Microbial Archaeology.
  Annu Rev Genomics Hum Genet. 2017 Aug 31;18:321-356. doi:
  10.1146/annurev-genom-091416-035526. Epub 2017 Apr 26. PMID: 28460196;
  PMCID: PMC5581243.
\item
  Orlando, L., Allaby, R., Skoglund, P. et al.~Ancient DNA analysis. Nat
  Rev Methods Primers 1, 14 (2021).
  https://doi.org/10.1038/s43586-020-00011-0
\end{enumerate}

\hypertarget{resources-1}{%
\section{Resources}\label{resources-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{KrakenUniq}: Breitwieser, F. P., Baker, D. N., \& Salzberg, S.
  L. (2018). KrakenUniq: confident and fast metagenomics classification
  using unique k-mer counts. Genome Biology, vol.~19(1), p.~1--10.
  http://www.ec.gc.ca/education/default.asp?lang=En\&n=44E5E9BB-1
\item
  \textbf{Samtools}: Heng Li, Bob Handsaker, Alec Wysoker, Tim Fennell,
  Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, Richard Durbin,
  1000 Genome Project Data Processing Subgroup, The Sequence
  Alignment/Map format and SAMtools, Bioinformatics, Volume 25, Issue
  16, 15 August 2009, Pages 2078--2079,
  https://doi.org/10.1093/bioinformatics/btp352
\item
  \textbf{PMDtools}: Skoglund P, Northoff BH, Shunkov MV, Derevianko AP,
  Pääbo S, Krause J, Jakobsson M. Separating endogenous ancient DNA from
  modern day contamination in a Siberian Neandertal. Proc Natl Acad Sci
  U S A. 2014 Feb 11;111(6):2229-34. doi: 10.1073/pnas.1318934111. Epub
  2014 Jan 27. PMID: 24469802; PMCID: PMC3926038.
\item
  \textbf{pyDamage}: Borry M, Hübner A, Rohrlach AB, Warinner C.
  PyDamage: automated ancient damage identification and estimation for
  contigs in ancient DNA de novo assembly. PeerJ. 2021 Jul 27;9:e11845.
  doi: 10.7717/peerj.11845. PMID: 34395085; PMCID: PMC8323603.
\item
  \textbf{SourceTracker}: Knights D, Kuczynski J, Charlson ES, Zaneveld
  J, Mozer MC, Collman RG, Bushman FD, Knight R, Kelley ST. Bayesian
  community-wide culture-independent microbial source tracking. Nat
  Methods. 2011 Jul 17;8(9):761-3. doi: 10.1038/nmeth.1650. PMID:
  21765408; PMCID: PMC3791591.
\item
  \textbf{deCOM}:
  https://www.biorxiv.org/content/10.1101/2023.01.26.525439v1, doi:
  https://doi.org/10.1101/2023.01.26.525439
\item
  \textbf{aMeta}:
  https://www.biorxiv.org/content/10.1101/2022.10.03.510579v1, doi:
  https://doi.org/10.1101/2022.10.03.510579
\item
  \textbf{Bowtie2}: Langmead, B., Salzberg, S. Fast gapped-read
  alignment with Bowtie 2. Nat Methods 9, 357--359 (2012).
  https://doi.org/10.1038/nmeth.1923
\item
  \textbf{cuperdec}:
  https://cran.r-project.org/web/packages/cuperdec/index.html
\item
  \textbf{decontam}:
  https://www.bioconductor.org/packages/release/bioc/html/decontam.html
\end{enumerate}

\part{Ancient Genomics}

A natural extension to any ancient \_meta\_genomics project is to
further investigate the specific genomes of the plethora of species and
strains you may have detected. In this section of the book, we will look
at the specific techniques used to reconstruct ancient genomes using
standard genomics reference-based methods, but as always in the context
of the short and damaged DNA fragments that are typical of ancient DNA.

\hypertarget{genome-mapping}{%
\section*{\texorpdfstring{\protect\hyperlink{genome-mapping-1}{Genome
Mapping}}{Genome Mapping}}\label{genome-mapping}}
\addcontentsline{toc}{section}{\protect\hyperlink{genome-mapping-1}{Genome
Mapping}}

\markright{Genome Mapping}

An important step in the reconstruction of full genomic sequences is
mapping. Even relatively short genomes usually cannot be sequenced as a
single consecutive piece. Instead, millions of short sequence reads are
generated from genomic fragments. These reads can be several hundred
nucleotides in length but are considerably shorter for ancient DNA
(aDNA).

For many applications involving comparative genomics these `reads' have
to be aligned to one or multiple already-reconstructed reference genomes
in order to identify differences between the sequenced genome and any
given contextual dataset. Aligning millions of short reads to much
longer genome sequences in a time-efficient and accurate manner is a
bioinformatics challenge for which numerous algorithms and tools have
been developed. Each of these programs comes with a variety of
parameters that can significantly alter the results and default settings
are often not optimal when working with aDNA. Furthermore, read mapping
procedures are often part of complex computational genomics pipelines
and are therefore not directly applied by many users.

In this chapter we will take a look at specific challenges during read
mapping when dealing with aDNA. We will get an overview of common input
and output formats and manually apply a read mapper to aDNA data
studying the direct effects of variation in mapping parameters. We will
conclude the session with an outlook on genotyping, which is an
important follow-up analysis step, that in turn is very relevant for
down-stream analyses such as phylogenetics.

\hypertarget{phylogenomics}{%
\section*{\texorpdfstring{\protect\hyperlink{introduction-to-phylogenomics}{Phylogenomics}}{Phylogenomics}}\label{phylogenomics}}
\addcontentsline{toc}{section}{\protect\hyperlink{introduction-to-phylogenomics}{Phylogenomics}}

\markright{Phylogenomics}

Phylogenetic trees are central tools for studying the evolution of
microorganisms, as they provide essential information about their
relationships and timing of divergence between microbial strains.

In this chapter, we will introduce basic phylogenetic concepts and
definitions, and provide guidance on how to interpret phylogenetic
trees. We will then learn how to reconstruct phylogenetic trees from DNA
sequences using various methods ranging from distance-based methods to
probabilistic approaches, including maximum likelihood and Bayesian
phylogenetics. In particular, we will learn how to use ancient genomic
data to reconstruct time-calibrated trees with BEAST2.

\hypertarget{genome-mapping-1}{%
\chapter{Genome Mapping}\label{genome-mapping-1}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/genome-mapping.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate genome{-}mapping}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-11}{%
\section{Lecture}\label{lecture-11}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{assets/images/chapters/genome-mapping/SPAAM\%20Summer\%20School\%202022\%20-\%204B\%20-\%20Genome\%20Mapping.pdf}{here}.

\hypertarget{mapping-to-a-reference-genome}{%
\section{Mapping to a Reference
Genome}\label{mapping-to-a-reference-genome}}

One way of reconstructing genomic information from DNA sequencing reads
is mapping/aligning them to a reference genome. This allows for
identification of differences between the genome from your sample and
the reference genome. This information can be used for example for
comparative analyses such as in phylogenetics. For a detailed
explanation of the read alignment problem and an overview of concepts
for solving it, please see
\url{https://doi.org/10.1146/annurev-genom-090413-025358}.

In this session we will map two samples to the \emph{Yersinia pestis}
(plague) genome using different parameter sets. We will do this
``manually'' in the sense that we will use all necessary commands one by
one in the terminal. These commands usually run in the background when
you apply DNA sequencing data processing pipelines.

\hypertarget{preparation-1}{%
\subsection{Preparation}\label{preparation-1}}

The data and conda environment \texttt{.yaml} file for this practical
session can be downloaded from here:
\url{https://doi.org/10.5281/zenodo.6983174}. See instructions on page.

We will open a terminal and then navigate to the working directory of
this session:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/genome{-}mapping/}
\end{Highlighting}
\end{Shaded}

Then, as already mentioned above, we need to activate the conda
environment of this session. By doing this all the necessary tools can
be accessed in the current terminal session:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate genome{-}mapping}
\end{Highlighting}
\end{Shaded}

We will be using the Burrows-Wheeler Aligner (Li et al.~2009 --
\url{http://bio-bwa.sourceforge.net}). There are different algorithms
implemented for different types of data (e.g.~different read lengths).
Here, we use BWA backtrack (\emph{bwa aln}), which is suitable for
Illumina sequences up to 100bp. Other algorithms are \emph{bwa mem} and
\emph{bwa sw} for longer reads.

\hypertarget{reference-genome}{%
\subsection{Reference Genome}\label{reference-genome}}

For mapping we need a reference genome in FASTA format. Ideally we use a
genome from the same species that our data relates to or, if not
available, a closely related species. The selection of the correct
reference genome is highly relevant. E.g. if the chosen genome differs
too much from the organism the data relates to, it might not be possible
to map most of the reads. Reference genomes can be retrieved from
comprehensive databases such as
\href{https://www.ncbi.nlm.nih.gov/}{NCBI}.

In your directory, you can find 2 samples and your reference. As a first
step we will index our reference genome (make sure you are inside your
directory).

The first index we will generate is for \emph{bwa}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{bwa}\NormalTok{ index YpestisCO92.fa}
\end{Highlighting}
\end{Shaded}

The second index will be used by the genome browser we will apply to our
results later on:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ faidx YpestisCO92.fa}
\end{Highlighting}
\end{Shaded}

We need to build a third index that is necessary for the genotyping
step, which comes later after mapping:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{picard}\NormalTok{ CreateSequenceDictionary R=YpestisCO92.fa}
\end{Highlighting}
\end{Shaded}

\hypertarget{mapping-parameters}{%
\subsection{Mapping Parameters}\label{mapping-parameters}}

We will be using \emph{bwa aln}, but we need to specify parameters. For
now we will concentrate on the ``seed length'' and the ``maximum edit
distance''. We will use the default setting for all other parameters
during this session. The choice of the right parameters depend on many
factors such as the type of data and the specific use case. One aspect
is the mapping sensitivity, i.e.~how different a read can be from the
chosen reference and still be mapped. In this context we generally
differentiate between \emph{strict} and \emph{lenient} mapping
parameters.

As many other mapping algorithms \emph{bwa} uses a so-called
``seed-and-extend'' approach. I.e. it initially maps the first \emph{N}
nucleotides of each read to the genome with relatively few mismatches
and thereby determines candidate positions for the more time-intensive
full alignment.

A short seed length will generate more such candidate positions and
therefore mapping will take longer, but it will also be more sensitive,
i.e.~there can be more differences between the read and the genome. Long
seeds are less sensitive but the mapping procedure is faster.

In this session we will use the following two parameter sets:

\textbf{Lenient}

Allow for more mismatches → -n 0.01

Short seed length → -l 16

\textbf{Strict}

Allow for less mismatches → -n 0.1

Long seed length → -l 32

We will be working with pre-processed files (\texttt{sample1.fastq.gz},
\texttt{sample2.fastq.gz}), i.e.~any quality filtering and removal of
sequencing adapters is already done.

We will map each file once with lenient and once with strict parameters.
For this, we will make 4 separate directories, to avoid mixing up files:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ sample1\_lenient sample2\_lenient sample1\_strict sample2\_strict}
\end{Highlighting}
\end{Shaded}

\hypertarget{mapping-sample1}{%
\subsection{Mapping Sample1}\label{mapping-sample1}}

Let's begin with a lenient mapping of sample1.

Go into the corresponding folder:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ sample1\_lenient}
\end{Highlighting}
\end{Shaded}

Perform the \emph{bwa} alignment, here for sample1, and specify lenient
mapping parameters:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{bwa}\NormalTok{ aln }\AttributeTok{{-}n}\NormalTok{ 0.01 }\AttributeTok{{-}l}\NormalTok{ 16 ../YpestisCO92.fa ../sample1.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_file.sai}
\end{Highlighting}
\end{Shaded}

Proceed with writing the mapping in \emph{sam} format
(\url{https://en.wikipedia.org/wiki/SAM_(file_format)}):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{bwa}\NormalTok{ samse }\AttributeTok{{-}r} \StringTok{\textquotesingle{}@RG\textbackslash{}tID:all\textbackslash{}tLB:NA\textbackslash{}tPL:illumina\textbackslash{}tPU:NA\textbackslash{}tSM:NA\textquotesingle{}}\NormalTok{ ../YpestisCO92.fa reads\_file.sai ../sample1.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.sam}
\end{Highlighting}
\end{Shaded}

Note that we have specified the sequencing platform (Illumina) by
creating a so-called ``Read Group'' (\texttt{-r}). This information is
used later during the genotyping step.

Convert SAM file to binary format (BAM file):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}b} \AttributeTok{{-}S}\NormalTok{ reads\_mapped.sam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.bam}
\end{Highlighting}
\end{Shaded}

For processing of \emph{sam} and \emph{bam} files we use \emph{SAMtools}
(Li et al.~2009 -- \url{http://samtools.sourceforge.net/}).

\texttt{-b} specifies to output in BAM format. (\texttt{-S} specifies
input is SAM, can be omitted in recent versions.)

Now we sort the \emph{bam} file → Sort alignments by leftmost
coordinates:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ sort reads\_mapped.bam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped\_sorted.bam}
\end{Highlighting}
\end{Shaded}

The sorted bam file needs to be indexed → more efficient for further
processing:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted.bam}
\end{Highlighting}
\end{Shaded}

Deduplication → Removal of reads from duplicated fragments:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ rmdup }\AttributeTok{{-}s}\NormalTok{ reads\_mapped\_sorted.bam reads\_mapped\_sorted\_dedup.bam}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted\_dedup.bam}
\end{Highlighting}
\end{Shaded}

Duplicated reads are usually a consequence of amplification of the DNA
fragments in the lab. Therefore, they are not biologically meaningful.

We have now completed the mapping procedure. Let's have a look at our
mapping results:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ view reads\_mapped\_sorted\_dedup.bam }\KeywordTok{|} \FunctionTok{less} \AttributeTok{{-}S}
\end{Highlighting}
\end{Shaded}

(exit by pressing q)

We can also get a summary about the number of mapped reads. For this we
use the \emph{samtools idxstats} command
(\url{http://www.htslib.org/doc/samtools-idxstats.html}):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samtools}\NormalTok{ idxstats reads\_mapped\_sorted\_dedup.bam}
\end{Highlighting}
\end{Shaded}

\hypertarget{genotyping}{%
\subsection{Genotyping}\label{genotyping}}

The next step we need to perform is genotyping, i.e.~the identification
of all SNPs that differentiate the sample from the reference. For this
we use the \emph{Genome Analysis Toolkit (GATK)} (DePristo et al.~2011
-- \url{http://www.broadinstitute.org/gatk/})

It uses the reference genome and the mapping as input and produces an
output in \emph{Variant Call Format (VCF)}
(\url{https://en.wikipedia.org/wiki/Variant_Call_Format}).

Perform genotyping on the mapping file:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{gatk3} \AttributeTok{{-}T}\NormalTok{ UnifiedGenotyper }\AttributeTok{{-}R}\NormalTok{ ../YpestisCO92.fa }\AttributeTok{{-}I}\NormalTok{ reads\_mapped\_sorted\_dedup.bam }\AttributeTok{{-}{-}output\_mode}\NormalTok{ EMIT\_ALL\_SITES }\AttributeTok{{-}o}\NormalTok{ mysnps.vcf}
\end{Highlighting}
\end{Shaded}

Let's have a look\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ mysnps.vcf }\KeywordTok{|} \FunctionTok{less} \AttributeTok{{-}S}
\end{Highlighting}
\end{Shaded}

(exit by pressing q)

\hypertarget{mapping-and-genotyping-for-the-other-samplesparameters}{%
\subsection{Mapping and Genotyping for the other
Samples/Parameters}\label{mapping-and-genotyping-for-the-other-samplesparameters}}

Let's now continue with mapping and genotyping for the other samples and
parameter settings.

\hypertarget{sample2-lenient}{%
\subsubsection{Sample2 Lenient}\label{sample2-lenient}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

This is a larger file and lenient mapping takes longer so this file will
likely take a few minutes. If you are short on time, proceed with the
other sample/parameter settings first and come back to this later if
there is time.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ ..}
\BuiltInTok{cd}\NormalTok{ sample2\_lenient}

\ExtensionTok{bwa}\NormalTok{ aln }\AttributeTok{{-}n}\NormalTok{ 0.01 }\AttributeTok{{-}l}\NormalTok{ 16 ../YpestisCO92.fa ../sample2.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_file.sai}

\ExtensionTok{bwa}\NormalTok{ samse }\AttributeTok{{-}r} \StringTok{\textquotesingle{}@RG\textbackslash{}tID:all\textbackslash{}tLB:NA\textbackslash{}tPL:illumina\textbackslash{}tPU:NA\textbackslash{}tSM:NA\textquotesingle{}}\NormalTok{ ../YpestisCO92.fa reads\_file.sai ../sample2.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.sam}

\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}b} \AttributeTok{{-}S}\NormalTok{ reads\_mapped.sam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.bam}

\ExtensionTok{samtools}\NormalTok{ sort reads\_mapped.bam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ rmdup }\AttributeTok{{-}s}\NormalTok{ reads\_mapped\_sorted.bam reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{gatk3} \AttributeTok{{-}T}\NormalTok{ UnifiedGenotyper }\AttributeTok{{-}R}\NormalTok{ ../YpestisCO92.fa }\AttributeTok{{-}I}\NormalTok{ reads\_mapped\_sorted\_dedup.bam }\AttributeTok{{-}{-}output\_mode}\NormalTok{ EMIT\_ALL\_SITES }\AttributeTok{{-}o}\NormalTok{ mysnps.vcf}
\end{Highlighting}
\end{Shaded}

\hypertarget{sample1-strict}{%
\subsubsection{Sample1 Strict}\label{sample1-strict}}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ ..}
\BuiltInTok{cd}\NormalTok{ sample1\_strict}

\ExtensionTok{bwa}\NormalTok{ aln }\AttributeTok{{-}n}\NormalTok{ 0.1 }\AttributeTok{{-}l}\NormalTok{ 32 ../YpestisCO92.fa ../sample1.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_file.sai}

\ExtensionTok{bwa}\NormalTok{ samse }\AttributeTok{{-}r} \StringTok{\textquotesingle{}@RG\textbackslash{}tID:all\textbackslash{}tLB:NA\textbackslash{}tPL:illumina\textbackslash{}tPU:NA\textbackslash{}tSM:NA\textquotesingle{}}\NormalTok{ ../YpestisCO92.fa reads\_file.sai ../sample1.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.sam}

\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}b} \AttributeTok{{-}S}\NormalTok{ reads\_mapped.sam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.bam}

\ExtensionTok{samtools}\NormalTok{ sort reads\_mapped.bam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ rmdup }\AttributeTok{{-}s}\NormalTok{ reads\_mapped\_sorted.bam reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{gatk3} \AttributeTok{{-}T}\NormalTok{ UnifiedGenotyper }\AttributeTok{{-}R}\NormalTok{ ../YpestisCO92.fa }\AttributeTok{{-}I}\NormalTok{ reads\_mapped\_sorted\_dedup.bam }\AttributeTok{{-}{-}output\_mode}\NormalTok{ EMIT\_ALL\_SITES }\AttributeTok{{-}o}\NormalTok{ mysnps.vcf}
\end{Highlighting}
\end{Shaded}

\hypertarget{sample2-strict}{%
\subsubsection{Sample2 Strict}\label{sample2-strict}}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ ..}
\BuiltInTok{cd}\NormalTok{ sample2\_strict}

\ExtensionTok{bwa}\NormalTok{ aln }\AttributeTok{{-}n}\NormalTok{ 0.1 }\AttributeTok{{-}l}\NormalTok{ 32 ../YpestisCO92.fa ../sample2.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_file.sai}

\ExtensionTok{bwa}\NormalTok{ samse }\AttributeTok{{-}r} \StringTok{\textquotesingle{}@RG\textbackslash{}tID:all\textbackslash{}tLB:NA\textbackslash{}tPL:illumina\textbackslash{}tPU:NA\textbackslash{}tSM:NA\textquotesingle{}}\NormalTok{ ../YpestisCO92.fa reads\_file.sai ../sample2.fastq.gz }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.sam}

\ExtensionTok{samtools}\NormalTok{ view }\AttributeTok{{-}b} \AttributeTok{{-}S}\NormalTok{ reads\_mapped.sam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped.bam}

\ExtensionTok{samtools}\NormalTok{ sort reads\_mapped.bam }\OperatorTok{\textgreater{}}\NormalTok{ reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted.bam}

\ExtensionTok{samtools}\NormalTok{ rmdup }\AttributeTok{{-}s}\NormalTok{ reads\_mapped\_sorted.bam reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{samtools}\NormalTok{ index reads\_mapped\_sorted\_dedup.bam}

\ExtensionTok{gatk3} \AttributeTok{{-}T}\NormalTok{ UnifiedGenotyper }\AttributeTok{{-}R}\NormalTok{ ../YpestisCO92.fa }\AttributeTok{{-}I}\NormalTok{ reads\_mapped\_sorted\_dedup.bam }\AttributeTok{{-}{-}output\_mode}\NormalTok{ EMIT\_ALL\_SITES }\AttributeTok{{-}o}\NormalTok{ mysnps.vcf}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparing-genotypes}{%
\subsection{Comparing Genotypes}\label{comparing-genotypes}}

In order to combine the results from multiple samples and parameter
settings we need to agregate and comparatively analyse the information
from all the \emph{vcf} files. For this we will use the software
\emph{MultiVCFAnalyzer}
(\url{https://github.com/alexherbig/MultiVCFAnalyzer}).

It produces various output files and summary statistics and can
integrate gene annotations for SNP effect analysis as done by the
program \emph{SnpEff} (Cingolani et al.~2012 -
\url{http://snpeff.sourceforge.net/}).

Run \emph{MultiVCFAnalyzer} on all 4 files at once. First \texttt{cd}
one level up (if you type \texttt{ls} you should see your 4 directories,
reference, etc.):

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ ..}
\end{Highlighting}
\end{Shaded}

Then make a new directory\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ vcf\_out}
\end{Highlighting}
\end{Shaded}

\ldots and run the programme:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{multivcfanalyzer}\NormalTok{ NA YpestisCO92.fa NA vcf\_out F 30 3 0.9 0.9 NA sample1\_lenient/mysnps.vcf sample1\_strict/mysnps.vcf sample2\_lenient/mysnps.vcf sample2\_strict/mysnps.vcf}
\end{Highlighting}
\end{Shaded}

Let's have a look in the `vcf\_out' directory (\texttt{cd} into it):

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ vcf\_out}
\end{Highlighting}
\end{Shaded}

Check the parameters we set earlier:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{less} \AttributeTok{{-}S}\NormalTok{ info.txt}
\end{Highlighting}
\end{Shaded}

(exit by pressing q)

Check results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{less} \AttributeTok{{-}S}\NormalTok{ snpStatistics.tsv}
\end{Highlighting}
\end{Shaded}

(exit by pressing q)

The file content should look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{SNP}\NormalTok{ statistics for 4 samples.}
\ExtensionTok{Quality}\NormalTok{ Threshold: 30.0}
\ExtensionTok{Coverage}\NormalTok{ Threshold: 3}
\ExtensionTok{Minimum}\NormalTok{ SNP allele frequency: 0.9}
\ExtensionTok{sample}\NormalTok{  SNP Calls }\ErrorTok{(}\ExtensionTok{all}\KeywordTok{)} \ExtensionTok{SNP}\NormalTok{ Calls }\ErrorTok{(}\ExtensionTok{het}\KeywordTok{)} \ExtensionTok{coverage}\ErrorTok{(}\FunctionTok{fold}\KeywordTok{)}  \ExtensionTok{coverage}\ErrorTok{(}\ExtensionTok{percent}\KeywordTok{)}
\ExtensionTok{refCall}\NormalTok{ allPos  noCall  discardedRefCall    discardedVarCall    filteredVarCall unhandledGenotype}
\ExtensionTok{sample1\_lenient}\NormalTok{ 213 0   16.38   92.69}
\ExtensionTok{4313387}\NormalTok{ 4653728 293297  46103   728 0   0}
\ExtensionTok{sample1\_strict}\NormalTok{  207 0   16.33   92.71}
\ExtensionTok{4314060}\NormalTok{ 4653728 293403  45633   425 0   0}
\ExtensionTok{sample2\_lenient}\NormalTok{ 1274    0   9.01    83.69}
\ExtensionTok{3893600}\NormalTok{ 4653728 453550  297471  7829    0   4}
\ExtensionTok{sample2\_strict}\NormalTok{  1218    0   8.94    83.76}
\ExtensionTok{3896970}\NormalTok{ 4653728 455450  295275  4815    0   0}
\end{Highlighting}
\end{Shaded}

First we find the most important parameter settings and then the table
of results. The first column contains the dataset name and the second
column the number of called SNPs. The genome coverage and the fraction
of the genome covered with the used threshold can be found in columns 4
and 5, respectively. For example, sample1 had 207 SNP calls with strict
parameters. The coverage is about 16-fold and about 93\% of the genome
are covered 3 fold or higher (The coverage threshold we set was 3).

\hypertarget{exploring-the-results}{%
\subsection{Exploring the Results}\label{exploring-the-results}}

For visual exploration of mapping results so-called ``Genome Browsers''
are used. Here we will use the \emph{Integrative Genomics Viewer (IGV)}
(\url{https://software.broadinstitute.org/software/igv/}).

To open IGV, simply type the following command and the app will open:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{igv}
\end{Highlighting}
\end{Shaded}

Note that you cannot use the terminal while IGV is open. If you want to
use it anyways, open a second terminal via the bar on the bottom.

Load your reference (\texttt{YpestisCO92.fa}):

\emph{→ Genomes → Load Genome from File}

\includegraphics{assets/images/chapters/genome-mapping/IGV_load_genome.png}

Load your \emph{bam} files (do this 4 times, once for each mapping):

\emph{→ File → Load from File}

\includegraphics{assets/images/chapters/genome-mapping/IGV_load_bam.png}

Try to explore the mapping results yourself. Here are some questions to
guide you. Please also have a look at the examples below.

What differences do you observe between the samples and parameters?

Differences in number of mapped reads, coverage, number of SNPs

Do you see any global patterns?

Which sample is more affected by changing the parameters?

Which of the two samples might be ancient, which is modern?

Let's examine some SNPs. Have a look at \texttt{snpTable.tsv}.

Can you identify SNPs that were called with lenient but not with strict
parameters or vice versa?

Let's check out some of these in IGV. Do you observe certain patterns in
these genomic regions?

\hypertarget{examples}{%
\subsection{Examples}\label{examples}}

Please find here a few examples for exploration. To get a better
visualization we only loaded \texttt{sample2\_lenient} (top track) and
\texttt{sample2\_strict} (bottom track):

\includegraphics{assets/images/chapters/genome-mapping/IGV_example_intro.png}

You can see all aligned reads in the current genomic region as stacks of
grey arrows. In the middle of the image you see brown dashes in all of
the reads. This is a SNP. You also see sporadically green or red dashes
in some reads but not all of them at a given position. These sporadic
differences are DNA damage such as we typically find it for ancient DNA.

For jumping to a specific coordinate you need to enter it into the
coordinate field at the top:

\includegraphics{assets/images/chapters/genome-mapping/IGV_coordinate_example.png}

E.g. if you enter \texttt{2326942} after the colon in the coordinate
field and hit enter, you will jump to the same position as in the
screenshot above.

Let's have a look at some positions.

For example position \texttt{36472}:

\includegraphics{assets/images/chapters/genome-mapping/IGV_SNP_36472.png}

In the middle of the image you see a SNP (\texttt{T}) that was called
with strict parameters (bottom) but not with lenient parameters (top).
But why would it not be called in the top track? It is not called
because there are three reads that cover the same position, but do not
contain the \texttt{T}. We can see that these reads have other
difference to the reference at other positions. That's why they are not
mapped with strict parameters. It is quite likely that they originate
from a different species. This example demonstrates that sensitive
mapping parameters might actually lead to a loss of certain SNP calls.

Does this mean that stricter parameters will always give us a clean
mapping? Let's have a look at position \texttt{219200}:

\includegraphics{assets/images/chapters/genome-mapping/IGV_SNP_219200.png}

You might need to zoom out a bit using the slider in the upper right
corner.

So, what is going on here? We see a lot of variation in most of the
reads. This is reduced a bit with strict mapping parameters (bottom
track) but the effect is still quite pronounced. Here, we see a region
that seems to be conserved in other species as well, so we have a lot of
mapping from other organisms. We can't compensate that with stricter
mapping parameters and we would have to apply some filtering on genotype
level to remove this variation from our genotyping. Removing false
positive SNP calls is important as it would interfere with downstream
analyses such as phylogenomics.

Such regions can be fairly large. For example, see this 20 kb region
around position \texttt{224750}:

\includegraphics{assets/images/chapters/genome-mapping/IGV_SNP_224750.png}

\hypertarget{conclusions}{%
\subsection{Conclusions}\label{conclusions}}

\begin{itemize}
\tightlist
\item
  Mapping DNA sequencing reads to a reference genome is a complex
  procedure that requires multiple steps.
\item
  Mapping results are the basis for genotyping, i.e.~the detection of
  differences to the reference.
\item
  The genotyping results can be aggregated from multiple samples and
  comparatively analysed e.g.~in the context of phylogenomics.
\item
  The chosen mapping parameters can have a strong influence on the
  results of any downstream analysis.
\item
  This is particularly true when dealing with ancient DNA samples as
  they tend to contain DNA from multiple organisms. This can lead to
  mismapped reads and therefore incorrect genotypes, which can further
  influence downstream analyses.
\end{itemize}

\hypertarget{introduction-to-phylogenomics}{%
\chapter{Introduction to
Phylogenomics}\label{introduction-to-phylogenomics}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/phylogenomics.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate phylogenomics}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-12}{%
\section{Lecture}\label{lecture-12}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{assets/images/chapters/phylogenomics/SPAAM\%20Summer\%20School\%202022\%20-\%205B\%20-\%20Phylogenomics.pdf}{here}.

\hypertarget{practical}{%
\section{Practical}\label{practical}}

\hypertarget{preparation-2}{%
\subsection{Preparation}\label{preparation-2}}

The data and conda environment \texttt{.yaml} file for this practical
session can be downloaded from here:
\url{https://doi.org/10.5281/zenodo.6983184}. See instructions on page.

Change into the session directory

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/phylogenomics/}
\end{Highlighting}
\end{Shaded}

The data in this folder should contain an alignment
(snpAlignment\_session5.fasta) and a txt file with the ages of the
samples that we are going to be working with in this session
(\texttt{samples.ages.txt})

Load the conda environment.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate phylogenomics}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualize-the-sequence-alignment}{%
\subsection{Visualize the sequence
alignment}\label{visualize-the-sequence-alignment}}

In this practical session, we will be working with an alignment produced
as you learned in the practical \emph{Genome mapping}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What is in the data?}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  the alignment is a SNP alignment (it contains only the variable
  genomic positions, not the full genomes)
\item
  it contains 33 \emph{Yersinia pestis} sequences and 1 \emph{Yersinia
  pseudotuberculosis} sequence which can be used as an outgroup
\item
  in this practical, we will investigate the phylogenetic position of
  four prehistorical \emph{Y. pestis} strains that we have recently
  discovered: KZL002, GZL002, CHC004 and VLI092
\end{itemize}

\end{tcolorbox}

We start by exploring the alignment in \emph{MEGA}. Open the \emph{MEGA}
desktop application by typing \texttt{mega\ \&} in the terminal.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Adding ``\&'' at the end of a commandline allows to run a program in the
background while letting the terminal accessible. This particularly
useful when starting a graphical interface from the terminal.

\end{tcolorbox}

Then, load the alignment by clicking on File -\textgreater{} Open A
File/Session -\textgreater{} Select the
\emph{snpAligment\_session5.fasta} (in the working directory of the
session).

\includegraphics{assets/images/chapters/phylogenomics/3.png}

It will you ask what you want to do with the alignment. In \emph{MEGA}
you can also produce an alignment, however, since our sequences are
already aligned we will press on \emph{Analyze}.

Then we will select \emph{Nucleotide Sequences} since we are working
with a DNA alignment. Note that \emph{MEGA} can also work with Protein
Sequences as well as Pairwise Distance Matrix (which we will cover
shortly). In the same window, we will change the character for
\emph{Missing Data} to \textbf{N} and click in \emph{OK}.

\includegraphics{assets/images/chapters/phylogenomics/4.png}

A window would open up asking if our alignment contains protein encoding
sequences, and we will select \emph{No}.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

If you had protein encoding sequences, you would have selected Yes. This
will allow you to treat different positions with different evolutionary
modes depending on their codon position. One can do this to take in
account that the third codon position can change to different
nucleotides without resulting in a different amino acid, while position
one and two of the codon are more restricted.

\end{tcolorbox}

To explore the alignment, you will then click on the box with \emph{TA}

\includegraphics{assets/images/chapters/phylogenomics/5.png}

You will see an alignment containing sequences from the bacterial
pathogen \emph{Yersinia pestis}. Within the alignment, we have four
sequences of interest (KZL002, GZL002, CHC004 and VLI092) that date
between 5000-2000 years Before Present (BP), and we want to know how
they relate to the rest of the \emph{Yersinia pestis} genomes in the
alignment.

\includegraphics{assets/images/chapters/phylogenomics/6.png}

\textbf{Questions:}

How many sequences are we analysing?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

We are analysing 34 sequences.

\end{tcolorbox}

What are the Ns in the sequences?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

They represent positions where we have missing data. We told \emph{MEGA}
to encode missing positions as \emph{N}

\end{tcolorbox}

What do you think the dots represent?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

The first line is a \textbf{consensus} sequence: it indicates the
nucleotide supported by the majority of the sequences in the alignment
(90\% of the sequences should agree, otherwise an N is displayed)

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

They represent positions that are the same as the consensus

\end{tcolorbox}

Once you know this, can you already tell by looking at the alignment
which sequence is the most divergent (scroll down)

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

We can easily see that the last sequence in the alignment (Y.
pseudotuberculosis) contains more disagreements to the consensus. This
is normal since this is the only genome not belonging to the \emph{Y.
pestis} species: we will use it as an outgroup

\end{tcolorbox}

\hypertarget{distance-based-phylogeny-neighbour-joining}{%
\subsection{Distance-based phylogeny: Neighbour
Joining}\label{distance-based-phylogeny-neighbour-joining}}

The Neighbour Joining (NJ) method is an agglomerative algorithm which
can be used to derive a phylogenetic tree from a pairwise distance
matrix. In essence, this method will be grouping taxa that have the
shortest distance together first, and will be doing this iteratively
until all the taxa/sequences included in your alignment have been placed
in a tree.

Here are the details of the calculations for a small NJ tree example
with 6 taxa:

\includegraphics{assets/images/chapters/phylogenomics/NJ_algorithm.png}

Luckily, you won't have to do this by hand since \emph{MEGA} allows you
to build a NJ tree. For that go back to \emph{MEGA} and click on the
\emph{Phylogeny} symbol (toolbar of the main menu window) and then
select \emph{Construct Neighbour Joining Tree}. Type ``Yes'' when you
are asked if you want to use the currenctly active date. In the window
that pop up, you will then chance the \emph{Model/Method} to
\emph{p-distance}. Then press \emph{OK} and a window with the calculated
phylogenetic tree will pop up.

\includegraphics{assets/images/chapters/phylogenomics/9.png}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{p-distances?}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

A NJ tree can be built from any type of distances. This includes:

\begin{itemize}
\tightlist
\item
  p-distances (also called raw distances): these are simply the
  proportion of differences between sequences
\item
  corrected distances: these are based on an underlying substitution
  model (JC69, K80, GTR\ldots) and account for multiple substitutions at
  the same sites (which would result in only one visible difference)
\item
  p-distances and corrected distances should be similar when the number
  of substitutions is low compared to the genome length
\end{itemize}

\textbf{note:} a ``substitution'' is a type of mutation in which a
nucleotide is replaced by another.

\end{tcolorbox}

Since the tree is not easily visualised in \emph{MEGA}, we will export
it in newick format (a standard text format for trees) and explore our
tree in \emph{FigTree}. This tool has a better interface for visually
manipulating trees and allows us to interact with the phylogenetic tree.

To do that you will click on \emph{File}, then \emph{Export current tree
(Newick)} and click on \emph{Branch Lengths} to include those in the
newick annotation. When you press \emph{OK}, a new window with the tree
in newick format will pop up and you will then press \emph{File}
-\textgreater{} \emph{Save} and saved it as \emph{NJ\_tree.nwk}. You can
then close the text editor and tree explorer windows (no need to save
the session).

\includegraphics{assets/images/chapters/phylogenomics/10.png}

As said above, we will explore own NJ tree in \emph{FigTree}. Open the
software by typing \texttt{figtree\ \&} in the terminal (if you use the
same terminal window as the one in which you ran mega, you might have to
press \texttt{enter} first). Then, open the NJ tree by clicking on
\emph{File} -\textgreater{} \emph{Open} and selecting the file with the
NJ tree \emph{NJ\_tree.nwk}

\includegraphics{assets/images/chapters/phylogenomics/11.png}

Note that even though a root is displayed by default in \emph{FigTree},
NJ trees are actually \textbf{unrooted}. We know that \emph{Yersinia
pseudotuberculosis} (labelled here as \emph{Y. pseudotuberculosis}) is
an outgroup to \emph{Yersinia pestis}. You can reroot the tree by
selecting \emph{Y.pseudotuberculosis} and pressing \emph{Reroot}.

\includegraphics{assets/images/chapters/phylogenomics/14.png}

Now we have a rooted tree.

\textbf{Questions:}

How much time did the NJ-tree calculation take?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textasciitilde1 second

\end{tcolorbox}

How many leaves/tips has our tree?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

34, i.e.~the number of sequences in our SNP alignment.

\end{tcolorbox}

Where are our taxa of interest? (KZL002, GZL002, CHC004 and VLI092)

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

They all fall ancestral to the rest of \emph{Yersinia pestis} in this
tree.

\end{tcolorbox}

Do they form a monophyletic group (a clade)?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Yes, they form a monophyletic group. We can also say that this group of
prehistoric strains form their own lineage.

\end{tcolorbox}

\hypertarget{probabilistic-methods-maximum-likelihood-and-bayesian-inference}{%
\subsection{Probabilistic methods: Maximum Likelihood and Bayesian
inference}\label{probabilistic-methods-maximum-likelihood-and-bayesian-inference}}

These are the most commonly used approach today. In general,
probabilistic methods are statistical techniques that are based on
models under which the observed data is generated through a stochastic
process depending on a set of parameters which we want to estimate. The
probability of the data given the model parameters is called the
likelihood.

\includegraphics{assets/images/chapters/phylogenomics/19.png}

\textbf{Question:}

In a phylogenetic probabilistic model, what are the data and what are
the parameters?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

In a phylogenetic probabilistic model, the data is the sequence
alignment and the parameters, are:

\begin{itemize}
\tightlist
\item
  the parameters of the chosen substitution model (substitution rates
  and base frequencies)
\item
  the phylogenetic tree
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/20.png}

\end{tcolorbox}

\hypertarget{maximum-likelihood-estimation-and-bootstrapping}{%
\subsubsection{Maximum likelihood estimation and
bootstrapping}\label{maximum-likelihood-estimation-and-bootstrapping}}

One way we can make inferences from a probabilistic model is by finding
the combination of parameters which maximises the likelihood. These
parameter values are called maximum likelihood (ML) estimates. We are
usually not able to compute the likelihood value for all possible
combinations of parameters and have to rely on heuristic algorithms to
find the maximum likelihood estimates.

\includegraphics{assets/images/chapters/phylogenomics/21.png}

The Maximum likelihood estimates are point estimates, i.e.~single
parameter values (for example, a tree), which does not allow to measure
the associated uncertainty. A classic method to measure the uncertainty
in ML trees is bootstrapping, which consists in repeatedly
``disturbing'' the alignment by masking sites randomly and estimating a
tree from each of these bootstrap alignments.

\includegraphics{assets/images/chapters/phylogenomics/22.png}

For each clade in the ML tree, a bootstrap support value is computed
which corresponds to the proportion of bootstrap trees containing the
clade. This gives an indication of how robustly the clade is supported
by the data (i.e.~whether it holds even after disturbing the dataset).
Bootstrapping can be used to measure the topology uncertainty of trees
estimated with any inference method.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Bootstrapping can be used to measure incertainty with any type of
inference method, including distance methods

\end{tcolorbox}

\textbf{Let's make our own ML tree!}

Here is a command to estimate an ML phylogenetic tree together with
bootstraps using \emph{RAxML} (you may find the list of parameters in
the \emph{RAxML} manual):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{raxmlHPC{-}PTHREADS} \AttributeTok{{-}m}\NormalTok{ GTRGAMMA }\AttributeTok{{-}T}\NormalTok{ 3 }\AttributeTok{{-}f}\NormalTok{ a }\AttributeTok{{-}x}\NormalTok{ 12345 }\AttributeTok{{-}p}\NormalTok{ 12345 }\AttributeTok{{-}N}\NormalTok{ autoMRE }\AttributeTok{{-}s}\NormalTok{ snpAlignment\_session5.fasta }\AttributeTok{{-}n}\NormalTok{ full\_dataset.tre}
\end{Highlighting}
\end{Shaded}

Here is the meaning of the chosen parameters:

\includegraphics{assets/images/chapters/phylogenomics/raxml_cmdline.png}

Once the analysis has been completed, you can open the tree using
\emph{Figtree} (RAxML\_bipartitions.full\_dataset.tre file, change
``label'' to ``bootstrap support'' at the prompt).

\includegraphics{assets/images/chapters/phylogenomics/figtree_prompt.png}

The tree estimated using this model is a substitution tree (branch
lengths represent genetic distances in substitutions/site). As for the
NJ tree,it is not oriented in time: this is an unrooted tree (displayed
with a random root in Figtree). You can reroot the tree in
\emph{Figtree} using \emph{Y. pseudotuberculosis} as an outgroup, as
previously.

\textbf{Questions:}

Can you confirm the position of our genomes of interest (KZL002, GZL002,
CHC004 and VLI092)?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Yes. Just as in the NJ tree, they form a clade which is basal to the
rest of the \emph{Y. pestis} diversity.

\end{tcolorbox}

Is that placement well-supported? (look at the bootstrap support value:
click on the ``Node Labels'' box and open the drop-down menu, change
``Node ages'' to ``bootstrap support'')

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The placement is strongly supported as indicated by a bootstrap support
of 100\% for this clade (it is not very easy to see, you probably need
to zoom in a bit)

\includegraphics{assets/images/chapters/phylogenomics/bootstrap_support.png}

\end{tcolorbox}

You can notice that the phylogeny is difficult to visualize due to the
long branch leading to \emph{Y. pseudotuberculosis}. Having a very
distant outgroup can also have deleterious effects on the estimated
phylogeny (due to the so-called ``long branch attraction'' effect). We
can construct a new phylogeny after removing the outgroup:

\begin{itemize}
\tightlist
\item
  go back to the alignment in mega, unclick \emph{Y.pseudotuberculosis},
  and export in fasta format (``Data'' -\textgreater{} ``Export Data''
  -\textgreater{} change ``Format'' to ``Fasta'' and click ``Ok''; you
  can save it as: ``snpAlignment\_without\_outgroup.fas'')
\item
  run raxml on this new alignment (change input to
  ``snpAlignment\_without\_outgroup.fas'' and output prefix to
  ``without\_outgroup'' in the commandline)
\item
  open the bipartition\ldots{} file in figtree and reroot the tree based
  on the knowledge we have gained previously: place the root on the
  branch leading to the prehistoric Y. pestis strains (KZL002, GZL002,
  CHC004 and VLI092).
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/raxml_tree_woOutgroup.png}

Lastly, we will export the rooted tree from figtree: File
-\textgreater{} Export trees -\textgreater{} select the ``save as
currently displayed'' box and save as ``ML\_tree\_rooted.tre'' (this
will be useful for the section ``Temporal signal assessment'' at the end
of this tutorial)

\includegraphics{assets/images/chapters/phylogenomics/export_tree.png}

\hypertarget{estimating-a-time-tree-using-bayesian-phylogenetics-beast2}{%
\subsubsection{\texorpdfstring{Estimating a time-tree using Bayesian
phylogenetics
(\emph{BEAST2})}{Estimating a time-tree using Bayesian phylogenetics (BEAST2)}}\label{estimating-a-time-tree-using-bayesian-phylogenetics-beast2}}

Now, we will try to use reconstruct a phylogeny in which the branch
lengths do not represent a number of substitutions but instead represent
the time of evolution. To do so, we will use the dates of ancient
genomes (C14 dates) to calibrate the tree in time. This assumes a
molecular clock hypothesis in which substitutions occur at a rate that
is relatively constant in time so that the time of evolution can be
estimated based on the number of substitutions.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

A great advantage of ancient pathogen genomes is that they provide key
calibration points to estimate molecular clocks and dated phylogenies.
This is more difficult to do with modern data alone.

\end{tcolorbox}

We will estimate a time-tree from our alignment using Bayesian inference
as implemented in the \emph{BEAST2} software. Bayesian inference is
based on a probability distribution that is different from the
likelihood: the posterior probability. The posterior probability is the
probability of the parameters given the data. It is easier to interpret
than the likelihood because it directly contains all the information
about the parameters: point estimates such as the median or the mean can
be directly estimated from it, but also percentile intervals which can
be used to measure uncertainty.

\includegraphics{assets/images/chapters/phylogenomics/23.png}

The Bayes theorem tells us that the posterior probability is
proportional to the product of the likelihood and the ``prior''
probability of the parameters:

\includegraphics{assets/images/chapters/phylogenomics/bayes_theorem.png}

Therefore, for Bayesian inference, we need to complement our
probabilistic model with prior distributions for all the parameters.
Because we want to estimate a time tree, we also add another parameter:
the molecular clock (average substitution rate in time units).

\includegraphics{assets/images/chapters/phylogenomics/24.png}

To characterize the full posterior distribution of each parameter, we
would need in theory to compute the posterior probability for each
possible combination of parameters. This is impossible, and we will
instead use an algorithm called Markov chain Monte Carlo (MCMC) to
approximate the posterior distribution. The MCMC is an algorithm which
iteratively samples values of the parameters from the posterior
distribution. Therefore, if the MCMC has run long enough, the (marginal)
posterior distribution of the parameters can be approximated by a
histogram of the sampled values.

\includegraphics{assets/images/chapters/phylogenomics/25.png}

\hypertarget{set-up-a-beast2-analysis}{%
\paragraph{\texorpdfstring{Set up a \emph{BEAST2}
analysis}{Set up a BEAST2 analysis}}\label{set-up-a-beast2-analysis}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

The \href{https://taming-the-beast.org/tutorials/}{``taming the beast''
website} has great tutorials to learn setting a \emph{BEAST2} analysis.
In particular, the ``Introduction to BEAST2'', ``Prior selection'' and
``Time-stamped data'' are good starts.

\end{tcolorbox}

The different components of the \emph{BEAST2} analysis can be set up in
the program \emph{BEAUti}:

\includegraphics{assets/images/chapters/phylogenomics/26.png}

Open BEAUTi by typing \texttt{beauti\ \&} in the terminal, and set up an
analysis as followed:

\begin{itemize}
\tightlist
\item
  load the alignment without outgroup in the ``Partitions'' tab
  (``File'' -\textgreater{} ``Import alignment''; select ``nucleotide'')
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_import_data.png}

\begin{itemize}
\tightlist
\item
  set the sampling dates in the ``Tip dates'' tab:

  \begin{itemize}
  \tightlist
  \item
    select ``Use tip dates''
  \item
    click on ``Auto-configure'' -\textgreater{} ``read from file'' and
    select the sample\_dates.txt file
  \item
    change ``Since some time in the past'' to ``Before present''
  \end{itemize}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_tip_dates.png}

\begin{itemize}
\tightlist
\item
  select the substitution model in the ``Site model'' tab:

  \begin{itemize}
  \tightlist
  \item
    chose a GTR model
  \item
    use 4 Gamma categories for the Gamma site model: this is to account
    for variations of the substitution rate accross sites
    (site=nucleotide position)
  \end{itemize}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_substitution_model.png}

\begin{itemize}
\tightlist
\item
  choose the molecular clock model in the ``Clock model'' tab:

  \begin{itemize}
  \tightlist
  \item
    use a relaxed clock lognormal model (this is to allow for some
    variation of the clock rate accross branches)
  \item
    change the initial value of the clock rate to 10-4
    substitution/site/year (\textbf{10-4 can be written 1E-4})
  \end{itemize}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_clock_model.png}

\begin{itemize}
\tightlist
\item
  choose the prior distribution of parameters in the ``Priors'' tab:

  \begin{itemize}
  \tightlist
  \item
    use a Coalescent Bayesian Skyline tree prior
  \item
    change the mean clock prior to a uniform distribution between 1E-6
    and 1E-3 subst/site/year
  \item
    leave everything else to default
  \end{itemize}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_priors.png}

\begin{itemize}
\tightlist
\item
  set up the MCMC in the ``MCMC'' tab:

  \begin{itemize}
  \tightlist
  \item
    use a chain length of 300M
  \item
    sample the monodimensional parameters and trees every 10,000
    iterations (unfold ``tracelog'' and ``treelog'' menus and change
    ``log every'' to 10,000)
  \end{itemize}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beauti_MCMC.png}

\begin{itemize}
\tightlist
\item
  save the analysis setup as an xml file: ``File'' -\textgreater{}
  ``Save as''; you can name the file ``beast\_analysis\_Y\_pestis.xml''
\end{itemize}

Now that the analysis is setup, we can run it using BEAST:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{beast}\NormalTok{ beast\_analysis\_Y\_pestis.xml}
\end{Highlighting}
\end{Shaded}

Once the analysis is running two files should have been created and are
continuously updated:

\begin{itemize}
\tightlist
\item
  the ``snpAlignment\_without\_outgroup.log'' file which contains the
  values sampled by the MCMC for various monodimensional parameters such
  as the clock rate, as well as other values that are a logged along the
  MCMC such as the posterior probability and the likelihood.
\item
  the ``snpAlignment\_without\_outgroup.trees'' file which contains the
  MCMC trees sampled by the MCMC
\end{itemize}

While the analysis is running, you can start reading the next section

\hypertarget{assessing-beast2-results}{%
\paragraph{\texorpdfstring{Assessing \emph{BEAST2}
results}{Assessing BEAST2 results}}\label{assessing-beast2-results}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reminder}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

We are using an MCMC algorithm to sample the posterior distribution of
parameters. If the MCMC has run long enough, we can use the sampled
parameters to approximate the posterior distribution itself. Therefore,
we have to check first that the MCMC chain has run long enough.

\end{tcolorbox}

We can assess the MCMC sampling using the program \emph{Tracer}.
\emph{Tracer} can read BEAST log files an generate statistics and plots
for each of the sampled parameters. Most importantly, \emph{Tracer}
provides:

\begin{itemize}
\tightlist
\item
  \textbf{trace plots}: show the sampled parameter values along the MCMC
  run. Trace plots are a very useful MCMC diagnostic tool.
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/tracer_traceplot.png}

The first thing that one needs to assess is wether the MCMC has passed
the so called ``burn-in'' phase. The MCMC starts with a random set of
parameters and will take some time to reach a zone of high posterior
probability density. The parameter values that are sampled during this
initial phase are usually considered as noise and discarded (by default,
tracer discards the first 10\% of samples). The burn-in phase can be
visualize on trace plots as an initial phase during which the posterior
probability of sampled parameters is constantly increasing before
reaching a plateau:

\includegraphics{assets/images/chapters/phylogenomics/32.png}

Once the burn-in phase is passed, one can look at the trace plots to
assess if the parameters have been sampled correctly and long enough.
Usually, when this is the case, the trace should be quite dense and
oscillating around a central value (nice trace plots should look like
``hairy caterpillars''). In the figure below, the trace on the left
doesn't look good, the one on the right does:

\includegraphics{assets/images/chapters/phylogenomics/33.png}

-- \textbf{ESS values}: tracer also calculates effective sample sizes
(ESS) for each of the sampled parameters. ESSs are estimates of the
number of sampled parameter values after correcting for auto-correlation
along the MCMC. As a rule of thumb, one usually considers that an MCMC
as run long enough if all parameter's ESS are \textgreater{} 200. Note
that if the trace looks like a hairy caterpillar, the corresponding ESS
value should be high.

\includegraphics{assets/images/chapters/phylogenomics/tracer_ESS.png}

-- \textbf{Parameter estimates}: \emph{Tracer} also provides statistics
and plots to explore the posterior distribution of the parameters. These
should be considered only if the trace plot and ESS values look fine. In
the ``Estimates'' tab, after selecting the chosen parameter in the left
panel, the upper-right panel shows point estimates (mean, median) and
measures of uncertainty (95\% HPD interval), and the bottom-right panel
shows a histogram of the sampled value:

\includegraphics{assets/images/chapters/phylogenomics/tracer_estimates.png}

Let's now load the ``snpAlignment\_without\_outgroup.log'' file into
\emph{Tracer}. Open a new terminal tab, activate the conda environemnt
with \texttt{conda\ activate\ phylogenomics}, open tracer with
\texttt{tracer\ \&}, and then ``File'' -\textgreater{} ``Import trace
file'' -\textgreater{} select ``snpAlignment\_without\_outgroup.log''.
Note that one can load a \emph{BEAST2} log file into tracer even if the
analysis is still running. This allows to assess if the MCMC is running
correctly or has run long enough before it's completed.

\textbf{Question:}

Has the MCMC run long enough?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

You have probably let you analysis run for 10-20 mins before looking at
the log file, and this is definitely not sufficient: the burnin phase
has recently been passed, the trace plots do not look very dense and ESS
values are low. It would probably take a few hours for the analysis to
complete. Luckily we have run the analysis in advance and saved the log
files for you in the ``intermediateFiles'' folder:
``snpAlignment\_without\_outgroup.log'' and
``snpAlignment\_without\_outgroup.trees''

\includegraphics{assets/images/chapters/phylogenomics/tracer_unsufficient_sampling.png}

\end{tcolorbox}

You can now load the
``intermediateFiles/snpAlignment\_without\_outgroup.log'' file into
\emph{Tracer}.

\textbf{Questions:}

Has the MCMC run long enough?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Yes! The trace plots look good and all ESSs are \textgreater{} 200
\includegraphics{assets/images/chapters/phylogenomics/beast_results_trace.png}

\end{tcolorbox}

What is your mean estimate of the clock rate (ucld mean)?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textasciitilde7.10-6 substitution/site/year. Note, however, that this
estimate is largely biased since we used a SNP alignment containing only
variable positions. In order to get an unbiased estimate of the
substitution rate, we should have used the full alignment or account for
the number of constant sites by using a ``filtered'' alignment (see
\href{https://www.beast2.org/2019/07/18/ascertainment-correction.html}{here}).
In general, this is good practice since not accounting for conserved
positions in the alignment can sometimes affect the tree as well
(although this should usually be minor, which is why we didn't bother to
do this here).

\includegraphics{assets/images/chapters/phylogenomics/beast_results_ucldMean.png}

\end{tcolorbox}

\hypertarget{mcc-tree}{%
\paragraph{MCC tree}\label{mcc-tree}}

Since we are working in a Bayesian framework, we do not obtain a single
phylogenetic tree as with Maximum likelihood, but a large set of trees
which should be representative of the posterior distribution. In
contrast with monodimensional parameters, a tree distribution cannot be
easily summarized with mean or median estimates. Instead, we need to use
specific tree-summarizing techniques. One of the most popular is the
maximum clade credibility (MCC) tree, which works as follow:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    For any node in any of the sampled trees, compute a
    \textbf{posterior support}: the proportion of trees in the sample
    which contain the node
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Select the MCC tree: this is the tree in which the product of node
    posterior supports is the highest
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Calculate node/branch statistics on the MCC tree: typically, the
    mean/median estimates and HPD interval of node ages are calculated
    from the full tree sample and annotated on the MCC tree
  \end{enumerate}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/MCC_tree_concept.png}

Let's generate an MCC tree from our tree sample. We can do this using
the \emph{TreeAnnotator} software, which has both a commnadline and
graphical interface. Let's use the commandline here and run the
following (using a burn-in of 10\%):

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{treeannotator} \AttributeTok{{-}burnin}\NormalTok{ 10 intermediateFiles/snpAlignment\_without\_outgroup.trees snpAlignment\_without\_outgroup.MCC.tree}
\end{Highlighting}
\end{Shaded}

Once this is completed, we can open the MCC tree with figtree. Let's
then add a few elements to the plot:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Tick the ``Scale Axis'' box, unfold the corresponding menu, and
    select ``Reverse axis'' (now the timescale is in years BP)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Tick the ``Node Labels'' box, unfold the corresponding menu, and
    select ``Display: posterior''. The posterior support of each node is
    now displayed. Note that the support value is a proportion (1=100\%)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Tick the ``Node Bars'' box, unfold the corresponding menu, and
    select ``Display: height\_95\%\_HPD''. The 95\% HPD intervals of
    node ages are now displayed.
  \end{enumerate}
\end{itemize}

\includegraphics{assets/images/chapters/phylogenomics/beast_results_MCC_tree.png}

\textbf{Questions:}

Is the root of the tree consistent with what we found previously?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

Yes! The root is placed between our prehistoric strains and the rest of
Y. pestis strains. Note that this time we didn't have to use an outgroup
because we estimated a time-tree: the root is identified as the oldest
node in the tree.

\end{tcolorbox}

What is your estimate for the age of the most recent common ancestor of
all Y. pestis strains?

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Answer}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\textasciitilde5800 years BP (HPD 95\%: \textasciitilde8000-4500 years
BP)

\end{tcolorbox}

\hypertarget{bonus-temporal-signal-assessment}{%
\subsubsection{Bonus: Temporal signal
assessment}\label{bonus-temporal-signal-assessment}}

It is a good practice to assess if the genetic sequences that we analyse
do indeed behave like molecular clocks before trying to estimate a time
tree (i.e.~we should have done this before the actual \emph{BEAST2}
analysis). A classic way to assess the temporal signal of a dataset is
the root-to-tip regression. The rationale of the root-to-tip regression
is to verify that the oldest a sequence is, the closer it should be to
the root in a (rooted) substitution tree because there was less time for
substitution to accumulate. In other words, their should be a
correlation between sample age and distance to the root, which we can
assess using a linear regression (root-to-tip regression). This can be
done using the program \emph{TempEst}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  open \emph{TempEst} by typing \texttt{tempest\ \&} and load the rooted
  ML tree that we produced previously (you should have saved it as
  ``ML\_tree\_rooted.tre'')
\item
  click on ``Import Dates'' in the ``Sample Dates'' tab, select the
  sample\_age.txt file and click ``OK''
\item
  still in the ``Sample Dates'' tab, change ``Since some time in the
  past'' to ``Before present'' (one might need to extand the
  \emph{TempEst} window to see the pull down menu)
\item
  look at the ``Root-to-tip tab'': is there a positive correlation
  between time and root-to-tip divergence as expected under the
  molecular clock hypothesis?
\end{enumerate}

\includegraphics{assets/images/chapters/phylogenomics/Tempest.png}

\part{Ancient Metagenomic Resources}

Following the earlier chapters, you should now be familiar with the
basic concepts of the practical aspects of the ancient metagenomic
project. However, as with all bioinformaticians, we like to automate as
much of our work as possible. In this chapter, we will introduce you to
some of the dedicated resources to address this, covering both where and
how to find ancient metagenomic data, but also how to speed up the
analyses introduced earlier through automated pipelines.

\hypertarget{accessing-ancient-metagenome-data}{%
\section*{\texorpdfstring{\protect\hyperlink{accessing-ancient-metagenomic-data}{Accessing
Ancient Metagenome
Data}}{Accessing Ancient Metagenome Data}}\label{accessing-ancient-metagenome-data}}
\addcontentsline{toc}{section}{\protect\hyperlink{accessing-ancient-metagenomic-data}{Accessing
Ancient Metagenome Data}}

\markright{Accessing Ancient Metagenome Data}

Finding relevant comparative data for your ancient metagenomic analysis
is not trivial. While palaeogenomicists are very good at uploading their
raw sequencing data to large sequencing data repositories such as the
EBI's ENA or NCBI's SRA archives in standardised file formats, these
files often have limited metadata. This often makes it difficult for
researchers to search for and download relevent published data they wish
to use use to augment their own analysis.

AncientMetagenomeDir is a community project from the SPAAM community to
make ancient metagenomic data more accessible. We curate a list of
standardised metadata of all published ancient metagenomic samples and
libraries, hosted on GitHub. In this chapter we will go through how to
use the AncientMetagenomeDir repository and associated tools to find and
download data for your own analyses. We will also discuss important
things to consider when publishing your own data to make it more
accessible for other researchers.

\hypertarget{ancient-metagenomic-pipelines}{%
\section*{\texorpdfstring{\protect\hyperlink{ancient-metagenomic-pipelines-1}{Ancient
Metagenomic
Pipelines}}{Ancient Metagenomic Pipelines}}\label{ancient-metagenomic-pipelines}}
\addcontentsline{toc}{section}{\protect\hyperlink{ancient-metagenomic-pipelines-1}{Ancient
Metagenomic Pipelines}}

\markright{Ancient Metagenomic Pipelines}

Analyses in the field of ancient DNA are growing, both in terms of the
number of samples processed and in the diversity of our research
questions and analytical methods. Computational pipelines are a solution
to the challenges of big data, helping researchers to perform analyses
efficiently and in a reproducible fashion. Today we will introduce
nf-core/eager, one of several pipelines designed specifically for the
preprocessing, analysis, and authentication of ancient next-generation
sequencing data.

In this chapter we will learn how to practically perform basic analyses
with nf-core/eager, starting from raw data and performing preprocessing,
alignment, and genotyping of several \emph{Yersinia pestis}-positive
samples. We will gain an appreciation of the diversity of analyses that
can be performed within nf-core eager, as well as where to find
additional information for customizing your own nf-core/eager runs.
Finally, we will learn how to use nf-core/eager to evaluate the quality
and authenticity of our ancient samples. After this session, you will be
ready to strike out into the world of nf-core/eager and build your own
analyses from scratch!

\hypertarget{accessing-ancient-metagenomic-data}{%
\chapter{Accessing Ancient Metagenomic
Data}\label{accessing-ancient-metagenomic-data}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/accessing-ancientmetagenomic-data.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate accessing{-}ancientmetagenomic{-}data}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-13}{%
\section{Lecture}\label{lecture-13}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/raw/main/docs/assets/slides/2022/2c-intro-to-ancientmetagenomedir/SPAAM\%20Summer\%20School\%202022\%20-\%202C\%20-\%20AncientMetagenomeDir.pdf}{here}.

\hypertarget{introduction-9}{%
\section{Introduction}\label{introduction-9}}

In most bioinformatic projects, we need to include publicly available
comparative data to expand or compare our newly generated data with.

Including public data can benefit ancient metagenomic studies in a
variety of ways. It can help increase our sample sizes (a common problem
when dealing with rare archaeological samples) - thus providing stronger
statistical power. Comparison with a range of previously published data
of different preservational levels can allow an estimate on the quality
of the new samples. When considering solely (re)using public data, we
can consider that this can also spawn new ideas, projects, and meta
analyses to allow further deeper exploration of ancient metagenomic data
(e.g., looking for correlations between various environmental factors
and preservation).

Fortunately for us, geneticists and particularly palaeogenomicists have
been very good at uploading raw sequencing data to well-established
databases {[}@Anagnostou2015-mz{]}.

In the vast majority of cases you will be able to find publically
available sequencing data on the \href{https://www.insdc.org/}{INSDC}
association of databases, namely the
\href{https://www.ebi.ac.uk/ena/}{EBI's European Nucleotide Archive}
(ENA), and \href{https://www.ncbi.nlm.nih.gov/sra}{NCBI} or
\href{https://www.ddbj.nig.ac.jp/dra/index-e.html}{DDBJ's} Sequence Read
Archives (SRA). However, you may in some cases find ancient metagenomic
data on institutional FTP servers, domain specific databases
(e.g.~\href{https://oagr.org}{OAGR}), \href{https://zenodo.org}{Zenodo},
\href{https://figshare.com}{Figshare}, or
\href{https://github.com}{GitHub}.

But while the data is publicly available, we need to ask whether it is
`FAIR'.

\hypertarget{finding-ancient-metagenomic-data}{%
\section{Finding Ancient Metagenomic
Data}\label{finding-ancient-metagenomic-data}}

\href{http://dx.doi.org/10.1038/sdata.2016.18}{FAIR principles} were
defined by researchers, librarians, and industry in 2016 to improve the
quality of data uploads - primarily by making data uploads more `machine
readable'. FAIR standards for:

\begin{itemize}
\tightlist
\item
  Findable
\item
  Accessible
\item
  Interoperable
\item
  Reproducible
\end{itemize}

When we consider ancient (meta)genomic data, we are pretty close to
this. Sequencing data is in most cases accessible (via the public
databases like ENA, SRA), interoperable and reproducible because we use
field standard formats such as FASTQ or BAM files. However
\emph{findable} remains an issue.

This is because the \emph{metadata} about each data file is dispersed
over many places, and very often not with the data files themselves.

In this case I am referring to metadata such as: What is the sample's
name? How old is it? Where is it from? Which enzymes were used for
library construction? What sequencing machine was this library sequenced
on?

To find this information about a given data file, you have to search
many places (main text, supplementary information, the database itself),
for different types of metadata (as authors report different things),
and also in different formats (text, tables, figures.

This very heterogenous landscape makes it difficult for machines to
index all this information (if at all), and thus means you cannot search
for the data you want to use for your own research in online search
engines.

\hypertarget{ancientmetagenomedir}{%
\section{AncientMetagenomeDir}\label{ancientmetagenomedir}}

This is where the SPAAM community project
`\href{https://github.com/spaam-community/AncientMetagenomeDir}{AncientMetagenomeDir}'
comes in {[}@Fellows\_Yates2021-rp{]}. AncientMetagenomeDir is a
resource of lists of metadata of all publishing and publicly available
ancient metagenomes and microbial genome-level enriched samples and
their associated libraries.

By aggregating and standardising metadata and accession codes of ancient
metagenomic samples and libraries, the project aims to make it easier
for people to find comparative data for their own projects,
appropriately re-analyse libraries, as well as help track the field over
time and facilitate meta analyses.

Currently the project is split over three main tables: host-associated
metagenomes (e.g.~ancient microbiomes), host-associated single-genomes
(e.g.~ancient pathogens), and environmental metagenomes (e.g.~lakebed
cores or cave sediment sequences).

The repository already contains more than 2000 samples and 5000
libraries, spanning the entire globe and as far back as hundreds of
thousands of years.

To make the lists of samples and their metadata as accessible and
interoperable as possible, we utilise simple text (TSV - tab separate
value) files - files that can be opened by pretty much all spreadsheet
tools (e.g., Microsoft Office excel, LibreOffice Calc) and languages (R,
Python etc.) (Figure~\ref{fig-accessingdata-exampledirtable}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-exampledirtable.png}

}

\caption{\label{fig-accessingdata-exampledirtable}Example few columns
and rows of an AncientMetagenomeDir table, including project name,
publication year, site name, latitude, longitude, country and sample
name.}

\end{figure}

Critically, by standardising the recorded all metadata across all
publications this makes it much easier for researchers to filter for
particular time periods, geographical regions, or sample types of their
interest - and then use the also recorded accession numbers to
efficiently download the data.

At their core all different AncientMetagenomeDir tables must have at 6
minimum metadata sets at the sample level:

\begin{itemize}
\tightlist
\item
  Publication information (doi)
\item
  Sample name(s)
\item
  Geographic location (e.g.~country, coordinates)
\item
  Age
\item
  Sample type (e.g.~bone, sediment, etc.)
\item
  Data Archive and accessions
\end{itemize}

Each table then has additional columns depending on the context
(e.g.~what time of microbiome is expected for host-associated
metagenomes, or species name of the genome that was reconstructed).

The AncientMetagenomeDir project already has 9 major releases, and will
continued to be regularly updated as the community continues to submit
new metadata of samples of new publications as they come out.

\hypertarget{amdirt}{%
\section{AMDirT}\label{amdirt}}

But how does one explore such a large dataset of tables with thousands
of rows? You could upload this into a spreadsheet tool or in a
programming language like R, but you would still have to do a lot of
manual filtering and parsing of the dataset to make it useful for
downstream analyses.

In response to this The SPAAM Community have also developed a companion
tool `AMDirT' to facilitate this. Amongst other functionality, AMDirT
allows you to load different releases of AncientMetagenomeDir, filter
and explore to specific samples or libraries of interest, and then
generate download scripts, configuration files for pipelines, and
reference BibTeX files for you, both via a command-line (CLI) or
graphical user interface (GUI)!

\hypertarget{running-amdirt-viewer}{%
\subsection{Running AMDirT viewer}\label{running-amdirt-viewer}}

We will now demonstrate how to use the AMDirT graphical user interface
to load a dataset, filter to samples of interest, and download some
configuration input files for downstream ancient DNA pipelines.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-amdirtlogo.png}

}

\caption{\label{fig-accessingdata-amdirtlogo}AMDirT logo: a cog with the
SPAAM icon in the middle with the word AMDirT to the side}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

This tutorial will require a web-browser! Make sure to run on your local
laptop/PC or, if on on a server, with X11 forwarding activated.

\end{tcolorbox}

First, we will need to activate a conda environment, and then install
the latest development version of the tool for you.

While in the \texttt{accessing-ancientmetagenomic-data} conda
environment, run the following command to load the GUI into your
web-browser. If the browser doesn't automatically load, copy the IP
address and paste it in your browser's URL bar.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{AMDirT}\NormalTok{ viewer}
\end{Highlighting}
\end{Shaded}

Your web browser should now load, and you should see a two panel page.

Under \textbf{Select a table} use the dropdown menu to select
`ancientsinglegenome-hostassociated'.

You should then see a table (Figure~\ref{fig-accessingdata-firstpage}),
pretty similar what you are familiar with with spreadsheet tools such as
Microsoft Excel or LibreOffice calc.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-firstpage.png}

}

\caption{\label{fig-accessingdata-firstpage}Main AMDirT Viewer page on
load. A toolbar on the left displays the AMDirT version, and dropdown
menus for the AncientMetagenomeDir release, table, and downloading tool
to use. The rest of the page shows a tabular window containing rows and
columns corresponding to sample metadata and rows of samples with
metadata such as project name, publication year and site name.}

\end{figure}

To navigate, you can scroll down to see more rows, and press shift and
scroll to see more columns, or use click on a cell and use your arrow
keys (⬆,⬇,⬅,➡) to move around the table.

You can reorder columns by clicking on the column name, and also filter
by pressing the little `burger' icon that appears on the column header
when you hover over a given column.

As an exercise, we will try filtering to a particular set of samples,
then generate some download scripts, and download the files.

First, filter the \textbf{project\_name} column to `Kocher2021' from
{[}@Kocher2021-vg, @fig-accessingdata-projectfilter{]}.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-projectfilter.png}

}

\caption{\label{fig-accessingdata-projectfilter}The AMDirT viewer with a
filter menu coming out of the project name column, with just the Kocher
2021 in the search bar.}

\end{figure}

Then scroll to the right, and filter the \textbf{geo\_loc\_name} to
`United Kingdom' (Figure~\ref{fig-accessingdata-geofilter}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-geofilter.png}

}

\caption{\label{fig-accessingdata-geofilter}The AMDirT viewer with a
filter menu coming out of the geo location column, with just the United
Kingdom written in the search bar and the entry ticked in the results.}

\end{figure}

You should be left with 4 rows.

Finally, scroll back to the first column and tick the boxes of these
four samples (Figure~\ref{fig-accessingdata-tickbox}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-tickbox.png}

}

\caption{\label{fig-accessingdata-tickbox}The AMDirT viewer with just
four rows with samples from Kocher2021 that are located in the United
Kingdom being displayed, and the tickboxes next to each one selected}

\end{figure}

Once you've selected the samples you want, you can press
\textbf{Validate selection}. You should then see a series
loading-spinner, and new a lot of buttons should appear
(Figure~\ref{fig-accessingdata-validated})!

\begin{figure}

{\centering \includegraphics{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-validated.png}

}

\caption{\label{fig-accessingdata-validated}The AMDirT viewer after
pressing the `validate selection' button. A range of buttons are
displayed at the bottom including a warning message, with the buttons
offering download of a range of files such as download scripts,
AncientMetagenomeDir library tables and input configuration sheets for a
range of ancient DNA bioinformatics pipelines}

\end{figure}

You should have four categories of buttons:

\begin{itemize}
\tightlist
\item
  \textbf{Download AncientMetagenomeDir Library Table}
\item
  \textbf{Download Curl sample download script}
\item
  \textbf{Download \textless tool/pipeline name\textgreater{} input TSV}
\item
  \textbf{Download Citations as BibText}
\end{itemize}

The first button is to download a table containing all the
AncientMetagenomeDir metadata of the selected amples. The second is for
generating a download script that will allow you to immediately download
all sequencing data of the samples you selected. The third set of
buttons generate (partially!) pre-configured input files for use in
dedicated ancient DNA pipeline such as nf-core/eager
{[}@Fellows\_Yates2021-jl{]}, PALAEOMIX {[}@Schubert2014-ps{]}, and/or
and aMeta {[}@Pochon2022-hj{]}. Finally, the fourth button generates a
text file with (in most cases) all the citations of the data you
downloaded, in a format accepted by most reference/citation managers. In
this case the reference metadata for that particular publication isn't
publicly available so there is a warning.

It's important to note you are not necessarily restricted to
\href{https://curl.se/}{Curl} for downloading the data. AMDirT aims to
add support for whatever tools or pipelines requested by the community.
For example, an already supported downloading tool alternative is the
\href{https://nf-co.re/fetchngs}{nf-core/fetchNGS} pipeline. You can
select these using the drop-down menus on the left hand-side.

Press the AncientMetagenomeDir library, curl download, and nf-core/eager
buttons to download two \texttt{.tsv} files, one \texttt{*.sh} file and
one '*bib' file. Once this is done, you can close the tab of the web
browser, and in the terminal you can press ctrl + c to shutdown the
tool.

\hypertarget{inspecting-amdirt-viewer-output}{%
\subsection{Inspecting AMDirT viewer
Output}\label{inspecting-amdirt-viewer-output}}

Lets look at the files that AMDirT has generated for you.

First you should \texttt{cd} into the directory that your web browser
downloaded the files into
(e.g.~\texttt{cd\ \textasciitilde{}/Downloads/}), then look inside the
directory. You should see the following three files

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ ls}
\ExtensionTok{AncientMetagenomeDir\_bibliography.bib}
\ExtensionTok{AncientMetagenomeDir\_curl\_download\_script.sh}
\ExtensionTok{AncientMetagenomeDir\_filtered\_libraries.csv}
\ExtensionTok{AncientMetagenomeDir\_nf\_core\_eager\_input\_table.tsv}
\end{Highlighting}
\end{Shaded}

We can simple run \texttt{cat} on each file to look inside. If you run
\texttt{cat} on the curl download script, you should see a series of
\texttt{curl} commands with the correct ENA links for you for each of
the samples you wish to download.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ cat AncientMetagenomeDir\_curl\_download\_script.sh}
\CommentTok{\#!/usr/bin/env bash}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053619.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053618.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053675.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053686.fastq.gz}
\end{Highlighting}
\end{Shaded}

By providing this script for you, AMDirT facilitates fast download of
files of interest by replacing the one-by-one download commands for each
sample with a \emph{single} command!

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ bash AncientMetagenomeDir\_curl\_download\_script.sh}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/009/ERR6053619/ERR6053619.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053619.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/008/ERR6053618/ERR6053618.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053618.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/005/ERR6053675/ERR6053675.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053675.fastq.gz}
\ExtensionTok{curl} \AttributeTok{{-}L}\NormalTok{ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR605/006/ERR6053686/ERR6053686.fastq.gz }\AttributeTok{{-}o}\NormalTok{ ERR6053686.fastq.gz}
\end{Highlighting}
\end{Shaded}

Running this command should result in progress logs of the downloading
of the data of the four selected samples!

Once the four samples are downloaded, AMDirT then facilitates fast
processing of the data, as the \emph{eager} script can be given directly
to nf-core/eager as input. Importantly by including the library metadata
(mentioned above), researchers can leverage the complex automated
processing that nf-core/eager can perform when given such relevant
metadata.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ cat AncientMetagenomeDir\_nf\_core\_eager\_input\_table.tsv}
\ExtensionTok{Sample\_Name}\NormalTok{ Library\_ID  Lane    Colour\_Chemistry    SeqType Organism    Strandedness    UDG\_Treatment   R1  R2  BAM}
\ExtensionTok{I0157}\NormalTok{   ERR6053618  0   4   SE  Homo sapiens    double  unknown ERX5692504\_ERR6053618.fastq.gz  NA  NA}
\ExtensionTok{I0161}\NormalTok{   ERR6053619  0   4   SE  Homo sapiens    double  unknown ERX5692505\_ERR6053619.fastq.gz  NA  NA}
\ExtensionTok{OAI017}\NormalTok{  ERR6053675  0   4   SE  Homo sapiens    double  half    ERX5692561\_ERR6053675.fastq.gz  NA  NA}
\ExtensionTok{SED009}\NormalTok{  ERR6053686  0   4   SE  Homo sapiens    double  half    ERX5692572\_ERR6053686.fastq.gz  NA  NA}
\end{Highlighting}
\end{Shaded}

Finally, we can look into the BibTeX citations file (\texttt{*bib})
which will provide you with the citation information of all the
downloaded data and AncientMetagenomeDir itself.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

The contents of this file is reliant on indexing of publications on
CrossRef. In some cases not all citations will be present (as per the
warning), so this should be double checked!

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ cat AncientMetagenomeDir\_bibliography.bib}
\ExtensionTok{@article\{Fellows\_Yates\_2021,}
    \ExtensionTok{doi}\NormalTok{ = \{10.1038/s41597{-}021{-}00816{-}y\},}
    \ExtensionTok{url}\NormalTok{ = \{https://doi.org/10.1038\%2Fs41597{-}021{-}00816{-}y\},}
    \ExtensionTok{year}\NormalTok{ = 2021,}
    \ExtensionTok{month}\NormalTok{ = \{jan\},}
    \ExtensionTok{publisher}\NormalTok{ = \{Springer Science and Business Media \{LLC\}\},}
    \ExtensionTok{volume}\NormalTok{ = \{8\},}
    \ExtensionTok{number}\NormalTok{ = \{1\},}
    \ExtensionTok{author}\NormalTok{ = \{James A. Fellows Yates and Aida Andrades Valtue\{}\DataTypeTok{\textbackslash{}\textasciitilde{}}\NormalTok{\{n\}\}a and \{}\DataTypeTok{\textbackslash{}A}\NormalTok{A\}shild J. V\{}\DataTypeTok{\textbackslash{}a}\NormalTok{a\}gene and}
    \ExtensionTok{Becky}\NormalTok{ Cribdon and Irina M. Velsko and Maxime Borry and Miriam J. Bravo{-}Lopez and Antonio Fernandez{-}Guerra}
    \ExtensionTok{and}\NormalTok{ Eleanor J. Green and Shreya L. Ramachandran and Peter D. Heintzman and Maria A. Spyrou and Alexander}
    \ExtensionTok{Hübner}\NormalTok{ and Abigail S. Gancz and Jessica Hider and Aurora F. Allshouse and Valentina Zaro and Christina Warinner\},}
    \ExtensionTok{title}\NormalTok{ = \{Community{-}curated and standardised metadata of published ancient metagenomic samples with \{AncientMetagenomeDir\}\},}
    \ExtensionTok{journal}\NormalTok{ = \{Scientific Data\}}
\ErrorTok{\}}
\end{Highlighting}
\end{Shaded}

This file can be easily loaded into most reference managers and then
have all the citations quickly added to your manuscripts.

\hypertarget{amdirt-convert}{%
\subsection{AMDirT convert}\label{amdirt-convert}}

If you're less of a GUI person and consider yourself a command-line
wizard, you can also use the \texttt{AMDirT\ convert} command instead of
the GUI version.

In this case you must supply your own filtered AncientMetagenomeDir
samples table, and use command line options to specify which files to
generate.

For example, lets say you used R to make the following
\href{assets/data/data-accessing-ancientmetagenomic-data/ancientsinglegenome-hostassociated_samples_Kocher2021_UnitedKingdom.tsv}{filtered
file} (basically the same Kocher et al.~2021 samples from the UK, as
filtered in the GUI part of this tutorial), you would supply this to
\texttt{AMDirT\ convert} like so:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ AMDirT convert }\AttributeTok{{-}o}\NormalTok{ . }\AttributeTok{{-}{-}bibliography} \AttributeTok{{-}{-}librarymetadata} \AttributeTok{{-}{-}curl} \AttributeTok{{-}{-}eager}\NormalTok{ ancientsinglegenome{-}hostassociated\_samples\_Kocher2021\_UnitedKingdom.tsv ancientsinglegenome{-}hostassociated}
\end{Highlighting}
\end{Shaded}

When running \texttt{ls} you should see the same resulting files as
before with the GUI!

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{$}\NormalTok{ ls}
\ExtensionTok{AncientMetagenomeDir\_bibliography.bib}
\ExtensionTok{AncientMetagenomeDir\_curl\_download\_script.sh}
\ExtensionTok{AncientMetagenomeDir\_filtered\_libraries.tsv}
\ExtensionTok{AncientMetagenomeDir\_nf\_core\_eager\_input\_table.tsv}
\ExtensionTok{ancientsinglegenome{-}hostassociated\_samples\_Kocher2021\_UnitedKingdom.tsv}
\end{Highlighting}
\end{Shaded}

In this case you say where to save the output files, then use different
flags to specify which scripts, bib, and configuration files you want,
and finally your filtered AncientMetagenomeDir TSV file and which table
it's derived from.

\hypertarget{git-practise}{%
\section{Git Practise}\label{git-practise}}

A critical factor of AncientMetagenomeDir is that it is community-based.
The community curates all new submissions to the repository, and this
all occurs with Git and GitHub.

The data is hosted and maintained on GitHub - new publications are
evaluated on issues, submissions created on branches, made by pull
requests, and PRs reviewed by other members of the community.

You can see the workflow in the image below from the
AncientMetagenomeDir
\href{https://doi.org/10.1038/s41597-021-00816-y}{publication}, and read
more about the workflow on the AncientMetagenomeDir
\href{https://github.com/SPAAM-community/AncientMetagenomeDir/wiki}{wiki}

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{assets/images/chapters/accessing-ancientmetagenomic-data/fig-accessingdata-thedirworkflow.png}

}

\caption{\label{fig-accessingdata-thedirworkflow}Overview of the
AncientMetagenomeDir contribution workflow. Potential publications are
added as a GitHub issue where they undergo relevance evaluation. Once
approved, a contributor makes a branch on the AncientMetagenomeDir
GitHub repository, adds the new lines of metadata for the relevant
samples and libraries, and once ready opens a pull request against the
main repository. The pull request undergoes an automated consistent
check against a schema and then undergoes a human-based peer review for
accuracy against AncientMetagenomeDir guidelines. Once approved, the
pull request is merged, and periodically the dataset is released on
GitHub and automatically archived on Zenodo.}

\end{figure}

As AncientMetagenomeDir is on GitHub, the means we can also use this
repository to try out our Git skills we learnt in the chapter
\protect\hyperlink{introduction-to-github}{Introduction to Git(Hub)}!

Your task is to complete the following steps however with \texttt{git}
terms removed (indicated by quotes). You should be able to complete the
steps and also note down what the \emph{correct} Git terminology:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make a `copy' the
  \href{https://github.com/jfy133/AncientMetagenomeDir}{jfy133/AncientMetagenomeDir}
  repository to your account

  \begin{itemize}
  \tightlist
  \item
    We are forking a personal fork of the main repository to ensure you
    don't accidentally edit the main dataset!
  \end{itemize}
\item
  `Download' the `copied' repo to your local machine
\item
  `Change' to the \texttt{dev} branch
\item
  Modify the file
  \texttt{ancientsinglegenome-hostassociated\_samples.tsv}

  \begin{itemize}
  \tightlist
  \item
    Click
    \href{assets/data/data-accessing-ancientmetagenomic-data/ancientmetagenomedir_example.tsv}{here}
    to get some example data to copy in to the end of the TSV file
  \end{itemize}
\item
  `Send' back to Git(Hub)
\item
  Open a `request' adding the changes to the original
  \texttt{jfy133/AncientMetagenomeDir} repo

  \begin{itemize}
  \tightlist
  \item
    Make sure to put `Summer school' in the title of the `Request'
  \end{itemize}
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Click me to reveal the correct terminology}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fork} the
  \href{https://github.com/jfy133/AncientMetagenomeDir}{jfy133/AncientMetagenomeDir}
  repository to your account
\item
  \textbf{Clone} the copied repo to your local machine
\item
  \textbf{Switch} to the \texttt{dev} branch
\item
  Modify \texttt{ancientsinglegenome-hostassociated\_samples.tsv}
\item
  \textbf{Commit} and \textbf{Push} back to your \textbf{Fork} on
  Git(Hub)
\item
  Open a \textbf{Pull Request} adding changes to the original
  jfy133/AncientMetagenomeDir repo

  \begin{itemize}
  \tightlist
  \item
    Make sure to put `Summer school' in the title of the pull request
  \end{itemize}
\end{enumerate}

\end{tcolorbox}

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

\begin{itemize}
\tightlist
\item
  Reporting of metadata messy! Consider when publishing your own work!

  \begin{itemize}
  \tightlist
  \item
    Use AncientMetagenomeDir as a template
  \end{itemize}
\item
  Use AncientMetagenomeDir and AMDirT (beta) to rapidly find public
  ancient metagenomic data
\item
  Contribute to AncientMetagenomeDir with git

  \begin{itemize}
  \tightlist
  \item
    Community curated!
  \end{itemize}
\end{itemize}

\hypertarget{resources-2}{%
\section{Resources}\label{resources-2}}

\begin{itemize}
\tightlist
\item
  \href{https://doi.org/10.1038/s41597-021-00816-y}{AncientMetagenomeDir
  paper}
\item
  \href{https://github.com/SPAAM-community/AncientMetagenomeDir}{AncientMetagenomeDir
  repository}
\item
  \href{https://www.spaam-community.org/AMDirT/}{AMDirT web server}
\item
  \href{https://amdirt.readthedocs.io/}{AMDirT documentation}
\item
  \href{https://github.com/SPAAM-community/AMDirT}{AMDirT repository}
\end{itemize}

\hypertarget{references-3}{%
\section{References}\label{references-3}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\hypertarget{ancient-metagenomic-pipelines-1}{%
\chapter{Ancient Metagenomic
Pipelines}\label{ancient-metagenomic-pipelines-1}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the \texttt{yml} file in the following
\href{https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/ancient-metagenomic-pipelines.yml}{link}
(right click and save as to download), and once created, activate the
environment with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate ancient{-}metagenomic{-}pipelines}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{lecture-14}{%
\section{Lecture}\label{lecture-14}}

Lecture slides and video from the
\href{https://www.spaam-community.org/wss-summer-school/\#/2022/README}{2022
edition of the summer school}.

PDF version of these slides can be downloaded from
\href{https://github.com/SPAAM-community/https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/raw/main/docs/assets/slides/2022/2d-intro-to-nfcoreeager/SPAAM\%20Summer\%20School\%202022\%20-\%202D\%20-\%20Introduction\%20to\%20nf-core_eager.pdf}{here}.

\hypertarget{introduction-10}{%
\section{Introduction}\label{introduction-10}}

A \textbf{pipeline} is a series of linked computational steps, where the
output of one process becomes the input of the next. Pipelines are
critical for managing the huge quantities of data that are now being
generated regularly as part of ancient DNA analyses. In this chapter we
will go through three dedicated ancient DNA pipelines - all with some
(or all!) functionality geared to ancient metagenomics - to show you how
you can speed up the more routine aspects of the basic analyses we've
learnt about earlier in this text book through workflow automation.

We will introduce:

\begin{itemize}
\tightlist
\item
  \href{https://nf-co.re/eager}{\textbf{nf-core/eager}} - a generalised
  aDNA `workhorse' pipeline that can do both ancient genomics and
  (basic) metagenomics {[}@Fellows\_Yates2021-jl{]}
\item
  \href{https://github.com/NBISweden/aMeta}{\textbf{aMeta}} - a pipeline
  for resource efficient and accurate ancient microbial detection and
  authentication {[}@Pochon2022-hj{]}
\item
  \href{https://nf-co.re/mag}{\textbf{nf-core/mag}} - a \emph{de novo}
  metagenomics assembly pipeline {[}@Krakau2022-we{]} that includes a
  dedicated ancient DNA mode for damage correction and validation.
\end{itemize}

Keep in mind that that there are many other pipelines that exist, and
picking which one often they come down to personal preference, such as
which functionality they support, which language they are written in,
and whether their computational requirements can fit in your available
resources.

Other examples of other ancient DNA genomic pipelines include
\href{https://paleomix.readthedocs.io/en/stable}{Paleomix}
{[}@Schubert2014-ps{]}, and
\href{https://github.com/sneuensc/mapache}{Mapache}
{[}@Neuenschwander2023-aj{]}, and for ancient metagenomics:
\href{https://bitbucket.org/Glouvel/metabit/src/master/}{metaBit}
{[}@Louvel2016-jo{]} and
\href{https://github.com/antonisdim/haystac}{HAYSTAC}
{[}@Dimopoulos2022-tp{]}.

\hypertarget{workflow-managers}{%
\section{Workflow managers}\label{workflow-managers}}

All the pipelines introduced in this chapter utilise \emph{workflow
managers}. These are software that allows users to `chain' together the
inputs and outputs distinct `atomic' steps of a bioinformatics analysis
- e.g.~separate terminal commands of different bioinformatic tools, so
that `you don't have to'. You have already seen very basic workflows or
`pipelines' when using the bash `pipe' (\texttt{\textbar{}}) in the
\protect\hyperlink{introduction-to-the-command-line}{Introduction to the
Command Line} chapter, where the each row of text the output of one
command was `streamed' into the next command.

However in the case of bioinformatics, we are often dealing with non-row
based text files, meaning that `classic' command line pipelines don't
really work. Instead this is where bioinformatic \emph{workflow
managers} come in: they handle the passing of files from one tool to the
next but in a \emph{reproducible} manager.

Modern computational bioinformatic workflow managers focus on a few main
concepts. To summarise @Wratten2021-es, these areas are: data
provenance, portability, scalability, re-entrancy - all together which
contribute to ensuring reproducibility of bioinformatic analyses.
\textbf{Data provenance} refers to the ability to track and visualise
where each file goes and gets processed, as well as \emph{metadata}
about each file and process (e.g., What version of a tool was used? What
parameters were used in that step? How much computing resources were
used). \textbf{Portability} follows from data provenance where it's not
just can the entire execution of the pipeline be reconstructed - but can
it also be run \emph{with the same results} on a different machine? This
is important to ensure that you can install and test the pipeline on
your laptop, but when you then need to do \emph{heavy} computation using
real data, that it will still be able to execute on a high-performance
computing cluster (HPC) or on the cloud - both that have very different
configurations. This is normally achieved through the use of reusable
software environments such as as
\href{https://docs.conda.io/en/latest/}{conda} or container engines such
as \href{https://www.docker.com/}{docker}, and tight integration with
HPC schedulers such as
\href{https://slurm.schedmd.com/documentation.html}{SLURM}. As mentioned
earlier, not having to run each command manually can be a great speed up
to your analysis. However this needs to be able to be \textbf{Scalable}
that it the workflow is still efficient regardless whether you're
running with one or ten thousands samples - modern workflow managers
perform resource requirement optimisation and scheduling to ensure that
all steps of the pipeline will be executed in the most resource
efficient manner so it completes as fast as possible - but regardless of
of the number of input data. Finally, as workflows get bigger and
longer, \textbf{re-entrancy} has become more important, i.e., the
ability to re-start a pipeline run that got stuck halfway through due to
an error.

All workflow managers have different ways of implementing the concepts
above, and these can be very simple
(e.g.~\href{https://en.wikipedia.org/wiki/Make_(software)}{Makefiles})
to very powerful and abstract
(e.g.~\href{https://github.com/openwdl/wdl}{Workflow Description
Language}). In this chapter we will use pipelines that use two popular
workflow managers in bioinformatics,
\href{https://nextflow.io}{Nextflow} and
\href{https://snakemake.github.io}{Snakemake}.

This chapter will not cover how to write your \emph{own} workflow, as
this would require a whole other textbook. However it is recommended to
learn and use workflow managers when carrying out repetitive or routine
bioinformatic analysis (\href{https://nextflow.io}{Nextflow} and
\href{https://snakemake.github.io/}{snakemake} being two popular ones in
bioinformatics). Use of workflow managers can help make your work more
efficiently (as you only run one command, rather than each step
separately), but also more \emph{reproducible} by reducing the risk of
user error when executing each step: the computer will do exactly what
you tell it, and if you don't change anything, will do the exact same
thing every time. If you're interested in writing your own workflows
using workflow managers, many training and tutorials exist on the
internet (e.g., for Nextflow there is the
\href{https://training.nextflow.io/}{official training} or from
\href{https://carpentries-incubator.github.io/workflows-nextflow/}{software
carpentries}, or the
\href{https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html\#tutorial}{official
training for snakemake}.

\hypertarget{what-is-nf-coreeager}{%
\section{What is nf-core/eager?}\label{what-is-nf-coreeager}}

nf-core/eager is a computational pipeline specifically designed for
preprocessing and analysis of ancient DNA data
(Figure~\ref{fig-ancientmetagenomicpipelines-eagermetromap}). It is a
reimplementation of the previously published EAGER (Efficient Ancient
Genome Reconstruction) pipeline {[}@Peltzer2016-ov{]} written in the
workflow manager Nextflow. In addition to reimplementing the original
genome mapping and variant calling pipeline, in a more reproducible and
portable manner the pipeline also included additional new functionality
particularly for researchers interested in microbial sciences, namely a
dedicated genotyper and consensus caller designed for low coverage
genomes, the ability to get breadth and depth coverage statistics for
particular genomic features (e.g.~virulence genes), but also automated
metagenomic screening and authentication of the off-target reads from
mapping (e.g.~against the host reference genome).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/eager2_metromap_complex.png}

}

\caption{\label{fig-ancientmetagenomicpipelines-eagermetromap}nf-core/eager
workflow summary in the form of a `metro map' style diagram. FASTQ or
BAM input for red and yellow lines (representing eukaryotic and
prokaryotic workflows) goes through FastQC, Adapter Removal, alignment
to a reference FASTA input with a range of alignments, before going
through BAM filtering, deduplication, \emph{in silico} damage removal,
and variant calling. Multiple statistics steps come out of the
deduplication `station'. The blue line represents the metagenomic
workflow, where offtarget reads come out of the BAM filtering `station',
and goes through compexity filtering with BBDuk, MALT or Kraken2, and
optionally into MaltExtract for MALT output.}

\end{figure}

A detailed description of steps in the pipeline is available as part of
nf-core/eager's extensive documentation. For more information, check out
the usage documentation \href{https://nf-co.re/eager/2.4.7/usage}{here}.

Briefly, nf-core/eager takes at a minimum standard input file types that
are shared across the genomics field, i.e., raw FASTQ files or aligned
reads in bam format, and a reference fasta. nf-core/eager performs
preprocessing of this raw data, including adapter clipping, read
merging, and quality control of adapter-trimmed data. nf-core/eager then
carries mapping using a variety of field-standard shot-read alignment
tools with default parameters adapted for short and damaged aDNA
sequences. The off-target reads from host DNA mapping can then go into
metagenomic classification and authentication (in the case of MALT).
After genomic mapping, BAM files go through deduplication of PCR
duplicates, damage profiling and removal, and finally variant calling. A
myriad of additional statistics can be generated depending on the users
preference. Finally, nf-core eager uses
\href{https://multiqc.info/}{MultiQC} to create an integrated html
report that summarizes the output/results from each of the pipeline
steps.

\hypertarget{running-nf-coreeager}{%
\subsection{Running nf-core/eager}\label{running-nf-coreeager}}

For the practical portion of this chapter, we will utilize sequencing
data from four aDNA libraries, which you should have already downloaded
from NCBI. If not, please see the \textbf{Preparation} section above .
We will use nf-core/eager to perform a typical microbial \emph{genomic}
analysis, i.e., reconstruction of an ancient genome to generate variant
calls that can be used for generating phylogenomic trees and other
evolutionary analysis, and gene feature coverage statistics to allow
insight into the functional aspects of the genome.

These four libraries come from from two ancient individuals, GLZ002 and
KZL002. GLZ002 comes from the Neolithic Siberian site of Glazkovskoe
predmestie {[}@Yu2020-zw{]} and was radiocarbon dated to 3081-2913
calBCE. KZL002 is an Iron Age individual from Kazakhstan, radiocarbon
dated to 2736-2457 calBCE {[}@Andrades\_Valtuena2022-tq{]}. Both
individuals were originally analysed for human population genetic
analysis, but when undergoing metagenomic screening of the off-target
reads, both set of authors identified reads from \emph{Yersinia pestis}
from these individuals - the bacterium that causes plague. Subsequently
the libraries from these individuals were processed using hybridization
capture to increase the number of \emph{Y. pestis} sequences available
for analysis.

Our aims in the following tutorial are to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preprocess the FASTQ files by trimming adapters and merging paired-end
  reads
\item
  Align reads to the \emph{Y. pestis} reference and compute the
  endogenous DNA percentage
\item
  Filter the aligned reads to remove host DNA
\item
  Remove duplicate reads for accurate coverage estimation and genotyping
\item
  Generate statistics on gene features (e.g.~virulence factors)
\item
  Merge data by sample and perform genotyping on the combined dataset
\item
  Review quality control data to evaluate the success of the previous
  steps
\end{enumerate}

Let's get started!

First, activate the conda environment that we downloaded during setup:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate activate ancient{-}metagenomic{-}pipelines}
\end{Highlighting}
\end{Shaded}

And change into our practical session directory:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/ancient{-}metagenomic{-}pipelines}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

nf-core/eager v2 requires an older version of Nextflow - if installing
manually, ensure you do not have Nextflow version any greater than
v22.10.6!

\end{tcolorbox}

Next, download the latest version of the nf-core/eager repo (or check
for updates if you have a previously-installed version):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{nextflow}\NormalTok{ pull nf{-}core/eager}
\end{Highlighting}
\end{Shaded}

Next we can re-visit AMDirT (see
\protect\hyperlink{accessing-ancient-metagenomic-data}{Accessing Ancient
Metagenomic Data}) to download a pre-prepared configuration file for
nf-core/eager

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load AMDirt (\texttt{AMDirT\ viewer}), and select the latest release
  and the `ancientsinglegenome-hostassociated' table
\item
  Filter the \texttt{sample\_name} column to just show KZL002 and
  GLZ002, and select these two rows

  \begin{itemize}
  \tightlist
  \item
    Press the burger icon on the column
  \item
    Press the filter tab and deselect everything
  \item
    Search GLZ002 and select in filter menu
  \item
    Search KLZ002 and select in filter menu
  \item
    Close filter menu and select the two rows
  \end{itemize}
\item
  Press the Validate Selection button
\item
  Press the `Download Curl sample download script', `Download
  nf-core/eager input TSV', and `Download Citations as BibTex' buttons
\item
  Move the follows into \texttt{eager/} of this tutorial's directory
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

We won't actually use the BibTex citations file for anything in this
tutorial, but it is good habit to \emph{always} to record and save all
citations of any software or data you use!

\end{tcolorbox}

To download the FASTQ files

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Move into the \texttt{eager/} directory
\item
  Run \texttt{bash\ AncientMetagenomeDir\_curl\_download\_script.sh} to
  download the files (this may take \textasciitilde3 minutes)
\end{enumerate}

Next we now inspect the AMDirT generated input TSV file for
nf-core/eager!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{ AncientMetagenomeDir\_nf\_core\_eager\_input\_table.tsv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Sample_Name Library_ID  Lane    Colour_Chemistry    SeqType Organism    Strandedness    UDG_Treatment   R1  R2  BAM
GLZ002  ERR4093961  0   4   PE  Homo sapiens    double  half    ERR4093961_1.fastq.gz   ERR4093961_2.fastq.gz   NA
GLZ002  ERR4093962  0   4   PE  Homo sapiens    double  full    ERR4093962_1.fastq.gz   ERR4093962_2.fastq.gz   NA
KZL002  ERR8958768  0   4   PE  Homo sapiens    double  half    ERR8958768_1.fastq.gz   ERR8958768_2.fastq.gz   NA
KZL002  ERR8958769  0   4   PE  Homo sapiens    double  half    ERR8958769_1.fastq.gz   ERR8958769_2.fastq.gz   NA
\end{verbatim}

Here we see 10 columns, all pre-filled. The first two columns correspond
to sample/library IDs that will be used for data provenance and
grouping. When you have sequencing multiple lanes you can speed up
preprocessing these independently before merging, so lane can specify
this (although not used in this case as we have independent libraries
per sample. You can indicate the colour chemistry to indicate whether
your data requires additional pre-processing to remove poly-G tails, and
then also strandedness and UDG damage treatment status of the libraries
if you require further damage manipulation. Finally you provide paths to
the FASTQ files or BAM files

Other than the raw FASTQ files, we will need a reference genome and
annotation coordinates of genes present on the genome. In this case you
will use a \emph{Yersinia pestis} (accession: GCF\_001293415.1)
reference genome (\texttt{.fna}) and gene feature file (\texttt{.gff})
from \href{https://www.ncbi.nlm.nih.gov/genome}{NCBI Genome}. This is
placed in the \texttt{reference/} directory.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: reference download and preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

To download the required reference genome and annotation file run the
following command to download from the NCBI Genome database.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Download from NCBI}
\ExtensionTok{curl} \AttributeTok{{-}OJX}\NormalTok{ GET }\StringTok{"https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF\_001293415.1/download?include\_annotation\_type=GENOME\_FASTA,GENOME\_GFF,RNA\_FASTA,CDS\_FASTA,PROT\_FASTA,SEQUENCE\_REPORT\&filename=GCF\_001293415.1.zip"} \AttributeTok{{-}H} \StringTok{"Accept: application/zip"}

\FunctionTok{unzip} \PreprocessorTok{*}\NormalTok{.zip}
\FunctionTok{mv}\NormalTok{ ncbi\_dataset/data/GCF\_001293415.1/}\PreprocessorTok{*}\NormalTok{ .}

\CommentTok{\#\# We have to sort the gff file to make it eager compatible}
\ExtensionTok{gffread}\NormalTok{ genomic.gff GCF\_001293415.1\_ASM129341v1\_genomic.gff}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

With all of these, we can run the pipeline!

First lets enter a screen session

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{screen} \AttributeTok{{-}R}\NormalTok{ eager}
\ExtensionTok{conda}\NormalTok{ activate ancient{-}metagenomic{-}pipelines}
\end{Highlighting}
\end{Shaded}

Now we can construct an eager command from within the \texttt{data/}
directory so that it looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{nextflow}\NormalTok{ run nf{-}core/eager }\AttributeTok{{-}r}\NormalTok{ 2.4.7 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}profile docker }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}fasta ../reference/GCF\_001293415.1\_ASM129341v1\_genomic.fna }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}input AncientMetagenomeDir\_nf\_core\_eager\_input\_table.tsv }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}anno\_file ../reference/GCF\_001293415.1\_ASM129341v1\_genomic.gff }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}run\_bam\_filtering }\AttributeTok{{-}{-}bam\_unmapped\_type}\NormalTok{ discard }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}skip\_preseq }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}run\_genotyping }\AttributeTok{{-}{-}genotyping\_tool}\NormalTok{ ug }\AttributeTok{{-}{-}gatk\_ug\_out\_mode}\NormalTok{ EMIT\_ALL\_SITES }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}run\_bcftools\_stats }\AttributeTok{{-}{-}run\_bedtools\_coverage}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

We don't normally recommend running analyses in the directory your data
is in! It is better to keep data and analysis results in separate
directories. Here we are just running eager alongside the data for
convenience (i.e., you don't have to modify the downloaded input TSV)

\end{tcolorbox}

So what is this command doing? The different parameters do the
following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tell nextflow to run the nf-core/eager pipeline with version 2.4.7
\item
  Specify which computing and software environment to use with
  \texttt{-profile}

  \begin{itemize}
  \tightlist
  \item
    In this case we are running locally so we don't specify a computing
    environment (such as a configuration for an institutional HPC)
  \item
    We use \texttt{docker} as our container engine, which downloads all
    the software and specific versions needed for nf-core/eager in
    immutable `containers', to ensure nothing get broken and is as
    nf-core/eager expects
  \end{itemize}
\item
  Provide the various paths to the input files (TSV with paths to FASTQ
  files, a reference fasta, and the reference fasta's annotations)
\item
  Activate the various of the steps of the pipeline you're interested in

  \begin{itemize}
  \tightlist
  \item
    We turn off preseq (e.g.~when you know you can't sequence more)
  \item
    We want to turn on BAM filtering, and specify to generate unmapped
    reads in FASTQ file (so you could check off target reads e.g.~for
    other pathogens)
  \item
    You turn on genotyping using GATK UnifiedGenotyper (preferred over
    HaplotypeCaller due to in compatibiity with that method to
    low-coverage data)
  \item
    We turn on variant statistics (from GATK) using bcftools, and
    coverage statistics of gene features using bedtools
  \end{itemize}
\end{enumerate}

For full parameter documentation, click
\href{https://nf-co.re/eager/2.4.5/parameters}{here}.

And now we wait\ldots{}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

As a reminder, to detach from the screen session type ctrl + a then d.
To reattach \texttt{screen\ -r\ eager}

\end{tcolorbox}

Specifically for ancient (meta)genomic data, the following parameters
and options may be useful to consider when running your own data:

\begin{itemize}
\tightlist
\item
  \texttt{-\/-mapper\ circularmapper}:

  \begin{itemize}
  \tightlist
  \item
    This aligner is a variant of \texttt{bwa\ aln} that allows even
    mapping across the end of linear references of circular genomes
  \item
    Allows reads spanning the start/end of the sequence to be correctly
    placed, and this provides better coverage estimates across the
    entire genome
  \end{itemize}
\item
  \texttt{-\/-hostremoval\_input\_fastq}:

  \begin{itemize}
  \tightlist
  \item
    Allows re-generation of input FASTQ files but with any reads aligned
    to the reference genome removed
  \item
    Can be useful when dealing with modern or ancient data where there
    are ethical restrictions on the publication of host DNA
  \item
    The output can be used as `raw data' upload to public data
    repositories
  \end{itemize}
\item
  \texttt{-\/-run\_bam\_filtering\ -\/-bam\_unmapped\_type}:

  \begin{itemize}
  \tightlist
  \item
    A pre-requisite for performing the metagenomic analysis steps of
    nf-core/eager
  \item
    Generates FASTQ files off the unmapped reads present in the
    reference mapping BAM file
  \end{itemize}
\item
  \texttt{-\/-run\_bedtools\_coverage\ -\/-anno\_file\ /\textless{}path\textgreater{}/\textless{}to\textgreater{}/\textless{}genefeature\textgreater{}.bed}

  \begin{itemize}
  \tightlist
  \item
    Turns on calculating depth/breadth of annotations in the provided
    bed or GFF file, useful for generating e.g.~virulence gene
    presence/absence plots
  \end{itemize}
\item
  \texttt{-\/-run\_genotyping\ -\/-genotyping\_tool\ ug\ -\/-gatk\_ug\_out\_mode\ EMIT\_ALL\_SITES}

  \begin{itemize}
  \tightlist
  \item
    Turns on genotyping with GATK UnifiedGenotyper
  \item
    A pre-requisite for running MultiVCFAnalyzer for consensus
    sequencing creation
  \item
    It's recommend to use either GATK UnifiedGenotyper or freeBayes
    (non-MultiVCFAnalyzer compatible!) for low-coverage data
  \item
    GATK HaplotypeCaller is \emph{not} recommended for low coverage data
    as it performs local \emph{de novo} assembly around possible variant
    sites, and this will fail with low-coverage short-read data
  \end{itemize}
\item
  \texttt{-\/-run\_multivcfanalyzer\ -\/-write\_allele\_frequencies}

  \begin{itemize}
  \tightlist
  \item
    Turns on SNP table and FASTA consensus sequence generation with
    MultiVCFAnalyzer (see pre-requisites above)
  \item
    By providing \texttt{-\/-write\_allele\_frequencies}, the SNP table
    will also provide the percentage of reads at that position
    supporting the call. This can help you evaluate the level of
    cross-mapping from related (e.g.~contaminating environmental)
    species and may question the reliablity of any resulting downstream
    analyses.
  \end{itemize}
\item
  \texttt{-\/-metagenomic\_complexity\_filtering}

  \begin{itemize}
  \tightlist
  \item
    An additional preprocessing step of raw reads before going into
    metagenomic screening to remove low-complexity reads (e.g.~mono- or
    di-nucleotide repeats)
  \item
    Removing these will speed up and lower computational requirements
    during classification, and will not bias profiles as these sequences
    provide no taxon-specific information (i.e., can be aligned against
    thousands to millions of genomes)
  \end{itemize}
\item
  \texttt{-\/-run\_metagenomic\_screening}

  \begin{itemize}
  \tightlist
  \item
    Turns on metagenomic screening with either MALT or Kraken2
  \item
    If running with MALT, can supply \texttt{-\/-run\_maltextract} to
    get authentication statistics and plots (damage, read lengths etc.)
    for evaluation.
  \end{itemize}
\end{itemize}

\hypertarget{top-tips-for-nf-coreeager-success}{%
\subsection{Top Tips for nf-core/eager
success}\label{top-tips-for-nf-coreeager-success}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Screen sessions

  Depending on your input data, infrastructure, and analyses, running
  nf-core/eager can take hours or even days. To avoid crashed due to
  loss of power or network connectivity, try running nf-core/eager in a
  \texttt{screen} or \texttt{tmux} session:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{screen} \AttributeTok{{-}R}\NormalTok{ eager}
\end{Highlighting}
\end{Shaded}
\item
  Multiple ways to supply input data

  In this tutorial, a tsv file to specify our input data files and
  formats. This is a powerful approach that allows nf-core eager to
  intelligently apply analyses to certain files only (e.g.~merging for
  paired-end but not single-end libraries). However inputs can also be
  specified using wildcards, which can be useful for fast analyses with
  simple input data types (e.g.~same sequencing configuration, file
  location, etc.).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{nextflow}\NormalTok{ run nf{-}core/eager }\AttributeTok{{-}r}\NormalTok{ 2.4.7 }\AttributeTok{{-}profile}\NormalTok{ docker }\AttributeTok{{-}{-}fasta}\NormalTok{ ../reference/GCF\_001293415.1\_ASM129341v1\_genomic.fna }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}input }\StringTok{"data/*\_\{1,2\}.fastq.gz"} \OperatorTok{\textless{}}\NormalTok{...}\OperatorTok{\textgreater{}} \DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}udg\_type half}
\end{Highlighting}
\end{Shaded}

  See the online \href{https://nf-co.re/eager/usage}{nf-core/eager
  documentation} for more details.
\item
  Get your MultiQC report via email

  If you have GNU mail or sendmail set up on your system, you can add
  the following flag to send the MultiQC html to your email upon run
  completion:

  \texttt{-\/-email\ "your\_address@something.com"}
\item
  Check out the EAGER GUI

  For researchers who might be less comfortable with the command line,
  check out the nf-core/eager
  \href{https://nf-co.re/launch?pipeline=eager\&release=2.4.7}{launch
  GUI}! The GUI also provides a full list of all pipeline options with
  short explanations for those interested in learning more about what
  the pipeline can do.
\item
  When something fails, all is not lost!

  When individual jobs fail, nf-core/eager will try to automatically
  resubmit that job with increased memory and CPUs (up to two times per
  job). When the whole pipeline crashes, you can save time and
  computational resources by resubmitting with the \texttt{-resume}
  flag. nf-core/eager will retrieve cached results from previous steps
  as long as the input is the same.
\item
  Monitor your pipeline in real time with the Nextflow Tower

  Regular users may be interested in checking out the Nextflow Tower, a
  tool for monitoring the progress of Nextflow pipelines in real time.
  Check \href{https://tower.nf}{here} for more information.
\end{enumerate}

\hypertarget{nf-coreeager-output}{%
\subsection{nf-core/eager output}\label{nf-coreeager-output}}

\hypertarget{results-files}{%
\subsubsection{Results files}\label{results-files}}

In the \texttt{results/} directory of your nf-core/eager run, you will
find a range of directories that will contain output files and tool log
of all the steps of the pipeline. nf-core/eager tries to only save the
`useful' files.

Everyday useful files for ancient (microbial) (meta)genomics typically
are in folders such as:

\begin{itemize}
\tightlist
\item
  \texttt{deduplication/} or \texttt{merged\_bams}

  \begin{itemize}
  \tightlist
  \item
    These contain the most basic BAM files you would want to use for
    downstream analyses (and used in the rest of the genomic workflow of
    the pipeline)
  \item
    They contain deduplicated BAM files (i.e., with PCR artefacts
    removed)
  \item
    \texttt{merged\_bams/}

    \begin{itemize}
    \tightlist
    \item
      This directory will contain BAMs where multiple libraries of one
      sample have been merged into one final BAM file, when these have
      been supplied
    \end{itemize}
  \end{itemize}
\item
  \texttt{damage\_rescaling/} or \texttt{trimmed\ bam/}

  \begin{itemize}
  \tightlist
  \item
    These contain the output BAM files from \emph{in silico} damage
    removal, if you have turned on e.g.~BAM trimming or damage rescaling
  \item
    If you have multiple libraries of one sample, the final BAMs you
    want will be in \texttt{merged\_bams/}
  \end{itemize}
\item
  \texttt{genotyping/}

  \begin{itemize}
  \tightlist
  \item
    This directory will contain your final VCF files from the genotyping
    step, e.g.~from the GATK or freeBayes tools
  \end{itemize}
\item
  \texttt{MultiVCFAnalyzer/}

  \begin{itemize}
  \tightlist
  \item
    This will contain consensus sequences and SNP tables from
    \texttt{MultiVCFAnalyzer}, which also allows generation of
    `multi-allelic' SNP position statistics (useful for the evaluation
    of cross-mapping from contaminants or relatives)
  \end{itemize}
\item
  \texttt{bedtools/}

  \begin{itemize}
  \tightlist
  \item
    This directory will contain the depth and breadth statistics of
    genomic features if a \texttt{gff} or \texttt{bed} file has been
    provided to the pipeline
  \item
    The files can be used to generate gene heatmaps, that can be used to
    visualise a comparative presence/absence of virulence factor across
    genomes (e.g.~for microbial pathogens)
  \end{itemize}
\item
  \texttt{metagenomic\_classification} or \texttt{malt\_extract}

  \begin{itemize}
  \tightlist
  \item
    This directory contains the output RMA6 files from MALT, the
    profiles and taxon count tables from Kraken2, or the aDNA
    authentication output statistics from maltExtract.
  \end{itemize}
\end{itemize}

Most other folders contain either intermediate files that are only
useful for technical evaluation in the case of problems, or statistics
files that are otherwise summarised in the run report.

So, before you delve into these folders, it's normally a good idea to do
a `quality check' of the pipeline run. You can do this using the
interactive MultiQC pipeline run report.

\hypertarget{multiqc-general-statistics}{%
\subsubsection{MultiQC General
Statistics}\label{multiqc-general-statistics}}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ multiqc/}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

If you're impatient, and your nf-core/eager run hasn't finished yet, you
can cancel the run with ctrl + c (possibly a couple of times), then you
can open a premade file in
\texttt{ancient-metagenomic-pipelines/multiqc\_report.html}

\end{tcolorbox}

In here you should see a bunch of files, but you should open the
\texttt{multiqc\_report.html} file in your browser. You can either do
this via the commandline (e.g.~for firefox
\texttt{firefox\ multiqc\_report.html}) or navigate to the file using
your file browser and double clicking on the HTML file.

Once opened you will see a table, and below it many figures and other
tables (Figure~\ref{fig-ancientmetagenomicpipelines-multiqcscreenshot}).
All of these statistics can help you evaluate the quality of your data,
pipeline run, and also possibly some initial results!

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/multiqc-screenshot.png}

}

\caption{\label{fig-ancientmetagenomicpipelines-multiqcscreenshot}Screenshot
of initial view of a MultiQC report. The left hand sidebar provides
links to various sections of the report, most of them containing summary
images of the outputs of all the different tools. On the main panel you
see the MultiQC and nf-core/eager logos, some descriptive information
about how the report was generated, and finally the top of the `General
Statistics' tables, that has columns such as Sample Name, Nr. Input
reads, length of input reads, \%trimmed, etc. Each numeric column
contains a coloured barchart to help readers to quickly evaluate the
number of a given sample against all others in the table}

\end{figure}

Typically you will look at the General Statistics table to get a rough
overview of the pipeline run. If you hover your cursor over the column
headers, you can see which tool the column's numbers came from, however
generally the columns go in order of left to right, where the left most
columns are from earlier in the pipeline run (e.g.~removing adapters),
to variant calling statistics (e.g.~number of variants called). You can
also configure which columns to display using the
\texttt{Configure\ columns} button. It's important to note that in
MultiQC tables, you may have duplicate rows from the same library or
sample. This is due to MultiQC trying to squish in as many statistics
from as many steps of the pipeline as possible (for example, statistics
on each of a pair of FASTQ files, and then statistics on the single
merged and mapped BAM file), so you should play around with the column
configuration to help you visualise the best way to initially evaluate
your data.

The bulk of the MultiQC report is made up of per-tool summary plots
(e.g., barcharts, linecharts etc). Most of these will be interactive,
i.e.~you can hover over lines and bars to get more specific numbers of
each plot. However the visualisations are aimed at helping you identify
possible outliers that may represent failed samples or libraries.

Evaluating how good the data is and how well the run went will vary
depending on the dataset and the options selected. However the
\href{https://nf-co.re/eager/usage\#tutorial---how-to-set-up-nf-coreeager-for-pathogen-genomics}{nf-core/eager
tutorials} have a good overview of questions you can ask from your
MultiQC report to see whether your data is good or not. We shamelessly
copy these questions here (as the overlap authors of both the the
nf-core/eager documentation and this text book is rather high).

Once completed, you can try going through the MultiQC report the command
you executed above, and compare against the questions below. Keep in
mind you have a sample \emph{N} of two, so many of the questions in
regards to identifying `outliers' may be difficult.

The following questions are by no means comprehensive, but rather
represent the `typical' questions the nf-core/eager developers asked of
their own data and reports. However they can act as a good framework for
thinking critically about your own results.

\hypertarget{general-stats-table}{%
\paragraph{General Stats Table}\label{general-stats-table}}

\begin{itemize}
\tightlist
\item
  Do I see the expected number of raw sequencing reads (summed across
  each set of FASTQ files per library) that was requested for
  sequencing?
\item
  Does the percentage of trimmed reads look normal for aDNA, and do
  lengths after trimming look short as expected of aDNA?
\item
  Does the Endogenous DNA (\%) columns look reasonable (high enough to
  indicate you have received enough coverage for downstream, and/or do
  you lose an unusually high reads after filtering )
\item
  Does ClusterFactor or `\% Dups' look high (e.g.~\textgreater2 or
  \textgreater10\% respectively - high values suggesting over-amplified
  or badly preserved samples i.e.~low complexity; note that
  genome-enrichment libraries may by their nature look higher).
\item
  Do you see an increased frequency of C\textgreater Ts on the 5' end of
  molecules in the mapped reads?
\item
  Do median read lengths look relatively low (normally \textless= 100
  bp) indicating typically fragmented aDNA?
\item
  Does the \% coverage decrease relatively gradually at each depth
  coverage, and does not drop extremely drastically
\item
  Does the Median coverage and percent \textgreater3x (or whatever you
  set) show sufficient coverage for reliable SNP calls and that a good
  proportion of the genome is covered indicating you have the right
  reference genome?
\item
  Do you see a high proportion of \% Hets, indicating many multi-allelic
  sites (and possibly presence of cross-mapping from other species, that
  may lead to false positive or less confident SNP calls)?
\end{itemize}

\hypertarget{fastqc-pre-adapterremoval}{%
\paragraph{FastQC
(pre-AdapterRemoval)}\label{fastqc-pre-adapterremoval}}

\begin{itemize}
\tightlist
\item
  Do I see any very early drop off of sequence quality scores suggesting
  problematic sequencing run?
\item
  Do I see outlier GC content distributions?
\item
  Do I see high sequence duplication levels?
\end{itemize}

\hypertarget{adapterremoval}{%
\paragraph{AdapterRemoval}\label{adapterremoval}}

\begin{itemize}
\tightlist
\item
  Do I see high numbers of singletons or discarded read pairs?
\end{itemize}

\hypertarget{fastqc-post-adapterremoval}{%
\paragraph{FastQC
(post-AdapterRemoval)}\label{fastqc-post-adapterremoval}}

\begin{itemize}
\tightlist
\item
  Do I see improved sequence quality scores along the length of reads?
\item
  Do I see reduced adapter content levels?
\end{itemize}

\hypertarget{samtools-flagstat-prepost-filter}{%
\paragraph{Samtools Flagstat (pre/post
Filter)}\label{samtools-flagstat-prepost-filter}}

\begin{itemize}
\tightlist
\item
  Do I see outliers, e.g.~with unusually low levels of mapped reads,
  (indicative of badly preserved samples) that require downstream closer
  assessment?
\end{itemize}

\hypertarget{deduppicard-markduplicates}{%
\paragraph{DeDup/Picard
MarkDuplicates}\label{deduppicard-markduplicates}}

\begin{itemize}
\tightlist
\item
  Do I see large numbers of duplicates being removed, possibly
  indicating over-amplified or badly preserved samples?
\end{itemize}

\hypertarget{malt-metagenomics-only}{%
\paragraph{MALT (metagenomics only)}\label{malt-metagenomics-only}}

\begin{itemize}
\tightlist
\item
  Do I have a reasonable level of mappability?

  \begin{itemize}
  \tightlist
  \item
    Somewhere between 10-30\% can be pretty normal for aDNA, whereas
    e.g.~\textless1\% requires careful manual assessment
  \end{itemize}
\item
  Do I have a reasonable taxonomic assignment success?

  \begin{itemize}
  \tightlist
  \item
    You hope to have a large number of the mapped reads (from the
    mappability plot) that also have taxonomic assignment.
  \end{itemize}
\end{itemize}

\hypertarget{preseq-genomics-only}{%
\paragraph{PreSeq (genomics only)}\label{preseq-genomics-only}}

\begin{itemize}
\tightlist
\item
  Do I see a large drop off of a sample's curve away from the
  theoretical complexity? If so, this may indicate it's not worth
  performing deeper sequencing as you will get few unique reads
  (vs.~duplicates that are not any more informative than the reads
  you've already sequenced)
\end{itemize}

\hypertarget{damageprofiler-genomics-only}{%
\paragraph{DamageProfiler (genomics
only)}\label{damageprofiler-genomics-only}}

\begin{itemize}
\tightlist
\item
  Do I see evidence of damage on the microbial DNA (i.e.~a \%
  C\textgreater T of more than \textasciitilde5\% in the first few
  nucleotide positions?) ? If not, possibly your mapped reads are
  deriving from modern contamination.
\end{itemize}

\hypertarget{qualimap-genomics-only}{%
\paragraph{QualiMap (genomics only)}\label{qualimap-genomics-only}}

\begin{itemize}
\tightlist
\item
  Do you see a peak of coverage (X) at a good level, e.g.~\textgreater=
  3x, indicating sufficient coverage for reliable SNP calls?
\end{itemize}

\hypertarget{multivcfanalyzer-genomics-only}{%
\paragraph{MultiVCFAnalyzer (genomics
only)}\label{multivcfanalyzer-genomics-only}}

\begin{itemize}
\tightlist
\item
  Do I have a good number of called SNPs that suggest the samples have
  genomes with sufficient nucleotide diversity to inform phylogenetic
  analysis?
\item
  Do you have a large number of discarded SNP calls?
\item
  Are the \% Hets very high indicating possible cross-mapping from
  off-target organisms that may confounding variant calling?
\end{itemize}

As above evaluating these outputs will vary depending on the data and or
pipeline settings, and very much. However the extensive
\href{https://nf-co.re/eager/2.4.7/output}{output documentation} of
nf-core/eager can guide you through every single table and plot to
assist you in continuing any type of ancient DNA project, assisted by
fun little cartoony schematic diagrams
(Figure~\ref{fig-ancient-metagenomic-pipelines-eageroutputexample})!

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/damageprofiler_deaminationpatterns.png}

}

\caption{\label{fig-ancient-metagenomic-pipelines-eageroutputexample}Example
of cartoon schematic diagram of the output from DamageProfiler in
different contexts. The six panels show different types of ancient DNA
damage line-plots from having no damage (flat red/blue lines), however
with a speech bubble noting that if the library was UDG treated, that
the flat lines might be valid, all the way to the `classic' ancient DNA
damage plot with both red and blue lines showing an exponential decay
from the ends of reads to the middle, with a motivational speech bubble.
Source: Zandra Fagernäs, CC-BY 4.0 via
\href{https://nf-co.re/eager/output}{nf-core/eager documentation}}

\end{figure}

\hypertarget{clean-up}{%
\subsection{Clean up}\label{clean-up}}

Before continuing onto the next section of this chapter, you will need
to deactivate from the conda environment

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ deactivate }
\end{Highlighting}
\end{Shaded}

You may also need to delete the contents of the eager directory

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/ancient{-}metagenomic{-}pipelines/eager}
\FunctionTok{rm} \AttributeTok{{-}r} \PreprocessorTok{*}
\end{Highlighting}
\end{Shaded}

\hypertarget{what-is-ameta}{%
\section{What is aMeta?}\label{what-is-ameta}}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

For this chapter's exercises, if not already performed, you will need to
create the \protect\hyperlink{creating-a-conda-environment}{conda
environment} from the following
\href{https://github.com/NBISweden/aMeta/blob/main/workflow/envs/environment.yaml}{\texttt{yml}
file}, and activate the environment:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate aMeta}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

While nf-core/eager is a solid pipeline for microbial genomics, and can
also perform metagenomic screening via the integrated HOPS pipeline or
Kraken2, in some cases you may wish to have a more accurate and resource
efficient pipeline In this section, we will demonstrate an example of
using aMeta, a snakemake workflow proposed by @Pochon2022-hj that aims
to minimise resource usage by combining both low-resource requiring
k-mer based taxonomic profiling as well as accurate read-alignment
(Figure~\ref{fig-ancientmetagenomicpipelines-ametadiagram}).

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/ameta-workflow.jpg}

}

\caption{\label{fig-ancientmetagenomicpipelines-ametadiagram}Schematic
diagram of the aMeta pipeline. Input samples initially undergo
generalised screening using the K-mer based KrakenUniq. For every hit
that the reads match inside this database, then sees the genome of that
hit then constructed into a MALT database where the reads undergo a
mapping step to generate alignments, a lowest-common-ancestor (LCA)
algorithm step to refine taxonomic assignments, and ancient DNA
authentication statistics generation}

\end{figure}

Rather than the very computationally heavy HOPS pipeline
{[}@Hubler2019-qw{]}, that requires extremely large computational nodes
with large RAM (\textgreater1 TB) to load MALT databases into memory,
aMeta does this via a two step approach. Firstly it uses KrakenUniq (a
k-mer based and thus memory efficient method) to do a screening of
sequencing reads against a broad generalised microbial database. Once
all the possible taxa have been detected, aMeta will then make a new
database of just the genomes of the taxa that were reported from
KrakenUniq (i.e.~a specific database) but using MALT. MALT on thus much
reduced database is then used to perform computationally much heavier
alignment against the reference genomes and LCA taxonomic reassignment.
The output from MALT is then sent to the MaltExtract program of the HOPS
pipeline for ancient DNA authentication statistics.

\hypertarget{running-ameta}{%
\subsection{Running aMeta}\label{running-ameta}}

In this tutorial we will try running the small test data that comes with
aMeta.

aMeta has been written in Snakemake, which means running the pipeline
has to be installed in a slightly different manner to the
\texttt{nextflow\ pull} command that can be used for nf-core/eager.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ ancient{-}metagenomic{-}pipelines/aMeta}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

The code repository, conda environment and database construction has
already been performed for you. If you are running this manually you'll
need to run the steps in the note block below

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: setting up aMeta for tutorial}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

\hypertarget{setting-up-ameta-manually}{%
\subsection{Setting up aMeta manually}\label{setting-up-ameta-manually}}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\textless{}}\NormalTok{!{-}{-} }\ExtensionTok{TODO:}\NormalTok{ replace with link to before{-}you{-}start }\AttributeTok{{-}{-}}\OperatorTok{\textgreater{}}
\FunctionTok{git}\NormalTok{ clone https://github.com/NBISweden/aMeta}
\BuiltInTok{cd}\NormalTok{ aMeta}
\ExtensionTok{conda}\NormalTok{ env create }\AttributeTok{{-}f}\NormalTok{ workflow/envs/environment.yaml}
\ExtensionTok{conda}\NormalTok{ activate aMeta}
\BuiltInTok{cd}\NormalTok{ ancient{-}metagenomic{-}pipelines/aMeta}

\CommentTok{\#\# Change into \textasciitilde{}/.test to set up all the required test resources (Databases etc.)}
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/.test}

\CommentTok{\#\# Set up conda envs}
\ExtensionTok{snakemake} \AttributeTok{{-}{-}use{-}conda} \AttributeTok{{-}{-}show{-}failed{-}logs} \AttributeTok{{-}j}\NormalTok{ 2 }\AttributeTok{{-}{-}conda{-}cleanup{-}pkgs}\NormalTok{ cache }\AttributeTok{{-}{-}conda{-}create{-}envs{-}only} \AttributeTok{{-}s}\NormalTok{ ../workflow/Snakefile }\AttributeTok{{-}{-}conda{-}frontend}\NormalTok{ conda}
\BuiltInTok{source} \VariableTok{$(}\FunctionTok{dirname} \VariableTok{$(}\FunctionTok{dirname} \VariableTok{$CONDA\_EXE))}\NormalTok{/etc/profile.d/conda.sh}

\CommentTok{\#\# Build dummy KrakenUniq database}
\VariableTok{env}\OperatorTok{=}\VariableTok{$(}\FunctionTok{grep}\NormalTok{ krakenuniq .snakemake/conda/}\PreprocessorTok{*}\NormalTok{yaml }\KeywordTok{|} \FunctionTok{awk} \StringTok{\textquotesingle{}\{print $1\}\textquotesingle{}} \KeywordTok{|} \FunctionTok{sed} \AttributeTok{{-}e} \StringTok{"s/.yaml://g"}\VariableTok{)}
\ExtensionTok{conda}\NormalTok{ activate }\VariableTok{$env}
\ExtensionTok{krakenuniq{-}build} \AttributeTok{{-}{-}db}\NormalTok{ resources/KrakenUniq\_DB }\AttributeTok{{-}{-}kmer{-}len}\NormalTok{ 21 }\AttributeTok{{-}{-}minimizer{-}len}\NormalTok{ 11 }\AttributeTok{{-}{-}jellyfish{-}bin} \VariableTok{$(}\BuiltInTok{pwd}\VariableTok{)}\NormalTok{/}\VariableTok{$env}\NormalTok{/bin/jellyfish}
\ExtensionTok{conda}\NormalTok{ deactivate}

\CommentTok{\#\# Get Krona taxonomy tax dump}
\VariableTok{env}\OperatorTok{=}\VariableTok{$(}\FunctionTok{grep}\NormalTok{ krona .snakemake/conda/}\PreprocessorTok{*}\NormalTok{yaml }\KeywordTok{|} \FunctionTok{awk} \StringTok{\textquotesingle{}\{print $1\}\textquotesingle{}} \KeywordTok{|} \FunctionTok{sed} \AttributeTok{{-}e} \StringTok{"s/.yaml://g"} \KeywordTok{|} \FunctionTok{head} \AttributeTok{{-}1}\VariableTok{)}
\ExtensionTok{conda}\NormalTok{ activate }\VariableTok{$env}
\BuiltInTok{cd} \VariableTok{$env}\NormalTok{/opt/krona}
\ExtensionTok{./updateTaxonomy.sh}\NormalTok{ taxonomy}
\BuiltInTok{cd} \AttributeTok{{-}}
\ExtensionTok{conda}\NormalTok{ deactivate}

\CommentTok{\#\# Adjust malt max memory usage}
\VariableTok{env}\OperatorTok{=}\VariableTok{$(}\FunctionTok{grep}\NormalTok{ hops .snakemake/conda/}\PreprocessorTok{*}\NormalTok{yaml }\KeywordTok{|} \FunctionTok{awk} \StringTok{\textquotesingle{}\{print $1\}\textquotesingle{}} \KeywordTok{|} \FunctionTok{sed} \AttributeTok{{-}e} \StringTok{"s/.yaml://g"} \KeywordTok{|} \FunctionTok{head} \AttributeTok{{-}1}\VariableTok{)}
\ExtensionTok{conda}\NormalTok{ activate }\VariableTok{$env}
\VariableTok{version}\OperatorTok{=}\VariableTok{$(}\ExtensionTok{conda}\NormalTok{ list malt }\AttributeTok{{-}{-}json} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ version }\KeywordTok{|} \FunctionTok{sed} \AttributeTok{{-}e} \StringTok{"s/}\DataTypeTok{\textbackslash{}"}\StringTok{//g"} \KeywordTok{|} \FunctionTok{awk} \StringTok{\textquotesingle{}\{print $2\}\textquotesingle{}}\VariableTok{)}
\BuiltInTok{cd} \VariableTok{$env}\NormalTok{/opt/malt{-}}\VariableTok{$version}
\FunctionTok{sed} \AttributeTok{{-}i} \AttributeTok{{-}e} \StringTok{"s/{-}Xmx64G/{-}Xmx3G/"}\NormalTok{ malt{-}build.vmoptions}
\FunctionTok{sed} \AttributeTok{{-}i} \AttributeTok{{-}e} \StringTok{"s/{-}Xmx64G/{-}Xmx3G/"}\NormalTok{ malt{-}run.vmoptions}
\BuiltInTok{cd} \AttributeTok{{-}}
\ExtensionTok{conda}\NormalTok{ deactivate}

\FunctionTok{touch}\NormalTok{ .initdb}

\CommentTok{\#\# Run a quick test}
\ExtensionTok{snakemake} \AttributeTok{{-}{-}use{-}conda} \AttributeTok{{-}{-}show{-}failed{-}logs} \AttributeTok{{-}{-}conda{-}cleanup{-}pkgs}\NormalTok{ cache }\AttributeTok{{-}s}\NormalTok{ ../workflow/Snakefile }\VariableTok{$@} \AttributeTok{{-}{-}conda{-}frontend}\NormalTok{ conda}

\ExtensionTok{snakemake} \AttributeTok{{-}s}\NormalTok{ ../workflow/Snakefile }\AttributeTok{{-}{-}report} \AttributeTok{{-}{-}report{-}stylesheet}\NormalTok{ ../workflow/report/custom.css }\AttributeTok{{-}{-}conda{-}frontend}\NormalTok{ conda}

\CommentTok{\#\# Now we move back into the main repository where we can symlink all the database files back to try running our \textquotesingle{}own\textquotesingle{} test}

\BuiltInTok{cd}\NormalTok{ ../}
\BuiltInTok{cd}\NormalTok{ resources/}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ ../.test/resources/}\PreprocessorTok{*}\NormalTok{ .}
\BuiltInTok{cd}\NormalTok{ ../config}
\FunctionTok{mv}\NormalTok{ config.yaml config.yaml.bkp}
\BuiltInTok{cd}\NormalTok{ ../}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ .test/data/ .}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{ameta-configuration}{%
\subsection{aMeta configuration}\label{ameta-configuration}}

First, we will need to configure the workflow. First, we need to create
a tab-delimited \texttt{samples.tsv} file inside \texttt{aMeta/config/}
and provide the names of the input fastq-files. In a text editor
(e.g.~\texttt{nano}), write the following names paths in TSV format:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{sample}\NormalTok{  fastq}
\ExtensionTok{foo}\NormalTok{ data/bar.fq.gz}
\ExtensionTok{bar}\NormalTok{ data/foo.fq.gz}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

Make sure when copy pasting into your test editor, tabs are not replaced
with spaces, otherwise the file might not be read!

\end{tcolorbox}

Then we need to write a config file. This tells aMeta where to find
things such as database files and other settings.

These go inside a \texttt{config.yaml} file inside
\texttt{aMeta/config/}. A minimal example \texttt{config.yaml} files can
look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{samplesheet:} \StringTok{"config/samples.tsv"}

\ExtensionTok{krakenuniq\_db:}\NormalTok{ resources/KrakenUniq\_DB}

\ExtensionTok{bowtie2\_db:}\NormalTok{ resources/ref.fa}
\ExtensionTok{bowtie2\_seqid2taxid\_db:}\NormalTok{ resources/seqid2taxid.pathogen.map}
\ExtensionTok{pathogenomesFound:}\NormalTok{ resources/pathogenomesFound.tab}

\ExtensionTok{malt\_nt\_fasta:}\NormalTok{ resources/ref.fa}
\ExtensionTok{malt\_seqid2taxid\_db:}\NormalTok{ resources/KrakenUniq\_DB/seqid2taxid.map}
\ExtensionTok{malt\_accession2taxid:}\NormalTok{ resources/accession2taxid.map}

\ExtensionTok{ncbi\_db:}\NormalTok{ resources/ncbi}

\ExtensionTok{n\_unique\_kmers:}\NormalTok{ 1000}
\ExtensionTok{n\_tax\_reads:}\NormalTok{ 200}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

As this is only a dummy run (due to the large-ish computational
resources required for KrakenUniq), we re-use some of the resource files
here. While this will produce nonsense output, it is used here to
demonstrate how you would execute the pipeline.

\end{tcolorbox}

\hypertarget{prepare-and-run-ameta}{%
\subsection{Prepare and run aMeta}\label{prepare-and-run-ameta}}

Make sure you're in the \texttt{aMeta} conda environment,

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate aMeta}
\end{Highlighting}
\end{Shaded}

and in the main aMeta directory with

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/ancient{-}metagenomic{-}pipelines/ameta/aMeta/}
\end{Highlighting}
\end{Shaded}

And, finally, we are ready to run aMeta!

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{snakemake} \AttributeTok{{-}{-}snakefile}\NormalTok{ workflow/Snakefile }\AttributeTok{{-}{-}use{-}conda} \AttributeTok{{-}j}\NormalTok{ 10 }\AttributeTok{{-}{-}conda{-}frontend}\NormalTok{ conda}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

You can modify \texttt{-j} to represent the number of available CPUs you
have on your machine.

\end{tcolorbox}

\hypertarget{ameta-output}{%
\subsection{aMeta output}\label{ameta-output}}

All output files of the workflow are located in \emph{aMeta/results}
directory. To get a quick overview of ancient microbes present in your
samples you should check a heatmap in
\emph{results/overview\_heatmap\_scores.pdf}.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/overview_heatmap_scores.png}

}

\caption{\label{fig-ancientmetagenomicpipelines-ametadiagram}Example
microbiome profiling summary heatmap from aMeta. The columns represent
different samples, and the rows of different species. The cells of the
heatmap are coloured from blue, to yellow, to red, representing aMeta
authentication scores from 0 to 10, with the higher the number the more
confident of the hit being both the correct taxonomic assignment and
that it is ancient. Numbers in the coloured cells also provide the
direct score number.}

\end{figure}

The heatmap demonstrates microbial species (in rows) authenticated for
each sample (in columns)
(Figure~\ref{fig-ancientmetagenomicpipelines-ametadiagram}). The colors
and the numbers in the heatmap represent authentications scores,
i.e.~numeric quantification of seven quality metrics that provide
information about microbial presence and ancient status. The
authentication scores can vary from 0 to 10, the higher is the score the
more likely that a microbe is present in a sample and is ancient.
Typically, scores from 8 to 10 (red color in the heatmap) provide good
confidence of ancient microbial presence in a sample. Scores from 5 to 7
(yellow and orange colors in the heatmap) can imply that either: a) a
microbe is present but not ancient, i.e.~modern contaminant, or b) a
microbe is ancient (the reads are damaged) but was perhaps aligned to a
wrong reference, i.e.~it is not the microbe you think about. The former
is a more common case scenario. The latter often happens when an ancient
microbe is correctly detected on a genus level but we are not confident
about the exact species, and might be aligning the damaged reads to a
non-optimal reference which leads to a lot of mismatches or poor evennes
of coverage. Scores from 0 to 4 (blue color in the heatmap) typically
mean that we have very little statistical evedence (very few reads) to
claim presence of a microbe in a sample.

To visually examine the seven quality metrics

\begin{itemize}
\tightlist
\item
  deamination profile,
\item
  evenness of coverage,
\item
  edit distance (amount of mismatches) for all reads,
\item
  edit distance (amount of mismatches) for damaged reads,
\item
  read length distribution,
\item
  PMD scores distribution,
\item
  number of assigned reads (depth of coverage),
\end{itemize}

Corresponding to the numbers and colors of the heatmap, one can find
them in
\texttt{results/AUTHENTICATION/sampleID/taxID/authentic\_\textless{}Sample\textgreater{}\_\textless{}sampleID\textgreater{}.trimmed.rma6\_\textless{}TaxID\textgreater{}\_\textless{}taxID\textgreater{}.pdf}
for each sample sampleID and each authenticated microbe taxID. An
example of such quality metrics is shown below in
Figure~\ref{fig-ancientmetagenomicpipelines-persampleplot}.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/aMeta_output.png}

}

\caption{\label{fig-ancientmetagenomicpipelines-persampleplot}Example of
sample/hit specific PDF output from aMeta, 9 panels represent different
figures that are useful for evaluating the authenticitiy of an ancient
metagenomic hit. From Left to Right, Top from bottom, the panels consist
of: 1. Edit distance (all reads) bar plot, 2. Edit distance (ancient
reads) bar plot, 3. Breadth of coverage line plot, 4. Deamination line
plot, 5. Read length distribution bar plot, 6. PMD score histogram, 7.
Percent identity bar plot, 8. table of similarity to hits from from
closest hit to least closest, and 10. a general statistics table
including the name of the taxonomic node, number of reads, duplicates,
and mean read length etc.}

\end{figure}

\hypertarget{clean-up-1}{%
\subsection{Clean up}\label{clean-up-1}}

Before continuing onto the next section of this chapter, you will need
to deactivate from the conda environment

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ deactivate }
\end{Highlighting}
\end{Shaded}

\hypertarget{what-is-nf-coremag}{%
\section{What is nf-core/mag?}\label{what-is-nf-coremag}}

nf-core/mag {[}@Krakau2022-we{]} is another Nextflow best-practise
pipeline for the \emph{de novo} assembly, binning, and annotation of
metagenomes (Figure~\ref{fig-ancientmetagenomicpipelines-magworkflow}).
While it was originally designed for modern metagenomes (with a focus on
optional support co-assembly with Nanopore long reads), it has since
been updated to include specific functionality and parameters for
ancient metagenomes too.

\begin{figure}

{\centering \includegraphics{assets/images/chapters/ancient-metagenomic-pipelines/mag_workflow.png}

}

\caption{\label{fig-ancientmetagenomicpipelines-magworkflow}Overview
diagram of the main steps and tools of the nf-core/mag pipeline. Taking
reads in FASTQ format or a samplesheet as input, reads can go through
adapter and/or quality trimming for specific tools for both short and
long reads. These reads can optionally go into taxonomic classification
and visualisation, at the same time as the prepared reads go into
sample-or group wise assemply, evaluation, as well as the ancient DNA
valdiation subworkflow. Resulting contigs can be functionally annotated
at the same time as optionally going through binning and binning
refinement and evaluation (with statistics generation). Bins and refined
bins can be taxonomically classified and annotated, with all final
reports going into MultiQC.}

\end{figure}

nf-core/mag covers the same major steps as you will have run if you
followed the chapter on
\protect\hyperlink{introduction-to-de-novo-genome-assembly}{De Novo
assembly}. Firstly, as with nf-core/eager, nf-core/mag do some initial
sequencing QC and cleanup with \texttt{fastp} or
\texttt{AdapterRemoval}, running FastQC before and after this step to
make sure that adapters and overly short reads have been removed. It can
then optionally do basic taxonomic classification of raw reads with
\texttt{Kraken2} or \texttt{Centrifuge}. The processed reads then ungo
assembly with MEGAHIT, SPAdes, or SPAdeshybrid (the latter being for
combining short and long reads), with contig annotation with
\texttt{Prodigal} to get gene prediction and assembly statistics with
\texttt{QUAST}. A unique feature of nf-core/mag over other \emph{de
novo} assembly pipelines is an aDNA validation and correction step:
contigs under go `reference free' damage profiling using
\texttt{pyDamage}, and then remaining damage bases that are mistakenly
incorporated by assemblies are corrected using \texttt{freebayes} and
\texttt{BCFTools}. Contigs then optionally grouped into genomic `bins'
with a range of metagenomic binners (\texttt{MetaBAT2},
\texttt{MaxBin2}, and \texttt{CONCOCT}), as well as also optional
binning refinement with \texttt{DAS\ Tool}. Bins are then re-annotated
with Prokka, taxonomically classified with either \texttt{CAT} or
\texttt{GTDB-Tk} Evaluation of the resulting bins is carried out with
BUSCO, CheckM, GUNC and/or QUAST to assess the bin completeness. And, as
with all nf-core pipelines, all the results are wrapped up into a
\texttt{MultiQC} report. Detailed documnetation can be see on the
nf-core/mag \href{https://nf-co.re/mag/2.3.2/parameters}{parameters} and
\href{https://nf-co.re/mag/2.3.2/docs/output}{output} pages.

\hypertarget{running-nf-coremag}{%
\subsection{Running nf-core/mag}\label{running-nf-coremag}}

In this tutorial, we will use the same data and aim to replicate the
steps taken in the
\protect\hyperlink{introduction-to-de-novo-genome-assembly}{De Novo
assembly} chapter. It is important to note that generally, \emph{de
novo} assembly, is very computational intensive. In many cases it will
not be able to run a \emph{de novo} assembly on a standard laptop,
therefore some of the parameters used here have been tweaked to get the
pipeline runable on powerful laptop level machines.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Self guided: reference download and preparation}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

This tutorial will re-use the data from the \emph{de novo} assembly
chapter. If you have not-run that practical, or deleted the data, you
can retrieve these with

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#\# IF YOU\textquotesingle{}VE INTO RUN/DELETE DENOVO ASSEMBLY THEN RUN THIS}
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/denovo{-}assembly/}
\FunctionTok{wget}\NormalTok{ https://share.eva.mpg.de/index.php/s/CtLq2R9iqEcAFyg/download/2612\_R1.fastq.gz}
\FunctionTok{wget}\NormalTok{ https://share.eva.mpg.de/index.php/s/mc5JrpDWdL4rC24/download/2612\_R2.fastq.gz}
\FunctionTok{wget}\NormalTok{ https://data.ace.uq.edu.au/public/CheckM\_databases/checkm\_data\_2015\_01\_16.tar.gz}
\end{Highlighting}
\end{Shaded}

Once you have done this, we will need to download the single copy marker
gene reference database for CheckM, which is used to assess completeness
of genome bins.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/ancient{-}metagenomic{-}pipelines/}
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ mag/data}
\BuiltInTok{cd}\NormalTok{ mag/data}

\FunctionTok{mkdir}\NormalTok{ checkm\_data\_2015\_01\_16}
\FunctionTok{tar} \AttributeTok{{-}{-}directory}\NormalTok{ checkm\_data\_2015\_01\_16/ }\AttributeTok{{-}xzvf}\NormalTok{ checkm\_data\_2015\_01\_16.tar.gz}

\BuiltInTok{cd}\NormalTok{ ..}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

The manual steps of the previous chapter included:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Read clean up with \texttt{fastp},
\item
  Assembly with \texttt{MEGAHIT},
\item
  Ancient DNA assessment and correction with \texttt{freebayes} and
  \texttt{pyDamage}
\item
  Binning using \texttt{MetaBAT2} and \texttt{MaxBin2} (originally using
  the \texttt{MetaWRAP} pipeline),
\item
  Bin assessment with \texttt{CheckM}
\item
  Contig taxonomic classification with \texttt{MMSeqs2} via the
  \texttt{GTDB} database
\item
  Genome annotation with \texttt{prokka}
\end{enumerate}

So, what command would we use to recreate these?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# 🛑 Don\textquotesingle{}t run this yet! 🛑}
\ExtensionTok{nextflow}\NormalTok{ run nf{-}core/mag }\AttributeTok{{-}r}\NormalTok{ 2.3.2 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}profile test,docker }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}input }\StringTok{\textquotesingle{}../../denovo{-}assembly/*\{R1,R2\}.fastq.gz\textquotesingle{}} \AttributeTok{{-}{-}outdir}\NormalTok{ ./results }\CommentTok{\#{-}c custom.config \textbackslash{}}
\ExtensionTok{{-}{-}ancient\_dna}  \AttributeTok{{-}{-}binning\_map\_mode}\NormalTok{ own }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}binqc\_tool checkm }\AttributeTok{{-}{-}checkm\_db}\NormalTok{ data/checkm\_data\_2015\_01\_16/ }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}centrifuge\_db false }\AttributeTok{{-}{-}kraken2\_db}\NormalTok{ false }\AttributeTok{{-}{-}skip\_krona} \AttributeTok{{-}{-}skip\_spades} \AttributeTok{{-}{-}skip\_spadeshybrid} \AttributeTok{{-}{-}skip\_maxbin2} \AttributeTok{{-}{-}skip\_concoct} \AttributeTok{{-}{-}skip\_prodigal} \AttributeTok{{-}{-}gtdb}\NormalTok{ false  }\AttributeTok{{-}{-}busco\_reference}\NormalTok{ false }
\end{Highlighting}
\end{Shaded}

In fact, nf-core/mag will do most of the steps for you! In this case, we
mostly just need to \emph{deactivate} most of the additional steps (the
last line). Otherwise, as with eager we have done the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Tell nextflow to run the nf-core/mag pipeline with version 2.3.2
\item
  Specify which computing and software environment to use with
  \texttt{-profile}

  \begin{itemize}
  \tightlist
  \item
    In this case we are running locally so we don't specify a computing
    environment (such as a configuration for an institutional HPC)
  \item
    We use \texttt{docker} as our container engine, which downloads all
    the software and specific versions needed for nf-core/mag in
    immutable `containers', to ensure nothing get broken and is as
    nf-core/mag expects
  \end{itemize}
\item
  Provide the various paths to the input files - the raw FASTQ files and
  the output directory
\item
  Specify we want to run the aDNA mode with mapping of reads back to the
  original contigs rather than for co-assembly

  \begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Binning mapping mode with ancient DNA}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, opacitybacktitle=0.6]

  The aDNA mode of nf-core/mag \emph{only} supports mapping reads back
  to the original contigs. The other mapping modes are when doing
  co-assembly, where you assume there are similar organisms across all
  your metagenomes, and wish to map reads from all samples in a group to
  a single sample's contig (for example). Doing this on aDNA reads would
  risk making false positive damage patterns, as the damaged reads may
  derive from reads from a different sample with damage, that are
  otherwise not present in the sample used for generating the given
  contig.

  \end{tcolorbox}
\item
  Specify we want to generate completeness statistics with
  \texttt{CheckM} (rather than \texttt{BUSCO}), with the associated
  pre-downloaded database

  \begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{nf-core pipelines and reference files}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

  Most nf-core pipelines will actually download reference databases,
  build reference indices for alignment, and in some cases reference
  genomes for you if you do not specify them as pipeline input. These
  download and reference building steps are often very time consuming,
  so it's recommended that once you've let the database download it
  once, you should move the files somewhere safe and you can re-use in
  subsequent pipeline runs.

  \end{tcolorbox}
\item
  Skip a few steps that are either `nice to have' (e.g.~read taxonomic
  classification), or require large computational resources
  (e.g.~\texttt{metaSPAdes}, \texttt{CONCOCT} or \texttt{BUSCO})

  \begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Information on skipped tests}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, opacitybacktitle=0.6]

  The steps we are skipping are: host/arefact removal (\texttt{Bowtie2}
  removal of phiX), taxonomic classification of reads
  (\texttt{centrifuge}, \texttt{kraken2}, \texttt{krona}), the
  additional assemblers (\texttt{metaSPAdes} and \texttt{SPAdeshybrid},
  as these require very large computational resources), additional
  binners (\texttt{MaxBin2} and \texttt{CONCOCT}, while they are used in
  the \emph{de novo} assembly chapter, they take a long time to run),
  ship raw contig annotation (with \texttt{Prodigal}, as we do this at
  the bin level), and contig taxonomic classification (with
  \texttt{GTDB} and \texttt{BUSCO} as they require very large
  databases).

  \end{tcolorbox}
\end{enumerate}

With this we are almost ready for running the pipeline!

\hypertarget{configuring-nextflow-pipelines}{%
\subsection{Configuring Nextflow
pipelines}\label{configuring-nextflow-pipelines}}

Before we execute the command however, we need to again consider about
the resource requirements. As described above, particularly \emph{de
novo} assembly (but also many other bioinformatic analyses) require a
lot of computational resources, depending on the type of data and
analyses you wish to run.

For most pipelines you must tweak the resource requirements to ensure a)
they will fit on your cluster, and b) will run optimally so you don't
under- or over- use your computational resources, where latter will
either make yourself (takes too long), or your cluser/server system
administrators (blocks other users) unhappy!

While nf-core pipelines all have `reasonable default' settings for
memory, CPU, and time limits, they will not work in all contexts. Here
we will give a brief example of how to tweak the parameters of the
pipeline steps so that they can run on our compute node or laptop.

For Nextflow, we must make a special `config' that defines the names of
each step of the pipeline you wish to tweak and the corresponding
resources.

For this our nf-core/mag run you will need to open your text editor an
empty file called \texttt{custom.config}, and copy and paste the
contents of the next code block.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{process }\OperatorTok{\{}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ FASTP }\OperatorTok{\{} 
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{8}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ BOWTIE2\_PHIX\_REMOVAL\_ALIGN }\OperatorTok{\{} 
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{8}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ BOWTIE2\_ASSEMBLY\_ALIGN }\OperatorTok{\{}    
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{8}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ PYDAMAGE\_ANALYZE }\OperatorTok{\{}      
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{8}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ PROKKA }\OperatorTok{\{}
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{4}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ MEGAHIT }\OperatorTok{\{}
\NormalTok{    cpus }\OperatorTok{=} \DecValTok{8}
  \OperatorTok{\}}

\NormalTok{  withName}\OperatorTok{:}\NormalTok{ CHECKM\_LINEAGEWF}\OperatorTok{\{}
\NormalTok{    memory }\OperatorTok{=} \FloatTok{24.G}\NormalTok{B}
\NormalTok{    cpus   }\OperatorTok{=} \DecValTok{8}
\NormalTok{    ext}\OperatorTok{.}\NormalTok{args }\OperatorTok{=} \StringTok{"{-}{-}reduced\_tree"}
  \OperatorTok{\}}

\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Here we set the number of CPUs for most tools to a maximum of
\texttt{8}, and then we limit the amount of memory for CheckM to 24 GB
(down from the
\href{https://github.com/nf-core/mag/blob/66cf53aff834d2a254b78b94fc54cd656b8b7b57/conf/base.config\#L37-L41}{default}
of 36 GB - which exceeds most laptop memory). We also give a specifc
\texttt{CheckM} flag command to use a smaller database.

We can save this \texttt{custom.config} file, and supply this to the
Nextflow command with the nextflow parameter (\texttt{-c}). And with
that, you can run your nf-core/mag pipeline!

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{nextflow}\NormalTok{ run nf{-}core/mag }\AttributeTok{{-}r}\NormalTok{ 2.3.2 }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}profile test,docker }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}input }\StringTok{\textquotesingle{}../../denovo{-}assembly/*\{R1,R2\}.fastq.gz\textquotesingle{}} \AttributeTok{{-}{-}outdir}\NormalTok{ ./results}
\ExtensionTok{{-}{-}ancient\_dna}  \AttributeTok{{-}{-}binning\_map\_mode}\NormalTok{ own }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}binqc\_tool checkm }\AttributeTok{{-}{-}checkm\_db}\NormalTok{ data/checkm\_data\_2015\_01\_16/ }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}{-}centrifuge\_db false }\AttributeTok{{-}{-}kraken2\_db}\NormalTok{ false }\AttributeTok{{-}{-}skip\_krona} \AttributeTok{{-}{-}skip\_spades} \AttributeTok{{-}{-}skip\_spadeshybrid} \AttributeTok{{-}{-}skip\_maxbin2} \AttributeTok{{-}{-}skip\_concoct} \AttributeTok{{-}{-}skip\_prodigal} \AttributeTok{{-}{-}gtdb}\NormalTok{ false  }\AttributeTok{{-}{-}busco\_reference}\NormalTok{ false }\DataTypeTok{\textbackslash{}}
\NormalTok{{-}c custom.config}
\end{Highlighting}
\end{Shaded}

The good thing about the config file, is you can re-use across runs on
the same machine! So set once, and never think about it again.

The pipeline run above should take around 20 minutes or so to complete.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, toprule=.15mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, titlerule=0mm, toptitle=1mm, left=2mm, colback=white, breakable, bottomtitle=1mm, arc=.35mm, rightrule=.15mm, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, opacitybacktitle=0.6]

Configuration files don't need be personal nor custom!

You can also generate a HPC / server specific profile which can be used
on all nf-core pipelines (and even your own!).

See \href{https://nf-co.re/configs}{nf-co.re/configs} for all existing
institutional configs

\end{tcolorbox}

\hypertarget{nf-coremag-output}{%
\subsection{nf-core/mag output}\label{nf-coremag-output}}

As with nf-core/eager, you will have a results directory containing many
folders representing different stages of the pipeline, and a MultiQC
report.

\hypertarget{results-files-1}{%
\subsubsection{Results Files}\label{results-files-1}}

Relevant directories for evaluting your assemblies are as follows:

\begin{itemize}
\tightlist
\item
  \texttt{QC\_shortreads/}

  \begin{itemize}
  \tightlist
  \item
    Contains various folders from the sequencing QC and raw read
    processing (e.g., FastQC, fastp/AdapterRemoval, host-removal results
    etc)
  \end{itemize}
\item
  \texttt{Taxonomy/}

  \begin{itemize}
  \tightlist
  \item
    If you have not already performed taxonomic classification of your
    reads, the output of the Kraken2 or Centrifuge integrated steps of
    nf-core/mag will be deposited here, including an optional Krona
    piechart
  \item
    If activated, the contig-level taxonomic assignments will be
    deposited here (e.g.~from CAT or GTDB)
  \end{itemize}
\item
  \texttt{Assembly/}

  \begin{itemize}
  \tightlist
  \item
    This directory will contain one directory per chosen assembler which
    will contain per-sample output files for each assembly.
  \item
    Within each of the assembler sub directories, a QC folder will
    contain the QUAST output
  \end{itemize}
\item
  \texttt{Ancient\_DNA}

  \begin{itemize}
  \tightlist
  \item
    This folder will contain the output of pyDamge contig authentication
    to allow you to select or filter for just putatively ancient contigs
  \end{itemize}
\item
  \texttt{variant\_calling}

  \begin{itemize}
  \tightlist
  \item
    This directory will contain the `damage-corrected' contigs that will
    be used in downstream steps of the pipeline, if the \texttt{aDNA}
    mode is turned on
  \end{itemize}
\item
  \texttt{GenomeBinning/}

  \begin{itemize}
  \tightlist
  \item
    This directory will contain all the output from each of any of the
    selected contig binning tools. As well as the binned FASTAs, you
    also have read-depth statistics from when input reads are re-mapped
    back to the contigs
  \item
    The directory will also contain the output from the \texttt{DASTool}
    binning refinement if turned on
  \item
    In the \texttt{QC} sub-directory of this folder, you will find the
    QUAST results on the genome \emph{bins} (rather than raw contigs as
    above), as well as BUSCO/CheckM/GUNC MAG completeness estimates
  \end{itemize}
\item
  \texttt{Prodigal} / \texttt{Prokka}

  \begin{itemize}
  \tightlist
  \item
    These directories will contain the genome annotation output of the
    raw contigs (\texttt{Prodigal}) or from bins (\texttt{Prokka})
  \end{itemize}
\end{itemize}

\hypertarget{report-table}{%
\subsubsection{Report Table}\label{report-table}}

As of v2.3.* the MultiQC report is unfortunately broken, however this
has been fixed for 2.4, and many of the questions asked in the
preprocessing section of the
\protect\hyperlink{nf-coreeager-output}{nf-core/eager} results
interpretation will also apply here.

Other than the QUAST results in the \texttt{Assembly/*/QC/} or
\texttt{GenomeBinning/*/QC/} directories, the main table used for
evaluating the output files is the \texttt{bin\_summary.tsv} table in
the \texttt{GenomeBinning/} directory.

In this file you woud typically want to assess the following columns:

\begin{itemize}
\tightlist
\item
  \texttt{\%\ Complete\ *} columns - with the higher the number of
  expected domain-specific genes, normally representing a better quality
  of the resulting bin.
\item
  \texttt{\#\ contigs\ (\textgreater{}=*\ bp)} columns - which represnts
  the number of contigs at different lengths, the fewer the shorter
  reads and the greater the longer reads you have again suggests a
  better assembly.
\item
  This can also be evaluated by the N50 or L75 columns (as described in
  the
  \protect\hyperlink{introduction-to-de-novo-genome-assembly}{\emph{De
  novo} assembly chapter}).
\item
  \texttt{fastani} and \texttt{closest\_placement\_taxonomy} - these can
  tell you if your particular bin has a genome very similar to existing
  species in taxonomic databases
\item
  \texttt{warnings} - for specific comments on each assignment
\end{itemize}

\hypertarget{clean-up-2}{%
\subsection{Clean up}\label{clean-up-2}}

Before continuing onto the next section of this chapter, you will need
to deactivate from the conda environment

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ deactivate }
\end{Highlighting}
\end{Shaded}

You may also need to delete the contents of the mag directory

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ /}\OperatorTok{\textless{}}\NormalTok{path}\OperatorTok{\textgreater{}}\NormalTok{/}\OperatorTok{\textless{}}\NormalTok{to}\OperatorTok{\textgreater{}}\NormalTok{/ancient{-}metagenomic{-}pipelines/mag}
\FunctionTok{rm} \AttributeTok{{-}r} \PreprocessorTok{*}
\end{Highlighting}
\end{Shaded}

\hypertarget{questions-to-think-about-6}{%
\section{Questions to think about}\label{questions-to-think-about-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why is it important to use a pipeline for genomic analysis of ancient
  data?
\item
  How can the design of pipelines such as nf-core/eager pipeline help
  researchers comply with the FAIR principles for management of
  scientific data?
\item
  What metrics do you use to evaluate the success/failure of ancient DNA
  sequencing experiments? How can these measures be evaluated when using
  nf-core/eager for data preprocessing and analysis?
\end{enumerate}

\hypertarget{references-4}{%
\section{References}\label{references-4}}

\bookmarksetup{startatroot}

\hypertarget{summary-3}{%
\chapter*{Summary}\label{summary-3}}
\addcontentsline{toc}{chapter}{Summary}

\markboth{Summary}{Summary}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\part{Appendices}

\hypertarget{resources-3}{%
\chapter{Resources}\label{resources-3}}

\hypertarget{introduction-to-ngs-sequencing-2}{%
\section{Introduction to NGS
Sequencing}\label{introduction-to-ngs-sequencing-2}}

\begin{itemize}
\tightlist
\item
  \url{https://www.youtube.com/watch?v=fCd6B5HRaZ8}
\item
  \url{https://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf}
\end{itemize}

\hypertarget{tools}{%
\chapter{Tools}\label{tools}}

This page all the software with links used and referred to in the
practical chapters of the book.

\hypertarget{introduction-to-r-and-the-tidyverse-1}{%
\section{Introduction to R and the
Tidyverse}\label{introduction-to-r-and-the-tidyverse-1}}

\begin{itemize}
\tightlist
\item
  \href{https://www.r-project.org/}{r}
\item
  \href{https://www.rstudio.com/products/rstudio/}{r studio (desktop)}
\item
  \href{https://www.tidyverse.org/}{tidyverse}
\end{itemize}

\hypertarget{introduction-to-python-and-pandas-1}{%
\section{Introduction to Python and
Pandas}\label{introduction-to-python-and-pandas-1}}

\begin{itemize}
\tightlist
\item
  \href{https://www.python.org/}{python}
\item
  \href{https://jupyter.org/}{jupyter}
\end{itemize}

\hypertarget{introduction-to-github-1}{%
\section{Introduction to Git(Hub)}\label{introduction-to-github-1}}

\begin{itemize}
\tightlist
\item
  \href{https://git-scm.com/}{git} (normally installed by default on all
  UNIX based operating systems e.g.~Linux, OSX)
\end{itemize}

\hypertarget{functional-profiling-2}{%
\section{Functional Profiling}\label{functional-profiling-2}}

\begin{itemize}
\tightlist
\item
  \href{https://www.r-project.org/}{r}
\item
  \href{https://www.rstudio.com/products/rstudio/}{r studio (desktop)}
\item
  \href{https://www.tidyverse.org/}{tidyverse}
\item
  \href{https://huttenhower.sph.harvard.edu/humann/}{humann3}
\end{itemize}

\hypertarget{de-novo-assembly-2}{%
\section{\texorpdfstring{\emph{De novo}
assembly}{De novo assembly}}\label{de-novo-assembly-2}}

\begin{itemize}
\tightlist
\item
  \href{https://github.com/OpenGene/fastp}{fastp}
\item
  \href{https://github.com/voutcn/megahit}{megahit}
\item
  \href{http://bowtie-bio.sourceforge.net/bowtie2/index.shtml}{bowtie2}
\item
  \href{http://www.htslib.org/}{samtools}
\item
  \href{https://github.com/lh3/bioawk}{bioawk}
\item
  \href{https://github.com/bbuchfink/diamond}{diamond}
\item
  \href{https://bitbucket.org/berkeleylab/metabat/src/master/}{metabat2}
\item
  \href{https://sourceforge.net/projects/maxbin/\#:~:text=MaxBin\%20is\%20a\%20software\%20for,coverage\%20information\%20or\%20sequencing\%20reads.}{maxbin2}
\item
  \href{https://concoct.readthedocs.io/en/latest/}{concoct}
\item
  \href{https://github.com/bxlab/metaWRAP}{metawrap}
\item
  \href{https://ecogenomics.github.io/CheckM/\#:~:text=CheckM\%20provides\%20a\%20set\%20of,copy\%20within\%20a\%20phylogenetic\%20lineage.}{checkm-genome}
\item
  \href{https://grp-bork.embl-community.io/gunc/}{gunc}
\item
  \href{https://pydamage.readthedocs.io/en/0.7/}{pydamage}
\item
  \href{https://github.com/tseemann/prokka}{prokka}
\end{itemize}

\hypertarget{genome-mapping-2}{%
\section{Genome Mapping}\label{genome-mapping-2}}

\begin{itemize}
\tightlist
\item
  \href{http://bio-bwa.sourceforge.net/}{bwa}
\item
  \href{https://igv.org/}{igv}
\item
  \href{https://gatk.broadinstitute.org/hc/en-us}{gatk}
\end{itemize}

\hypertarget{phylogenomics-1}{%
\section{Phylogenomics}\label{phylogenomics-1}}

\begin{itemize}
\tightlist
\item
  \href{https://www.beast2.org/}{beast2}\_
\item
  \href{http://tree.bio.ed.ac.uk/software/tracer/}{tracer}
\item
  \href{https://beast.community/tempest}{tempest}
\item
  \href{https://www.megasoftware.net/}{mega}
\end{itemize}

\hypertarget{ancient-metagenomic-pipelines-2}{%
\section{Ancient Metagenomic
Pipelines}\label{ancient-metagenomic-pipelines-2}}

\begin{itemize}
\tightlist
\item
  \href{https://www.nextflow.io/}{nextflow}
\item
  \href{https://nf-co.re/tools}{nf-core tools}
\item
  \href{https://nf-co.re/eager}{nf-core/eager}
\end{itemize}


\backmatter

\printindex

\end{document}
