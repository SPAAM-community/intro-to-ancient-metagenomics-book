@article{Schuster2008-qx,
  title    = {Next-generation sequencing transforms today's biology},
  author   = {Schuster, Stephan C},
  journal  = {Nature methods},
  volume   = 5,
  number   = 1,
  pages    = {16--18},
  abstract = {A new generation of non-Sanger-based sequencing technologies has
              delivered on its promise of sequencing DNA at unprecedented speed,
              thereby enabling impressive scientific achievements and novel
              biological applications. However, before stepping into the
              limelight, next-generation sequencing had to overcome the inertia
              of a field that relied on Sanger-sequencing for 30 years.},
  month    = jan,
  year     = 2008,
  url      = {http://dx.doi.org/10.1038/nmeth1156},
  doi      = {10.1038/nmeth1156},
  issn     = {1548-7105,1548-7091},
  language = {en},
  pmid     = 18165802
}

@article{Shendure2008-fh,
  title    = {Next-generation {DNA} sequencing},
  author   = {Shendure, Jay and Ji, Hanlee},
  journal  = {Nature biotechnology},
  volume   = 26,
  number   = 10,
  pages    = {1135--1145},
  abstract = {DNA sequence represents a single format onto which a broad range
              of biological phenomena can be projected for high-throughput data
              collection. Over the past three years, massively parallel DNA
              sequencing platforms have become widely available, reducing the
              cost of DNA sequencing by over two orders of magnitude, and
              democratizing the field by putting the sequencing capacity of a
              major genome center in the hands of individual investigators.
              These new technologies are rapidly evolving, and near-term
              challenges include the development of robust protocols for
              generating sequencing libraries, building effective new approaches
              to data-analysis, and often a rethinking of experimental design.
              Next-generation DNA sequencing has the potential to dramatically
              accelerate biological and biomedical research, by enabling the
              comprehensive analysis of genomes, transcriptomes and interactomes
              to become inexpensive, routine and widespread, rather than
              requiring significant production-scale efforts.},
  month    = oct,
  year     = 2008,
  url      = {http://dx.doi.org/10.1038/nbt1486},
  doi      = {10.1038/nbt1486},
  issn     = {1546-1696,1087-0156},
  language = {en},
  pmid     = 18846087
}

@article{Slatko2018-hg,
  title    = {Overview of Next-Generation Sequencing Technologies},
  author   = {Slatko, Barton E and Gardner, Andrew F and Ausubel, Frederick M},
  journal  = {Current protocols in molecular biology / edited by Frederick M.
              Ausubel ... [et al.]},
  volume   = 122,
  number   = 1,
  pages    = {e59},
  abstract = {High throughput DNA sequencing methodology (next generation
              sequencing; NGS) has rapidly evolved over the past 15 years and
              new methods are continually being commercialized. As the
              technology develops, so do increases in the number of
              corresponding applications for basic and applied science. The
              purpose of this review is to provide a compendium of NGS
              methodologies and associated applications. Each brief discussion
              is followed by web links to the manufacturer and/or web-based
              visualizations. Keyword searches, such as with Google, may also
              provide helpful internet links and information. © 2018 by John
              Wiley \& Sons, Inc.},
  month    = apr,
  year     = 2018,
  url      = {http://dx.doi.org/10.1002/cpmb.59},
  keywords = {NGS; Sanger sequencing; next-generation sequencing},
  doi      = {10.1002/cpmb.59},
  issn     = {1934-3647,1934-3639},
  pmc      = {PMC6020069},
  language = {en},
  pmid     = 29851291
}

@article{Van_Dijk2014-ep,
  title    = {Ten years of next-generation sequencing technology},
  author   = {van Dijk, Erwin L and Auger, Hélène and Jaszczyszyn, Yan and
              Thermes, Claude},
  journal  = {Trends in genetics},
  volume   = 30,
  number   = 9,
  pages    = {418--426},
  abstract = {Ten years ago next-generation sequencing (NGS) technologies
              appeared on the market. During the past decade, tremendous
              progress has been made in terms of speed, read length, and
              throughput, along with a sharp reduction in per-base cost.
              Together, these advances democratized NGS and paved the way for
              the development of a large number of novel NGS applications in
              basic science as well as in translational research areas such as
              clinical diagnostics, agrigenomics, and forensic science. Here we
              provide an overview of the evolution of NGS and discuss the most
              significant improvements in sequencing technologies and library
              preparation protocols. We also explore the current landscape of
              NGS applications and provide a perspective for future
              developments.},
  month    = sep,
  year     = 2014,
  url      = {http://dx.doi.org/10.1016/j.tig.2014.07.001},
  keywords = {ChIP-seq; DNA-seq; NGS library preparation; Next-generation
              sequencing (NGS); RNA-seq; genomics},
  doi      = {10.1016/j.tig.2014.07.001},
  issn     = {0168-9525},
  language = {en},
  pmid     = 25108476
}

@article{Kircher2012-fg,
  title    = {Double indexing overcomes inaccuracies in multiplex sequencing on
              the Illumina platform},
  author   = {Kircher, Martin and Sawyer, Susanna and Meyer, Matthias},
  journal  = {Nucleic acids research},
  volume   = 40,
  number   = 1,
  pages    = {e3},
  abstract = {Due to the increasing throughput of current DNA sequencing
              instruments, sample multiplexing is necessary for making
              economical use of available sequencing capacities. A widely used
              multiplexing strategy for the Illumina Genome Analyzer utilizes
              sample-specific indexes, which are embedded in one of the library
              adapters. However, this and similar multiplex approaches come with
              a risk of sample misidentification. By introducing indexes into
              both library adapters (double indexing), we have developed a
              method that reveals the rate of sample misidentification within
              current multiplex sequencing experiments. With ~0.3\% these rates
              are orders of magnitude higher than expected and may severely
              confound applications in cancer genomics and other fields
              requiring accurate detection of rare variants. We identified the
              occurrence of mixed clusters on the flow as the predominant source
              of error. The accuracy of sample identification is further
              impaired if indexed oligonucleotides are cross-contaminated or if
              indexed libraries are amplified in bulk. Double-indexing
              eliminates these problems and increases both the scope and
              accuracy of multiplex sequencing on the Illumina platform.},
  month    = jan,
  year     = 2012,
  url      = {http://dx.doi.org/10.1093/nar/gkr771},
  doi      = {10.1093/nar/gkr771},
  issn     = {1362-4962,0305-1048},
  pmc      = {PMC3245947},
  pmid     = 22021376
}

@article{Meyer2010-qc,
  title    = {Illumina sequencing library preparation for highly multiplexed
              target capture and sequencing},
  author   = {Meyer, Matthias and Kircher, Martin},
  journal  = {Cold Spring Harbor protocols},
  volume   = 2010,
  number   = 6,
  pages    = {db.prot5448},
  abstract = {The large amount of DNA sequence data generated by high-throughput
              sequencing technologies often allows multiple samples to be
              sequenced in parallel on a single sequencing run. This is
              particularly true if subsets of the genome are studied rather than
              complete genomes. In recent years, target capture from sequencing
              libraries has largely replaced polymerase chain reaction (PCR) as
              the preferred method of target enrichment. Parallelizing target
              capture and sequencing for multiple samples requires the
              incorporation of sample-specific barcodes into sequencing
              libraries, which is necessary to trace back the sample source of
              each sequence. This protocol describes a fast and reliable method
              for the preparation of barcoded (``indexed'') sequencing libraries
              for Illumina's Genome Analyzer platform. The protocol avoids
              expensive commercial library preparation kits and can be performed
              in a 96-well plate setup using multi-channel pipettes, requiring
              not more than two or three days of lab work. Libraries can be
              prepared from any type of double-stranded DNA, even if present in
              subnanogram quantity.},
  month    = jun,
  year     = 2010,
  url      = {http://dx.doi.org/10.1101/pdb.prot5448},
  doi      = {10.1101/pdb.prot5448},
  issn     = {1559-6095,1940-3402},
  pmid     = 20516186
}

@article{Ma2019-lg,
  title    = {Analysis of error profiles in deep next-generation sequencing data},
  author   = {Ma, Xiaotu and Shao, Ying and Tian, Liqing and Flasch, Diane A and
              Mulder, Heather L and Edmonson, Michael N and Liu, Yu and Chen,
              Xiang and Newman, Scott and Nakitandwe, Joy and Li, Yongjin and
              Li, Benshang and Shen, Shuhong and Wang, Zhaoming and Shurtleff,
              Sheila and Robison, Leslie L and Levy, Shawn and Easton, John and
              Zhang, Jinghui},
  journal  = {Genome biology},
  volume   = 20,
  number   = 1,
  pages    = 50,
  abstract = {BACKGROUND: Sequencing errors are key confounding factors for
              detecting low-frequency genetic variants that are important for
              cancer molecular diagnosis, treatment, and surveillance using deep
              next-generation sequencing (NGS). However, there is a lack of
              comprehensive understanding of errors introduced at various steps
              of a conventional NGS workflow, such as sample handling, library
              preparation, PCR enrichment, and sequencing. In this study, we use
              current NGS technology to systematically investigate these
              questions. RESULTS: By evaluating read-specific error
              distributions, we discover that the substitution error rate can be
              computationally suppressed to 10-5 to 10-4, which is 10- to
              100-fold lower than generally considered achievable (10-3) in the
              current literature. We then quantify substitution errors
              attributable to sample handling, library preparation, enrichment
              PCR, and sequencing by using multiple deep sequencing datasets. We
              find that error rates differ by nucleotide substitution types,
              ranging from 10-5 for A>C/T>G, C>A/G>T, and C>G/G>C changes to
              10-4 for A>G/T>C changes. Furthermore, C>T/G>A errors exhibit
              strong sequence context dependency, sample-specific effects
              dominate elevated C>A/G>T errors, and target-enrichment PCR led to
              ~ 6-fold increase of overall error rate. We also find that more
              than 70\% of hotspot variants can be detected at 0.1 ~ 0.01\%
              frequency with the current NGS technology by applying in silico
              error suppression. CONCLUSIONS: We present the first comprehensive
              analysis of sequencing error sources in conventional NGS
              workflows. The error profiles revealed by our study highlight new
              directions for further improving NGS analysis accuracy both
              experimentally and computationally, ultimately enhancing the
              precision of deep sequencing.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1186/s13059-019-1659-6},
  keywords = {Deep sequencing; Detection; Error rate; Hotspot mutation;
              Subclonal; Substitution},
  doi      = {10.1186/s13059-019-1659-6},
  issn     = {1465-6906},
  pmc      = {PMC6417284},
  language = {en},
  pmid     = 30867008
}
@unpublished{Sinha2017-zo,
  title    = {Index switching causes “spreading-of-signal” among multiplexed
              samples in Illumina {HiSeq} 4000 {DNA} sequencing},
  author   = {Sinha, Rahul and Stanley, Geoff and Gulati, Gunsagar Singh and
              Ezran, Camille and Travaglini, Kyle Joseph and Wei, Eric and Chan,
              Charles Kwok Fai and Nabhan, Ahmad N and Su, Tianying and
              Morganti, Rachel Marie and Conley, Stephanie Diana and Chaib,
              Hassan and Red-Horse, Kristy and Longaker, Michael T and Snyder,
              Michael P and Krasnow, Mark A and Weissman, Irving L},
  journal  = {bioRxiv},
  pages    = 125724,
  abstract = {Illumina-based next generation sequencing (NGS) has accelerated
              biomedical discovery through its ability to generate thousands of
              gigabases of sequencing output per run at a fraction of the time
              and cost of conventional technologies. The process typically
              involves four basic steps: library preparation, cluster
              generation, sequencing, and data analysis. In 2015, a new
              chemistry of cluster generation was introduced in the newer
              Illumina machines (HiSeq 3000/4000/X Ten) called exclusion
              amplification (ExAmp), which was a fundamental shift from the
              earlier method of random cluster generation by bridge
              amplification on a non-patterned flow cell. The ExAmp chemistry,
              in conjunction with patterned flow cells containing nanowells at
              fixed locations, increases cluster density on the flow cell,
              thereby reducing the cost per run. It also increases sequence read
              quality, especially for longer read lengths (up to 150 base
              pairs). This advance has been widely adopted for genome sequencing
              because greater sequencing depth can be achieved for lower cost
              without compromising the quality of longer reads. We show that
              this promising chemistry is problematic, however, when
              multiplexing samples. We discovered that up to 5-10\% of
              sequencing reads (or signals) are incorrectly assigned from a
              given sample to other samples in a multiplexed pool. We provide
              evidence that this “spreading-of-signals” arises from low levels
              of free index primers present in the pool. These index primers can
              prime pooled library fragments at random via complementary 3′
              ends, and get extended by DNA polymerase, creating a new library
              molecule with a new index before binding to the patterned flow
              cell to generate a cluster for sequencing. This causes the
              resulting read from that cluster to be assigned to a different
              sample, causing the spread of signals within multiplexed samples.
              We show that low levels of free index primers persist after the
              most common library purification procedure recommended by
              Illumina, and that the amount of signal spreading among samples is
              proportional to the level of free index primer present in the
              library pool. This artifact causes homogenization and
              misclassification of cells in single cell RNA-seq experiments.
              Therefore, all data generated in this way must now be carefully
              re-examined to ensure that “spreading-of-signals” has not
              compromised data analysis and conclusions. Re-sequencing samples
              using an older technology that uses conventional bridge
              amplification for cluster generation, or improved library cleanup
              strategies to remove free index primers, can minimize or eliminate
              this signal spreading artifact.},
  month    = apr,
  year     = 2017,
  url      = {http://dx.doi.org/10.1101/125724},
  doi      = {10.1101/125724},
  language = {en}
}

@article{Van_der_Valk2019-to,
  title    = {Index hopping on the Illumina {HiseqX} platform and its
              consequences for ancient {DNA} studies},
  author   = {van der Valk, Tom and Vezzi, Francesco and Ormestad, Mattias and
              Dalén, Love and Guschanski, Katerina},
  journal  = {Molecular ecology resources},
  abstract = {The high-throughput capacities of the Illumina sequencing
              platforms and the possibility to label samples individually have
              encouraged wide use of sample multiplexing. However, this practice
              results in read misassignment (usually <1\%) across samples
              sequenced on the same lane. Alarmingly high rates of read
              misassignment of up to 10\% were reported for lllumina sequencing
              machines with exclusion amplification chemistry. This may make use
              of these platforms prohibitive, particularly in studies that rely
              on low-quantity and low-quality samples, such as historical and
              archaeological specimens. Here, we use barcodes, short sequences
              that are ligated to both ends of the DNA insert, to directly
              quantify the rate of index hopping in 100-year old
              museum-preserved gorilla (Gorilla beringei) samples. Correcting
              for multiple sources of noise, we identify on average 0.470\% of
              reads containing a hopped index. We show that sample-specific
              quantity of misassigned reads depends on the number of reads that
              any given sample contributes to the total sequencing pool, so that
              samples with few sequenced reads receive the greatest proportion
              of misassigned reads. This particularly affects ancient DNA
              samples, as these frequently differ in their DNA quantity and
              endogenous content. Through simulations we show that even low
              rates of index hopping, as reported here, can lead to biases in
              ancient DNA studies when multiplexing samples with vastly
              different quantities of endogenous material.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1111/1755-0998.13009},
  keywords = {ancient DNA; index switching; multiplexing; museum specimens;
              next-generation sequencing; read misassignment},
  doi      = {10.1111/1755-0998.13009},
  issn     = {1755-0998,1755-098X},
  language = {en},
  pmid     = 30848092
}

@article{Ju2006-cl,
  title    = {Four-color {DNA} sequencing by synthesis using cleavable
              fluorescent nucleotide reversible terminators},
  author   = {Ju, Jingyue and Kim, Dae Hyun and Bi, Lanrong and Meng, Qinglin
              and Bai, Xiaopeng and Li, Zengmin and Li, Xiaoxu and Marma, Mong
              Sano and Shi, Shundi and Wu, Jian and Edwards, John R and Romu,
              Aireen and Turro, Nicholas J},
  journal  = {Proceedings of the National Academy of Sciences of the United
              States of America},
  volume   = 103,
  number   = 52,
  pages    = {19635--19640},
  abstract = {DNA sequencing by synthesis (SBS) on a solid surface during
              polymerase reaction offers a paradigm to decipher DNA sequences.
              We report here the construction of such a DNA sequencing system
              using molecular engineering approaches. In this approach, four
              nucleotides (A, C, G, T) are modified as reversible terminators by
              attaching a cleavable fluorophore to the base and capping the
              3'-OH group with a small chemically reversible moiety so that they
              are still recognized by DNA polymerase as substrates. We found
              that an allyl moiety can be used successfully as a linker to
              tether a fluorophore to 3'-O-allyl-modified nucleotides, forming
              chemically cleavable fluorescent nucleotide reversible
              terminators, 3'-O-allyl-dNTPs-allyl-fluorophore, for application
              in SBS. The fluorophore and the 3'-O-allyl group on a DNA
              extension product, which is generated by incorporating
              3'-O-allyl-dNTPs-allyl-fluorophore in a polymerase reaction, are
              removed simultaneously in 30 s by Pd-catalyzed deallylation in
              aqueous buffer solution. This one-step dual-deallylation reaction
              thus allows the reinitiation of the polymerase reaction and
              increases the SBS efficiency. DNA templates consisting of
              homopolymer regions were accurately sequenced by using this class
              of fluorescent nucleotide analogues on a DNA chip and a four-color
              fluorescent scanner.},
  month    = dec,
  year     = 2006,
  url      = {http://dx.doi.org/10.1073/pnas.0609513103},
  doi      = {10.1073/pnas.0609513103},
  pmc      = {PMC1702316},
  pmid     = 17170132,
  issn     = {0027-8424},
  language = {en}
}

@article{Heather2015-zo,
  title    = {The Sequence of Sequencers: The History of Sequencing {DNA}},
  author   = {Heather, James M and Chain, Benjamin},
  journal  = {Genomics},
  abstract = {Determining the order of nucleic acid residues in biological
              samples is an integral component of a wide variety of research
              applications. Over the last fifty years large numbers of
              researchers have applied themselves to the production of
              techniques and technologies to facilitate this feat, sequencing
              DNA and RNA molecules. This time-scale has witnessed tremendous
              changes, moving from sequencing short oligonucleotides to millions
              of bases, from struggling towards the deduction of the coding
              sequence of a single gene to rapid and widely available whole
              genome sequencing. This article traverses those years, iterating
              through the different generations of sequencing technology,
              highlighting some of the key discoveries, researchers, and
              sequences along the way.},
  month    = nov,
  year     = 2015,
  url      = {http://dx.doi.org/10.1016/j.ygeno.2015.11.003},
  keywords = {DNA; History; RNA; Sequencer; Sequencing},
  doi      = {10.1016/j.ygeno.2015.11.003},
  pmid     = 26554401,
  issn     = {0888-7543,1089-8646}
}

@article{Sanger1977-zw,
  title     = {{DNA} sequencing with chain-terminating inhibitors},
  author    = {Sanger, F and Nicklen, S and Coulson, A R},
  journal   = {Proceedings of the National Academy of Sciences of the United
               States of America},
  publisher = {Proceedings of the National Academy of Sciences},
  volume    = 74,
  number    = 12,
  pages     = {5463--5467},
  abstract  = {A new method for determining nucleotide sequences in DNA is
               described. It is similar to the ``plus and minus'' method
               [Sanger, F. \& Coulson, A. R. (1975) J. Mol. Biol. 94, 441-448]
               but makes use of the 2',3'-dideoxy and arabinonucleoside
               analogues of the normal deoxynucleoside triphosphates, which act
               as specific chain-terminating inhibitors of DNA polymerase. The
               technique has been applied to the DNA of bacteriophage varphiX174
               and is more rapid and more accurate than either the plus or the
               minus method.},
  month     = dec,
  year      = 1977,
  url       = {https://www.pnas.org/doi/abs/10.1073/pnas.74.12.5463},
  doi       = {10.1073/pnas.74.12.5463},
  pmc       = {PMC431765},
  pmid      = 271968,
  issn      = {0027-8424,1091-6490},
  language  = {en}
}
@article{Bentley2008-oj,
  title     = {Accurate whole human genome sequencing using reversible
               terminator chemistry},
  author    = {Bentley, David R and Balasubramanian, Shankar and Swerdlow,
               Harold P and Smith, Geoffrey P and Milton, John and Brown, Clive
               G and Hall, Kevin P and Evers, Dirk J and Barnes, Colin L and
               Bignell, Helen R and Boutell, Jonathan M and Bryant, Jason and
               Carter, Richard J and Keira Cheetham, R and Cox, Anthony J and
               Ellis, Darren J and Flatbush, Michael R and Gormley, Niall A and
               Humphray, Sean J and Irving, Leslie J and Karbelashvili, Mirian S
               and Kirk, Scott M and Li, Heng and Liu, Xiaohai and Maisinger,
               Klaus S and Murray, Lisa J and Obradovic, Bojan and Ost, Tobias
               and Parkinson, Michael L and Pratt, Mark R and Rasolonjatovo,
               Isabelle M J and Reed, Mark T and Rigatti, Roberto and
               Rodighiero, Chiara and Ross, Mark T and Sabot, Andrea and Sankar,
               Subramanian V and Scally, Aylwyn and Schroth, Gary P and Smith,
               Mark E and Smith, Vincent P and Spiridou, Anastassia and
               Torrance, Peta E and Tzonev, Svilen S and Vermaas, Eric H and
               Walter, Klaudia and Wu, Xiaolin and Zhang, Lu and Alam, Mohammed
               D and Anastasi, Carole and Aniebo, Ify C and Bailey, David M D
               and Bancarz, Iain R and Banerjee, Saibal and Barbour, Selena G
               and Baybayan, Primo A and Benoit, Vincent A and Benson, Kevin F
               and Bevis, Claire and Black, Phillip J and Boodhun, Asha and
               Brennan, Joe S and Bridgham, John A and Brown, Rob C and Brown,
               Andrew A and Buermann, Dale H and Bundu, Abass A and Burrows,
               James C and Carter, Nigel P and Castillo, Nestor and Chiara E
               Catenazzi, Maria and Chang, Simon and Neil Cooley, R and Crake,
               Natasha R and Dada, Olubunmi O and Diakoumakos, Konstantinos D
               and Dominguez-Fernandez, Belen and Earnshaw, David J and Egbujor,
               Ugonna C and Elmore, David W and Etchin, Sergey S and Ewan, Mark
               R and Fedurco, Milan and Fraser, Louise J and Fuentes Fajardo,
               Karin V and Scott Furey, W and George, David and Gietzen,
               Kimberley J and Goddard, Colin P and Golda, George S and
               Granieri, Philip A and Green, David E and Gustafson, David L and
               Hansen, Nancy F and Harnish, Kevin and Haudenschild, Christian D
               and Heyer, Narinder I and Hims, Matthew M and Ho, Johnny T and
               Horgan, Adrian M and Hoschler, Katya and Hurwitz, Steve and
               Ivanov, Denis V and Johnson, Maria Q and James, Terena and Huw
               Jones, T A and Kang, Gyoung-Dong and Kerelska, Tzvetana H and
               Kersey, Alan D and Khrebtukova, Irina and Kindwall, Alex P and
               Kingsbury, Zoya and Kokko-Gonzales, Paula I and Kumar, Anil and
               Laurent, Marc A and Lawley, Cynthia T and Lee, Sarah E and Lee,
               Xavier and Liao, Arnold K and Loch, Jennifer A and Lok, Mitch and
               Luo, Shujun and Mammen, Radhika M and Martin, John W and
               McCauley, Patrick G and McNitt, Paul and Mehta, Parul and Moon,
               Keith W and Mullens, Joe W and Newington, Taksina and Ning, Zemin
               and Ling Ng, Bee and Novo, Sonia M and O'Neill, Michael J and
               Osborne, Mark A and Osnowski, Andrew and Ostadan, Omead and
               Paraschos, Lambros L and Pickering, Lea and Pike, Andrew C and
               Pike, Alger C and Chris Pinkard, D and Pliskin, Daniel P and
               Podhasky, Joe and Quijano, Victor J and Raczy, Come and Rae,
               Vicki H and Rawlings, Stephen R and Chiva Rodriguez, Ana and Roe,
               Phyllida M and Rogers, John and Rogert Bacigalupo, Maria C and
               Romanov, Nikolai and Romieu, Anthony and Roth, Rithy K and
               Rourke, Natalie J and Ruediger, Silke T and Rusman, Eli and
               Sanches-Kuiper, Raquel M and Schenker, Martin R and Seoane,
               Josefina M and Shaw, Richard J and Shiver, Mitch K and Short,
               Steven W and Sizto, Ning L and Sluis, Johannes P and Smith,
               Melanie A and Ernest Sohna Sohna, Jean and Spence, Eric J and
               Stevens, Kim and Sutton, Neil and Szajkowski, Lukasz and
               Tregidgo, Carolyn L and Turcatti, Gerardo and Vandevondele,
               Stephanie and Verhovsky, Yuli and Virk, Selene M and Wakelin,
               Suzanne and Walcott, Gregory C and Wang, Jingwen and Worsley,
               Graham J and Yan, Juying and Yau, Ling and Zuerlein, Mike and
               Rogers, Jane and Mullikin, James C and Hurles, Matthew E and
               McCooke, Nick J and West, John S and Oaks, Frank L and Lundberg,
               Peter L and Klenerman, David and Durbin, Richard and Smith,
               Anthony J},
  journal   = {Nature},
  publisher = {Nature Publishing Group},
  volume    = 456,
  number    = 7218,
  pages     = {53--59},
  abstract  = {DNA sequence information underpins genetic research, enabling
               discoveries of important biological or medical benefit.
               Sequencing projects have traditionally used long (400-800 base
               pair) reads, but the existence of reference sequences for the
               human and many other genomes makes it possible to develop new,
               fast approaches to re-sequencing, whereby shorter reads are
               compared to a reference to identify intraspecies genetic
               variation. Here we report an approach that generates several
               billion bases of accurate nucleotide sequence per experiment at
               low cost. Single molecules of DNA are attached to a flat surface,
               amplified in situ and used as templates for synthetic sequencing
               with fluorescent reversible terminator deoxyribonucleotides.
               Images of the surface are analysed to generate high-quality
               sequence. We demonstrate application of this approach to human
               genome sequencing on flow-sorted X chromosomes and then scale the
               approach to determine the genome sequence of a male Yoruba from
               Ibadan, Nigeria. We build an accurate consensus sequence from
               >30x average depth of paired 35-base reads. We characterize four
               million single-nucleotide polymorphisms and four hundred thousand
               structural variants, many of which were previously unknown. Our
               approach is effective for accurate, rapid and economical
               whole-genome re-sequencing and many other biomedical
               applications.},
  month     = nov,
  year      = 2008,
  url       = {https://www.nature.com/articles/nature07517},
  doi       = {10.1038/nature07517},
  pmc       = {PMC2581791},
  pmid      = 18987734,
  issn      = {0028-0836,1476-4687},
  language  = {en}
}

@article{Van_der_Valk2019-to,
  title    = {Index hopping on the Illumina {HiseqX} platform and its
              consequences for ancient {DNA} studies},
  author   = {van der Valk, Tom and Vezzi, Francesco and Ormestad, Mattias and
              Dalén, Love and Guschanski, Katerina},
  journal  = {Molecular ecology resources},
  abstract = {The high-throughput capacities of the Illumina sequencing
              platforms and the possibility to label samples individually have
              encouraged wide use of sample multiplexing. However, this practice
              results in read misassignment (usually <1\%) across samples
              sequenced on the same lane. Alarmingly high rates of read
              misassignment of up to 10\% were reported for lllumina sequencing
              machines with exclusion amplification chemistry. This may make use
              of these platforms prohibitive, particularly in studies that rely
              on low-quantity and low-quality samples, such as historical and
              archaeological specimens. Here, we use barcodes, short sequences
              that are ligated to both ends of the DNA insert, to directly
              quantify the rate of index hopping in 100-year old
              museum-preserved gorilla (Gorilla beringei) samples. Correcting
              for multiple sources of noise, we identify on average 0.470\% of
              reads containing a hopped index. We show that sample-specific
              quantity of misassigned reads depends on the number of reads that
              any given sample contributes to the total sequencing pool, so that
              samples with few sequenced reads receive the greatest proportion
              of misassigned reads. This particularly affects ancient DNA
              samples, as these frequently differ in their DNA quantity and
              endogenous content. Through simulations we show that even low
              rates of index hopping, as reported here, can lead to biases in
              ancient DNA studies when multiplexing samples with vastly
              different quantities of endogenous material.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1111/1755-0998.13009},
  keywords = {ancient DNA; index switching; multiplexing; museum specimens;
              next-generation sequencing; read misassignment},
  doi      = {10.1111/1755-0998.13009},
  pmid     = 30848092,
  issn     = {1755-098X,1755-0998},
  language = {en}
}

@article{Zhang2023-vz,
  title     = {Deduplication improves cost-efficiency and yields of {DE} Novo
               assembly and binning of shotgun metagenomes in microbiome
               research},
  author    = {Zhang, Zhiguo and Zhang, Lu and Zhang, Guoqing and Zhao, Ze and
               Wang, Hui and Ju, Feng},
  journal   = {Microbiology spectrum},
  publisher = {American Society for Microbiology 1752 N St., N.W., Washington,
               DC},
  pages     = {e0428222},
  abstract  = {In the last decade, metagenomics has greatly revolutionized the
               study of microbial communities. However, the presence of
               artificial duplicate reads raised mainly from the preparation of
               metagenomic DNA sequencing libraries and their impacts on
               metagenomic assembly and binning have never been brought to
               attention. Here, we explicitly investigated the effects of
               duplicate reads on metagenomic assemblies and binning based on
               analyses of five groups of representative metagenomes with
               distinct microbiome complexities. Our results showed that
               deduplication considerably increased the binning yields (by 3.5\%
               to 80\%) for most of the metagenomic data sets examined thanks to
               the improved contig length and coverage profiling of
               metagenome-assembled contigs, whereas it slightly decreased the
               binning yields of metagenomes with low complexity (e.g., human
               gut metagenomes). Specifically, 411 versus 397, 331 versus 317,
               104 versus 88, and 9 versus 5 metagenome-assembled genomes (MAGs)
               were recovered from MEGAHIT assemblies of bioreactor sludge,
               surface water, lake sediment, and forest soil metagenomes,
               respectively. Noticeably, deduplication significantly reduced the
               computational costs of the metagenomic assembly, including the
               elapsed time (9.0\% to 29.9\%) and the maximum memory requirement
               (4.3\% to 37.1\%). Collectively, we recommend the removal of
               duplicate reads in metagenomes with high complexity before
               assembly and binning analyses, for example, the forest soil
               metagenomes examined in this study. IMPORTANCE Duplicated reads
               in shotgun metagenomes are usually considered technical
               artifacts. Their presence in metagenomes would theoretically not
               only introduce bias into the quantitative analysis but also
               result in mistakes in the coverage profile, leading to adverse
               effects on or even failures in metagenomic assembly and binning,
               as the widely used metagenome assemblers and binners all need
               coverage information for graph partitioning and assembly binning,
               respectively. However, this issue was seldom noticed, and its
               impacts on downstream essential bioinformatic procedures (e.g.,
               assembly and binning) remained unclear. In this study, we
               comprehensively evaluated for the first time the implications of
               duplicate reads for the de novo assembly and binning of real
               metagenomic data sets by comparing the assembly qualities,
               binning yields, and requirements for computational resources with
               and without the removal of duplicate reads. It was revealed that
               deduplication considerably increased the binning yields of
               metagenomes with high complexity and significantly reduced the
               computational costs, including the elapsed time and the maximum
               memory requirement, for most of the metagenomes studied. These
               results provide empirical references for more cost-efficient
               metagenomic analyses in microbiome research.},
  month     = feb,
  year      = 2023,
  url       = {https://journals.asm.org/doi/10.1128/spectrum.04282-22},
  keywords  = {assembly; binning; duplicate reads; metagenomes; microbiome;
               shotgun metagenomes},
  doi       = {10.1128/spectrum.04282-22},
  pmc       = {PMC10101064},
  pmid      = 36744896,
  issn      = {2165-0497},
  language  = {en}
}


@article{Longo2011-qd,
  title    = {Abundant human {DNA} contamination identified in non-primate
              genome databases},
  author   = {Longo, Mark S and O'Neill, Michael J and O'Neill, Rachel J},
  journal  = {PloS one},
  volume   = 6,
  number   = 2,
  pages    = {e16410},
  abstract = {During routine screens of the NCBI databases using human
              repetitive elements we discovered an unlikely level of nucleotide
              identity across a broad range of phyla. To ascertain whether
              databases containing DNA sequences, genome assemblies and trace
              archive reads were contaminated with human sequences, we performed
              an in depth search for sequences of human origin in non-human
              species. Using a primate specific SINE, AluY, we screened 2,749
              non-primate public databases from NCBI, Ensembl, JGI, and UCSC and
              have found 492 to be contaminated with human sequence. These
              represent species ranging from bacteria (B. cereus) to plants (Z.
              mays) to fish (D. rerio) with examples found from most phyla. The
              identification of such extensive contamination of human sequence
              across databases and sequence types warrants caution among the
              sequencing community in future sequencing efforts, such as human
              re-sequencing. We discuss issues this may raise as well as present
              data that gives insight as to how this may be occurring.},
  month    = feb,
  year     = 2011,
  url      = {http://dx.doi.org/10.1371/journal.pone.0016410},
  doi      = {10.1371/journal.pone.0016410},
  pmc      = {PMC3040168},
  pmid     = 21358816,
  issn     = {1932-6203},
  language = {en}
}

@article{Mukherjee2015-vc,
  title    = {Large-scale contamination of microbial isolate genomes by Illumina
              {PhiX} control},
  author   = {Mukherjee, Supratim and Huntemann, Marcel and Ivanova, Natalia and
              Kyrpides, Nikos C and Pati, Amrita},
  journal  = {Standards in genomic sciences},
  volume   = 10,
  pages    = 18,
  abstract = {With the rapid growth and development of sequencing technologies,
              genomes have become the new go-to for exploring solutions to some
              of the world's biggest challenges such as searching for
              alternative energy sources and exploration of genomic dark matter.
              However, progress in sequencing has been accompanied by its share
              of errors that can occur during template or library preparation,
              sequencing, imaging or data analysis. In this study we screened
              over 18,000 publicly available microbial isolate genome sequences
              in the Integrated Microbial Genomes database and identified more
              than 1000 genomes that are contaminated with PhiX, a control
              frequently used during Illumina sequencing runs. Approximately
              10\% of these genomes have been published in literature and 129
              contaminated genomes were sequenced under the Human Microbiome
              Project. Raw sequence reads are prone to contamination from
              various sources and are usually eliminated during downstream
              quality control steps. Detection of PhiX contaminated genomes
              indicates a lapse in either the application or effectiveness of
              proper quality control measures. The presence of PhiX
              contamination in several publicly available isolate genomes can
              result in additional errors when such data are used in comparative
              genomics analyses. Such contamination of public databases have
              far-reaching consequences in the form of erroneous data
              interpretation and analyses, and necessitates better measures to
              proofread raw sequences before releasing them to the broader
              scientific community.},
  month    = mar,
  year     = 2015,
  url      = {http://dx.doi.org/10.1186/1944-3277-10-18},
  keywords = {Comparative genomics; Contamination; Next-generation sequencing;
              PhiX},
  doi      = {10.1186/1944-3277-10-18},
  pmc      = {PMC4511556},
  pmid     = 26203331,
  issn     = {1944-3277},
  language = {en}
}

@article{Merchant2014-eu,
  title    = {Unexpected cross-species contamination in genome sequencing
              projects},
  author   = {Merchant, Samier and Wood, Derrick E and Salzberg, Steven L},
  journal  = {PeerJ},
  volume   = 2,
  pages    = {e675},
  abstract = {The raw data from a genome sequencing project sometimes contains
              DNA from contaminating organisms, which may be introduced during
              sample collection or sequence preparation. In some instances,
              these contaminants remain in the sequence even after assembly and
              deposition of the genome into public databases. As a result,
              searches of these databases may yield erroneous and confusing
              results. We used efficient microbiome analysis software to scan
              the draft assembly of domestic cow, Bos taurus, and identify 173
              small contigs that appeared to derive from microbial contaminants.
              In the course of verifying these findings, we discovered that one
              genome, Neisseria gonorrhoeae TCDC-NG08107, although putatively a
              complete genome, contained multiple sequences that actually
              derived from the cow and sheep genomes. Our findings illustrate
              the need to carefully validate findings of anomalous DNA that rely
              on comparisons to either draft or finished genomes.},
  month    = nov,
  year     = 2014,
  url      = {http://dx.doi.org/10.7717/peerj.675},
  keywords = {Bioinformatics; DNA sequencing; Genome assembly; Genomics;
              Microbiome; Sequence analysis},
  doi      = {10.7717/peerj.675},
  pmc      = {PMC4243333},
  pmid     = 25426337,
  issn     = {2167-8359},
  language = {en}
}

@article{Steinegger2020-br,
  title    = {Terminating contamination: large-scale search identifies more than
              2,000,000 contaminated entries in {GenBank}},
  author   = {Steinegger, Martin and Salzberg, Steven L},
  journal  = {Genome biology},
  volume   = 21,
  number   = 1,
  pages    = 115,
  abstract = {Genomic analyses are sensitive to contamination in public
              databases caused by incorrectly labeled reference sequences. Here,
              we describe Conterminator, an efficient method to detect and
              remove incorrectly labeled sequences by an exhaustive
              all-against-all sequence comparison. Our analysis reports
              contamination of 2,161,746, 114,035, and 14,148 sequences in the
              RefSeq, GenBank, and NR databases, respectively, spanning the
              whole range from draft to ``complete'' model organism genomes. Our
              method scales linearly with input size and can process 3.3 TB in
              12 days on a 32-core computer. Conterminator can help ensure the
              quality of reference databases. Source code (GPLv3):
              https://github.com/martin-steinegger/conterminator.},
  month    = may,
  year     = 2020,
  url      = {http://dx.doi.org/10.1186/s13059-020-02023-1},
  keywords = {Contamination; GenBank; Genomes; RefSeq; Software},
  doi      = {10.1186/s13059-020-02023-1},
  pmc      = {PMC7218494},
  pmid     = 32398145,
  issn     = {1465-6906},
  language = {en}
}

@article{Breitwieser2019-iz,
  title    = {Human contamination in bacterial genomes has created thousands of
              spurious proteins},
  author   = {Breitwieser, Florian P and Pertea, Mihaela and Zimin, Aleksey and
              Salzberg, Steven L},
  journal  = {Genome research},
  volume   = 29,
  pages    = {954--960},
  abstract = {Contaminant sequences that appear in published genomes can cause
              numerous problems for downstream analyses, particularly for
              evolutionary studies and metagenomics projects. Our large-scale
              scan of complete and draft bacterial and archaeal genomes in the
              NCBI RefSeq database reveals that 2250 genomes are contaminated by
              human sequence. The contaminant sequences derive primarily from
              high-copy human repeat regions, which themselves are not
              adequately represented in the current human reference genome,
              GRCh38. The absence of the sequences from the human assembly
              offers a likely explanation for their presence in bacterial
              assemblies. In some cases, the contaminating contigs have been
              erroneously annotated as containing protein-coding sequences,
              which over time have propagated to create spurious protein
              ``families'' across multiple prokaryotic and eukaryotic genomes.
              As a result, 3437 spurious protein entries are currently present
              in the widely-used nr and TrEMBL protein databases. We report here
              an extensive list of contaminant sequences in bacterial genome
              assemblies and the proteins associated with them. We found that
              nearly all contaminants occurred on small contigs in draft
              genomes, which suggests that filtering out small contigs from
              draft genome assemblies may mitigate the issue of contamination
              while still keeping nearly all of the genuine genomic sequences.},
  month    = may,
  year     = 2019,
  url      = {http://dx.doi.org/10.1101/gr.245373.118},
  doi      = {10.1101/gr.245373.118},
  pmid     = 31064768,
  issn     = {1088-9051,1549-5469},
  language = {en}
}

@article{Kryukov2016-my,
  title    = {Human Contamination in Public Genome Assemblies},
  author   = {Kryukov, Kirill and Imanishi, Tadashi},
  journal  = {PloS one},
  volume   = 11,
  number   = 9,
  pages    = {e0162424},
  abstract = {Contamination in genome assembly can lead to wrong or confusing
              results when using such genome as reference in sequence
              comparison. Although bacterial contamination is well known, the
              problem of human-originated contamination received little
              attention. In this study we surveyed 45,735 available genome
              assemblies for evidence of human contamination. We used lineage
              specificity to distinguish between contamination and conservation.
              We found that 154 genome assemblies contain fragments that with
              high confidence originate as contamination from human DNA.
              Majority of contaminating human sequences were present in the
              reference human genome assembly for over a decade. We recommend
              that existing contaminated genomes should be revised to remove
              contaminated sequence, and that new assemblies should be
              thoroughly checked for presence of human DNA before submitting
              them to public databases.},
  month    = sep,
  year     = 2016,
  url      = {http://dx.doi.org/10.1371/journal.pone.0162424},
  doi      = {10.1371/journal.pone.0162424},
  pmc      = {PMC5017631},
  pmid     = 27611326,
  issn     = {1932-6203},
  language = {en}
}

@article{Chen2018-vg,
  title     = {fastp: an ultra-fast all-in-one {FASTQ} preprocessor},
  author    = {Chen, Shifu and Zhou, Yanqing and Chen, Yaru and Gu, Jia},
  journal   = {Bioinformatics},
  publisher = {Oxford University Press},
  volume    = 34,
  number    = 17,
  pages     = {i884--i890},
  abstract  = {Quality control and preprocessing of FASTQ files are essential to
               providing clean data for downstream analysis. Traditionally, a
               different tool is used for each operation, such as quality
               control, adapter trimming and quality filtering. These tools are
               often insufficiently fast as most are developed using high-level
               programming languages (e.g. Python and Java) and provide limited
               multi-threading support. Reading and loading data multiple times
               also renders preprocessing slow and I/O inefficient.We developed
               fastp as an ultra-fast FASTQ preprocessor with useful quality
               control and data-filtering features. It can perform quality
               control, adapter trimming, quality filtering, per-read quality
               pruning and many other operations with a single scan of the FASTQ
               data. This tool is developed in C++ and has multi-threading
               support. Based on our evaluation, fastp is 2–5 times faster than
               other FASTQ preprocessing tools such as Trimmomatic or Cutadapt
               despite performing far more operations than similar tools.The
               open-source code and corresponding instructions are available at
               https://github.com/OpenGene/fastp.},
  month     = sep,
  year      = 2018,
  url       = {https://academic.oup.com/bioinformatics/article/34/17/i884/5093234},
  doi       = {10.1093/bioinformatics/bty560},
  issn      = {1367-4803}
}

@article{Andreeva2022-oe,
  title     = {Methodologies for ancient {DNA} extraction from bones for genomic
               analysis: Approaches and guidelines},
  author    = {Andreeva, T V and Malyarchuk, A B and Soshkina, A D and Dudko, N
               A and Plotnikova, M Yu and Rogaev, E I},
  journal   = {Russian journal of genetics},
  publisher = {Pleiades Publishing Ltd},
  volume    = 58,
  number    = 9,
  pages     = {1017--1035},
  abstract  = {Abstract The emergence and development of massive parallel
               sequencing (new generation sequencing) methods have opened up new
               prospects in the study of ancient organisms, including extinct
               ones. Numerous skeletal remains from archaeological and museum
               collections are often the only source of information on ancient
               species and populations. In this review, we discuss the features
               of human bone tissue and the advantages and disadvantages of bone
               material as a source of DNA for genomic analysis of ancient
               people. Here we present new methodological approaches to DNA
               extraction from ancient human skeletal remains and its
               preparation for large-scale parallel sequencing are presented, as
               well as prospects and directions for further research in a new
               interdisciplinary field, paleogenomics.},
  month     = sep,
  year      = 2022,
  url       = {https://link.springer.com/article/10.1134/S1022795422090034},
  doi       = {10.1134/s1022795422090034},
  issn      = {1022-7954,1608-3369},
  language  = {en}
}

@article{Danaeifar2022-sk,
  title     = {New horizons in developing cell lysis methods: A review},
  author    = {Danaeifar, Mohsen},
  journal   = {Biotechnology and bioengineering},
  publisher = {Wiley},
  volume    = 119,
  number    = 11,
  pages     = {3007--3021},
  abstract  = {Cell lysis is an essential step in many studies related to
               biology and medicine. Based on the scale and medium that cell
               lysis is carried out, there are three main types of the cell
               lysis: (1) lysis of the cells in the surrounding environment, (2)
               lysis of the isolated or cultured cells, and (3) single cell
               lysis. Conventionally, several cell lysis methods have been
               developed, such as freeze-thawing, bead beating, incursion in
               liquid nitrogen, sonication, and enzymatic and chemical-based
               approaches. In recent years, various novel technologies have been
               employed to develop new methods of cell lysis. The aim of studies
               in this field is to introduce more precise and efficient tools or
               to reduce the costs of cell lysis procedures. Nanostructure-based
               lysis methods, acoustic oscillation, electrical current,
               irradiation, bacteria-mediated cell lysis, magnetic ionic
               liquids, bacteriophage genes, monolith columns, hydraulic forces,
               and steam explosion are some examples of newly developed cell
               lysis methods. Besides the significant advances in this field,
               there are still many challenges and tools must be further
               improved.},
  month     = nov,
  year      = 2022,
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bit.28198},
  keywords  = {cell lysis; membrane disruption; microfluidics; new methods},
  doi       = {10.1002/bit.28198},
  pmid      = 35900072,
  issn      = {1097-0290,0006-3592},
  language  = {en}
}

@article{Schloss2020-gp,
  title     = {Cultivating {DNA} sequencing technology after the Human Genome
               Project},
  author    = {Schloss, Jeffery A and Gibbs, Richard A and Makhijani, Vinod B
               and Marziali, Andre},
  journal   = {Annual review of genomics and human genetics},
  publisher = {Annual Reviews},
  volume    = 21,
  number    = 1,
  pages     = {117--138},
  abstract  = {When the Human Genome Project was completed in 2003, automated
               Sanger DNA sequencing with fluorescent dye labels was the
               dominant technology. Several nascent alternative methods based on
               older ideas that had not been fully developed were the focus of
               technical researchers and companies. Funding agencies recognized
               the dynamic nature of technology development and that, beyond the
               Human Genome Project, there were growing opportunities to deploy
               DNA sequencing in biological research. Consequently, the National
               Human Genome Research Institute of the National Institutes of
               Health created a program-widely known as the Advanced Sequencing
               Technology Program-that stimulated all stages of development of
               new DNA sequencing methods, from innovation to advanced
               manufacturing and production testing, with the goal of reducing
               the cost of sequencing a human genome first to $100,000 and then
               to $1,000. The events of this period provide a powerful example
               of how judicious funding of academic and commercial partners can
               rapidly advance core technology developments that lead to
               profound advances across the scientific landscape.},
  month     = aug,
  year      = 2020,
  url       = {https://www.annualreviews.org/content/journals/10.1146/annurev-genom-111919-082433},
  keywords  = {\$1,000 genome technology program; Advanced Sequencing Technology
               Program; National Human Genome Research Institute; National
               Institutes of Health; funding agency; funding model; nanopore DNA
               sequencing; next-generation DNA sequencing; peer review},
  doi       = {10.1146/annurev-genom-111919-082433},
  pmid      = 32283947,
  issn      = {1527-8204,1545-293X},
  language  = {en}
}

@article{Holt2008-wk,
  title     = {The new paradigm of flow cell sequencing},
  author    = {Holt, Robert A and Jones, Steven J M},
  journal   = {Genome research},
  publisher = {Cold Spring Harbor Laboratory},
  volume    = 18,
  number    = 6,
  pages     = {839--846},
  abstract  = {DNA sequencing is in a period of rapid change, in which capillary
               sequencing is no longer the technology of choice for most
               ultra-high-throughput applications. A new generation of
               instruments that utilize primed synthesis in flow cells to
               obtain, simultaneously, the sequence of millions of different DNA
               templates has changed the field. We compare and contrast these
               new sequencing platforms in terms of stage of development,
               instrument configuration, template format, sequencing chemistry,
               throughput capability, operating cost, data handling issues, and
               error models. While these platforms outperform capillary
               instruments in terms of bases per day and cost per base, the
               short length of sequence reads obtained from most instruments and
               the limited number of samples that can be run simultaneously
               imposes some practical constraints on sequencing applications.
               However, recently developed methods for paired-end sequencing and
               for array-based direct selection of desired templates from
               complex mixtures extend the utility of these platforms for genome
               analysis. Given the ever increasing demand for DNA sequence
               information, we can expect continuous improvement of this new
               generation of instruments and their eventual replacement by even
               more powerful technology.},
  month     = jun,
  year      = 2008,
  url       = {http://genome.cshlp.org/content/18/6/839.abstract},
  doi       = {10.1101/gr.073262.107},
  pmid      = 18519653,
  issn      = {1088-9051,1549-5469},
  language  = {en}
}
