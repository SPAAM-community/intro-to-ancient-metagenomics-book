@article{Schuster2008-qx,
  title    = {Next-generation sequencing transforms today's biology},
  author   = {Schuster, Stephan C},
  journal  = {Nature methods},
  volume   = 5,
  number   = 1,
  pages    = {16--18},
  abstract = {A new generation of non-Sanger-based sequencing technologies has
              delivered on its promise of sequencing DNA at unprecedented speed,
              thereby enabling impressive scientific achievements and novel
              biological applications. However, before stepping into the
              limelight, next-generation sequencing had to overcome the inertia
              of a field that relied on Sanger-sequencing for 30 years.},
  month    = jan,
  year     = 2008,
  url      = {http://dx.doi.org/10.1038/nmeth1156},
  doi      = {10.1038/nmeth1156},
  issn     = {1548-7105,1548-7091},
  language = {en},
  pmid     = 18165802
}

@article{Shendure2008-fh,
  title    = {Next-generation {DNA} sequencing},
  author   = {Shendure, Jay and Ji, Hanlee},
  journal  = {Nature biotechnology},
  volume   = 26,
  number   = 10,
  pages    = {1135--1145},
  abstract = {DNA sequence represents a single format onto which a broad range
              of biological phenomena can be projected for high-throughput data
              collection. Over the past three years, massively parallel DNA
              sequencing platforms have become widely available, reducing the
              cost of DNA sequencing by over two orders of magnitude, and
              democratizing the field by putting the sequencing capacity of a
              major genome center in the hands of individual investigators.
              These new technologies are rapidly evolving, and near-term
              challenges include the development of robust protocols for
              generating sequencing libraries, building effective new approaches
              to data-analysis, and often a rethinking of experimental design.
              Next-generation DNA sequencing has the potential to dramatically
              accelerate biological and biomedical research, by enabling the
              comprehensive analysis of genomes, transcriptomes and interactomes
              to become inexpensive, routine and widespread, rather than
              requiring significant production-scale efforts.},
  month    = oct,
  year     = 2008,
  url      = {http://dx.doi.org/10.1038/nbt1486},
  doi      = {10.1038/nbt1486},
  issn     = {1546-1696,1087-0156},
  language = {en},
  pmid     = 18846087
}

@article{Slatko2018-hg,
  title    = {Overview of Next-Generation Sequencing Technologies},
  author   = {Slatko, Barton E and Gardner, Andrew F and Ausubel, Frederick M},
  journal  = {Current protocols in molecular biology / edited by Frederick M.
              Ausubel ... [et al.]},
  volume   = 122,
  number   = 1,
  pages    = {e59},
  abstract = {High throughput DNA sequencing methodology (next generation
              sequencing; NGS) has rapidly evolved over the past 15 years and
              new methods are continually being commercialized. As the
              technology develops, so do increases in the number of
              corresponding applications for basic and applied science. The
              purpose of this review is to provide a compendium of NGS
              methodologies and associated applications. Each brief discussion
              is followed by web links to the manufacturer and/or web-based
              visualizations. Keyword searches, such as with Google, may also
              provide helpful internet links and information. © 2018 by John
              Wiley \& Sons, Inc.},
  month    = apr,
  year     = 2018,
  url      = {http://dx.doi.org/10.1002/cpmb.59},
  keywords = {NGS; Sanger sequencing; next-generation sequencing},
  doi      = {10.1002/cpmb.59},
  issn     = {1934-3647,1934-3639},
  pmc      = {PMC6020069},
  language = {en},
  pmid     = 29851291
}

@article{Van_Dijk2014-ep,
  title    = {Ten years of next-generation sequencing technology},
  author   = {van Dijk, Erwin L and Auger, Hélène and Jaszczyszyn, Yan and
              Thermes, Claude},
  journal  = {Trends in genetics},
  volume   = 30,
  number   = 9,
  pages    = {418--426},
  abstract = {Ten years ago next-generation sequencing (NGS) technologies
              appeared on the market. During the past decade, tremendous
              progress has been made in terms of speed, read length, and
              throughput, along with a sharp reduction in per-base cost.
              Together, these advances democratized NGS and paved the way for
              the development of a large number of novel NGS applications in
              basic science as well as in translational research areas such as
              clinical diagnostics, agrigenomics, and forensic science. Here we
              provide an overview of the evolution of NGS and discuss the most
              significant improvements in sequencing technologies and library
              preparation protocols. We also explore the current landscape of
              NGS applications and provide a perspective for future
              developments.},
  month    = sep,
  year     = 2014,
  url      = {http://dx.doi.org/10.1016/j.tig.2014.07.001},
  keywords = {ChIP-seq; DNA-seq; NGS library preparation; Next-generation
              sequencing (NGS); RNA-seq; genomics},
  doi      = {10.1016/j.tig.2014.07.001},
  issn     = {0168-9525},
  language = {en},
  pmid     = 25108476
}

@article{Kircher2012-fg,
  title    = {Double indexing overcomes inaccuracies in multiplex sequencing on
              the Illumina platform},
  author   = {Kircher, Martin and Sawyer, Susanna and Meyer, Matthias},
  journal  = {Nucleic acids research},
  volume   = 40,
  number   = 1,
  pages    = {e3},
  abstract = {Due to the increasing throughput of current DNA sequencing
              instruments, sample multiplexing is necessary for making
              economical use of available sequencing capacities. A widely used
              multiplexing strategy for the Illumina Genome Analyzer utilizes
              sample-specific indexes, which are embedded in one of the library
              adapters. However, this and similar multiplex approaches come with
              a risk of sample misidentification. By introducing indexes into
              both library adapters (double indexing), we have developed a
              method that reveals the rate of sample misidentification within
              current multiplex sequencing experiments. With ~0.3\% these rates
              are orders of magnitude higher than expected and may severely
              confound applications in cancer genomics and other fields
              requiring accurate detection of rare variants. We identified the
              occurrence of mixed clusters on the flow as the predominant source
              of error. The accuracy of sample identification is further
              impaired if indexed oligonucleotides are cross-contaminated or if
              indexed libraries are amplified in bulk. Double-indexing
              eliminates these problems and increases both the scope and
              accuracy of multiplex sequencing on the Illumina platform.},
  month    = jan,
  year     = 2012,
  url      = {http://dx.doi.org/10.1093/nar/gkr771},
  doi      = {10.1093/nar/gkr771},
  issn     = {1362-4962,0305-1048},
  pmc      = {PMC3245947},
  pmid     = 22021376
}

@article{Meyer2010-qc,
  title    = {Illumina sequencing library preparation for highly multiplexed
              target capture and sequencing},
  author   = {Meyer, Matthias and Kircher, Martin},
  journal  = {Cold Spring Harbor protocols},
  volume   = 2010,
  number   = 6,
  pages    = {db.prot5448},
  abstract = {The large amount of DNA sequence data generated by high-throughput
              sequencing technologies often allows multiple samples to be
              sequenced in parallel on a single sequencing run. This is
              particularly true if subsets of the genome are studied rather than
              complete genomes. In recent years, target capture from sequencing
              libraries has largely replaced polymerase chain reaction (PCR) as
              the preferred method of target enrichment. Parallelizing target
              capture and sequencing for multiple samples requires the
              incorporation of sample-specific barcodes into sequencing
              libraries, which is necessary to trace back the sample source of
              each sequence. This protocol describes a fast and reliable method
              for the preparation of barcoded (``indexed'') sequencing libraries
              for Illumina's Genome Analyzer platform. The protocol avoids
              expensive commercial library preparation kits and can be performed
              in a 96-well plate setup using multi-channel pipettes, requiring
              not more than two or three days of lab work. Libraries can be
              prepared from any type of double-stranded DNA, even if present in
              subnanogram quantity.},
  month    = jun,
  year     = 2010,
  url      = {http://dx.doi.org/10.1101/pdb.prot5448},
  doi      = {10.1101/pdb.prot5448},
  issn     = {1559-6095,1940-3402},
  pmid     = 20516186
}

@article{Ma2019-lg,
  title    = {Analysis of error profiles in deep next-generation sequencing data},
  author   = {Ma, Xiaotu and Shao, Ying and Tian, Liqing and Flasch, Diane A and
              Mulder, Heather L and Edmonson, Michael N and Liu, Yu and Chen,
              Xiang and Newman, Scott and Nakitandwe, Joy and Li, Yongjin and
              Li, Benshang and Shen, Shuhong and Wang, Zhaoming and Shurtleff,
              Sheila and Robison, Leslie L and Levy, Shawn and Easton, John and
              Zhang, Jinghui},
  journal  = {Genome biology},
  volume   = 20,
  number   = 1,
  pages    = 50,
  abstract = {BACKGROUND: Sequencing errors are key confounding factors for
              detecting low-frequency genetic variants that are important for
              cancer molecular diagnosis, treatment, and surveillance using deep
              next-generation sequencing (NGS). However, there is a lack of
              comprehensive understanding of errors introduced at various steps
              of a conventional NGS workflow, such as sample handling, library
              preparation, PCR enrichment, and sequencing. In this study, we use
              current NGS technology to systematically investigate these
              questions. RESULTS: By evaluating read-specific error
              distributions, we discover that the substitution error rate can be
              computationally suppressed to 10-5 to 10-4, which is 10- to
              100-fold lower than generally considered achievable (10-3) in the
              current literature. We then quantify substitution errors
              attributable to sample handling, library preparation, enrichment
              PCR, and sequencing by using multiple deep sequencing datasets. We
              find that error rates differ by nucleotide substitution types,
              ranging from 10-5 for A>C/T>G, C>A/G>T, and C>G/G>C changes to
              10-4 for A>G/T>C changes. Furthermore, C>T/G>A errors exhibit
              strong sequence context dependency, sample-specific effects
              dominate elevated C>A/G>T errors, and target-enrichment PCR led to
              ~ 6-fold increase of overall error rate. We also find that more
              than 70\% of hotspot variants can be detected at 0.1 ~ 0.01\%
              frequency with the current NGS technology by applying in silico
              error suppression. CONCLUSIONS: We present the first comprehensive
              analysis of sequencing error sources in conventional NGS
              workflows. The error profiles revealed by our study highlight new
              directions for further improving NGS analysis accuracy both
              experimentally and computationally, ultimately enhancing the
              precision of deep sequencing.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1186/s13059-019-1659-6},
  keywords = {Deep sequencing; Detection; Error rate; Hotspot mutation;
              Subclonal; Substitution},
  doi      = {10.1186/s13059-019-1659-6},
  issn     = {1465-6906},
  pmc      = {PMC6417284},
  language = {en},
  pmid     = 30867008
}
@unpublished{Sinha2017-zo,
  title    = {Index switching causes “spreading-of-signal” among multiplexed
              samples in Illumina {HiSeq} 4000 {DNA} sequencing},
  author   = {Sinha, Rahul and Stanley, Geoff and Gulati, Gunsagar Singh and
              Ezran, Camille and Travaglini, Kyle Joseph and Wei, Eric and Chan,
              Charles Kwok Fai and Nabhan, Ahmad N and Su, Tianying and
              Morganti, Rachel Marie and Conley, Stephanie Diana and Chaib,
              Hassan and Red-Horse, Kristy and Longaker, Michael T and Snyder,
              Michael P and Krasnow, Mark A and Weissman, Irving L},
  journal  = {bioRxiv},
  pages    = 125724,
  abstract = {Illumina-based next generation sequencing (NGS) has accelerated
              biomedical discovery through its ability to generate thousands of
              gigabases of sequencing output per run at a fraction of the time
              and cost of conventional technologies. The process typically
              involves four basic steps: library preparation, cluster
              generation, sequencing, and data analysis. In 2015, a new
              chemistry of cluster generation was introduced in the newer
              Illumina machines (HiSeq 3000/4000/X Ten) called exclusion
              amplification (ExAmp), which was a fundamental shift from the
              earlier method of random cluster generation by bridge
              amplification on a non-patterned flow cell. The ExAmp chemistry,
              in conjunction with patterned flow cells containing nanowells at
              fixed locations, increases cluster density on the flow cell,
              thereby reducing the cost per run. It also increases sequence read
              quality, especially for longer read lengths (up to 150 base
              pairs). This advance has been widely adopted for genome sequencing
              because greater sequencing depth can be achieved for lower cost
              without compromising the quality of longer reads. We show that
              this promising chemistry is problematic, however, when
              multiplexing samples. We discovered that up to 5-10\% of
              sequencing reads (or signals) are incorrectly assigned from a
              given sample to other samples in a multiplexed pool. We provide
              evidence that this “spreading-of-signals” arises from low levels
              of free index primers present in the pool. These index primers can
              prime pooled library fragments at random via complementary 3′
              ends, and get extended by DNA polymerase, creating a new library
              molecule with a new index before binding to the patterned flow
              cell to generate a cluster for sequencing. This causes the
              resulting read from that cluster to be assigned to a different
              sample, causing the spread of signals within multiplexed samples.
              We show that low levels of free index primers persist after the
              most common library purification procedure recommended by
              Illumina, and that the amount of signal spreading among samples is
              proportional to the level of free index primer present in the
              library pool. This artifact causes homogenization and
              misclassification of cells in single cell RNA-seq experiments.
              Therefore, all data generated in this way must now be carefully
              re-examined to ensure that “spreading-of-signals” has not
              compromised data analysis and conclusions. Re-sequencing samples
              using an older technology that uses conventional bridge
              amplification for cluster generation, or improved library cleanup
              strategies to remove free index primers, can minimize or eliminate
              this signal spreading artifact.},
  month    = apr,
  year     = 2017,
  url      = {http://dx.doi.org/10.1101/125724},
  doi      = {10.1101/125724},
  language = {en}
}

@article{Van_der_Valk2019-to,
  title    = {Index hopping on the Illumina {HiseqX} platform and its
              consequences for ancient {DNA} studies},
  author   = {van der Valk, Tom and Vezzi, Francesco and Ormestad, Mattias and
              Dalén, Love and Guschanski, Katerina},
  journal  = {Molecular ecology resources},
  abstract = {The high-throughput capacities of the Illumina sequencing
              platforms and the possibility to label samples individually have
              encouraged wide use of sample multiplexing. However, this practice
              results in read misassignment (usually <1\%) across samples
              sequenced on the same lane. Alarmingly high rates of read
              misassignment of up to 10\% were reported for lllumina sequencing
              machines with exclusion amplification chemistry. This may make use
              of these platforms prohibitive, particularly in studies that rely
              on low-quantity and low-quality samples, such as historical and
              archaeological specimens. Here, we use barcodes, short sequences
              that are ligated to both ends of the DNA insert, to directly
              quantify the rate of index hopping in 100-year old
              museum-preserved gorilla (Gorilla beringei) samples. Correcting
              for multiple sources of noise, we identify on average 0.470\% of
              reads containing a hopped index. We show that sample-specific
              quantity of misassigned reads depends on the number of reads that
              any given sample contributes to the total sequencing pool, so that
              samples with few sequenced reads receive the greatest proportion
              of misassigned reads. This particularly affects ancient DNA
              samples, as these frequently differ in their DNA quantity and
              endogenous content. Through simulations we show that even low
              rates of index hopping, as reported here, can lead to biases in
              ancient DNA studies when multiplexing samples with vastly
              different quantities of endogenous material.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1111/1755-0998.13009},
  keywords = {ancient DNA; index switching; multiplexing; museum specimens;
              next-generation sequencing; read misassignment},
  doi      = {10.1111/1755-0998.13009},
  issn     = {1755-0998,1755-098X},
  language = {en},
  pmid     = 30848092
}

@article{Ju2006-cl,
  title    = {Four-color {DNA} sequencing by synthesis using cleavable
              fluorescent nucleotide reversible terminators},
  author   = {Ju, Jingyue and Kim, Dae Hyun and Bi, Lanrong and Meng, Qinglin
              and Bai, Xiaopeng and Li, Zengmin and Li, Xiaoxu and Marma, Mong
              Sano and Shi, Shundi and Wu, Jian and Edwards, John R and Romu,
              Aireen and Turro, Nicholas J},
  journal  = {Proceedings of the National Academy of Sciences of the United
              States of America},
  volume   = 103,
  number   = 52,
  pages    = {19635--19640},
  abstract = {DNA sequencing by synthesis (SBS) on a solid surface during
              polymerase reaction offers a paradigm to decipher DNA sequences.
              We report here the construction of such a DNA sequencing system
              using molecular engineering approaches. In this approach, four
              nucleotides (A, C, G, T) are modified as reversible terminators by
              attaching a cleavable fluorophore to the base and capping the
              3'-OH group with a small chemically reversible moiety so that they
              are still recognized by DNA polymerase as substrates. We found
              that an allyl moiety can be used successfully as a linker to
              tether a fluorophore to 3'-O-allyl-modified nucleotides, forming
              chemically cleavable fluorescent nucleotide reversible
              terminators, 3'-O-allyl-dNTPs-allyl-fluorophore, for application
              in SBS. The fluorophore and the 3'-O-allyl group on a DNA
              extension product, which is generated by incorporating
              3'-O-allyl-dNTPs-allyl-fluorophore in a polymerase reaction, are
              removed simultaneously in 30 s by Pd-catalyzed deallylation in
              aqueous buffer solution. This one-step dual-deallylation reaction
              thus allows the reinitiation of the polymerase reaction and
              increases the SBS efficiency. DNA templates consisting of
              homopolymer regions were accurately sequenced by using this class
              of fluorescent nucleotide analogues on a DNA chip and a four-color
              fluorescent scanner.},
  month    = dec,
  year     = 2006,
  url      = {http://dx.doi.org/10.1073/pnas.0609513103},
  doi      = {10.1073/pnas.0609513103},
  pmc      = {PMC1702316},
  pmid     = 17170132,
  issn     = {0027-8424},
  language = {en}
}

@article{Heather2015-zo,
  title    = {The Sequence of Sequencers: The History of Sequencing {DNA}},
  author   = {Heather, James M and Chain, Benjamin},
  journal  = {Genomics},
  abstract = {Determining the order of nucleic acid residues in biological
              samples is an integral component of a wide variety of research
              applications. Over the last fifty years large numbers of
              researchers have applied themselves to the production of
              techniques and technologies to facilitate this feat, sequencing
              DNA and RNA molecules. This time-scale has witnessed tremendous
              changes, moving from sequencing short oligonucleotides to millions
              of bases, from struggling towards the deduction of the coding
              sequence of a single gene to rapid and widely available whole
              genome sequencing. This article traverses those years, iterating
              through the different generations of sequencing technology,
              highlighting some of the key discoveries, researchers, and
              sequences along the way.},
  month    = nov,
  year     = 2015,
  url      = {http://dx.doi.org/10.1016/j.ygeno.2015.11.003},
  keywords = {DNA; History; RNA; Sequencer; Sequencing},
  doi      = {10.1016/j.ygeno.2015.11.003},
  pmid     = 26554401,
  issn     = {0888-7543,1089-8646}
}

@article{Sanger1977-zw,
  title     = {{DNA} sequencing with chain-terminating inhibitors},
  author    = {Sanger, F and Nicklen, S and Coulson, A R},
  journal   = {Proceedings of the National Academy of Sciences of the United
               States of America},
  publisher = {Proceedings of the National Academy of Sciences},
  volume    = 74,
  number    = 12,
  pages     = {5463--5467},
  abstract  = {A new method for determining nucleotide sequences in DNA is
               described. It is similar to the ``plus and minus'' method
               [Sanger, F. \& Coulson, A. R. (1975) J. Mol. Biol. 94, 441-448]
               but makes use of the 2',3'-dideoxy and arabinonucleoside
               analogues of the normal deoxynucleoside triphosphates, which act
               as specific chain-terminating inhibitors of DNA polymerase. The
               technique has been applied to the DNA of bacteriophage varphiX174
               and is more rapid and more accurate than either the plus or the
               minus method.},
  month     = dec,
  year      = 1977,
  url       = {https://www.pnas.org/doi/abs/10.1073/pnas.74.12.5463},
  doi       = {10.1073/pnas.74.12.5463},
  pmc       = {PMC431765},
  pmid      = 271968,
  issn      = {0027-8424,1091-6490},
  language  = {en}
}
@article{Bentley2008-oj,
  title     = {Accurate whole human genome sequencing using reversible
               terminator chemistry},
  author    = {Bentley, David R and Balasubramanian, Shankar and Swerdlow,
               Harold P and Smith, Geoffrey P and Milton, John and Brown, Clive
               G and Hall, Kevin P and Evers, Dirk J and Barnes, Colin L and
               Bignell, Helen R and Boutell, Jonathan M and Bryant, Jason and
               Carter, Richard J and Keira Cheetham, R and Cox, Anthony J and
               Ellis, Darren J and Flatbush, Michael R and Gormley, Niall A and
               Humphray, Sean J and Irving, Leslie J and Karbelashvili, Mirian S
               and Kirk, Scott M and Li, Heng and Liu, Xiaohai and Maisinger,
               Klaus S and Murray, Lisa J and Obradovic, Bojan and Ost, Tobias
               and Parkinson, Michael L and Pratt, Mark R and Rasolonjatovo,
               Isabelle M J and Reed, Mark T and Rigatti, Roberto and
               Rodighiero, Chiara and Ross, Mark T and Sabot, Andrea and Sankar,
               Subramanian V and Scally, Aylwyn and Schroth, Gary P and Smith,
               Mark E and Smith, Vincent P and Spiridou, Anastassia and
               Torrance, Peta E and Tzonev, Svilen S and Vermaas, Eric H and
               Walter, Klaudia and Wu, Xiaolin and Zhang, Lu and Alam, Mohammed
               D and Anastasi, Carole and Aniebo, Ify C and Bailey, David M D
               and Bancarz, Iain R and Banerjee, Saibal and Barbour, Selena G
               and Baybayan, Primo A and Benoit, Vincent A and Benson, Kevin F
               and Bevis, Claire and Black, Phillip J and Boodhun, Asha and
               Brennan, Joe S and Bridgham, John A and Brown, Rob C and Brown,
               Andrew A and Buermann, Dale H and Bundu, Abass A and Burrows,
               James C and Carter, Nigel P and Castillo, Nestor and Chiara E
               Catenazzi, Maria and Chang, Simon and Neil Cooley, R and Crake,
               Natasha R and Dada, Olubunmi O and Diakoumakos, Konstantinos D
               and Dominguez-Fernandez, Belen and Earnshaw, David J and Egbujor,
               Ugonna C and Elmore, David W and Etchin, Sergey S and Ewan, Mark
               R and Fedurco, Milan and Fraser, Louise J and Fuentes Fajardo,
               Karin V and Scott Furey, W and George, David and Gietzen,
               Kimberley J and Goddard, Colin P and Golda, George S and
               Granieri, Philip A and Green, David E and Gustafson, David L and
               Hansen, Nancy F and Harnish, Kevin and Haudenschild, Christian D
               and Heyer, Narinder I and Hims, Matthew M and Ho, Johnny T and
               Horgan, Adrian M and Hoschler, Katya and Hurwitz, Steve and
               Ivanov, Denis V and Johnson, Maria Q and James, Terena and Huw
               Jones, T A and Kang, Gyoung-Dong and Kerelska, Tzvetana H and
               Kersey, Alan D and Khrebtukova, Irina and Kindwall, Alex P and
               Kingsbury, Zoya and Kokko-Gonzales, Paula I and Kumar, Anil and
               Laurent, Marc A and Lawley, Cynthia T and Lee, Sarah E and Lee,
               Xavier and Liao, Arnold K and Loch, Jennifer A and Lok, Mitch and
               Luo, Shujun and Mammen, Radhika M and Martin, John W and
               McCauley, Patrick G and McNitt, Paul and Mehta, Parul and Moon,
               Keith W and Mullens, Joe W and Newington, Taksina and Ning, Zemin
               and Ling Ng, Bee and Novo, Sonia M and O'Neill, Michael J and
               Osborne, Mark A and Osnowski, Andrew and Ostadan, Omead and
               Paraschos, Lambros L and Pickering, Lea and Pike, Andrew C and
               Pike, Alger C and Chris Pinkard, D and Pliskin, Daniel P and
               Podhasky, Joe and Quijano, Victor J and Raczy, Come and Rae,
               Vicki H and Rawlings, Stephen R and Chiva Rodriguez, Ana and Roe,
               Phyllida M and Rogers, John and Rogert Bacigalupo, Maria C and
               Romanov, Nikolai and Romieu, Anthony and Roth, Rithy K and
               Rourke, Natalie J and Ruediger, Silke T and Rusman, Eli and
               Sanches-Kuiper, Raquel M and Schenker, Martin R and Seoane,
               Josefina M and Shaw, Richard J and Shiver, Mitch K and Short,
               Steven W and Sizto, Ning L and Sluis, Johannes P and Smith,
               Melanie A and Ernest Sohna Sohna, Jean and Spence, Eric J and
               Stevens, Kim and Sutton, Neil and Szajkowski, Lukasz and
               Tregidgo, Carolyn L and Turcatti, Gerardo and Vandevondele,
               Stephanie and Verhovsky, Yuli and Virk, Selene M and Wakelin,
               Suzanne and Walcott, Gregory C and Wang, Jingwen and Worsley,
               Graham J and Yan, Juying and Yau, Ling and Zuerlein, Mike and
               Rogers, Jane and Mullikin, James C and Hurles, Matthew E and
               McCooke, Nick J and West, John S and Oaks, Frank L and Lundberg,
               Peter L and Klenerman, David and Durbin, Richard and Smith,
               Anthony J},
  journal   = {Nature},
  publisher = {Nature Publishing Group},
  volume    = 456,
  number    = 7218,
  pages     = {53--59},
  abstract  = {DNA sequence information underpins genetic research, enabling
               discoveries of important biological or medical benefit.
               Sequencing projects have traditionally used long (400-800 base
               pair) reads, but the existence of reference sequences for the
               human and many other genomes makes it possible to develop new,
               fast approaches to re-sequencing, whereby shorter reads are
               compared to a reference to identify intraspecies genetic
               variation. Here we report an approach that generates several
               billion bases of accurate nucleotide sequence per experiment at
               low cost. Single molecules of DNA are attached to a flat surface,
               amplified in situ and used as templates for synthetic sequencing
               with fluorescent reversible terminator deoxyribonucleotides.
               Images of the surface are analysed to generate high-quality
               sequence. We demonstrate application of this approach to human
               genome sequencing on flow-sorted X chromosomes and then scale the
               approach to determine the genome sequence of a male Yoruba from
               Ibadan, Nigeria. We build an accurate consensus sequence from
               >30x average depth of paired 35-base reads. We characterize four
               million single-nucleotide polymorphisms and four hundred thousand
               structural variants, many of which were previously unknown. Our
               approach is effective for accurate, rapid and economical
               whole-genome re-sequencing and many other biomedical
               applications.},
  month     = nov,
  year      = 2008,
  url       = {https://www.nature.com/articles/nature07517},
  doi       = {10.1038/nature07517},
  pmc       = {PMC2581791},
  pmid      = 18987734,
  issn      = {0028-0836,1476-4687},
  language  = {en}
}

@article{Van_der_Valk2019-to,
  title    = {Index hopping on the Illumina {HiseqX} platform and its
              consequences for ancient {DNA} studies},
  author   = {van der Valk, Tom and Vezzi, Francesco and Ormestad, Mattias and
              Dalén, Love and Guschanski, Katerina},
  journal  = {Molecular ecology resources},
  abstract = {The high-throughput capacities of the Illumina sequencing
              platforms and the possibility to label samples individually have
              encouraged wide use of sample multiplexing. However, this practice
              results in read misassignment (usually <1\%) across samples
              sequenced on the same lane. Alarmingly high rates of read
              misassignment of up to 10\% were reported for lllumina sequencing
              machines with exclusion amplification chemistry. This may make use
              of these platforms prohibitive, particularly in studies that rely
              on low-quantity and low-quality samples, such as historical and
              archaeological specimens. Here, we use barcodes, short sequences
              that are ligated to both ends of the DNA insert, to directly
              quantify the rate of index hopping in 100-year old
              museum-preserved gorilla (Gorilla beringei) samples. Correcting
              for multiple sources of noise, we identify on average 0.470\% of
              reads containing a hopped index. We show that sample-specific
              quantity of misassigned reads depends on the number of reads that
              any given sample contributes to the total sequencing pool, so that
              samples with few sequenced reads receive the greatest proportion
              of misassigned reads. This particularly affects ancient DNA
              samples, as these frequently differ in their DNA quantity and
              endogenous content. Through simulations we show that even low
              rates of index hopping, as reported here, can lead to biases in
              ancient DNA studies when multiplexing samples with vastly
              different quantities of endogenous material.},
  month    = mar,
  year     = 2019,
  url      = {http://dx.doi.org/10.1111/1755-0998.13009},
  keywords = {ancient DNA; index switching; multiplexing; museum specimens;
              next-generation sequencing; read misassignment},
  doi      = {10.1111/1755-0998.13009},
  pmid     = 30848092,
  issn     = {1755-098X,1755-0998},
  language = {en}
}

@article{Zhang2023-vz,
  title     = {Deduplication improves cost-efficiency and yields of {DE} Novo
               assembly and binning of shotgun metagenomes in microbiome
               research},
  author    = {Zhang, Zhiguo and Zhang, Lu and Zhang, Guoqing and Zhao, Ze and
               Wang, Hui and Ju, Feng},
  journal   = {Microbiology spectrum},
  publisher = {American Society for Microbiology 1752 N St., N.W., Washington,
               DC},
  pages     = {e0428222},
  abstract  = {In the last decade, metagenomics has greatly revolutionized the
               study of microbial communities. However, the presence of
               artificial duplicate reads raised mainly from the preparation of
               metagenomic DNA sequencing libraries and their impacts on
               metagenomic assembly and binning have never been brought to
               attention. Here, we explicitly investigated the effects of
               duplicate reads on metagenomic assemblies and binning based on
               analyses of five groups of representative metagenomes with
               distinct microbiome complexities. Our results showed that
               deduplication considerably increased the binning yields (by 3.5\%
               to 80\%) for most of the metagenomic data sets examined thanks to
               the improved contig length and coverage profiling of
               metagenome-assembled contigs, whereas it slightly decreased the
               binning yields of metagenomes with low complexity (e.g., human
               gut metagenomes). Specifically, 411 versus 397, 331 versus 317,
               104 versus 88, and 9 versus 5 metagenome-assembled genomes (MAGs)
               were recovered from MEGAHIT assemblies of bioreactor sludge,
               surface water, lake sediment, and forest soil metagenomes,
               respectively. Noticeably, deduplication significantly reduced the
               computational costs of the metagenomic assembly, including the
               elapsed time (9.0\% to 29.9\%) and the maximum memory requirement
               (4.3\% to 37.1\%). Collectively, we recommend the removal of
               duplicate reads in metagenomes with high complexity before
               assembly and binning analyses, for example, the forest soil
               metagenomes examined in this study. IMPORTANCE Duplicated reads
               in shotgun metagenomes are usually considered technical
               artifacts. Their presence in metagenomes would theoretically not
               only introduce bias into the quantitative analysis but also
               result in mistakes in the coverage profile, leading to adverse
               effects on or even failures in metagenomic assembly and binning,
               as the widely used metagenome assemblers and binners all need
               coverage information for graph partitioning and assembly binning,
               respectively. However, this issue was seldom noticed, and its
               impacts on downstream essential bioinformatic procedures (e.g.,
               assembly and binning) remained unclear. In this study, we
               comprehensively evaluated for the first time the implications of
               duplicate reads for the de novo assembly and binning of real
               metagenomic data sets by comparing the assembly qualities,
               binning yields, and requirements for computational resources with
               and without the removal of duplicate reads. It was revealed that
               deduplication considerably increased the binning yields of
               metagenomes with high complexity and significantly reduced the
               computational costs, including the elapsed time and the maximum
               memory requirement, for most of the metagenomes studied. These
               results provide empirical references for more cost-efficient
               metagenomic analyses in microbiome research.},
  month     = feb,
  year      = 2023,
  url       = {https://journals.asm.org/doi/10.1128/spectrum.04282-22},
  keywords  = {assembly; binning; duplicate reads; metagenomes; microbiome;
               shotgun metagenomes},
  doi       = {10.1128/spectrum.04282-22},
  pmc       = {PMC10101064},
  pmid      = 36744896,
  issn      = {2165-0497},
  language  = {en}
}


@article{Longo2011-qd,
  title    = {Abundant human {DNA} contamination identified in non-primate
              genome databases},
  author   = {Longo, Mark S and O'Neill, Michael J and O'Neill, Rachel J},
  journal  = {PloS one},
  volume   = 6,
  number   = 2,
  pages    = {e16410},
  abstract = {During routine screens of the NCBI databases using human
              repetitive elements we discovered an unlikely level of nucleotide
              identity across a broad range of phyla. To ascertain whether
              databases containing DNA sequences, genome assemblies and trace
              archive reads were contaminated with human sequences, we performed
              an in depth search for sequences of human origin in non-human
              species. Using a primate specific SINE, AluY, we screened 2,749
              non-primate public databases from NCBI, Ensembl, JGI, and UCSC and
              have found 492 to be contaminated with human sequence. These
              represent species ranging from bacteria (B. cereus) to plants (Z.
              mays) to fish (D. rerio) with examples found from most phyla. The
              identification of such extensive contamination of human sequence
              across databases and sequence types warrants caution among the
              sequencing community in future sequencing efforts, such as human
              re-sequencing. We discuss issues this may raise as well as present
              data that gives insight as to how this may be occurring.},
  month    = feb,
  year     = 2011,
  url      = {http://dx.doi.org/10.1371/journal.pone.0016410},
  doi      = {10.1371/journal.pone.0016410},
  pmc      = {PMC3040168},
  pmid     = 21358816,
  issn     = {1932-6203},
  language = {en}
}

@article{Mukherjee2015-vc,
  title    = {Large-scale contamination of microbial isolate genomes by Illumina
              {PhiX} control},
  author   = {Mukherjee, Supratim and Huntemann, Marcel and Ivanova, Natalia and
              Kyrpides, Nikos C and Pati, Amrita},
  journal  = {Standards in genomic sciences},
  volume   = 10,
  pages    = 18,
  abstract = {With the rapid growth and development of sequencing technologies,
              genomes have become the new go-to for exploring solutions to some
              of the world's biggest challenges such as searching for
              alternative energy sources and exploration of genomic dark matter.
              However, progress in sequencing has been accompanied by its share
              of errors that can occur during template or library preparation,
              sequencing, imaging or data analysis. In this study we screened
              over 18,000 publicly available microbial isolate genome sequences
              in the Integrated Microbial Genomes database and identified more
              than 1000 genomes that are contaminated with PhiX, a control
              frequently used during Illumina sequencing runs. Approximately
              10\% of these genomes have been published in literature and 129
              contaminated genomes were sequenced under the Human Microbiome
              Project. Raw sequence reads are prone to contamination from
              various sources and are usually eliminated during downstream
              quality control steps. Detection of PhiX contaminated genomes
              indicates a lapse in either the application or effectiveness of
              proper quality control measures. The presence of PhiX
              contamination in several publicly available isolate genomes can
              result in additional errors when such data are used in comparative
              genomics analyses. Such contamination of public databases have
              far-reaching consequences in the form of erroneous data
              interpretation and analyses, and necessitates better measures to
              proofread raw sequences before releasing them to the broader
              scientific community.},
  month    = mar,
  year     = 2015,
  url      = {http://dx.doi.org/10.1186/1944-3277-10-18},
  keywords = {Comparative genomics; Contamination; Next-generation sequencing;
              PhiX},
  doi      = {10.1186/1944-3277-10-18},
  pmc      = {PMC4511556},
  pmid     = 26203331,
  issn     = {1944-3277},
  language = {en}
}

@article{Merchant2014-eu,
  title    = {Unexpected cross-species contamination in genome sequencing
              projects},
  author   = {Merchant, Samier and Wood, Derrick E and Salzberg, Steven L},
  journal  = {PeerJ},
  volume   = 2,
  pages    = {e675},
  abstract = {The raw data from a genome sequencing project sometimes contains
              DNA from contaminating organisms, which may be introduced during
              sample collection or sequence preparation. In some instances,
              these contaminants remain in the sequence even after assembly and
              deposition of the genome into public databases. As a result,
              searches of these databases may yield erroneous and confusing
              results. We used efficient microbiome analysis software to scan
              the draft assembly of domestic cow, Bos taurus, and identify 173
              small contigs that appeared to derive from microbial contaminants.
              In the course of verifying these findings, we discovered that one
              genome, Neisseria gonorrhoeae TCDC-NG08107, although putatively a
              complete genome, contained multiple sequences that actually
              derived from the cow and sheep genomes. Our findings illustrate
              the need to carefully validate findings of anomalous DNA that rely
              on comparisons to either draft or finished genomes.},
  month    = nov,
  year     = 2014,
  url      = {http://dx.doi.org/10.7717/peerj.675},
  keywords = {Bioinformatics; DNA sequencing; Genome assembly; Genomics;
              Microbiome; Sequence analysis},
  doi      = {10.7717/peerj.675},
  pmc      = {PMC4243333},
  pmid     = 25426337,
  issn     = {2167-8359},
  language = {en}
}

@article{Steinegger2020-br,
  title    = {Terminating contamination: large-scale search identifies more than
              2,000,000 contaminated entries in {GenBank}},
  author   = {Steinegger, Martin and Salzberg, Steven L},
  journal  = {Genome biology},
  volume   = 21,
  number   = 1,
  pages    = 115,
  abstract = {Genomic analyses are sensitive to contamination in public
              databases caused by incorrectly labeled reference sequences. Here,
              we describe Conterminator, an efficient method to detect and
              remove incorrectly labeled sequences by an exhaustive
              all-against-all sequence comparison. Our analysis reports
              contamination of 2,161,746, 114,035, and 14,148 sequences in the
              RefSeq, GenBank, and NR databases, respectively, spanning the
              whole range from draft to ``complete'' model organism genomes. Our
              method scales linearly with input size and can process 3.3 TB in
              12 days on a 32-core computer. Conterminator can help ensure the
              quality of reference databases. Source code (GPLv3):
              https://github.com/martin-steinegger/conterminator.},
  month    = may,
  year     = 2020,
  url      = {http://dx.doi.org/10.1186/s13059-020-02023-1},
  keywords = {Contamination; GenBank; Genomes; RefSeq; Software},
  doi      = {10.1186/s13059-020-02023-1},
  pmc      = {PMC7218494},
  pmid     = 32398145,
  issn     = {1465-6906},
  language = {en}
}

@article{Breitwieser2019-iz,
  title    = {Human contamination in bacterial genomes has created thousands of
              spurious proteins},
  author   = {Breitwieser, Florian P and Pertea, Mihaela and Zimin, Aleksey and
              Salzberg, Steven L},
  journal  = {Genome research},
  volume   = 29,
  pages    = {954--960},
  abstract = {Contaminant sequences that appear in published genomes can cause
              numerous problems for downstream analyses, particularly for
              evolutionary studies and metagenomics projects. Our large-scale
              scan of complete and draft bacterial and archaeal genomes in the
              NCBI RefSeq database reveals that 2250 genomes are contaminated by
              human sequence. The contaminant sequences derive primarily from
              high-copy human repeat regions, which themselves are not
              adequately represented in the current human reference genome,
              GRCh38. The absence of the sequences from the human assembly
              offers a likely explanation for their presence in bacterial
              assemblies. In some cases, the contaminating contigs have been
              erroneously annotated as containing protein-coding sequences,
              which over time have propagated to create spurious protein
              ``families'' across multiple prokaryotic and eukaryotic genomes.
              As a result, 3437 spurious protein entries are currently present
              in the widely-used nr and TrEMBL protein databases. We report here
              an extensive list of contaminant sequences in bacterial genome
              assemblies and the proteins associated with them. We found that
              nearly all contaminants occurred on small contigs in draft
              genomes, which suggests that filtering out small contigs from
              draft genome assemblies may mitigate the issue of contamination
              while still keeping nearly all of the genuine genomic sequences.},
  month    = may,
  year     = 2019,
  url      = {http://dx.doi.org/10.1101/gr.245373.118},
  doi      = {10.1101/gr.245373.118},
  pmid     = 31064768,
  issn     = {1088-9051,1549-5469},
  language = {en}
}

@article{Kryukov2016-my,
  title    = {Human Contamination in Public Genome Assemblies},
  author   = {Kryukov, Kirill and Imanishi, Tadashi},
  journal  = {PloS one},
  volume   = 11,
  number   = 9,
  pages    = {e0162424},
  abstract = {Contamination in genome assembly can lead to wrong or confusing
              results when using such genome as reference in sequence
              comparison. Although bacterial contamination is well known, the
              problem of human-originated contamination received little
              attention. In this study we surveyed 45,735 available genome
              assemblies for evidence of human contamination. We used lineage
              specificity to distinguish between contamination and conservation.
              We found that 154 genome assemblies contain fragments that with
              high confidence originate as contamination from human DNA.
              Majority of contaminating human sequences were present in the
              reference human genome assembly for over a decade. We recommend
              that existing contaminated genomes should be revised to remove
              contaminated sequence, and that new assemblies should be
              thoroughly checked for presence of human DNA before submitting
              them to public databases.},
  month    = sep,
  year     = 2016,
  url      = {http://dx.doi.org/10.1371/journal.pone.0162424},
  doi      = {10.1371/journal.pone.0162424},
  pmc      = {PMC5017631},
  pmid     = 27611326,
  issn     = {1932-6203},
  language = {en}
}

@article{Chen2018-vg,
  title     = {fastp: an ultra-fast all-in-one {FASTQ} preprocessor},
  author    = {Chen, Shifu and Zhou, Yanqing and Chen, Yaru and Gu, Jia},
  journal   = {Bioinformatics},
  publisher = {Oxford University Press},
  volume    = 34,
  number    = 17,
  pages     = {i884--i890},
  abstract  = {Quality control and preprocessing of FASTQ files are essential to
               providing clean data for downstream analysis. Traditionally, a
               different tool is used for each operation, such as quality
               control, adapter trimming and quality filtering. These tools are
               often insufficiently fast as most are developed using high-level
               programming languages (e.g. Python and Java) and provide limited
               multi-threading support. Reading and loading data multiple times
               also renders preprocessing slow and I/O inefficient.We developed
               fastp as an ultra-fast FASTQ preprocessor with useful quality
               control and data-filtering features. It can perform quality
               control, adapter trimming, quality filtering, per-read quality
               pruning and many other operations with a single scan of the FASTQ
               data. This tool is developed in C++ and has multi-threading
               support. Based on our evaluation, fastp is 2–5 times faster than
               other FASTQ preprocessing tools such as Trimmomatic or Cutadapt
               despite performing far more operations than similar tools.The
               open-source code and corresponding instructions are available at
               https://github.com/OpenGene/fastp.},
  month     = sep,
  year      = 2018,
  url       = {https://academic.oup.com/bioinformatics/article/34/17/i884/5093234},
  doi       = {10.1093/bioinformatics/bty560},
  issn      = {1367-4803}
}

@article{Andreeva2022-oe,
  title     = {Methodologies for ancient {DNA} extraction from bones for genomic
               analysis: Approaches and guidelines},
  author    = {Andreeva, T V and Malyarchuk, A B and Soshkina, A D and Dudko, N
               A and Plotnikova, M Yu and Rogaev, E I},
  journal   = {Russian journal of genetics},
  publisher = {Pleiades Publishing Ltd},
  volume    = 58,
  number    = 9,
  pages     = {1017--1035},
  abstract  = {Abstract The emergence and development of massive parallel
               sequencing (new generation sequencing) methods have opened up new
               prospects in the study of ancient organisms, including extinct
               ones. Numerous skeletal remains from archaeological and museum
               collections are often the only source of information on ancient
               species and populations. In this review, we discuss the features
               of human bone tissue and the advantages and disadvantages of bone
               material as a source of DNA for genomic analysis of ancient
               people. Here we present new methodological approaches to DNA
               extraction from ancient human skeletal remains and its
               preparation for large-scale parallel sequencing are presented, as
               well as prospects and directions for further research in a new
               interdisciplinary field, paleogenomics.},
  month     = sep,
  year      = 2022,
  url       = {https://link.springer.com/article/10.1134/S1022795422090034},
  doi       = {10.1134/s1022795422090034},
  issn      = {1022-7954,1608-3369},
  language  = {en}
}

@article{Danaeifar2022-sk,
  title     = {New horizons in developing cell lysis methods: A review},
  author    = {Danaeifar, Mohsen},
  journal   = {Biotechnology and bioengineering},
  publisher = {Wiley},
  volume    = 119,
  number    = 11,
  pages     = {3007--3021},
  abstract  = {Cell lysis is an essential step in many studies related to
               biology and medicine. Based on the scale and medium that cell
               lysis is carried out, there are three main types of the cell
               lysis: (1) lysis of the cells in the surrounding environment, (2)
               lysis of the isolated or cultured cells, and (3) single cell
               lysis. Conventionally, several cell lysis methods have been
               developed, such as freeze-thawing, bead beating, incursion in
               liquid nitrogen, sonication, and enzymatic and chemical-based
               approaches. In recent years, various novel technologies have been
               employed to develop new methods of cell lysis. The aim of studies
               in this field is to introduce more precise and efficient tools or
               to reduce the costs of cell lysis procedures. Nanostructure-based
               lysis methods, acoustic oscillation, electrical current,
               irradiation, bacteria-mediated cell lysis, magnetic ionic
               liquids, bacteriophage genes, monolith columns, hydraulic forces,
               and steam explosion are some examples of newly developed cell
               lysis methods. Besides the significant advances in this field,
               there are still many challenges and tools must be further
               improved.},
  month     = nov,
  year      = 2022,
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bit.28198},
  keywords  = {cell lysis; membrane disruption; microfluidics; new methods},
  doi       = {10.1002/bit.28198},
  pmid      = 35900072,
  issn      = {1097-0290,0006-3592},
  language  = {en}
}

@article{Schloss2020-gp,
  title     = {Cultivating {DNA} sequencing technology after the Human Genome
               Project},
  author    = {Schloss, Jeffery A and Gibbs, Richard A and Makhijani, Vinod B
               and Marziali, Andre},
  journal   = {Annual review of genomics and human genetics},
  publisher = {Annual Reviews},
  volume    = 21,
  number    = 1,
  pages     = {117--138},
  abstract  = {When the Human Genome Project was completed in 2003, automated
               Sanger DNA sequencing with fluorescent dye labels was the
               dominant technology. Several nascent alternative methods based on
               older ideas that had not been fully developed were the focus of
               technical researchers and companies. Funding agencies recognized
               the dynamic nature of technology development and that, beyond the
               Human Genome Project, there were growing opportunities to deploy
               DNA sequencing in biological research. Consequently, the National
               Human Genome Research Institute of the National Institutes of
               Health created a program-widely known as the Advanced Sequencing
               Technology Program-that stimulated all stages of development of
               new DNA sequencing methods, from innovation to advanced
               manufacturing and production testing, with the goal of reducing
               the cost of sequencing a human genome first to $100,000 and then
               to $1,000. The events of this period provide a powerful example
               of how judicious funding of academic and commercial partners can
               rapidly advance core technology developments that lead to
               profound advances across the scientific landscape.},
  month     = aug,
  year      = 2020,
  url       = {https://www.annualreviews.org/content/journals/10.1146/annurev-genom-111919-082433},
  keywords  = {\$1,000 genome technology program; Advanced Sequencing Technology
               Program; National Human Genome Research Institute; National
               Institutes of Health; funding agency; funding model; nanopore DNA
               sequencing; next-generation DNA sequencing; peer review},
  doi       = {10.1146/annurev-genom-111919-082433},
  pmid      = 32283947,
  issn      = {1527-8204,1545-293X},
  language  = {en}
}

@article{Holt2008-wk,
  title     = {The new paradigm of flow cell sequencing},
  author    = {Holt, Robert A and Jones, Steven J M},
  journal   = {Genome research},
  publisher = {Cold Spring Harbor Laboratory},
  volume    = 18,
  number    = 6,
  pages     = {839--846},
  abstract  = {DNA sequencing is in a period of rapid change, in which capillary
               sequencing is no longer the technology of choice for most
               ultra-high-throughput applications. A new generation of
               instruments that utilize primed synthesis in flow cells to
               obtain, simultaneously, the sequence of millions of different DNA
               templates has changed the field. We compare and contrast these
               new sequencing platforms in terms of stage of development,
               instrument configuration, template format, sequencing chemistry,
               throughput capability, operating cost, data handling issues, and
               error models. While these platforms outperform capillary
               instruments in terms of bases per day and cost per base, the
               short length of sequence reads obtained from most instruments and
               the limited number of samples that can be run simultaneously
               imposes some practical constraints on sequencing applications.
               However, recently developed methods for paired-end sequencing and
               for array-based direct selection of desired templates from
               complex mixtures extend the utility of these platforms for genome
               analysis. Given the ever increasing demand for DNA sequence
               information, we can expect continuous improvement of this new
               generation of instruments and their eventual replacement by even
               more powerful technology.},
  month     = jun,
  year      = 2008,
  url       = {http://genome.cshlp.org/content/18/6/839.abstract},
  doi       = {10.1101/gr.073262.107},
  pmid      = 18519653,
  issn      = {1088-9051,1549-5469},
  language  = {en}
}

@misc{Andrews2010-pd,
  title  = {{FastQC}: A quality control tool for high throughput sequence data},
  author = {Andrews, Simon},
  year   = 2010,
  url    = {http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}
}

@article{Maixner2021-sg,
  title    = {Hallstatt miners consumed blue cheese and beer during the Iron Age
              and retained a non-Westernized gut microbiome until the Baroque
              period},
  author   = {Maixner, Frank and Sarhan, Mohamed S and Huang, Kun D and Tett,
              Adrian and Schoenafinger, Alexander and Zingale, Stefania and
              Blanco-Míguez, Aitor and Manghi, Paolo and Cemper-Kiesslich, Jan
              and Rosendahl, Wilfried and Kusebauch, Ulrike and Morrone, Seamus
              R and Hoopmann, Michael R and Rota-Stabelli, Omar and Rattei,
              Thomas and Moritz, Robert L and Oeggl, Klaus and Segata, Nicola
              and Zink, Albert and Reschreiter, Hans and Kowarik, Kerstin},
  journal  = {Current biology: CB},
  abstract = {We subjected human paleofeces dating from the Bronze Age to the
              Baroque period (18th century AD) to in-depth microscopic,
              metagenomic, and proteomic analyses. The paleofeces were preserved
              in the underground salt mines of the UNESCO World Heritage site of
              Hallstatt in Austria. This allowed us to reconstruct the diet of
              the former population and gain insights into their ancient gut
              microbiome composition. Our dietary survey identified bran and
              glumes of different cereals as some of the most prevalent plant
              fragments. This highly fibrous, carbohydrate-rich diet was
              supplemented with proteins from broad beans and occasionally with
              fruits, nuts, or animal food products. Due to these traditional
              dietary habits, all ancient miners up to the Baroque period have
              gut microbiome structures akin to modern non-Westernized
              individuals whose diets are also mainly composed of unprocessed
              foods and fresh fruits and vegetables. This may indicate a shift
              in the gut community composition of modern Westernized populations
              due to quite recent dietary and lifestyle changes. When we
              extended our microbial survey to fungi present in the paleofeces,
              in one of the Iron Age samples, we observed a high abundance of
              Penicillium roqueforti and Saccharomyces cerevisiae DNA.
              Genome-wide analysis indicates that both fungi were involved in
              food fermentation and provides the first molecular evidence for
              blue cheese and beer consumption in Iron Age Europe.},
  month    = oct,
  year     = 2021,
  url      = {http://dx.doi.org/10.1016/j.cub.2021.09.031},
  keywords = {Hallstatt; beer; cheese; diet; fermented food; microbiome;
              paleofeces; protohistory; salt mine},
  doi      = {10.1016/j.cub.2021.09.031},
  pmid     = 34648730,
  issn     = {0960-9822,1879-0445},
  language = {en}
}

@article{Bronner2013-sv,
  title     = {Improved protocols for Illumina sequencing},
  author    = {Bronner, Iraad F and Quail, Michael A and Turner, Daniel J and
               Swerdlow, Harold},
  journal   = {et al [Current protocols in human genetics]},
  publisher = {Wiley},
  volume    = 79,
  number    = 1,
  pages     = {18.2.1--18.2.42},
  abstract  = {AbstractIn this unit, we describe a set of improvements that have
               been made to the standard Illumina protocols to make the
               sequencing process more reliable in a high‐throughput
               environment, reduce amplification bias, narrow the distribution
               of insert sizes, and reliably obtain high yields of data. Curr.
               Protoc. Hum. Genet. 79:18.2.1‐18.2.42. © 2013 by John Wiley \&
               Sons, Inc.},
  month     = oct,
  year      = 2013,
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471142905.hg1802s79},
  keywords  = {Illumina; next-generation; sequencer; protocols},
  doi       = {10.1002/0471142905.hg1802s79},
  issn      = {1934-8258,1934-8266},
  language  = {en}
}

@article{Van_der_Valk2018-xx,
  author       = {Valk, Tom van der and Vezzi, Francesco and Ormestad, Mattias and Dal{\'e}n, Love and Guschanski, Katerina},
  title        = {Index hopping on the Illumina HiseqX platform and its consequences for ancient DNA studies},
  elocation-id = {179028},
  year         = {2018},
  doi          = {10.1101/179028},
  publisher    = {Cold Spring Harbor Laboratory},
  abstract     = {The high-throughput capacities of the Illumina sequencing platforms and the possibility to label samples individually have encouraged a wide use of sample multiplexing. However, this practice results in read misassignment (usually \&lt;1\%) across samples sequenced on the same lane. Alarmingly high rates of read misassignment of up to 10\% were reported for the latest generation of lllumina sequencing machines. This may make future use of the newest generation of platforms prohibitive, particularly in studies that rely on low quantity and quality samples, such as historical and archaeological specimens. Here, we rely on barcodes, short sequences that are ligated to both ends of the DNA insert, to directly quantify the rate of index hopping in 100-year old museum-preserved gorilla (Gorilla beringei) samples. Correcting for multiple sources of noise, we identify on average 0.470\% of reads containing a hopped index. We show that sample-specific quantity of misassigned reads depend on the number of reads that any given sample contributes to the total sequencing pool, so that samples with few sequenced reads receive the greatest proportion of misassigned reads. Ancient DNA samples are particularly affected, since they often differ widely in endogenous content. Through extensive simulations we show that even low index-hopping rates lead to biases in ancient DNA studies when multiplexing samples with different quantities of input material.},
  url          = {https://www.biorxiv.org/content/early/2018/09/10/179028},
  eprint       = {https://www.biorxiv.org/content/early/2018/09/10/179028.full.pdf},
  journal      = {bioRxiv}
}


@article{Dabney2013-zo,
  title    = {Ancient {DNA} damage},
  author   = {Dabney, Jesse and Meyer, Matthias and Pääbo, Svante},
  journal  = {Cold Spring Harbor perspectives in biology},
  volume   = 5,
  number   = 7,
  abstract = {Under favorable conditions DNA can survive for thousands of years
              in the remains of dead organisms. The DNA extracted from such
              remains is invariably degraded to a small average size by
              processes that at least partly involve depurination. It also
              contains large amounts of deaminated cytosine residues that are
              accumulated toward the ends of the molecules, as well as several
              other lesions that are less well characterized.},
  month    = jul,
  year     = 2013,
  url      = {http://dx.doi.org/10.1101/cshperspect.a012567},
  doi      = {10.1101/cshperspect.a012567},
  pmc      = {PMC3685887},
  pmid     = 23729639,
  issn     = {1943-0264},
  language = {en}
}

@article{Shendure2011-om,
  title     = {Overview of {DNA} sequencing strategies},
  author    = {Shendure, Jay A and Porreca, Gregory J and Church, George M and
               Gardner, Andrew F and Hendrickson, Cynthia L and Kieleczawa, Jan
               and Slatko, Barton E},
  journal   = {et al [Current protocols in molecular biology]},
  publisher = {Wiley},
  volume    = {Chapter 7},
  number    = 1,
  pages     = {Unit7.1},
  abstract  = {Efficient and cost-effective DNA sequencing technologies are
               critical to the progress of molecular biology. This overview of
               DNA sequencing strategies provides a high-level review of seven
               distinct approaches to DNA sequencing: (a) dideoxy sequencing;
               (b) solid phase sequencing; (c) sequencing-by-hybridization; (d)
               mass spectrometry; (e) cyclic array sequencing; (f)
               microelectrophoresis; and (g) nanopore sequencing. Other
               platforms currently in development are also briefly described.
               The primary focus here is on Sanger dideoxy sequencing, which has
               been the dominant technology since 1977, and on cyclic array
               strategies, for which several competitive implementations have
               been developed since 2005. Because the field of DNA sequencing is
               changing rapidly, this unit represents a snapshot as of
               September, 2011.},
  month     = oct,
  year      = 2011,
  url       = {http://dx.doi.org/10.1002/0471142727.mb0701s96},
  keywords  = {DNA sequencing; next-generation sequencing; polony; cyclic array
               sequencing; genomics; sequencing by ligation; nanopore sequencing},
  doi       = {10.1002/0471142727.mb0701s96},
  pmid      = 21987056,
  issn      = {1934-3647,1934-3639},
  language  = {en}
}

@incollection{Valencia2013-wk,
  title     = {Sanger sequencing principles, history, and landmarks},
  author    = {Valencia, C Alexander and Pervaiz, M Ali and Husami, Ammar and
               Qian, Yaping and Zhang, Kejian},
  booktitle = {SpringerBriefs in Genetics},
  publisher = {Springer New York},
  address   = {New York, NY},
  pages     = {3--11},
  abstract  = {The first DNA sequencing (1968) was performed 15 years after the
               discovery of the double helix (1953) (Hutchison 2007). However,
               the chemical method of Maxam and Gilbert and the dideoxy method
               of Sanger began in the mid-1970s (Fig. 1.1). The profound
               insights into genetic organization were shown by Nicklen and
               Coulson with the first complete DNA sequence of phage ϕX174. As
               sequencing output improved larger molecules greater than 200 kb
               (human cytomegalovirus) were sequenced and computational analysis
               and bioinformatics was born. Sequencing efforts reached new
               heights with the initiation of the US Human Genome Project
               culminating in the first “sequencing factory” by 1992 (Hutchison
               2007). With this effort came the sequencing of the first
               bacterial genome, by 1995, and other small eubacterial,
               archaebacterial, and eukaryotic genomes soon thereafter.
               Published in 2001, the working draft of the human genome sequence
               was the result of the competition between the public Human Genome
               Project and Celera Genomics (Fig. 1.1). The new “massively
               parallel” sequencing methods (Chap. 2) are greatly increasing
               sequencing capacity, but further innovations are needed to
               achieve the “thousand dollar genome” that many feel is the
               prerequisite to personalized genomic medicine (Fig. 1.1). These
               advances will also allow new approaches to a variety of problems
               in biology, evolution, and the environment.},
  year      = 2013,
  url       = {http://dx.doi.org/10.1007/978-1-4614-9032-6_1},
  doi       = {10.1007/978-1-4614-9032-6\_1},
  isbn      = {9781461490319,9781461490326},
  issn      = {2191-5571,2191-5563},
  language  = {en}
}

@article{Koboldt2013-kk,
  title     = {The Next-Generation Sequencing Revolution and Its Impact on
               Genomics},
  author    = {Koboldt, Daniel C and Steinberg, Karyn Meltz and Larson, David E
               and Wilson, Richard K and Mardis, Elaine R},
  journal   = {Cell},
  publisher = {Elsevier},
  volume    = 155,
  number    = 1,
  pages     = {27--38},
  abstract  = {Genomics is a relatively new scientific discipline, having DNA
               sequencing as its core technology. As technology has improved the
               cost and scale of genome characterization over sequencing's
               40-year history, the scope of inquiry has commensurately
               broadened. Massively parallel sequencing has proven
               revolutionary, shifting the paradigm of genomics to address
               biological questions at a genome-wide scale. Sequencing now
               empowers clinical diagnostics and other aspects of medical care,
               including disease risk, therapeutic identification, and prenatal
               testing. This Review explores the current state of genomics in
               the massively parallel sequencing era.},
  month     = sep,
  year      = 2013,
  url       = {http://dx.doi.org/10.1016/j.cell.2013.09.006},
  doi       = {10.1016/j.cell.2013.09.006},
  pmid      = 24074859,
  issn      = {0092-8674,1097-4172},
  language  = {en}
}

@article{Church1988-cr,
  title     = {Multiplex {DNA} sequencing},
  author    = {Church, G M and Kieffer-Higgins, S},
  journal   = {Science (New York, N.Y.)},
  publisher = {American Association for the Advancement of Science (AAAS)},
  volume    = 240,
  number    = 4849,
  pages     = {185--188},
  abstract  = {The increasing demand for DNA sequences can be met by replacement
               of each DNA sample in a device with a mixture of N samples so
               that the normal throughput is increased by a factor of N. Such a
               method is described. In order to separate the sequence
               information at the end of the processing, the DNA molecules of
               interest are ligated to a set of oligonucleotide ``tags'' at the
               beginning. The tagged DNA molecules are pooled, amplified, and
               chemically fragmented in 96-well plates. The resulting reaction
               products are fractionated by size on sequencing gels and
               transferred to nylon membranes. These membranes are then probed
               as many times as there are types of tags in the original pools,
               producing, in each cycle of probing, autoradiographs similar to
               those from standard DNA sequencing methods. Thus, each reaction
               and gel yields a quantity of data equivalent to that obtained
               from conventional reactions and gels multiplied by the number of
               probes used. To date, even after 50 successive probings, the
               original signal strength and the image quality are retained, an
               indication that the upper limit for the number of reprobings may
               be considerably higher.},
  month     = apr,
  year      = 1988,
  url       = {http://dx.doi.org/10.1126/science.3353714},
  doi       = {10.1126/science.3353714},
  pmid      = 3353714,
  issn      = {0036-8075,1095-9203},
  language  = {en}
}

@article{Fu2025-oq,
  title    = {A recurrent sequencing artifact on Illumina sequencers with
              two-color fluorescent dye chemistry and its impact on somatic
              variant detection},
  author   = {Fu, Beverly J and Viswanadham, Vinayak V and Maziec, Dominika and
              Jin, Hu and Park, Peter J},
  journal  = {bioRxiv.org: the preprint server for biology},
  pages    = {2025.09.27.678978},
  abstract = {Background: The sequencing-by-synthesis technology by Illumina,
              Inc. enables efficient and scalable readouts of mutations from
              genomic data. To enhance sequencing speed and efficiency, Illumina
              has shifted from the four-color base calling chemistry of the
              HiSeq series to a two-color fluorescent dye chemistry in the
              NovaSeq series. Benchmarking sequencing artifacts due to biases in
              the newer chemistry is important to evaluate the quality of
              identified mutations. Results: We re-analyzed a series of
              whole-genome sequencing experiments in which the same samples were
              sequenced on the NovaSeq 6000 (two-color) and HiSeq X10
              (four-color) platforms by independent groups. In several samples,
              we observed a higher frequency of T-to-G and A-to-C substitutions
              (``T>G'') at the read level for NovaSeq 6000 versus HiSeq X10. As
              the per-base error rate is still low, the artifactual
              substitutions have a negligible effect in identifying germline or
              high variant allele frequency (VAF) somatic mutations. However,
              such errors can confound the detection of low-VAF somatic variants
              in high-depth sequencing samples, particularly in studies of
              mosaic mutations in normal tissues, where variants have low read
              support and are called without a matched normal. The artifactual
              T>G variant calls disproportionately occur at NT[TG]
              trinucleotides, and we leveraged this observation to
              bioinformatically reduce the T>G excess in somatic mutation
              callsets. Conclusions: We identified a recurrent artifact specific
              to the Illumina two-color chemistry platform on the NovaSeq 6000
              with the potential to contaminate low-VAF somatic mutation calls.
              Thus, an unexpected enrichment of T>G mutations in mosaicism
              studies warrants caution.},
  month    = sep,
  year     = 2025,
  url      = {https://www.biorxiv.org/content/10.1101/2025.09.27.678978v1.abstract},
  keywords = {Illumina NovaSeq 6000; Next-generation sequencing; mosaic
              mutations; sequencing artifacts; somatic mutations},
  doi      = {10.1101/2025.09.27.678978},
  pmc      = {PMC12621940},
  pmid     = 41256534,
  language = {en}
}

@article{Ewing1998-su,
  title     = {Base-calling of automated sequencer traces using phred. {II}.
               Error probabilities},
  author    = {Ewing, B and Green, P},
  journal   = {Genome research},
  publisher = {Cold Spring Harbor Laboratory},
  volume    = 8,
  number    = 3,
  pages     = {186--194},
  abstract  = {Elimination of the data processing bottleneck in high-throughput
               sequencing will require both improved accuracy of data processing
               software and reliable measures of that accuracy. We have
               developed and implemented in our base-calling program phred the
               ability to estimate a probability of error for each base-call, as
               a function of certain parameters computed from the trace data.
               These error probabilities are shown here to be valid (correspond
               to actual error rates) and to have high power to discriminate
               correct base-calls from incorrect ones, for read data collected
               under several different chemistries and electrophoretic
               conditions. They play a critical role in our assembly program
               phrap and our finishing program consed.},
  month     = mar,
  year      = 1998,
  url       = {http://dx.doi.org/10.1101/gr.8.3.186},
  doi       = {10.1101/gr.8.3.186},
  pmid      = 9521922,
  issn      = {1088-9051,1549-5469},
  language  = {en}
}

@article{De-Kayne2021-pc,
  title     = {Sequencing platform shifts provide opportunities but pose
               challenges for combining genomic data sets},
  author    = {De-Kayne, Rishi and Frei, David and Greenway, Ryan and Mendes,
               Sofia L and Retel, Cas and Feulner, Philine G D},
  journal   = {Molecular Ecology Resources},
  publisher = {John Wiley \& Sons, Ltd},
  volume    = 21,
  number    = 3,
  pages     = {653--660},
  abstract  = {Technological advances in DNA sequencing over the last decade now
               permit the production and curation of large genomic data sets in
               an increasing number of nonmodel species. Additionally, these new
               data provide the opportunity for combining data sets, resulting
               in larger studies with a broader taxonomic range. Whilst the
               development of new sequencing platforms has been beneficial,
               resulting in a higher throughput of data at a lower per-base
               cost, shifts in sequencing technology can also pose challenges
               for those wishing to combine new sequencing data with data
               sequenced on older platforms. Here, we outline the types of
               studies where the use of curated data might be beneficial, and
               highlight potential biases that might be introduced by combining
               data from different sequencing platforms. As an example of the
               challenges associated with combining data across sequencing
               platforms, we focus on the impact of the shift in Illumina's base
               calling technology from a four-channel system to a two-channel
               system. We caution that when data are combined from these two
               systems, erroneous guanine base calls that result from the
               two-channel chemistry can make their way through a bioinformatic
               pipeline, eventually leading to inaccurate and potentially
               misleading conclusions. We also suggest solutions for dealing
               with such potential artefacts, which make samples sequenced on
               different sequencing platforms appear more differentiated from
               one another than they really are. Finally, we stress the
               importance of archiving tissue samples and the associated
               sequences for the continued reproducibility and reusability of
               sequencing data in the face of ever-changing sequencing platform
               technology.},
  month     = apr,
  year      = 2021,
  url       = {http://dx.doi.org/10.1111/1755-0998.13309},
  keywords  = {HiSeq; NGS; NovaSeq; poly-G; reproducibility; reusability},
  doi       = {10.1111/1755-0998.13309},
  issn      = {1755-0998},
  language  = {en}
}

@article{Metzker2010-qw,
  title     = {Sequencing technologies - the next generation},
  author    = {Metzker, Michael L},
  journal   = {Nature reviews. Genetics},
  publisher = {Springer Science and Business Media LLC},
  volume    = 11,
  number    = 1,
  pages     = {31--46},
  abstract  = {Demand has never been greater for revolutionary technologies that
               deliver fast, inexpensive and accurate genome information. This
               challenge has catalysed the development of next-generation
               sequencing (NGS) technologies. The inexpensive production of
               large volumes of sequence data is the primary advantage over
               conventional methods. Here, I present a technical review of
               template preparation, sequencing and imaging, genome alignment
               and assembly approaches, and recent advances in current and
               near-term commercially available NGS instruments. I also outline
               the broad range of applications for NGS technologies, in addition
               to providing guidelines for platform selection to address
               biological questions of interest.},
  month     = jan,
  year      = 2010,
  url       = {http://dx.doi.org/10.1038/nrg2626},
  doi       = {10.1038/nrg2626},
  pmid      = 19997069,
  issn      = {1471-0056,1471-0064},
  language  = {en}
}

@article{Cacho2016-ci,
  title     = {A comparison of base-calling algorithms for Illumina sequencing
               technology},
  author    = {Cacho, Ashley and Smirnova, Ekaterina and Huzurbazar, Snehalata
               and Cui, Xinping},
  journal   = {Briefings in bioinformatics},
  publisher = {Oxford Academic},
  volume    = 17,
  number    = 5,
  pages     = {786--795},
  abstract  = {Recent advances in next-generation sequencing technology have
               yielded increasing cost-effectiveness and higher throughput
               produced per run, in turn, greatly influencing the analysis of
               DNA sequences. Among the various sequencing technologies,
               Illumina is by far the most widely used platform. However, the
               Illumina sequencing platform suffers from several imperfections
               that can be attributed to the chemical processes inherent to the
               sequencing-by-synthesis technology. With the enormous amounts of
               reads produced, statistical methodologies and computationally
               efficient algorithms are required to improve the accuracy and
               speed of base-calling. Over the past few years, several papers
               have proposed methods to model the various imperfections, giving
               rise to accurate and/or efficient base-calling algorithms. In
               this article, we provide a comprehensive comparison of the
               performance of recently developed base-callers and we present a
               general statistical model that unifies a large majority of these
               base-callers.},
  month     = sep,
  year      = 2016,
  url       = {https://dx.doi.org/10.1093/bib/bbv088},
  keywords  = {Illumina; base-calling; next-generation sequencing},
  doi       = {10.1093/bib/bbv088},
  pmid      = 26443614,
  issn      = {1467-5463,1477-4054},
  language  = {en}
}

@article{Magoc2011-xl,
  title     = {{FLASH}: fast length adjustment of short reads to improve genome
               assemblies},
  author    = {Magoč, Tanja and Salzberg, Steven L},
  journal   = {Bioinformatics (Oxford, England)},
  publisher = {Oxford University Press (OUP)},
  volume    = 27,
  number    = 21,
  pages     = {2957--2963},
  abstract  = {MOTIVATION: Next-generation sequencing technologies generate very
               large numbers of short reads. Even with very deep genome
               coverage, short read lengths cause problems in de novo
               assemblies. The use of paired-end libraries with a fragment size
               shorter than twice the read length provides an opportunity to
               generate much longer reads by overlapping and merging read pairs
               before assembling a genome. RESULTS: We present FLASH, a fast
               computational tool to extend the length of short reads by
               overlapping paired-end reads from fragment libraries that are
               sufficiently short. We tested the correctness of the tool on one
               million simulated read pairs, and we then applied it as a
               pre-processor for genome assemblies of Illumina reads from the
               bacterium Staphylococcus aureus and human chromosome 14. FLASH
               correctly extended and merged reads >99\% of the time on
               simulated reads with an error rate of <1\%. With adequately set
               parameters, FLASH correctly merged reads over 90\% of the time
               even when the reads contained up to 5\% errors. When FLASH was
               used to extend reads prior to assembly, the resulting assemblies
               had substantially greater N50 lengths for both contigs and
               scaffolds. AVAILABILITY AND IMPLEMENTATION: The FLASH system is
               implemented in C and is freely available as open-source code at
               http://www.cbcb.umd.edu/software/flash. CONTACT:
               t.magoc@gmail.com.},
  month     = nov,
  year      = 2011,
  url       = {https://academic.oup.com/bioinformatics/article-pdf/27/21/2957/48863763/bioinformatics_27_21_2957.pdf},
  doi       = {10.1093/bioinformatics/btr507},
  pmc       = {PMC3198573},
  pmid      = 21903629,
  issn      = {1367-4803,1367-4811},
  language  = {en}
}

@article{Ayling2020-ra,
  title    = {New approaches for metagenome assembly with short reads},
  author   = {Ayling, Martin and Clark, Matthew D and Leggett, Richard M},
  journal  = {Briefings in bioinformatics},
  volume   = 21,
  number   = 2,
  pages    = {584--594},
  abstract = {In recent years, the use of longer range read data combined with
              advances in assembly algorithms has stimulated big improvements in
              the contiguity and quality of genome assemblies. However, these
              advances have not directly transferred to metagenomic data sets,
              as assumptions made by the single genome assembly algorithms do
              not apply when assembling multiple genomes at varying levels of
              abundance. The development of dedicated assemblers for metagenomic
              data was a relatively late innovation and for many years,
              researchers had to make do using tools designed for single
              genomes. This has changed in the last few years and we have seen
              the emergence of a new type of tool built using different
              principles. In this review, we describe the challenges inherent in
              metagenomic assemblies and compare the different approaches taken
              by these novel assembly tools.},
  month    = mar,
  year     = 2020,
  url      = {http://dx.doi.org/10.1093/bib/bbz020},
  keywords = {Metagenomics; algorithms; assembly; sequencing},
  doi      = {10.1093/bib/bbz020},
  pmid     = 30815668,
  issn     = {1467-5463,1477-4054},
  language = {en}
}

@article{Rougemont2008-ug,
  title     = {Probabilistic base calling of Solexa sequencing data},
  author    = {Rougemont, Jacques and Amzallag, Arnaud and Iseli, Christian and
               Farinelli, Laurent and Xenarios, Ioannis and Naef, Felix},
  journal   = {BMC bioinformatics},
  publisher = {Springer Science and Business Media LLC},
  volume    = 9,
  number    = 1,
  pages     = 431,
  abstract  = {BACKGROUND: Solexa/Illumina short-read ultra-high throughput DNA
               sequencing technology produces millions of short tags (up to 36
               bases) by parallel sequencing-by-synthesis of DNA colonies. The
               processing and statistical analysis of such high-throughput data
               poses new challenges; currently a fair proportion of the tags are
               routinely discarded due to an inability to match them to a
               reference sequence, thereby reducing the effective throughput of
               the technology. RESULTS: We propose a novel base calling
               algorithm using model-based clustering and probability theory to
               identify ambiguous bases and code them with IUPAC symbols. We
               also select optimal sub-tags using a score based on information
               content to remove uncertain bases towards the ends of the reads.
               CONCLUSION: We show that the method improves genome coverage and
               number of usable tags as compared with Solexa's data processing
               pipeline by an average of 15\%. An R package is provided which
               allows fast and accurate base calling of Solexa's fluorescence
               intensity files and the production of informative diagnostic
               plots.},
  month     = oct,
  year      = 2008,
  url       = {http://dx.doi.org/10.1186/1471-2105-9-431},
  doi       = {10.1186/1471-2105-9-431},
  pmc       = {PMC2575221},
  pmid      = 18851737,
  issn      = {1471-2105,1471-2105},
  language  = {en}
}


@article{Cock2010-qs,
  title     = {The Sanger {FASTQ} file format for sequences with quality scores,
               and the Solexa/Illumina {FASTQ} variants},
  author    = {Cock, Peter J A and Fields, Christopher J and Goto, Naohisa and
               Heuer, Michael L and Rice, Peter M},
  journal   = {Nucleic acids research},
  publisher = {Oxford University Press (OUP)},
  volume    = 38,
  number    = 6,
  pages     = {1767--1771},
  abstract  = {FASTQ has emerged as a common file format for sharing sequencing
               read data combining both the sequence and an associated per base
               quality score, despite lacking any formal definition to date, and
               existing in at least three incompatible variants. This article
               defines the FASTQ format, covering the original Sanger standard,
               the Solexa/Illumina variants and conversion between them, based
               on publicly available information such as the MAQ documentation
               and conventions recently agreed by the Open Bioinformatics
               Foundation projects Biopython, BioPerl, BioRuby, BioJava and
               EMBOSS. Being an open access publication, it is hoped that this
               description, with the example files provided as Supplementary
               Data, will serve in future as a reference for this important file
               format.},
  month     = apr,
  year      = 2010,
  url       = {https://dx.doi.org/10.1093/nar/gkp1137},
  keywords  = {personality character; color},
  doi       = {10.1093/nar/gkp1137},
  pmc       = {PMC2847217},
  pmid      = 20015970,
  issn      = {0305-1048,1362-4962},
  language  = {en}
}

@article{Weyrich2019-am,
  title    = {Laboratory contamination over time during low-biomass sample
              analysis},
  author   = {Weyrich, Laura S and Farrer, Andrew G and Eisenhofer, Raphael and
              Arriola, Luis A and Young, Jennifer and Selway, Caitlin A and
              Handsley-Davis, Matilda and Adler, Christina J and Breen, James
              and Cooper, Alan},
  journal  = {Molecular ecology resources},
  volume   = 19,
  number   = 4,
  pages    = {982--996},
  abstract = {Bacteria are not only ubiquitous on earth but can also be
              incredibly diverse within clean laboratories and reagents. The
              presence of both living and dead bacteria in laboratory
              environments and reagents is especially problematic when examining
              samples with low endogenous content (e.g., skin swabs, tissue
              biopsies, ice, water, degraded forensic samples or ancient
              material), where contaminants can outnumber endogenous
              microorganisms within samples. The contribution of contaminants
              within high-throughput studies remains poorly understood because
              of the relatively low number of contaminant surveys. Here, we
              examined 144 negative control samples (extraction blank and
              no-template amplification controls) collected in both typical
              molecular laboratories and an ultraclean ancient DNA laboratory
              over 5 years to characterize long-term contaminant diversity. We
              additionally compared the contaminant content within a home-made
              silica-based extraction method, commonly used to analyse low
              endogenous content samples, with a widely used commercial DNA
              extraction kit. The contaminant taxonomic profile of the
              ultraclean ancient DNA laboratory was unique compared to modern
              molecular biology laboratories, and changed over time according to
              researcher, month and season. The commercial kit also contained
              higher microbial diversity and several human-associated taxa in
              comparison to the home-made silica extraction protocol. We
              recommend a minimum of two strategies to reduce the impacts of
              laboratory contaminants within low-biomass metagenomic studies:
              (a) extraction blank controls should be included and sequenced
              with every batch of extractions and (b) the contributions of
              laboratory contamination should be assessed and reported in each
              high-throughput metagenomic study.},
  month    = jul,
  year     = 2019,
  url      = {http://dx.doi.org/10.1111/1755-0998.13011},
  keywords = {ancient DNA; contaminant; contamination; metagenomics; microbiome;
              microbiota},
  doi      = {10.1111/1755-0998.13011},
  pmc      = {PMC6850301},
  pmid     = 30887686,
  issn     = {1755-098X,1755-0998},
  language = {en}
}

@article{Gloor2017-ml,
  title    = {Microbiome Datasets Are Compositional: And This Is Not Optional},
  author   = {Gloor, Gregory B and Macklaim, Jean M and Pawlowsky-Glahn, Vera
              and Egozcue, Juan J},
  journal  = {Frontiers in microbiology},
  volume   = 8,
  pages    = 2224,
  abstract = {Datasets collected by high-throughput sequencing (HTS) of 16S rRNA
              gene amplimers, metagenomes or metatranscriptomes are commonplace
              and being used to study human disease states, ecological
              differences between sites, and the built environment. There is
              increasing awareness that microbiome datasets generated by HTS are
              compositional because they have an arbitrary total imposed by the
              instrument. However, many investigators are either unaware of this
              or assume specific properties of the compositional data. The
              purpose of this review is to alert investigators to the dangers
              inherent in ignoring the compositional nature of the data, and
              point out that HTS datasets derived from microbiome studies can
              and should be treated as compositions at all stages of analysis.
              We briefly introduce compositional data, illustrate the
              pathologies that occur when compositional data are analyzed
              inappropriately, and finally give guidance and point to resources
              and examples for the analysis of microbiome datasets using
              compositional data analysis.},
  month    = nov,
  year     = 2017,
  url      = {http://dx.doi.org/10.3389/fmicb.2017.02224},
  keywords = {Bayesian estimation; compositional data; correlation; count
              normalization; high-throughput sequencing; microbiota; relative
              abundance},
  doi      = {10.3389/fmicb.2017.02224},
  pmc      = {PMC5695134},
  pmid     = 29187837,
  issn     = {1664-302X},
  language = {en}
}

@article{Modi2021-ka,
  title     = {The Illumina Sequencing Protocol and the {NovaSeq} 6000 System},
  author    = {Modi, Alessandra and Vai, Stefania and Caramelli, David and Lari,
               Martina},
  journal   = {Bacterial Pangenomics},
  publisher = {Springer US},
  pages     = {15--42},
  abstract  = {The NovaSeq 6000 is a sequencing platform from Illumina that
               enables the sequencing of short reads with an output up to 6 Tb.
               The NovaSeq 6000 uses the typical Illumina sequencing workflow
               based on library preparation, cluster generation by in situ
               amplification, and sequencing by synthesis. Flexibility is one of
               the major features of the NovaSeq 6000. Several types of
               sequencing kits coupled with dual flow cell mode enable high
               scalability of sequencing outputs to match a wide range of
               applications from complete genome sequencing to metagenomics
               analysis. In this chapter, after explaining how to assemble a
               normalized pool of libraries for sequencing, we will describe the
               experimental steps required to run the pools on the NovaSeq 6000
               platform.},
  year      = 2021,
  url       = {http://dx.doi.org/10.1007/978-1-0716-1099-2_2},
  doi       = {10.1007/978-1-0716-1099-2\_2},
  issn      = {1940-6029},
  language  = {en}
}

@article{Daley2013-kn,
  title    = {Predicting the molecular complexity of sequencing libraries},
  author   = {Daley, Timothy and Smith, Andrew D},
  journal  = {Nature methods},
  volume   = 10,
  number   = 4,
  pages    = {325--327},
  abstract = {Predicting the molecular complexity of a genomic sequencing
              library is a critical but difficult problem in modern sequencing
              applications. Methods to determine how deeply to sequence to
              achieve complete coverage or to predict the benefits of additional
              sequencing are lacking. We introduce an empirical bayesian method
              to accurately characterize the molecular complexity of a DNA
              sample for almost any sequencing application on the basis of
              limited preliminary sequencing.},
  month    = apr,
  year     = 2013,
  url      = {http://dx.doi.org/10.1038/nmeth.2375},
  doi      = {10.1038/nmeth.2375},
  pmc      = {PMC3612374},
  pmid     = 23435259,
  issn     = {1548-7091,1548-7105},
  language = {en}
}

@article{Rohland2015-xn,
  title    = {Partial uracil-{DNA}-glycosylase treatment for screening of
              ancient {DNA}},
  author   = {Rohland, Nadin and Harney, Eadaoin and Mallick, Swapan and
              Nordenfelt, Susanne and Reich, David},
  journal  = {Philosophical transactions of the Royal Society of London. Series
              B, Biological sciences},
  volume   = 370,
  number   = 1660,
  pages    = 20130624,
  abstract = {The challenge of sequencing ancient DNA has led to the development
              of specialized laboratory protocols that have focused on reducing
              contamination and maximizing the number of molecules that are
              extracted from ancient remains. Despite the fact that success in
              ancient DNA studies is typically obtained by screening many
              samples to identify a promising subset, ancient DNA protocols have
              not, in general, focused on reducing the time required to screen
              samples. We present an adaptation of a popular ancient library
              preparation method that makes screening more efficient. First, the
              DNA extract is treated using a protocol that causes characteristic
              ancient DNA damage to be restricted to the terminal nucleotides,
              while nearly eliminating it in the interior of the DNA molecules,
              allowing a single library to be used both to test for ancient DNA
              authenticity and to carry out population genetic analysis. Second,
              the DNA molecules are ligated to a unique pair of barcodes, which
              eliminates undetected cross-contamination from this step onwards.
              Third, the barcoded library molecules include incomplete adapters
              of short length that can increase the specificity of
              hybridization-based genomic target enrichment. The adapters are
              completed just before sequencing, so the same DNA library can be
              used in multiple experiments, and the sequences distinguished. We
              demonstrate this protocol on 60 ancient human samples.},
  month    = jan,
  year     = 2015,
  url      = {http://dx.doi.org/10.1098/rstb.2013.0624},
  keywords = {ancient DNA; authenticity; barcodes; flexibility; library
              preparation; target capture},
  doi      = {10.1098/rstb.2013.0624},
  pmc      = {PMC4275898},
  pmid     = 25487342,
  issn     = {0962-8436,1471-2970},
  language = {en}
}
