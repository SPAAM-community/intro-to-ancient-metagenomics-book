[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Ancient Metagenomics",
    "section": "",
    "text": "Introduction\nAncient metagenomics applies cutting-edge metagenomic methods to the degraded DNA content of archaeological and palaeontological specimens. The rapidly growing field is currently uncovering a wealth of novel information for both human and natural history, from identifying the causes of devastating pandemics such as the Black Death, to revealing how past ecosystems changed in response to long-term climatic and anthropogenic change, to reconstructing the microbiomes of extinct human relatives. However, as the field grows, the techniques, methods, and workflows used to analyse such data are rapidly changing and improving.\nIn this book we will go through the main steps of ancient metagenomic bioinformatic workflows, familiarising students with the command line, demonstrating how to process next-generation-sequencing (NGS) data, and showing how to perform de novo metagenomic assembly. Focusing on host-associated ancient metagenomics, the book consists of a combination of theory and hands-on exercises, allowing readers to become familiar with the types of questions and data researchers work with.\nBy the end of the textbook, readers will have an understanding of how to effectively carry out the major bioinformatic components of an ancient metagenomic project in an open and transparent manner.\n\n\n\n\n\n\nNote\n\n\n\nIf you export the PDF or ePub versions of this book, some sections maybe excluded (such as videos, and embedded slide decks). Always refer to this website in doubt.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe PDF/ePub version of the book is currently still under construction, and is likely misformatted and missing much of the content. It is not recommended for use.\n\n\nAll material was originally developed for the SPAAM Summer School: Introduction to Ancient Metagenomics",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "citing-this-book.html",
    "href": "citing-this-book.html",
    "title": "Citing this book",
    "section": "",
    "text": "The source material for this book is located on GitHub:\nhttps://github.com/SPAAM-community/intro-to-ancient-metagenomics-book.\nIf you wish to cite this book, please use the following bibliographic information\n\nJames A. Fellows Yates, Christina Warinner, Alina Hiß, Arthur Kocher, Clemens Schmid, Irina Velsko, Maxime Borry, Megan Michel, Nikolay Oskolkov, Sebastian Duchene, Thiseas Lamnidis, Aida Andrades Valtueña, Alexander Herbig, Alexander Hübner, Kevin Nota, Robin Warner, Meriam Guellil. (2023). Introduction to Ancient Metagenomics (Edition 2023). Zenodo. DOI: 10.5281/zenodo.8027281\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Citing this book"
    ]
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "The creation of this text book was developed through a series of summer schools run by the SPAAM community, and financially supported by the Werner Siemens-Stiftung. The have contributed to the development of this textbook.\n\n\n\n\n\n\n\n\n2022-2023\n\n🇬🇧 James Fellows Yates is an archaeology-trained biomolecular archaeologist and convert to palaeogenomics, and is recently pivoting to bioinformatics. He specialises in ancient metagenomics analysis, generating tools and high-throughput approaches and high-quality pipelines for validating and analysing ancient (oral) microbiomes and palaeogenomic data.\n\n\n2022-2023\n\n🇺🇸 Christina Warinner is Group Leader of Microbiome Sciences at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and Associate Professor of Anthropology at Harvard University. She serves on the Leadership Team of the Max Planck-Harvard Research Center for the Archaeoscience of the Ancient Mediterranean (MHAAM), and is a Professor in the Faculty of Biological Sciences at Friedrich Schiller University in Jena, Germany. Her research focuses on the use of metagenomics and paleoproteomics to better understand past human diet, health, and the evolution of the human microbiome.\n\n\n2022-2023\n\n🇪🇸 Aida Andrades Valtueña is a geneticist interested in pathogen evolution, with particular interest in prehistoric pathogens. She has been exploring new methods to analyse ancient pathogen data to understand their past function and ecology to inform models of pathogen emergence.\n\n\n2022-2023\n\n🇩🇪 Alexander Herbig is a bioinformatician and group leader for Computational Pathogenomics at the Max Planck Institute for Evolutionary Anthropology. His main interest is in studying the evolution of human pathogens and in methods development for pathogen detection and bacterial genomics.\n\n\n2022-2023\n\n🇩🇪 Alex Hübner is a computational biologist, who originally studied biotechnology, before switching to evolutionary biology during his PhD. For his postdoc in the Warinner lab, he focuses on investigating whether new methods in the field of modern metagenomics can be directly applied to ancient DNA data. Here, he is particularly interested in the de novo assembly of ancient metagenomic sequencing data and the subsequent analysis of its results.\n\n\n2022-2023\n\n🇩🇪 Alina Hiss is a PhD student in the Computational Pathogenomics group at the Max Planck Institute for Evolutionary Anthropology. She is interested in the evolution of human pathogens and working on material from the Carpathian basin to gain insights about the presence and spread of pathogens in the region during the Early Medieval period.\n\n\n2022-2023\n\n🇫🇷 Arthur Kocher initially trained as a veterinarian. He then pursued a PhD in the field of disease ecology, during which he studied the impact of biodiversity changes on the transmission of zoonotic diseases using molecular tools such as DNA metabarcoding. During his Post-Docs, he extended his research focus to evolutionary aspects of pathogens, which he currently investigates using ancient genomic data and Bayesian phylogenetics.\n\n\n2022-2023\n\n🇩🇪 Clemens Schmid is a computational archaeologist pursuing a PhD in the group of Stephan Schiffels at the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. He is trained both in archaeology and computer science and currently develops computational methods for the spatiotemporal co-analysis of archaeological and ancient genomic data. He worked in research projects on the European Neolithic, Copper and Bronze age and maintains research software in R, C++ and Haskell.\n\n\n2022\n\n🇺🇸 Irina Velsko is a postdoc in the Microbiome group of the department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology. She did her PhD work on oral microbiology and immunology of the living, and now works on oral microbiomes of the living and the dead. Her work focuses on the evolution and ecology of dental plaque biofilms, both modern and ancient, and the complex interplay between microbiomes and their hosts.\n\n\n2022-2023\n\n🇫🇷 Maxime Borry is a doctoral researcher in bioinformatics at the Max Planck Institute for Evolutionary Anthropology in Germany. After an undergraduate in life sciences and a master in Ecology, followed by a master in bioinformatics, he is now working on the completion of his PhD, focused on developing new tools and data analysis of ancient metagenomic samples.\n\n\n2022\n\n🇺🇸 Megan Michel is a PhD student jointly affiliated with the Archaeogenetics Department at the Max Planck Institute for Evolutionary Anthropology and the Human Evolutionary Biology Department at Harvard University. Her research focuses on using computational genomic analyses to understand how pathogens have co-evolved with their hosts over the course of human history.\n\n\n2022-2023\n\n🇷🇺 Nikolay Oskolkov is a bioinformatician at Lund University and the bioinformatics platform of SciLifeLab, Sweden. He defended his PhD in theoretical physics in 2007, and switched to life sciences in 2012. His research interests include mathematical statistics and machine learning applied to genetics and genomics, single cell and ancient metagenomics data analysis.\n\n\n2022\n\n🇦🇺 Sebastian Duchene is an Australian Research Council Fellow at the Doherty Institute for Infection and Immunity at the University of Melbourne, Australia. Prior to joining the University of Melbourne he obtained his PhD and conducted postdoctoral work at the University of Sydney. His research is in molecular evolution and epidemiology of infectious pathogens, notably viruses and bacteria, and developing Bayesian phylodynamic methods.\n\n\n2022-2023\n\n🇬🇷 Thiseas Lamnidis is a human population geneticist interested in European population history after the Bronze Age. To gain the required resolution to differentiate between Iron Age European populations, he is developing analytical methods based on the sharing of rare variation between individuals. He has also contributed to pipelines that streamline the processing and analysis of genetic data in a reproducible manner, while also facilitating dissemination of information among interdisciplinary colleagues.\n\n\n2023\n\n🇳🇱 Kevin Nota has a PhD in molecular paleoecology from Uppsala University. Currently he is a postdoc in the Max Planck Research Group for Ancient Environmental Genomics. His main research interest is in population genomics from ancient environmental samples.\n\n\n2023\n\n🇦🇹 Meriam Guellil is an expert in ancient microbial phylogenomics and metagenomics, particularly of human pathogens. She is particularly interested in the study of diseases that are invisible in the archaeological and osteological record, and the study of their evolution throughout human history. Her previous research includes studies on microbial species such as Yersinia pestis, Haemophilus influenzae, Borrelia recurrentis and Herpes simplex 1.\n\n\n2023\n\n🇩🇪 Robin Warner is a MSc bioinformatics student at the Leipzig University. He is currently writing his master’s thesis in the Max Planck Research Group for Ancient Environmental Genomics about the comparison of ancient sedimentary DNA capture methods and shotgun sequencing.",
    "crumbs": [
      "Authors"
    ]
  },
  {
    "objectID": "foreword.html",
    "href": "foreword.html",
    "title": "Foreword",
    "section": "",
    "text": "Thank you\nWe would like to extend a special thank you to the following people who contributed to the development of the course and companion textbook:",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "foreword.html#thank-you",
    "href": "foreword.html#thank-you",
    "title": "Foreword",
    "section": "",
    "text": "Aida Andrades Valtueña\nMaxime Borry\nKeri Burge\nRemí Densie\nSebastian Duchene\nJasmin Frangenberg\nMeriam Guellil\nAlexander Herbig\nAlina Hiß\nAlexander Hübner\nArthur Kocher\nDenise Kühnert\nThiseas Lamnidis\nMegan Michel\nNikolay Oskolkov\nBetsy Nelson\nGunnar Neumann\nKevin Nota\nAleksandra Laura Pach\nVilma Pérez\nClemens Schmid\nIrina Velsko\nRobin Warner\nChristina Warinner\nJames Fellows Yates\nGuilia Zampirolo\nTessa Zeibig",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "foreword.html#acknowledgements",
    "href": "foreword.html#acknowledgements",
    "title": "Foreword",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are very grateful to Matthias Meyer and Pierre Stallforth for generously reading our early drafts of sections of our textbook and providing valuable feedback and suggestions.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Financial Support\nThe content of this textbook was developed from the SPAAM Summer School: Introduction to Ancient Metagenomics summer school series, sponsored by the Werner Siemens-Stiftung (Grant: Paleobiotechnology, awarded to Pierre Stallforth, Hans-Knöll Institute, and Christina Warinner, Max Planck Institute for Evolutionary Anthropology)",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "acknowledgements.html#institutional-support",
    "href": "acknowledgements.html#institutional-support",
    "title": "Acknowledgements",
    "section": "Institutional Support",
    "text": "Institutional Support",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "acknowledgements.html#infrastructural-support",
    "href": "acknowledgements.html#infrastructural-support",
    "title": "Acknowledgements",
    "section": "Infrastructural Support",
    "text": "Infrastructural Support\n\nThe practical sessions of the summers schools work was supported by the BMBF-funded de.NBI Cloud within the German Network for Bioinformatics Infrastructure (de.NBI) (031A532B, 031A533A, 031A533B, 031A534A, 031A535A, 031A537A, 031A537B, 031A537C, 031A537D, 031A538A). z",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "before-you-start.html",
    "href": "before-you-start.html",
    "title": "Before you Start",
    "section": "",
    "text": "Basic requirements\nTo following the practical chapters of this text book, you will require:\nFor each chapter, if it requires pre-prepared data, the top of the page will have a box called ‘Self guided: chapter envionment setup’ that link to a .tar archive. This contain the raw data will be available to download for following the chapter Furthermore, the same box will describe how to use a conda .yml file that specifies the software environment for that chapter will also be available for you to install.\nSee the rest of this page on how to install conda (if not already available to you), and also how to create conda software environments",
    "crumbs": [
      "Before you Start"
    ]
  },
  {
    "objectID": "before-you-start.html#basic-requirements",
    "href": "before-you-start.html#basic-requirements",
    "title": "Before you Start",
    "section": "",
    "text": "Warning\n\n\n\nBioinformatics often involve large computing resource requirements! While we aim to make example data and processing as efficient as possible, we cannot guarantee that they will all be able to work on standard laptops or desktop computing - most likely due to memory/RAM requirements. As a guide, the cloud nodes used during the summer school had 16 cores and 32 GB of RAM.\n\n\n\n\nA unix based operating system (e.g., Linux, MacOS, or possibly Windows with Linux Subsystem - however the latter has not be tested )\nA corresponding Unix terminal\nAn internet connection\nA web browser\nA conda installation with bioconda configured.\n\nConda is a very popular package manager for installing software in bioinformatics. bioconda is currently the most popular distribution source of bioinformatics software for conda.",
    "crumbs": [
      "Before you Start"
    ]
  },
  {
    "objectID": "before-you-start.html#software-environments",
    "href": "before-you-start.html#software-environments",
    "title": "Before you Start",
    "section": "Software Environments",
    "text": "Software Environments\nBefore loading the environment for the exercises, the software environment will need to be created using the .yml with the instructions below, and then activated. A list of the software in each chapter’s environment can be found in the Appendix.\nIf you’ve not yet installed conda, please follow the instructions in the box below.\n\n\n\n\n\n\nQuick guide to installing conda\n\n\n\n\n\nThese instructions have been tested on Ubuntu 22.04, but should apply to most Linux operating systems. For OSX you may need to download a different file from here.\n\nChange directory to somewhere suitable for installing a few gigabytes of software, e.g. mkdir ~/bin/ && cd ~/bin/\nDownload miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nRun the install script\nbash bash Miniconda3-latest-Linux-x86_64.sh\n\nReview license\nAgree to license\nMake sure to install miniconda to the correct directory! e.g. /home/&lt;YOUR_USER&gt;/bin/miniconda3\nYes to running conda init\nCopy the conda config command\nClose the terminal (e.g. with exit or ctrl + d)\nOpen the terminal again and run the command you copied (i.e., conda config --set auto_activate_base false)\nExit and open the terminal again\nType conda --version to check conda is installed and working\nSet up bioconda\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n\n\n\n\n\nOnce conda is installed and bioconda configured, at the beginning of each chapter, to create the conda environment from the yml file, you will need to run the following:\n\nDownload and unpack the conda env file the top of the chapter by right clicking on the link and pressing ‘save as’. Once uncompressed, change into the directory.\nThen you can run the following conda command to install the software into it’s dedicated environment\nconda env create -f /&lt;PATH/&lt;TO&gt;/&lt;DOWNLOADED_FILE&gt;.yml\n\n\n\n\n\n\n\nNote\n\n\n\nYou only have to run the environment creation once!\n\n\n\nFollow the instructions as prompted. Once created, you can see a list of installed environments with\nconda env list\nTo load the relevant environment, you can run\nconda activate &lt;NAME_OF_ENVIRONMENT&gt;\nOnce finished with the chapter, you can deactivate the environment with\nconda deactivate\n\nTo reuse the environment, just run step 4 and 5 as necessary.\n\n\n\n\n\n\nTip\n\n\n\nTo delete a conda software environment, run conda remove --name &lt;NAME_OF_ENV&gt; --all -y",
    "crumbs": [
      "Before you Start"
    ]
  },
  {
    "objectID": "before-you-start.html#additional-software",
    "href": "before-you-start.html#additional-software",
    "title": "Before you Start",
    "section": "Additional Software",
    "text": "Additional Software\nFor some chapters you may need the following software/and or data manually installed, which are not available on bioconda:\n\nIntroduction to the command line\n\nrename (if not already installed, e.g. on OSX)\nsudo apt install rename\n\n\n\nDe novo assembly\n\nMetaWRAP\nconda create -n metawrap-env python=2.7\nconda activate metawrap-env\nconda install -c bioconda biopython=1.68 bwa=0.7.17 maxbin2=2.2.7 metabat2 samtools=1.9 checkm-genome=1.0.12\ncd /&lt;path&gt;/&lt;to&gt;/denovo-assembly\ngit clone https://github.com/bxlab/metaWRAP.git\n## don't forget to update path/to!\necho \"export PATH=$PATH:/&lt;path&gt;/&lt;to&gt;/metaWRAP/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n\n\nFunctional Profiling\n\nHUMAnN3 UniRef database (where the functional providing conda environment is already activated - see the Functional Profiling chapter for more details)\nhumann3_databases --download uniref uniref90_ec_filtered_diamond /&lt;path&gt;/&lt;to&gt;/functional-profiling/humann3_db\n\n\n\nAuthentication and Decontamination \n\n\n\n\nPhylogenomics\n\nTempest (v1.5.3)\n\nIt is also recommended to assign the following bash variable so you can access the tool without the full path\ntar -xvf TempEst_v1.5.3.tgz\ncd TempEst_V1.5.3\nexport tempest='bash /&lt;PATH&gt;/&lt;TO&gt;/TempEst_v1.5.3/bin/tempest'\nIf you get an error like Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Can't load library: /usr/lib/jvm/java-11-openjdk-amd64/lib/libawt_xawt.so, make sure you have Java installed e.g. \nsudo apt install openjdk-11-jdk\n\nMEGAX (v11.0.11)\n\n\n\nAncient metagenomic pipelines\n\nDocker (installation method will vary depending on your OS)\n\nStandard install\nLinux-nerd install\n\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\necho \\\n\"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n\"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\nsudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n## May need to do a reboot or something here\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\nsudo reboot ## will kick you out, but it'll be back in a minute or two\naMeta (make sure you’ve already downloaded the data directory as per the chapter instructions)\ncd /&lt;path&gt;/&lt;to&gt;/ancient-metagenomic-pipelines/\ngit clone https://github.com/NBISweden/aMeta\ncd aMeta\n## We have to patch the environment to use an old version of Snakemake as aMeta is not compatible with the latest version\nsed -i 's/snakemake-minimal&gt;=5.18/snakemake &lt;=6.3.0/' workflow/envs/environment.yaml\nconda env create -f workflow/envs/environment.yaml",
    "crumbs": [
      "Before you Start"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html",
    "href": "introduction-to-ngs-sequencing.html",
    "title": "1  Introduction to NGS Sequencing",
    "section": "",
    "text": "1.1 Introduction\nWe will make a start. So in this session, introduction to NGS data, I want to very briefly recap the basics of DNA for a good reason. Then introduce what DNA sequence is and how that works, and then also explain how Illumina NGS data, sequencing data is generated. The reason why I’m focusing on Illumina is because this is what the vast, very vast majority of ancient DNA will be sequenced on for reasons which I will explain later.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#basic-structure-of-dna",
    "href": "introduction-to-ngs-sequencing.html#basic-structure-of-dna",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.2 Basic structure of DNA",
    "text": "1.2 Basic structure of DNA\nSo first, to actually understand how sequencing works, we need to look at, just do a very, very briefly recap of what DNA is. So DNA, as you probably, everybody knows, is a double helix molecule which is stored in every single cell in your body (Figure 1.1).\nAnd this double helix is actually made up of four main components called nucleotides, which you can see here (Figure 1.2), and they consist of two strands in which these four nucleotides will come together and bind together in a particular order. And these four nucleotides are made up of two groups, pyrimidines and purines. So pyrimidines are cytosines and thiamines, and purines are guanines and adenines.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Animation of 3D DNA double helix. Source: Erin Rod, CC BY 4.0, via Wikimedia Commons\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: 2D molecular diagram of DNA helix, with sugar-phosphate backbone and amine groups labelled indicated. Source: Pradana Aumars, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\n\n\n\nAnd this base pairing, which brings the two strands of the DNA together, always consists of one pyrimidine and one pyrimine. I always remember which group go together, and so I remember it as C always goes with G, think CGI, and then go A with T. What’s some of the best CGI you’ve ever seen? It’s from Star Wars, so think AT-AT Walker (Figure 1.3), Antid Walker, you have been able to remember that. So what this means, and because they go together, it always means they’re complementary. So whenever you find a C on one strand of the DNA molecule, you will see a G on the other and vice versa, and again, A with a T on the other. So depending on which strand you are reading, you can always get the order of the base on the other strand because of this complementary base pairing.\n\n\n\n\n\n\nFigure 1.3: Lego construction of AT AT walker from Star Wars. Source: Tim Moreillon, CC BY-SA 2.0 Generic, via Wikimedia Commons",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#dna-replication",
    "href": "introduction-to-ngs-sequencing.html#dna-replication",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.3 DNA replication",
    "text": "1.3 DNA replication\nAnd this is important because this is essentially how replication occurs, so when you basically make a copy of the DNA strand. And in very simple terms, what we do is, or what the body does, is unwinds the multiple various macro structures, including double helix. You then separate the strands into two, so you have, all of your nucleotides are basically exposed, so you don’t have them binding together. And then you get an enzyme called DNA polymerase, which basically attaches onto the strands that’s reading along it, and when it finds an exposed nucleotide, it will say, well, okay, I recognize, for example, this is a C that’s being exposed, and it waits to basically pick up a free nucleotide floating around in the cellular gunk, and then basically allows it to bind together in that position. Then it basically will move along to the next exposed nucleotide, see what it is, let’s say it’s an A, and then it will basically wait to find the T and fix it on the strand, and eventually another enzyme, I believe, comes along, I can’t remember the name, I didn’t name this, comes along and basically repairs the backbone, like, no, it doesn’t matter, prepares the backbone to basically then have your two, strands from your original strand. And just remember basically having this enzyme picking up free nucleotides and adding it to the new strand, because this is the important thing the sequencing is within a minute.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#extracting-dna",
    "href": "introduction-to-ngs-sequencing.html#extracting-dna",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.4 Extracting DNA",
    "text": "1.4 Extracting DNA\n\nModern DNA\nAs a reminder, how we get DNA, so when you do this, you basically get your sample, you then have to break down the cells, the cell walls, and membranes, and then you basically have to destroy a lot of stuff inside the cell, there’s not the DNAs, things like RNAs, proteins, and things like this, because this can basically inhibit your DNA replication downstream in your molecular steps. You then separate out the DNA from the rest of the broken stuff, which you can then pull out (Figure 1.4). And normally in modern DNA, this can actually look like a long, this sort of spaghetti-like thing.\n\n\n\n\n\n\nFigure 1.4: Schematic diagram of DNA replication, with a helicase unwinding DNA strand with DNA polymerases on the leading and lagging strand incorporating free nucleotides. Source: Christine Imiller, CC BY-SA 4.0 Generic, via Wikimedia Commons\n\n\n\n\n\nAncient DNA\nHowever, ancient DNA is a bit different. The process is the same, you basically have to break down the tissue, in this case it’s, let’s say, bone, so you have to demineralize it to release all of the biomolecules, and you have to degrade all the other stuff, but the DNA molecules are also degraded, so they’re already fragmented, so they’re very short, they’re very damaged, so they have modified nucleotides, and they also have contamination. So basically, your small fragments DNA is sitting in a super-modern DNA, so a lot of these things will be covered in more detail in other later sessions, but you have to remember that they’re damaged, they’re old, they’re very, very short.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#dna-sequencing",
    "href": "introduction-to-ngs-sequencing.html#dna-sequencing",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.5 DNA sequencing",
    "text": "1.5 DNA sequencing\nAnd this brings us on to DNA sequencing, which is essentially the conversion of the chemical, that’s better, chemical nucleotides of a DNA molecule to the human-readable ACTG on your computer screen, and what we basically do all of our analyses on in genetics and genomics. And the way this works is pretty common across most methods, which is you replicate a strand, as I described a minute ago, but instead of adding just a standard nucleotide, you add a fluorophore-modified nucleotide, so a fluorophore is a little molecule which essentially, when you excite it somehow, will emit a color.\nAnd in the case of DNA, you can have four different nucleotides, and each one will emit a different color Figure 1.5. And so quite often, the way you excite the fluorophore is firing a laser, which then emits the light, and you record the color. And so when you’re basically adding your nucleotide each time, you fire a laser, take a picture of the color being emitted, and then you know which base it is, and then you repeat on the next base, and next base, and next base.\n\n\n\n\n\n\nFigure 1.5: Molecular diagram of the four DNA nucleotides with coloured fluorophore amine groups. Source: (Ju et al. 2006)\n\n\n\n\nSanger sequencing\nSo historically, the first, let’s say, mass production sequencing method was called Sanger sequencing (Figure 1.6). So what this involved was taking a DNA molecule, making lots and lots of copies of it, but also fragmenting it in a random manner, so all of the, oh sorry, fragmenting it, no. I’m getting confused, one step ahead. Sorry, taking a DNA molecule, making lots of copies of it. Then you start extending the molecule, but instead of adding just no standard nucleotides, you mix in a few special modified nucleotides, which include essentially a blocker. And what this means is that once the polymerase gets to this particular blocking nucleotide, it will not extend it any further. However, as you added a mixture of standard nucleotides and also these blocker nucleotides, your DNA molecules will be extended to somewhat a random length each time, because you have many, many different copies.\n\n\n\n\n\n\nFigure 1.6: Diagram of sanger sequencing, with a primer, template and free blocking nucleotides being added to ends of molecules at random lengths, and then being sent through a size-selecting capillary with a laser dector to identify which of the four blocking nucleotides are passing through the capillary and thus ready ina chromatopgraph. Source: Estevezj, [CC-BY-SA 3.0 Unported](https://creativecommons.org/licenses/by-sa/3.0/) via Wikimedia Commons\n\n\n\n\n\n\n\n\n\nFigure 1.7: Example of gel-based chromatogram with four columns with bands indicating which colour is the blocking nucleotide of that particular sized DNA molecule, and on the size an intensity graph of each colour given off by each flourophore to indicate which colour it is. Source: Morse Phoque (Abizar), [CC-BY-SA 3.0 Unported](https://creativecommons.org/licenses/by-sa/3.0/) via Wikimedia Commons\n\n\n\nIn the end, you will essentially have the entirety of your original molecule covered, as you can sort of see in this sort of step-like manner here. And what would happen once you basically have your randomly extended molecules, but with these blockers, you would then send it through a capillary gel, which basically separates out the DNA molecules based on its length, and then you would fire a laser. And the important thing about these blocking nucleotides is also they were essentially fluorophores, so when you fire the laser, it’ll emit a light. And as you basically have your DNA molecules going through the capillary gel, so the shorter ones going faster through the gel because of resistance, and the longer ones going slower, you can basically record the order of the molecules going through the capillary gel. And then according to the light, you can basically see the different colors, and then with that, you can basically count which base is in your DNA molecule, as it goes through the capillary gel, and basically reconstruct the sequence by this method (Figure 1.7). However, this is, the approach was actually not so good for high-throughput DNA, so trying to reconstruct whole genomes, a lot of the original human genome was generated with this, but this tooyears, was extremely expensive, and it’s also very, very resource-hungry, you have to use a lot of preliminary, you have to use a lot of DNA, which again, when we deal with ancient DNA, which you get very small amounts, because it’s very degraded, it doesn’t really work.\n\n\nNext generation sequencing\nAnd then in about 2005, next-generation sequencing, which is a bit of a misnomer, to be honest, came along, where you can see sequence billions and billions of DNA molecules at once. It was very fast and cheap, and very much revolutionized genetics, and pushed us into a near of genomics. And the market leader was, and still is, is a company called Illumina. There are others called PacBio and IonTorrent, but really Illumina is the one that pretty much everybody uses nowadays for at least short read sequencing. And as I’m sure you’re all aware, this is sort of, these machines are more second generation now, we have new machines like Oxford Nanopore, which basically do very long reads, PacBio to a certain extent, and that’s sort of more a third generation that we’re entering right now.\nOh, poop. Oh, dear, so unfortunately, my pretty picture has been, a video has been deleted, but what I want you to imagine is a big black window with lots of colored points. And when I press play, on all of these points, they’re going to start changing colors, going from red to green to yellow to blue, and this happening thousands of times at once across this big black screen. And this is essentially the process of sequencing.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#illumina-sequencing",
    "href": "introduction-to-ngs-sequencing.html#illumina-sequencing",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.6 Illumina sequencing",
    "text": "1.6 Illumina sequencing\nSo what this black window that you should have been seeing would have been is something called a flow cell. This is a glass slide, and on this glass slide, it’s embedded sort of bound to the base of this, is lots of short DNA sequences, synthetic DNA sequences called oligos. And what you do is essentially take your DNA, and you basically inject it into the flow cell, and your DNA will spread across the flow cell, and start binding to this lawn. So imagine, literally imagine like a grass lawn on the base of this glass slide, and basically all of your DNA molecules are attaching to these random synthetic nucleotides, oligos.\n\nAdapters\nBut the question is, how is your DNA actually binding onto this lawn, and not basically getting washed away as your solution flows through the flow cell? What you do before injecting into the flow cell is convert your DNA samples into something called a library. A library is an adapters, and see these are basically the complement sequence of the oligos, of these synthetic oligos, which allow you to bind to the lawn flow cell. But in addition to the adapters, you add onto DNA sequence, things called priming sites, so this is where the enzymes actually bind on to start copying the DNA.\nAnd also, when you’re sequencing multiple samples at once on the same sequencing run, you can add things called indices, or known as barcodes, which are basically sample specific. So it allows you to mix multiple samples at once into one sequencing run, sequencing all at the same time, later on separate them out. So this is ultimately a slightly simplified version of a Illuminate DNA construct. So the X, X, X, X, X in the middle, this is your target. So this is actually your DNA sequence from your sample.\nAnd then at both ends, you essentially have a target primer. So this is where your polymerase will bind onto to start, then actually going, reading into your target, or your insert is another phrase that you can call it. Then prior to the target primer, you also have an index. So this is actually your sample specific barcode. This is actually added typically onto the adapter, and so this happens in the, no, sorry. And the library construction, sorry. And then also you have the adapter and index primer right at the beginning. So this is both what binds onto your flow cell, but also acts as a primer for actually reading the index because you also need a sequence index to know which DNA molecule, to which sample DNA molecule it’s coming from.\n\n\nClustering\nSo once you’ve done this, in Illuminate sequencing, you have the problem that the fluorophores will be adding to your DNA molecule. The eliterated is not enough for a camera to pick up. It’s much, much, much too small. You’re dealing with these very, very small biomolecules. And so what you have to do is once your DNA molecule is bound to your flow cell, sort of some randomly, you start to make lots and lots and lots of copies. So when you have lots of copies, that means that all the copies will make the same nucleotide, sorry, the same color at once, and make the emissions strong enough that a camera in the sequencer can actually take a picture and record a lot base it is.\nAnd the procedure making these copies is called clustering. So what this consists of is you have your DNA molecule, which is bound to one of the synthetic oligos on the lawn. And you basically do some of the bridge amplification where you basically bend over the DNA molecule. Sorry, sorry, I forgot. Your DNA molecule is single stranded at the moment. So when you bind this over, bend this over so that the reverse complement will bind to the other oligo, a primer can bind on and start reading across the molecule to reconstruct your double stranded DNA. So you basically get the complement or reverse strand of your DNA molecule to double stranded DNA. Then you had a primer to basically cut at one end or each end of the DNA molecule on the two different sort of forward and reverse adapters to result in two single stranded again DNA molecules. So basically the reverse complement of the DNA molecule, but you have one, sorry, two strands. Then you do the same thing again, bend it over, reconstruct the full double stranded molecule again, separate it out and do this many, many, many, many, many different times. And basically you have loads and loads and loads of copies of the same molecule in the same tiny little cluster, which is why you could go clustering in the same point in the flow cell.\nThen prior to sequencing, you actually will cut all of one of these types off. So let’s say you’d only be left with the purple ones to make sure you only have the same sequence. So not the reverse complements, but just the same sequence in one spot on your flow cell. Sorry, one second. Basically this onto the actual sequencing process.\n\n\nSequencing-by-synthesis\nLike I mentioned before with the Sanger sequencing and replication, what you do is you basically have your single stranded molecule, you’d had a prime, the primer, the priming site, sorry, priming site at the enzyme for the polymerase. And then you start to add free floating modified nucleotides, which have these fluorophores which will basically emit your light. So there is a slight difference in Sanger sequencing, however. So once you start adding, your polymerase goes along, it starts adding a free nucleotide and then you have the fluorophore. Now this fluorophore does block the next nucleotide being added on. So this makes sure that on all of the molecules in your cluster, and you’re only adding the correct nucleotide at one point. You then defy the laser, so the light is emitted, you take a photo.\nHowever, the difference here between Sanger sequencing and why actually Illumina sequencing or the sequencing by synthesis is much more resource-sufficient is you can actually cut off this fluorophore and then basically repeat the process again. So rather than basically having your DNA molecule having one use of single use, you can sort of recycle the same DNA molecule to basically then add the next nucleotide. So this case will be, let’s say this blue one, and I’ll attach that on. Fire a laser, take a photo, remove the fluorophore, and then back again. And basically you can reconstruct the entire sequence of the molecule without having to basically throw away the DNA molecule once you’ve taken the picture of just that single nucleotide. And this normally happens at the Illumina sequencing somewhere betweein either categories of 50, polyester and helicopter. Normally happens at 50, 75times. And these are also known as cycles.\nOkay, this is sort of what you should have been seeing like earlier in this animation. You should see a big black square with all these colored dots. So the black, black square is your flow cell and all these colored dots are these different clusters, each representing a single DNA molecule which we copy lots and lots and lots of times. And so what you can imagine sort of like in like a video or a film where basically you’ve got hundreds and hundreds of thousands of single shots just sped up over time, you add the first, so you have your flow cell, your DNA cluster is bound to the flow cell. You add the nucleotides, you fire the laser. And when you fire the laser, you should see basically a color being emitted. So in this case, green here and here, this is another DNA molecule, which is yellow, blue. And you take the picture and then you know that a green is only attached to teas. And so basically you see a tea molecule being picked up. So you record a tea, you remove the fluorophore, you add, wash those away, add the new fluorophores, the next base pair, the nucleotide will be bound onto the molecule, you fire the laser, it will be this time a blue. And this means you have a G. Then you again, cut the fluorophore off, add the new ones, fire the laser, emits a red. And so this is how you basically reconstruct all of these molecules. And again, the flow cell has millions and millions of these points, of these colored dots. And this is why you can see so many different DNA molecules all at once. Now, again, you have in your head these four colors before because you’ve seen the animation, also this picture, but there is differences to this. So there’s actually two different, with the luminous sequencing, there’s two different methods of actually emitting light. One is called the four channel system, or four color chemistry, where each nucleotide has a distinct color.\n\n\nColour chemistry\nBut there’s also on particularly the next seek and no seek machines, a slight different system, where they called, they tried to make it a bit cheaper by only using two colors or two dyes. And the approach they take there is that, again, a T is green, red is, C is a red. However, in this case, an A is actually two colors mixed at once. So if you emit that both, so if the machine picks up two colors, two wavelengths are being emitted, that is an A. However, if there was no color being emitted at all in that cluster, the machine reaches as a G, or a no detected dye. So this is very important for some caveats or things you have to consider when processing your data a bit later on.\n\n\nPaired-end sequencing\nSo something you have to consider though is we’re dealing with biology. It is not like, I don’t know, chemistry or even sort of physics, where things are perfect and wonderful. Over time, areas start happening, we’re not perfect. And essentially the imaging reagents start getting tired, the polymerases start adding mistakes, or don’t bind on properly and more areas will occur. So sometimes your nucleotides will not bind, meaning you’ll skip a base or you’ll get multiple nucleotides being added once and you go forward one. And essentially within your cluster, the DNA molecules being sort of replicated and emitting light will get desynced. So the color will get a bit blurry and less clear.\nAnd so first the machine does calculate some sort of base quality. So it captures the probability that it thinks it captured the right color, so the right nucleotide of each photo. But to the point where if it has no idea, it will call a dead base call, which will be reported as an N. And this became more and more of a problem, particularly in the early days of sequencing. And so people thought, how can we improve or correct such sort of errors? And the idea that came up was paired end sequencing.\nSo what this means is you do one sequencing in one direction, then you flip the entire read over and then read it from the other end. So whereas you have to consider that when you’re in the forward direction, over time you’re getting more and more errors. So the further you get into the molecule, the less confident you are. And so the more errors are gonna be. You can then turn it over and start the whole thing from scratch with fresh reagents. By going from the reverse end forward, you basically can correct the mistakes that were occurring at the end of the forward sequencing, but get the high quality calls from the other end from the beginning. So you can sort of, that’s a bad explanation. But you can sort of see here, you read it once in this direction. So you read the DNA insert and the prime, the index. You turn it over and you do the same thing, but from the beginning. So you basically sequence the same DNA molecule twice. An added bonus of this is also you get more cycles. So if your DNA molecule is a single molecule, it’s actually longer than the cycles. It’s 50% fair, five base pairs. By going from the other end, you can capture any DNA nucleotides which you’re missing from the forward sequencing. This is not so necessarily relevant to DNA. We will typically very short DNA molecules, but in some cases that will apply.\n\n\nDemultiplexing\nThen it comes on to biological to computational sequence. How do we take these sort of very raw nucleotide sequences and put it into a format that a computer can read? This typically happens in a step of demultiplexing. It’s actually very rare that you yourself as students will have to deal with this or do with this. Typically this is done by sequencing centers or by your lab team. But what this essentially consists of is normally or rather often nowadays, there is two steps in which you’re stuck together, which colloquially are no demultiplexing.\nThe first step is called base calling. This is where you basically take your photo files of every single nucleotide and convert this into an ACTNG. So taking image files, putting into a text file. But also in most cases, we have multiple sequence, multiple samples at once. And all the samples have these barcodes or indices. And we need to actually separate these out. So you know that all of these DNA molecules come from this sample or these DNA sample, sorry, DNA molecules from this sample. And this happens in this demultiplexing step. So essentially a computer program will read in each DNA molecule whether it finds such a barcode across from one sample, let’s say an example here, the red one that corresponds to sample one and the reverse here. And then the machine will basically sort or order the DNA molecules accordingly. So all of the DNA molecules which have these combinations and indices will put into sample one, sample two. So the blue and yellow will go here and so on. So this is something you, again, rarely will have to do yourself, but it’s something just to keep in mind.\n\n\nFASTQ File\nAnd the output of this demultiplexing step is something called a fastq file. So this is a text-based format for storing biological sequences itimes of cases nucleotide sequences, but also with the base quality scores. So these are the things where the computer tries to estimate the probability that it thinks that the nucleotide call was correct. And both are encoded basically with ASCII text characters. Doesn’t really matter what they are, if you’re not familiar with that. So this is a very, very small example. These files can be gigabytes in size, so huge, huge text files typically compressed, but still a gigabyte start to being compressed.\nBut what they, all they are made of, of basically a repeating set of four different lines. You have the first line up here, and this is called the ID line. This stores multiple information from the sequencer. So the sequencing, or the sequencing machine ID, then a run number, a flow cell ID, so each flow cell will have an ID from the manufacturer. And then you have essentially a bunch of coordinates going on here, which basically tells you from which cluster on their flow cell has the DNA molecule come from. Then you can have a bunch of extra information. This is often quite random depending on the sequencing and what they put in here, but often people put things like barcodes, maybe sort of how many errors were allowed during gene multiplexing, because you could do a certain enough filtering during that step. On the second line, you have the DNA sequence itself. So in this case, it starts with an N, this is dead-based call. This is actually quite often common, because this is when the camera is still calibrating itself. And so the first base is often a bit rubbish, but then the rest of the molecule, as you can see, sorry, the sequence here is A, C, T, and G, so it’s usually a new molecule. You’ll then have a plus, which is a separator. And the fourth line of this repeating set of four is then the base quality scores. So these are random, sort of a random set of characters, which I’ll explain in a second. But basically this tells you the confidence of that, how good we think that that nucleotide call was.\nAnd then you can basically see the same thing here on the next line, and it repeats as follows. So you can see, for example, this number is a bit different, because it’s from a slightly different coordinate cluster. So these quality scores, they are not uniform, I have to say. So it depends on both the age of the machine and what the manufacturer selected. But typically they will look something like this, where there’s a fixed order in the ASCII characters. And each one will correspond to a different probability of Fred’s score. I won’t explain exactly what that is, it’s a bit mazzy. But essentially what it means is that the higher the character along this score, the more confident you are of the base call, because the probability that it’s incorrect is sort of low.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#sequencing-recap",
    "href": "introduction-to-ngs-sequencing.html#sequencing-recap",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.7 Sequencing recap",
    "text": "1.7 Sequencing recap\nSo to recap, DNA molecules are essentially made of nucleotides, A, C, T, and Gs. We have two strands, which is complementary based pairings, C, G, CGI, AT, Atatoka, Star Wars, it’s great. Modern DNA is very long, but AT DNA is very short. And this is very good for NGS sequencing, where we do this massive multiplexing. So where rather than trying to sequence few very long DNA molecules, we sequence lots and lots and lots of very short ones at very high accuracy, which we can then reconstruct the long sequences later on, if you’ve got a good DNA. So the main steps are adding adapters to create something called a library, which allows your DNA molecules to bind to the glass slide, the flow cell, or something called a lawn. You then basically make a new strand, each cycle, you basically add a fluorescent nucleotide, which you can fire a laser, AT, which emits a colour, you can take a photo, and by basically recording, the order of the colours being emitted in a single point in the flow cell, you can reconstruct the DNA sequence. This de-syncing of clusters results in lower based quality scores over time, so you can also improve this by paired end sequencing, where you basically sequence from one end, and then do turn it around, then you sequence from the other end with fresh reagents.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#sequencing-and-considerations-for-ancient-metagenomics",
    "href": "introduction-to-ngs-sequencing.html#sequencing-and-considerations-for-ancient-metagenomics",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.8 Sequencing and considerations for ancient metagenomics",
    "text": "1.8 Sequencing and considerations for ancient metagenomics\nSo for the last section of this lecture, I want to give you a few ideas of things you should consider when you’re dealing with DNA for ancient genomics. Some of those are applicable to modern genomics too, but it’s things I find that, through my career, people forget about in some cases.\n\nLow DNA preservation\nSo firstly is low DNA preservation. So when you’re dealing with an ancient DNA, your samples are very old, and only have very little DNA in the sample, and during library preparation, you may have to do lots and lots and lots of amplification, make lots and lots and lots of copies of your DNA to make a sufficient amount to actually put sequencing. And this is important, and it will be discussed later during the week, but this is important because during library construction, you can actually inflate your counts in terms of DNA molecules that come from a particular taxon, micro-brothaxon, for example, and basically skew your estimate of which species are in your sample or not, or were in your sample because they’re now dead. Also, by overamplifying your DNA molecules, you reduce the number of sequencers you actually get out there. By having these duplicate molecules, you’re not actually providing any more extra information about your DNA library, and your sequencing flow cell only has a fixed number of sequencing slots, which basically, if you’ve overamplified your library, will basically fill up your slots and you will not sequence as many unique reads, and so the amount of information you’re getting out of your DNA molecule can be problematic. So this is actually quite, I’ve jumped ahead quite a bit in terms of detail here, but this is, I wanted to have the slides here for you to go back to and recap later.\n\n\nIndex hopping\nAnother thing during sequencing, you have to consider is something called index hopping. So this is a problem, particularly with Illumina Sequ, or people are aware of with Illumina sequencing, and it’s a challenge when you’re doing multiplex sequencing. When you’re sequencing lots and lots of samples at once, and you have to have these indexes or barcodes, which allow you to identify this DNA molecule comes from this sample. This seems to happen more often on a type of flow cell, which you find on Illumina HiSeq X and NovaSeq machines, and is ultimately caused by free floating index primers. And why that is a problem, is that if you do not sufficiently clean up your primers, during the clustering process, you can accidentally start adding on or switching barcodes between DNA molecules. This does happen at a relatively low rate, but it does happen. And so what it means is that essentially you switch the barcodes and you may accidentally assign a DNA molecule from one sample into another sample when you’re doing demultiplexing. And so let’s say you’re dealing with a microbiome sample, and you have lots of, let’s say, unless you have an oral microbiome sample and a gut microbiome sample, you may start seeing, for example, oral species popping up in your, or sort of oral species which are only found in the mouth, ending up in your gut samples, which may be a bit weird. This can also be a particular problem if you’re doing microbial genomics, so doing, let’s say, pathogen reconstruction, if you’re working on that. And you mix capture results with your shotgun samples because then you may start picking up the high amount of capture results in your shotgun samples where you’re doing screening. So you may start getting false positives there. There’s quite a few papers on this, also in the context of ancient DNA, so van der Waag has got quite a good paper to understand that and also had to correct such, or estimate the level of this in your studies.\n\n\nSequencing errors\nThey go back to the sequencing, you have to consider your sequencing errors. So if you don’t sufficiently quality control and check for these errors happening on DNA sequencing, you may actually start incorporating errors into your analysis downstream. So for example, what may happen is if you have a low base quality score, the machine may have picked up the wrong base or the wrong nucleotide. And this means that your DNA molecule or sequence, when you compare to reference genome databases or reference genomes, may start going to the wrong place or match the wrong reference genome because you have the wrong nucleotides, the wrong sequence. Which can be a problem. Also, it can reduce your chance of getting sufficient overlap during the assembly, which is where you basically stick together all of the overlapping DNA molecules together to try and reconstruct the entire molecule. This is something that Alex will present on Thursday, like Subna. And also if you’re doing variant calling for phylogenomics and you have very low coverage, this may also start increasing, increasing the add errors and you will do the wrong SNP call, which means that basically your, the relationship between your genomes in your tree, for example, will be skewed. And so it’s always very important to check for such errors and check that your sequencing run was high quality.\n\n\nDirty Genomes\nYou also have to consider, so this is again sort of jumping ahead, but there’s a reason why I’m putting in this presentation, is dirty genomes. So unfortunately, there are many reference genomes which are very dirty. So dirty, I mean, for example, having a lot of adapters still in there. And this means you have the problem where if you yourself have not sufficiently cleaned up your DNA library to remove the adapters and remove also pre-processing, you will start seeing weird results. For example, I very, very highly expect that if any of you screen against the NTPI-NT nucleotide database, you will start seeing carp everywhere. And the reason why is because people somehow got onto the NCBI a whole carp genome without removing any of their adapters. So there’s adapters sequences everywhere in the genome. And so whenever you have an adapter in your library, this will basically align to the carp genome and then you’ll get carp, which particularly if you’re trying to look at diet, for example, or ancient diet in, like say microbiome studies, where you look at some calculus, you may start seeing carp even if you’re, I don’t know, from, got samples from, I don’t know, Chile or somewhere where carp is not expected to find. You also often will find this with zebrafish. So often many, many, many, I think it’s Darius Varini or something like that, which makes no sense because all of these fish comes from one lake in Africa, but you see it everywhere in your metronome examples. And again, it’s because of dirty genomes where they think of adapters or vectors in the genome.\n\n\nLow Sequence Diversity\nAnother thing is low sequence diversity. So what I mean by this is mononucleotide reads, like GGGGG, or dinoucleotide repeats. This is not so much of an error necessarily when you’re doing metagenomics, but when you come into genomics, this can be a problem. So the problem with such DNA molecules, like this one here, GGGGG, is they’re very unspecific. They give you no information. That can come from any species everywhere because they are very common across all genomes. So the problem here is that firstly, it slows down your processing because basically you are aligning against, or comparing of DNA sequence against many, many, many different genomes to ultimately say, I don’t know which one it comes from, which is unnecessary. And also in some cases, it can inflate counts at higher nodes when you’re doing an LCA. This will be described later on. But this can be very common. So if you remember this two-color chemistry which I mentioned earlier, where you don’t have one color per base, but rather if there is no color emitted, the machine uses a G, particularly with an ek-seq and no-seq data, you will have a lot of these Gs, particularly as we’re dealing with ancient DNA, which is very short. When you have very short ancient DNA and you don’t reach all of your cycles, so let’s say you’re doinbase pair cycles, but your DNA molecules are onlbase pairs, you will basically get to the end of your DNA molecules and not add anything else. So you start getting these very long tails of Gs at the end of your molecules. And if you don’t remove these, this will make it very difficult to correctly align your DNA molecule to a reference genome or sequence. So be aware to look for these and remove these. Also because it will speed up your processing. So to recap the considerations, a lot of this will make more sense later on in the other sessions, but consider your duplication rates. You check for lots of copies of same DNA molecule in your library. It’s a good idea to check for index hopping, so making sure that the index combinations that you have in your library are correct and you’ve sorted your samples correctly. Always check for sequencing error and remove low quality bases if possible. Check for adapters, so to make sure you don’t start finding CARP everywhere. And also it’s a good idea to look for low-seq and diversity reads. For example, particularly if you put next-seq or no-seq data, because it just slows down your results and you get lower quality text-long assignment.\n\n\nQ and A\n\nHow to design barcodes\nOkay. How to design the barcodes. That’s a question from UD. That is quite tricky because there’s a lot of considerations you have to make when making sure there’s a balance and they’re not too similar to each other. Often manufacturers have tools which allow you to basically generate this. I believe they also to sentence and have standard sets which they can also send you that you request. So you yourself do not have to necessarily design these. It depends on your lab. So often I’d say speak to your sequencing center if you have them. Because they all have advice. All check manufacturers, I think most like Agilent and Illumina will also basically have such things for you then. Yeah, sometimes they make it a little bit defined but you can’t find them. And if you read the Meyer and Chercher article for buildinnew libraries, it would be provided in that set. Yeah. Could everyone hear Tina then? No? Okay. So she said that often the manufacturer make it a bit hard to find such functionality on their websites and stuff, but you can often do that. But also if you read the sort of classic paper by Meyer and Chercher 2011, 12, they actually have the set of barcodes that you can use yourself. So I think that link, that paper is on the website somewhere but we can also share with you.\n\n\nHow many indicies can you use\nHow many indices can you use? So this is a good question. This depends on your strategy and is slowly changing over time. So you can actually choose one index if you want, which is at the beginning of your, this is from Laura, which is the beginning of your molecule. What has been recommended and what the Meyer and Chercher paper introduces double indexing where you have two at the end. And these are attached to your adapters. What people are commonly doing now is actually adding additional barcodes called inline barcodes. So these are very short sequences, about seven base pairs I think, which you actually attach immediately to the DNA molecule before library preparation. So after extraction before library preparation. And this actually helps you with, sorry, with correcting for index hopping. So if you read the van der Waal paper, which I mentioned earlier, we can send the link again later. They also describe how they use these internal barcodes to separate out. So for example, you can get from a manufacturer about 100, let’s say barcodes, but you will normally per library can have somewhere between one to four separate identifiers.\n\n\nWhy Gs called more often in two colour chemistry sequencing\nSo I have a question regarding the gene calling for Gs, right? You mentioned that Gs are a characteristic of ancient DNA or sequencing errors. Why is it specifically Gs that are called more often rather than the other bases? The reason, okay, so that is because your DNA molecule is very short. So let’s say your DNA sequence is onlbase pairs long, but your sequencing cycles, so the number of cycles of imaging your machine is gonna do is let’s sabase pairs. Once you’ve got through thbase pairs, there is nothing to sequence anymore. Your lights are not going to emit anything. So when you’re on nobody-connected data, if no, sorry, machines, if no light is emitted, it reads it as a G. And you have to remember that the machine is not going to stop imaging once that one DNA molecule is finished. The DNA, the sequencing machine will keep taking photos until it’s reached to the number of cycles you’ve set, whether in this case, 75. So once you’ve exhausted your DNA molecule, there’s nothing to sequence, nothing to image anymore. So basically the machine will just keep picking up G for every remaining cycle of the run. Does that make sense? Yes. Yes, it does. But still I don’t get why G and not like AT. Why is it specifically the space? Because on nobody can make sick data, whatever reason they’ve decided Gs means nothing. There’s no color. Once it runs out of the…\nYeah, so basically the problem with ancient DNA is we often have very short reads. So you might do say two bsequencing, which is very common, but you might have a read that’s onlbases long. And so once it kind of runs out of DNA, it will just, it won’t sequence anymore. So there’ll be no more fluorophores. And because the NovaSeq and the next you can interpret that as a G, you’ll just get these polyG tails, but actually it just means no more data. And another thing to… Could you make it up for yourself? And I think this may also happen in modern data as well. If you’ve fragmented your modern data too short, you’ll also get that. It’s just that in the complex of ancient DNA, the reason why I said that is because we are naturally already very, very short because of the degradation. And I’ll also say this is, if you’re used to doing modern DNA, this is where it’s really different because let’s say you’re sequencing a regular library. What you would normally do for modern DNA is you have genomic DNA, which is huge. So for your microbial DNA, each genome is something likmillion bases long, and that’s way too big for an aluminum machine. So what you would do is you would shear it either enzymatically or by sonication, usually to an average size of aboubases. And then you do your alumina sequencing usually two by 150. So you kind of measure one side, then you measure the other side, and you get a total obases sequenced out of thbase pair read. You never run out of DNA. You never actually get to the end of the molecule when you’re sequencing. And so for most people that do modern DNA sequencing, they’ve never dealt with this problem before because they never see it because they’re always sequencing a DNA molecule longer than what their sequencing chemistry can actually do. For ancient DNA, it’s very different. We actually don’t shear. We take advantage of the fact that because our DNA is short, we don’t have to shear. And because we don’t shear, it actually allows us to exclude some of the modern contamination because any modern DNA that’s in there will be so long that it won’t build a proper library and it won’t be sequenced. And so it kind of helps us clear out some of the modern DNA that might be present. And so we will only sequence the short DNA sequences, which are more likely to actually be ancient. But the problem there is we’re dealing with the real size of the ancient DNA, which might bbases, 30 basesbasesbases. We don’t have necessarily the kind of consistency you would have if you were intentionally shearing modern DNA. So we do have some sequences that are very short.\n\n\nWhat is full genome seuqencing\nOkay, so Liasat asked, when we’re talking about whole genome sequencing, so WGS and full genome sequencing, FGS, is it the same? I’ve never heard of FGS. So yes, I would say it probably is.\n\n\nTools for generating indicies\nAnd then Jaime asks, is, I hope I’m saying that right. Is this kind of program publicly available? Is this to? Sorry? It was the index checker to make sure your pool is not. It was the index checker, yes. So again, lots of tools online, I think, basically, to make sure there’s no overlap. Normally the manufacturer will offer such thing.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#readings",
    "href": "introduction-to-ngs-sequencing.html#readings",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.9 Readings",
    "text": "1.9 Readings\n\nReviews\n(Schuster 2008)\n(Shendure and Ji 2008)\n(Slatko, Gardner, and Ausubel 2018)\n(Dijk et al. 2014)\n\n\nSequencing Library Construction\n(Kircher, Sawyer, and Meyer 2012)\n(Meyer and Kircher 2010)\n\n\nErrors and Considerations\n(Ma et al. 2019)\n(Sinha et al. 2017)\n(Valk et al. 2019)",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "href": "introduction-to-ngs-sequencing.html#questions-to-think-about",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.10 Questions to think about",
    "text": "1.10 Questions to think about\n\nWhy is Illumina sequencing technologies useful for aDNA?\nWhat problems can the 2-colour chemistry technology of NextSeq and NovaSeqs cause in downstream analysis?\nWhy is ‘Index-Hopping’ a problem?\nWhat is good software to evaluate the quality of your sequencing runs?",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ngs-sequencing.html#references",
    "href": "introduction-to-ngs-sequencing.html#references",
    "title": "1  Introduction to NGS Sequencing",
    "section": "1.11 References",
    "text": "1.11 References\n\n\n\n\nDijk, Erwin L van, Hélène Auger, Yan Jaszczyszyn, and Claude Thermes. 2014. “Ten Years of Next-Generation Sequencing Technology.” Trends in Genetics 30 (9): 418–26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nJu, Jingyue, Dae Hyun Kim, Lanrong Bi, Qinglin Meng, Xiaopeng Bai, Zengmin Li, Xiaoxu Li, et al. 2006. “Four-Color DNA Sequencing by Synthesis Using Cleavable Fluorescent Nucleotide Reversible Terminators.” Proceedings of the National Academy of Sciences of the United States of America 103 (52): 19635–40. https://doi.org/10.1073/pnas.0609513103.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. “Double Indexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina Platform.” Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder, Michael N Edmonson, Yu Liu, et al. 2019. “Analysis of Error Profiles in Deep Next-Generation Sequencing Data.” Genome Biology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. “Illumina Sequencing Library Preparation for Highly Multiplexed Target Capture and Sequencing.” Cold Spring Harbor Protocols 2010 (6): db.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. “Next-Generation Sequencing Transforms Today’s Biology.” Nature Methods 5 (1): 16–18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. “Next-Generation DNA Sequencing.” Nature Biotechnology 26 (10): 1135–45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle Joseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017. “Index Switching Causes ‘Spreading-of-Signal’ Among Multiplexed Samples in Illumina HiSeq 4000 DNA Sequencing.” bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018. “Overview of Next-Generation Sequencing Technologies.” Current Protocols in Molecular Biology / Edited by Frederick M. Ausubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dalén, and Katerina Guschanski. 2019. “Index Hopping on the Illumina HiseqX Platform and Its Consequences for Ancient DNA Studies.” Molecular Ecology Resources, March. https://doi.org/10.1111/1755-0998.13009.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to NGS Sequencing</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html",
    "href": "introduction-to-ancient-dna.html",
    "title": "2  Introduction to Ancient DNA",
    "section": "",
    "text": "2.1 Ancient DNA beginnings\nThe origins of the field of palaeogenomics can be traced back to 1984 with the publication of the first ancient DNA study by Russell Higuichi and colleagues (Higuchi et al. 1984). Working in Allan Wilson’s lab at the University of California at Berkeley, Higuchi investigated DNA from a museum soft tissue specimen of the quagga, a subspecies of zebra that had gone extinct in the late 19th century (Figure 2.1). Compared to today, 1984 was a very different world in terms of technological capabilities within biology and genetics. To help put this into perspective, Figure 2.1 shows the DNA sequences that were published in this first paper. They consist of partial DNA sequences from two mitochondrial genes – cytochrome oxidase I and a second unidentified gene that was later determined to be NADH dehydrogenase I. Sequencing these two short segments of DNA required an enormous amount of time and effort to achieve, and all of the work was analog. In fact, most of the digital tools we take for granted today did not yet exist in 1984. Journals did not have websites, manuscripts were submitted by post, and genetic data was mostly tabulated and analysed by hand. Microsoft Excel, which is today a kind of ubiquitous software for basic spreadsheet data manipulation, wasn’t even invented and released until 1985 (for Mac) and 1987 (for Windows).\nThis first ancient DNA study, published at the very beginning of the era of genetic sequencing in the journal Nature, illustrates what an important achievement it was in the 1980s to be able to sequence even this small amount of quagga DNA. However, the methods they used are very different from the methods we use today. Higuchi and colleagues achieved the genetic sequencing reported in their paper using a technique called primed-synthesis dideoxynucleoside chain-termination sequencing, a method that had been developed by Frederick Sanger and colleagues only seven years before in 1977 (Sanger, Nicklen, and Coulson 1977). Sanger’s method, which came to be known as Sanger Sequencing, was the first truly successful and useful method for sequencing DNA, and it became the primary method of DNA sequencing for the next 35 years. This was a breakthrough moment in the history of genetics, and it is worth exploring how the method works.\nIn the previous chapter, you learned some of the basic concepts of DNA sequencing that are used for next generation sequencing (NGS). NGS builds upon the basic principles of the original Sanger method, and so it is worth examining the Sanger method in more detail here. The Sanger method works by using a DNA polymerase to copy a large pool of identical DNA templates using a mixture of ordinary nucleotides and nucleotides containing a blocking component. As the polymerase extends along each DNA template molecule, it incorporates the available nucleotides, and when it incorporates a blocking nucleotide it prematurely stops. The random incorporation of blocking nucleotides at different points along the template molecules results in DNA of different extension lengths, which can then be separated and ordered by size using gel electrophoresis. In the original Sanger method, each round of sequencing required four separate reactions - each containing blocking nucleotides for a different base. Following gel electrophoresis, the different banding patterns of the four reactions could then be used to determine the base present at each particular position in the sequence, and thus determining the overall DNA sequence of the template.\nFigure 2.2 shows an example of how this works from another early ancient DNA study by Matthias Höss and Svante Pääbo on a different extinct equid (Höss and Pääbo 1993). The bands are “read” from bottom to top, which represents the 5’ to 3’ orientation of the template molecule. The four lanes of the gel represent the four separate polymerase extension reactions, each containing a mixture of ordinary nucleotides and blocking nucleotides corresponding to the base annotated at the top of each lane. The shortest DNA sequences travel fastest through the gel and are visualised as bands at the bottom of the gel; these represent the beginning of the DNA sequence. The longest DNA sequences travel more slowly through the gel and are visualised as bands at the top; these represent the end of the DNA sequence. Determining the DNA sequence was initially an entirely analogue process generally carried out with a ruler and pencil. Band by band, the bases would be recorded from bottom to top, resulting in a consensus sequence. This process, setting aside the prior laboratory work necessary to produce sufficient template DNA for sequencing, would itself have taken a full workday in order to set up the reactions, perform the extensions, run the gel, image the gel, develop the film, and then manually read out the DNA sequences. Although automated sequencing and basecalling first became available for the Sanger method in 1987 with the release of the Applied Biosystems ABI 370 instrument, which utilised fluorophores instead of radioactive molecules to detect the DNA bases (making it safer) and a personal computer for digital basecalling (making it faster), analog methods continued to be widely used well into the 1990s.\nRemarkably, the original quagga sequences were obtained using Sanger Sequencing without the use of polymerase chain reaction (PCR). Although PCR was invented by Kary Mullis in the 1980s (Saiki et al. 1985, 1988), it did not become widely accessible until the early 1990s (Bartlett and Stirling 2003; Mullis et al. 1992). Thus Higuchi and colleagues achieved the first ancient DNA sequences through even more laborious methods based on restriction enzymes and bacterial cloning. In total, their study reports two ancient DNA sequences, one 114 bp long and the other 112 bp long, from one ancient sample. Now contrast this to today, when it is possible to routinely generate &gt;50 billion high quality ancient DNA sequences every 48 hours on an Illumina NovaSeq X instrument (https://emea.illumina.com/systems/sequencing-platforms/novaseq-x-plus.html).\nAncient DNA sequencing has come a long way since the 1980s, and grappling with these changes is the rationale for developing this textbook and companion course on Ancient Metagenomics, which focuses on bioinformatic coding and scripting. During the Sanger sequencing era, from the 1980s to the 2000s, the vast majority of the hours of work that went into ancient DNA analysis was in the laboratory and consisted of extensive wet lab preparation of samples to reach the point where a relatively short DNA sequence could be generated. It is no wonder then that this era demanded intensive training in biochemistry and molecular biology. The situation is very different today, in which the laboratory methods have become highly standardised but the data output is enormous. Since the rise of high-throughput sequencing in 2010s, the main challenge for ancient DNA researchers has become how to manage this deluge of genetic data, and this requires specialised computational and bioinformatics skills to be able to handle, process, and interpret the vast amounts of data that make up today’s ancient DNA datasets.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#ancient-dna-beginnings",
    "href": "introduction-to-ancient-dna.html#ancient-dna-beginnings",
    "title": "2  Introduction to Ancient DNA",
    "section": "",
    "text": "Figure 2.1: The first ancient DNA sequences. In 1984, ancient DNA was successfully sequenced from a museum specimen of muscle tissue from a quagga, an extinct subspecies of zebra. Short DNA sequences from two genes were obtained using Sanger Sequencing, resulting in a total of 226 bp of reconstructed quagga DNA. Quagga image by Frederick York, 1870, Wikipedia, public domain. DNA sequences from (Higuchi et al. 1984).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Example of ancient DNA sequenced by the Sanger method. In 1993, mitochondrial DNA was successfully sequenced from a 25,000-year-old horse bone using Sanger Sequencing. The portion of the DNA sequence alignment visible on the gel is highlighted in gray. Adapted from (Höss and Pääbo 1993).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#from-quagga-to-ancient-microbes",
    "href": "introduction-to-ancient-dna.html#from-quagga-to-ancient-microbes",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.2 From quagga to ancient microbes",
    "text": "2.2 From quagga to ancient microbes\nHow did we get from quagga to ancient microbes? It has been an incredible journey, made all the more remarkable by the fact that nearly everything we now know about ancient microbes has been achieved through the study of ancient DNA. The rest of this chapter will focus on how genetic technologies are used to study ancient microbes. But first, let’s start with some important questions: Where exactly does one find the DNA of ancient microbes? How does one archaeologically recover organisms that are too small to see? It turns out there are several reliable sources of ancient microbes that have proven to be productive and informative about the past. Here, we will review six sources that are particularly important in archaeology: teeth, bones, historic medical specimens, palaeofaeces, cultural objects, and sediments (Figure 2.3).\n\n\n\n\n\n\nFigure 2.3: Sources of ancient microbial DNA. (a) teeth; (b) pathological bone; (c) historic soft tissue medical specimens; (d) palaeofaeces; (e) cultural artifact residues; (f) sediments. Images a-b and d-f are courtesy of the author. Images a-b, d-f are courtesy of the author Christina Warinner. Image c depicts a detail of an 1897 medical illustration of liver hepatitis; the image is licensed under a CC-BY 4.0 license and comes from Wellcome Images (image 577059i), a website operated by Wellcome Trust, a global charitable foundation based in the United Kingdom.\n\n\n\n\n2.2.1 Teeth\nPerhaps the single greatest source of ancient microbial DNA is teeth. As the only part of the skeleton normally visible during life, teeth are also the only part of the skeleton typically colonised by microbes. Figure 2.3 shows the mandible (lower jaw) of a woman who died around a thousand years ago at the medieval monastic site of Dalheim in Germany (Warinner et al. 2014; Radini et al. 2019). Several important microbially-associated features are visible. First, there are mineral accretions on the surface of her teeth. These accretions are called dental calculus, and they form through the periodic calcification of dental plaque (Fagernäs and Warinner 2023; Warinner, Speller, and Collins 2015). Below the calculus, the alveolar bone (which holds the teeth) is thickened, recessed, and porous - indications that the periodontium (the soft and hard tissues supporting the teeth) was chronically inflamed by dental plaque bacteria during life. Left untreated, this inflammation has caused partial destruction of the alveolar bone, a medical condition known as periodontitis. Thus, even from a simple visual inspection we can already see signs of ancient microbial activity that occurred during this woman’s life.\nFigure 2.4 shows a closer view of one of the teeth in cross-section using scanning electron microscopy (SEM). There are three main sources of microbial DNA that can be obtained from this tooth. On the surface of the enamel, which appears as white, the mineral accretions of dental calculus are visible. Within the dental calculus are thousands, if not millions, of individual bacterial cells that were calcified in situ (Figure 2.4, panel A). They are, in a sense, frozen in time, along with their DNA (Fagernäs and Warinner 2023). The process by which dental plaque calcifies into dental calculus happens at regular intervals over an individual’s lifetime, building up a calcified biofilm that can become several millimetres thick. This process occurs during life, and in fact dental calculus is the only part of your body that routinely fossilises while you are still alive. What you see in Figure 2.4 are the calcified remains of the oral microbiome, and specifically the dental plaque bacteria that grow on the surfaces of teeth. Although most bacteria within dental calculus are commensals, respiratory and other pathogens affecting the oral cavity have also been identified within calculus, including Klebsiella pneumoniae (R. M. Austin et al. 2022) and Mycobacterium leprae (Fotakis et al. 2020).\n\n\n\n\n\n\nFigure 2.4: Archaeological tooth in cross-section and magnified views of different sources of microbial DNA. (a) Semi-fossilised oral microbiome bacteria in dental calculus; (b) pathogen DNA preserved on the walls of the dental pulp chamber; (c) necrobiome bacteria contributing to the decomposition of tooth dentine. Images produced using scanning electron microscopy (SEM) (Warinner 2022).\n\n\n\nFigure 2.4 (panel B) shows a close-up view of the pulp chamber of the tooth. During life, the pulp chamber of the tooth is vascularised. There are blood vessels that run through the root canal and feed the dental pulp, which is at the center of each tooth. This connects the teeth to the broader circulatory system. During life, if a person has a blood-borne infection, any pathogens that are circulating in the blood will also circulate through the teeth in the dental pulp chamber. If this person dies while the infection is active, the pathogens will remain in the tooth and decay in place, and their DNA will end up becoming smeared along the walls of the dental pulp chamber. The dental pulp chamber is one of the best known sources of DNA from pathogens involved in infectious blood-borne diseases around the time of death (Spyrou et al. 2019). Several pathogens have been identified archaeologically by analysing ancient DNA in the dental pulp chamber, including Yersinia pestis (Bos et al., 2011), Salmonella enterica Paratyphi C (Vågene et al. 2018), smallpox (Mühlemann et al. 2020), and hepatitis B virus (Kocher et al. 2021).\nFigure 2.4 (panel C) shows a part of the tooth dentine that is actively undergoing decomposition. This decomposition appears as discolouration in the SEM, and tissue degradation gives the dentine a ragged appearance. Such changes are caused by the activity of the necrobiome, the bacteria that contribute to the decay and decomposition of the teeth. Within the figure, many small dark bacterial cells are visible. They are dark in colour because they are not mineralised, and they are not mineralised because they are alive. These are the many living bacteria that are slowly breaking down and decomposing the tooth. Most necrobiome bacteria are environmental bacteria that originate from the burial sediments (Warinner et al. 2014), but in some cases oral bacteria can also contribute to decomposition (Mann et al. 2018). One thing that is important to keep in mind about the necrobiome is that these bacteria can also be quite old. Bacteria invade the body and start to break it down within hours or days of death. When these bacteria die, their DNA will also accumulate damage and appear ancient. When analysing the necrobiome, it is typical to observe a mixture of DNA from bacteria of different ages. Some of the necrobiome DNA will originate from dead bacteria that contributed to the early stages of decomposition, and these dead bacteria may be nearly as old as the tooth itself and will show signs of DNA damage. Other necrobiome DNA will originate from living bacteria, including those that have more recently colonised the tooth.\n\n\n2.2.2 Bone\nBone, and especially pathological bone, is another important source of ancient microbial DNA (Figure 2.3, panel B). Some diseases that infect the skeleton, especially those that are chronic and progress over many months or years, may cause lesions or other characteristic alterations in bone. Chronic pulmonary infections, such as tuberculosis, can escape the lungs and infect neighbouring tissues, forming lesions on the interior of the ribs or infecting the thoracic vertebrae. Invasion of the vertebrae by the bacterial pathogen Mycobacterium tuberculosis can cause large lesions and cavities to form within the vertebral bodies (Figure 2.3, panel B), which weakens and destabilises the spine. This can then result in the vertebrae collapsing forward, a condition called kyphosis. Kyphosis is responsible for the characteristic hunching of the back that accompanies advanced spinal tuberculosis, and it is depicted in a wide range of ancient art. Examples of spinal tuberculosis causing kyphosis have been described at the Chiribaya sites along the southern coast of Peru dating to ca. 1000 ya (Bos et al. 2014). Leprosy is another disease that produces characteristic alterations to the skeleton. Leprosy is caused by Mycobacterium leprae, and as the disease progresses it results in a widening of the nasal aperture, destruction of the bone around the maxillary dentition, and the destruction and loss of bones in the extremities, especially fingers and toes. These features can be recognised osteologically in the archaeological record and have led to the recovery of M. leprae DNA from skeletons across Eurasia (Schuenemann et al. 2018).\n\n\n2.2.3 Historic medical specimens\nHistoric medical specimens represent a large and varied source of ancient microbial DNA (Figure 2.3, panel C). During the 18th, 19th, and early 20th centuries, many hospitals and medical schools amassed large collections of pathological specimens, which range from biopsies to complete organs. Such specimens were typically preserved and stored in alcohol or formalin, although precise documentation of their treatment is often lacking. Specimens from the early and mid-20th century also include histology slides and tissue blocks, often formalin-fixed and paraffin embedded (FFPE). RNA and DNA can be recovered from such specimens, although success rates are often low due to DNA cross-linking caused by the formalin treatment (Gryseels et al. 2020; Stiller et al. 2016). Investigation of historic medical specimens, such as vaccination kits, have led to important discoveries regarding the diversity of vaccinia virus and orthopoxvirus strains used in 19th century smallpox vaccination efforts (Duggan et al. 2020), and the study of ancient RNA in FFPE tissue blocks has shown that HIV was locally circulating in Central Africa prior to 1960, more than two decades before its global outbreak (Gryseels et al. 2020; Worobey et al. 2008; Zhu et al. 1998).\n\n\n2.2.4 Palaeofaeces\npalaeofaeces are an important source of ancient microbial DNA relating to the gut microbiome and gastrointestinal pathogens (Figure 2.3, panel D). faeces do not preserve in most environments, but specific conditions that immobilise or eliminate water, such as freezing temperatures, extreme dryness, high salinity, or mineralization, can lead to the long-term preservation of archaeological palaeofaeces and coprolites (Shillito et al. 2020). Permafrost, dry caves (Wibowo et al. 2021), and salt mines (Maixner et al. 2021) are among the best environments for the long-term preservation of palaeofaeces. In addition to microbial DNA, palaeofaeces are also a good source of dietary and parasite DNA.\n\n\n2.2.5 Cultural artifact residues\nResidues within cultural artifacts such as serving dishes and food and beverage containers are potential sources of ancient culinary bacteria (Figure 2.3, panel E). For example, food residues from baskets and ceramic bowls in western China have yielded molecular evidence for a variety of yeasts and lactic acid bacteria involved in the making of dairy products (Xie et al. 2016) and sourdough bread (Shevchenko et al. 2014).\n\n\n2.2.6 Sediments\nFinally, archaeological sediments are a rich source of environmental DNA (eDNA) containing the genetic remains of ancient microbes (Figure 2.3, panel F). Sediment samples can be collected through coring, and resin impregnated sediment blocks previously prepared for micromorphology analysis have also been shown to successfully yield ancient DNA (Massilani et al. 2022). The vast majority of research conducted on ancient sediments to date has focused on plant and mammalian DNA in order to reconstruct ancient ecosystems (Linderholm 2021; Capo et al. 2022; Kjær et al. 2022; Zavala et al. 2021). However, interest in characterizing ancient microorganisms, such as plankton, is increasing (Armbrecht 2020), and the analysis of ancient bacteria is within reach (Fernandez-Guerra et al. 2023), although distinguishing between live and dead pools can be challenging (Ellegaard et al. 2020; Wegner et al. 2023).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#defining-ancient-dna",
    "href": "introduction-to-ancient-dna.html#defining-ancient-dna",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.3 Defining ancient DNA",
    "text": "2.3 Defining ancient DNA\nHaving defined the main sources of ancient microbial DNA, we might next ask: What makes something ancient? What is “ancient DNA” as opposed to just DNA? What makes it specifically ancient? Ancient DNA can be defined as any DNA from a non-living source that shows evidence of molecular degradation. You will notice this definition does not specify a particular range of time. That is because ancient DNA is not defined by a fixed age, but rather by its condition. For example, 100,000-year-old Neanderthal oral microbiome DNA from dental calculus (Klapper et al. 2023) would obviously qualify as ancient DNA. And so would 5,000-year-old hepatitis B virus DNA from teeth (Kocher et al. 2021), 3,000-year-old gut microbiome DNA from palaeofaeces (Maixner et al. 2021), 600-year-old plague DNA from skeletons (Bos et al. 2014), oral bacteria from 19th century great apes in a museum (Fellows Yates, Velsko, et al. 2021), vaccinia virus DNA from 19th century medical specimens (Duggan et al. 2020), and even leprosy DNA from mid-20th century FFPE tissue blocks (Blevins et al. 2020). These are all examples of ancient DNA because they all show similar types of DNA damage that require special handling to be able to analyse. Essentially, ancient DNA is DNA that has undergone specific forms of degradation and damage.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#genome-basics",
    "href": "introduction-to-ancient-dna.html#genome-basics",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.4 Genome basics",
    "text": "2.4 Genome basics\nBefore going further, let’s first take a step back and ask: Why does this matter? Why is ancient DNA defined by its state of preservation rather than its chronological age? DNA degradation matters when considering the diverse composition, organization, size, and copy number of the genomes we are trying to reconstruct and understand. So before moving on, we will now review some genome basics.\nFirst, let’s consider viruses. Viral genomes can be made up of DNA or RNA, and they can be single-stranded, double-stranded, or partially double-stranded. Their genome can consist of a single molecule of nucleic acid or many molecules, and their genomes can be linear or circular. Figure 2.5 shows the seven different types of viral genomes classified within the Baltimore system of viral classification (Koonin, Krupovic, and Agol 2021). The genomes of viruses are very diverse in structure, organization, and composition.\nBacterial genomes are more consistent in their organization (Bobay and Ochman 2017). A bacterial genome typically consists of one long, continuous circle of double-stranded DNA (Figure 2.5), although some bacteria, such as Streptomyces, have linear genomes (Galperin 2007; Hinnebusch and Tilly 1993). A bacterial genome is often referred to as a chromosome, and while this terminology is not universally accepted due to differences in the compaction and staining properties of prokaryotic and eukaryotic genomes, the term chromosome is widely used by large data archives and platforms, including members of the International Nucleotide Database Collaboration (INSDC). In addition to the main genome, bacteria can also have accessory DNA called plasmids, which are typically smaller circles of DNA that can be exchanged with other bacteria.\nThe genomes of animals, which are eukaryotes, are more complex. Animals have two genomes: a nuclear genome and a mitochondrial genome (Figure 2.5). The nuclear genome is composed of multiple linear strings of double-stranded DNA, which are called chromosomes (after the Greek chroma, meaning colour, which refers to the staining properties of DNA-protein complexes in eukaryotic chromatin). Each chromosome is one very long linear piece of DNA that is tightly coiled around histones and packaged within the cell nucleus. Animal cells are usually diploid, meaning they have two of each chromosome in their nuclear genome. The mitochondrial genome (mitogenome) is made up of circular double-stranded DNA, and multiple mitogenome copies are present in each of the mitochondria, the energy-producing organelles in the cell cytoplasm. Each cell moreover contains many mitochondria, resulting in an average of 1,000-5,000 mitogenomes per animal cell, depending on cell type (Moraes 2001). The reason mitochondria have circular DNA is because they originate from bacteria that entered into other cells at the beginning of the formation of eukaryotes (Dyall, Brown, and Johnson 2004; Martin, Garg, and Zimorski 2015). While they have undergone substantial genome reduction and transfer of genes to the nuclear genome over time, they themselves have retained the circular organization of their DNA within eukaryotic cells for more than a billion years.\nPlant cells are similar in many ways to animal cells, but they also contain additional genomes (Figure 2.5). Like animals, plants have a nuclear genome made up of multiple linear strings of double-stranded DNA folded up within their cell nucleus, but unlike animal cells they are often polyploid, meaning that they can have many copies of each chromosome in their nuclear genome - typically two (diploid), four (tetraploid), six (hexaploid), or eight (octoploid). Like animals, plant cells also have a mitochondrial genome that is circular and double-stranded and present in many copies per cell, with the largest number of copies in root cells (Preuten et al. 2010). However, plants also have additional genomes within other organelles, such as chloroplasts. The plant chloroplast genome is double-stranded and partially circular (Arnold J. Bendich 2004). It originates from a photosynthetic cyanobacteria that entered into cells of plant ancestors early in the development of eukaryotes (Dyall, Brown, and Johnson 2004; Martin, Garg, and Zimorski 2015).\n\n\n\n\n\n\nFigure 2.5: Genome basics. There are major differences in the composition, organization, size, and copy number of genomes in viruses, bateria, animal cells, and plant cells. Image by Christina Warinner under a CC-BY-SA 4.0\n\n\n\nIn sum, the term genome is used to refer to a variety of different configurations of nucleic acids within viruses, prokaryotes, and eukaryotes. What are the relative genome sizes of these different types of organisms? Viruses have the smallest genomes, with most viral genomes falling between about 5 and 100 thousand bp long (Campillo-Balderas, Lazcano, and Becerra 2015). Bacteria are bigger. Bacterial genomes are, on average, about 1 to 10 million bp long (Westoby et al. 2021), and their plasmids can vary in size from 1 to 400 thousand bp long (Thomas and Summers 2020). Large plasmids tend to be present in fewer copies than small plasmids, and plasmid copy number is usually 1-200 per bacterial cell (Ilhan et al. 2019; Thomas and Summers 2020). Overall, eukaryotes have the largest genomes, and whereas viral and bacterial genome sizes tend to scale linearly with their number of genes, no such correlation exists in eukaryotes (Bobay and Ochman 2017), a phenomenon that has been described as the C-value paradox or C-value enigma (Gregory 2005). Among animals, birds and mammals generally have nuclear genomes of about 1 to 6 billion bp long (Kapusta, Suh, and Feschotte 2017) and smaller mitogenomes that are 15-20 thousand bp long but present in 1,000-5,000 copies per cell (Boore 1999; Moraes 2001). Plants clock in with an average nuclear genome size of 0.5 to 14 billion base pairs (Michael 2014) and rather large mitogenomes (200 thousand to 2 million bp) and chloroplast genomes (100-200 thousand bp) that are each present in multiple copies per cell (A. J. Bendich 1987; B. R. Green 2011; Morley and Nielsen 2017; Preuten et al. 2010). It is thus clear that genome sizes vary enormously among these different types of organisms, with viruses and microbes typically having much smaller genomes than animals or plants.\nDespite these trends, there are nevertheless many genomic outliers that have much larger or smaller genomes than expected. For example, the world’s largest known virus, Pandoravirus salinus, has a genome of 2.5 million bases, which puts it firmly within the typical genome size range of bacteria (Campillo-Balderas, Lazcano, and Becerra 2015). The world’s smallest known bacterial genome, the endosymbiont Carsonella ruddi, is barely larger than a virus at 160 thousand bases long (Nakabachi et al. 2006), while the largest known bacterial genome, Sorangium cellulosum, is 13 million bases long (Schneiker et al. 2007). The largest known animal genome is that of the lungfish Protopterus aethiopicus, at 130 billion bases (Hidalgo et al. 2017). And among plants, the largest known genome is that of a small flowering plant, Paris japonica, at 149 billion bases (Hidalgo et al. 2017). Among eukaryotes, the size of the genome is not related to the complexity of the organism, and currently the organism with the largest known genome is a microscopic single-celled amoeba called Polychaos dubium, with an estimated genome size of 670 billion bases (Parfrey, Lahr, and Katz 2008), although its precise genome size is disputed and difficult to measure (Hidalgo et al. 2017). When trying to reconstruct ancient genomes from short, degraded ancient DNA, the genome size of the target organism matters a lot.\nNext, let’s put this information about genome sizes in context. Let’s take the human genome as an example because it is the genome with which we are most familiar. The haploid size, or C-value, of the human genome is approximately 3 gigabase pairs, so 3 billion bp. Our body cells are diploid, so each of our cells contains two copies of the human genome, for a total of 6 billion bp. Humans have 23 chromosome pairs, for a total of 46 chromosomes, and each chromosome ranges in size between 50 and 250 million bp. And recall that chromosomes are really just long linear strings of DNA - very, very, very, very long DNA strings Humans also have a mitogenome, which by comparison is very tiny. It is only about 16,500 bases long, but it is present in thousands of copies per cell. On average, each cell contains 1,000-5,000 mitogenomes, but a mature human egg cell may contain as many as 1.5 million copies of the mitogenome (Cecchino and Garcia-Velasco 2019).\nHave you ever wondered how chromosome 1 came to be named chromosome 1, and why chromosome 12 is called chromosome 12? The chromosomes are numbered in order of their length, or at least the length as they were originally calculated using cytogenetics (Figure 2.6). Thus, chromosome 1 is the largest chromosome in the human genome, and it measures nearly 250 million bp long. In contrast, the shortest chromosomes in the human genome, chromosomes 21 and 22, are each around 50 million bp long.\n\n\n\n\n\n\nFigure 2.6: Karyogram micrograph showing the autosomal and sex chromosomes of a human male. Inset shows micrograph of human mitochondrion at a different scale. Chromosome lengths are from (Piovesan et al. 2019). Karyogram photo credit: National Human Genome Research Institute; image is in public domain. Mitochondrial TEM photo credit: Louisa Howard; image is in public domain.\n\n\n\nToday, sequencing and analysing high molecular weight genomic DNA from living cells is relatively straightforward, especially with the recent advent of long-read sequences like PacBio and Oxford Nanopore (Koren and Phillippy 2015; Kovaka et al. 2023). However, ancient DNA is very different from the DNA shown in the karyogram in Figure 2.6. Ancient DNA is broken and degraded into thousands, if not millions, of pieces. Let’s take, for example, chromosome 1. In life, it is one continuous string of DNA that is 248,956,422 bp long. After death, it fragments into pieces that are each approximately 50 bp long. So to use a metaphor, I like to refer to ancient DNA as the world’s worst jigsaw puzzle. Because if you had to put together a jigsaw puzzle of chromosome 1 from ancient DNA fragments, it would be a puzzle with over 5 million pieces. Hopefully this puts into perspective the kinds of challenges researchers face when working with ancient DNA.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#how-dna-degrades",
    "href": "introduction-to-ancient-dna.html#how-dna-degrades",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.5 How DNA degrades",
    "text": "2.5 How DNA degrades\nHow does DNA degrade? What kind of damage accumulates in ancient DNA over time? Some forms of DNA damage occur during life, for example during disease progression, immune response, and tissue necrosis. Such DNA damage is largely driven by the activity of nucleases, which create a variety of double-stranded breaks (Bokelmann, Glocke, and Meyer 2020; Vladimir V. Didenko 2008; Vladimir V. Didenko, Ngo, and Baskin 2003; V. V. Didenko and Hornsby 1996; Harkins et al. 2020). Other forms of DNA damage occur after death and are largely driven by chemical changes. In 1993, Tomas Lindahl was the first to examine the latter forms of postmortem damage in detail (Lindahl 1993). Lindahl presented a theoretical model of DNA degradation based on the chemical properties of DNA and identified hydrolytic and oxidative decomposition as the most likely types of damage to accumulate in fossil DNA. He also predicted that conditions of high ionic strength, adsorption to hydroxyapatite, and partial dehydration may contribute to long-term DNA preservation. Michael Hofreiter and colleagues expanded Lindahl’s model of ancient DNA degradation in an influential review article in 2001, noting that the empirically observed excess of C-&gt;T an G-&gt;A miscoding lesions in PCR-amplified ancient DNA likely results from deamination of cytosine residues in ancient DNA template molecules (Hofreiter et al. 2001).\nThe chemical bonds within DNA that are most vulnerable to degradation are indicated with arrows in Figure 2.7. Although DNA is vulnerable in many places, subsequent research has shown that some of these bonds undergo much faster rates of degradation than others, and that DNA damage generally proceeds in a predictable sequence.\n\n\n\n\n\n\nFigure 2.7: DNA degradation. Model of DNA degradation highlighting chemical bonds that are more susceptible to chemical attack and are thus most likely to show evidence of damage in ancient DNA. Four nucleotides are shown with the sequence: 5’-GACT-3’. The phosphate backbone and deoxyribose sugar parts of the DNA molecule are shown in black. The nitrogenous bases are coloured: guanine (G), purple; adenine (A), green; cytosine (C), blue; thymine (T), red. G and A are purines; C and T are pyrimidines. Sites of hydrolytic damage are indicated by black arrows; sites of oxidative damage are indicated by gray arrows. The typical order of damage accumulation at major damage sites (large arrows) is indicated by numbers: (1) depurination; (2) nicking; (3) cytosine deamination. Adapted from (Hofreiter et al. 2001; Lindahl 1993)\n\n\n\n\n2.5.1 Depurination and nicking\nThe first step in DNA degradation is typically a hydrolytic attack of the bond attaching the nitrogenous base of purines to the nucleotide’s deoxyribose sugar, a process called depurination (Figure 2.8). Guanines and adenines are purines, so this step leads to the removal of G and A bases, producing abasic sites along the DNA molecule. These abasic sites act like holes along the DNA molecule, and they make the phosphate backbone more exposed and therefore susceptible to attack.\nHydrolytic attack of the phosphate backbone occurs next, and this typically occurs 3’ to the abasic site (Figure 2.8). This process is called nicking. These nicks occur randomly along the DNA wherever there are abasic sites. Because they only affect one strand at any given position, they are called single-stranded nicks. As nicks accumulate along the DNA molecule, the DNA becomes destabilised and the hydrogen bonds between the complementary bases become the primary force holding the DNA together. If two nicks form close to each other, the hydrogen bonds between the nicks may not be strong enough to hold the DNA together and the double helix may separate, a process known as melting. Melting occurs when the vibrational energy of the two DNA strands exceeds that of the hydrogen bonds holding them together. Because two nicks rarely form at the same position on the two DNA strands, most nicking results in DNA fragments with single-stranded DNA overhangs (Figure 2.8).\n\n\n\n\n\n\nFigure 2.8: Process of DNA fragmentation. Depurination exposes the phosphate backbone of DNA to hydrolytic attack, which leads to nicking 3’ to the abasic sites. (A) When nicks are spaced far apart, the DNA between the nicks is unlikely to melt. (B) When nicks are spaced close together, the DNA between the nicks is likely to melt, producing DNA fragments with single-stranded overhangs. Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\nDNA has different local melting points depending on its composition. G and C form a triple hydrogen bond, while A and T form a double hydrogen bond. Consequently, the melting point of GC rich sequences is higher than AT rich sequences. For example, at neutral pH a 12 bp fragment of DNA with the sequence 5’-GCGCGCGCGCGC-3’ will have a melting point of 63.9˚C, while a 12 bp fragment of DNA with the sequence 5’-ATATATATATAT-3’ will have a melting point of 7.8˚C1. Thus, if two single stranded nicks form within 12 bp of each other on opposite strands of the double helix, the DNA is likely to melt and fragment if the DNA is locally AT rich. When nicks are even closer together, the temperature required to melt the strands is even lower. For example, a 6 bp fragment of DNA with the sequence 5’-GCGCGC-3’ will have a melting point of 24˚C, while a 6 bp fragment of DNA with the sequence 5’-ATATAT -3’ will have a melting point of 0˚C. Changes in pH and ion concentration can affect the precise melting temperature, but closely spaced nicks are the major cause of ancient DNA fragmentation at ambient temperatures. Empirically, most ancient DNA overhangs have been found to be very short, with one- and two-nucleotide overhangs being most common, but overhangs can also extend 20 or more nucleotides into the ancient DNA molecule (Bokelmann, Glocke, and Meyer 2020).\nThe cumulative effect of nicking over time is a high degree of DNA fragmentation. A smoothed histogram of bacterial DNA fragment lengths measured from ancient dental calculus is shown in Figure 2.9. The mode DNA length is around 50-60 bp long, but the curve sharply declines such that there is almost no DNA present at lengths 150 bp and higher. Almost all DNA within archaeological dental calculus is short, and this is the direct result of nicking and the melting of DNA between nicks. This is how time chops up high molecular weight modern DNA into tiny fragmented ancient DNA.\n\n\n\n\n\n\nFigure 2.9: Smoothed histogram of bacterial DNA fragment lengths from ancient dental calculus. Ancient DNA quickly fragments into pieces &lt;150 bp long, after which the rate of DNA fragmentation appears to slow. Here DNA ranging in age from 150 years old to nearly 5,000 years old all show a similar fragment size distribution. Data from (Mann et al. 2018).\n\n\n\n\n\n2.5.2 Cytosine deamination\nOnce fragmented, nucleotides within the single-stranded overhangs become exposed to further chemical attack. The amine group of the cytosine nitrogenous base is particularly vulnerable to hydrolytic attack when not hydrogen-bonded to its complementary guanine, and it undergoes cytosine deamination Figure 2.7. During cytosine deamination, the NH2 group is lost as ammonia, turning the cytosine nitrogenous base into a uracil Figure 2.10. Uracils are not normally found in DNA, and are instead usually found in RNA. Although cytosine deamination may occur at any time in DNA, the rate of its formation is &gt;100 times higher in single-stranded DNA (Frederico, Kunkel, and Shaw 1993; Lindahl and Nyberg 1974). Consequently, ancient DNA molecules tend to accumulate uracils at fragment termini, where they are single-stranded (Bokelmann, Glocke, and Meyer 2020). Most DNA polymerases are not able to correctly recognise uracils, and they generally interpret uracil as a thymine during DNA extension. As a result, ancient DNA sequences contain an overrepresentation of thymines at positions where cytosines are expected. This process produces the characteristic patterns of C-&gt;T transitions observed in ancient DNA datasets, as well as the corresponding G-&gt;A transitions in the complementary strand.\nOne important variant of the cytosine deamination process occurs when the cytosine is methylated, as is common in CpG islands involved in epigenetic DNA regulation. Methylated cytosines can also undergo deamination, and when that occurs the cytosine turns into a thymine, not a uracil (Figure 2.10). Using enzymes that specifically target uracils in DNA, some ancient DNA researchers have taken advantage of differences between unmethylated and methylated cytosine deamination products to investigate ancient epigenetics (Hanghøj and Orlando 2019; Briggs et al. 2010; Gokhman et al. 2014).\n\n\n\n\n\n\nFigure 2.10: Deamination of cytosine and 5-methylcytosine. Deamination of cytosine results in the formation of uracil. Deamination of 5-methylcytosine results in the formation of thymine. Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\n\n\n2.5.3 Damage review\nLet’s review the typical progression of DNA damage Figure 2.11. First, DNA undergoes depurination, which results in the random loss of G and A bases. Next, the phosphate backbone undergoes hydrolytic attack at the newly formed abasic sites resulting in single-stranded nicking. If the nicks are closely spaced, the hydrogen bonding between the bases will not be strong enough to hold the double helix together, and the strands will melt. This produces DNA fragments with single-stranded overhangs. The length of the overhang corresponds to the distance between the nicks in the melted strands. Cytosines along the single-stranded overhangs undergo faster rates of cytosine deamination. Unmethylated cytosines deaminate into uracils, while methylated cytosines deaminate into thymines. DNA polymerases interpret both forms of deaminated cytosines as thymines, resulting in an excess of thymines in ancient DNA datasets, particularly at the ends of DNA sequences.\n\n\n\n\n\n\nFigure 2.11: Deamination of cytosine and 5-methylcytosine. Deamination of cytosine results in the formation of uracil. Deamination of 5-methylcytosine results in the formation of thymine. Image by Christina Warinner under a CC-BY-SA 4.0.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#getting-a-handle-on-damage-in-ancient-dna",
    "href": "introduction-to-ancient-dna.html#getting-a-handle-on-damage-in-ancient-dna",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.6 Getting a handle on damage in ancient DNA",
    "text": "2.6 Getting a handle on damage in ancient DNA\nBut how was this all figured out? How do we know how ancient DNA decays? The empirical process by which this was determined is a fascinating story, and it occurred relatively recently. Prior to the advent of NGS, it was known that ancient DNA was fragmented, but the length distribution of ancient DNA could not be precisely measured. As recently as 2004, there were still large uncertainties about the typical length of ancient DNA (Pääbo et al. 2004). This was in large part because at the time there were no effective methods for measuring the length of very short, very low abundance DNA. Short DNA is very easily lost during DNA extraction, and many common DNA extraction methods, such as salting-out without a carrier (Gaillard and Strauss 1990), are completely unsuitable for recovering ancient DNA for this reason (Jeong et al. 2016). Low quantities of unamplified ancient DNA also do not visualise well using gel electrophoresis, and if modern soil DNA is also present in the sample, such contaminant DNA tends to overwhelm and obscure the visualization of any ancient DNA. Consequently, prior to 2007 there was great uncertainty about just how long or short ancient DNA really was. Many articles published in the 1990s and early 2000s provide estimates of around 100-500 bp, but those were really just educated guesses. Scientists could tell that ancient DNA was short, but they did not really know how short.\nEarly ancient DNA studies primarily used a targeted PCR approach to try to amplify DNA templates of about 300-500 bp, but these attempts had very high PCR failure rates and vexing contamination problems (J. J. Austin et al. 1997; Champlot et al. 2010; Hagelberg and Clegg 1991; Handt et al. 1994). Today we know why - it is because ancient DNA is much shorter than this - but at the time this was not yet known. PCR amplification is very sensitive and it can reliably generate amplicons from as few as 4 template molecules under a variety of conditions (Forootan et al. 2017). This makes PCR highly susceptible to even trace amounts of contamination, and if PCR reactions are designed to amplify templates that exceed the length of ancient DNA, the only molecules that will amplify are contaminants (Figure 2.12, panel A). Ancient DNA studies during the 1990s and 2000s, when direct PCR was the primary method of ancient DNA analysis, were thus plagued with intractable problems of irreproducibility and contamination (Cooper and Poinar 2000). Nevertheless, there were successful examples of PCR amplification of ancient DNA, and researchers noted the occasional presence of C-&gt;T and G-&gt;A miscoding lesions in seemingly authentic ancient DNA amplicons, but they could not yet empirically confirm the precise mechanisms of how they got there (Gilbert et al. 2003; Hofreiter et al. 2001).\nAt this point in the pre-NGS era, damage was just seen as a problem for the ancient DNA community. It created uncertainty in DNA sequence reconstructions, and it was not recognised to have a benefit. This changed with the development of NGS. NGS represented a radical redesign of the methods used to amplify and sequence ancient DNA, and it eliminated most of the problems that had mired earlier PCR-based ancient DNA studies in ambiguity and uncertainty (Figure 2.12, panel B). With NGS, it was no longer necessary to design PCR primer-binding sites on the actual ancient DNA template, which meant that much shorter DNA fragments could be analysed and sequenced. Instead, oligos containing universal priming sites were simply ligated onto the ends of the ancient DNA molecules. This made it possible for the first time to recover all of the ancient DNA in a sample, and, in most cases, to sequence each ancient DNA molecule in its entirety–from beginning to end.\n\n\n\n\n\n\nFigure 2.12: Comparison of PCR and NGS methods for analysing a pool of 6 ancient and contaminant DNA molecules. DNA sequences are from intron 13 of the MCM6 gene in humans, which contains a polymorphic site conferring lactase persistence (bold). Ancient DNA molecules are shown in black; contaminant DNA molecules are shown in purple; primers are shown as gray bars; sequenceable portions of the DNA template using each method are highlighted in green. (A) Using direct PCR, none of the ancient DNA molecules would amplify because the primer design requires a 111 bp template on which both primers can bind, and the ancient DNA template molecules are too short. Only the contaminant sequence would amplify, yielding an incorrect genotype (CC). (B) Using NGS, all of the DNA molecules would amplify and yield sequences. Downstream data analysis would allow the identification and removal of the contaminant sequence due to its excessive length (&gt;300 bp) and lack of DNA damage. The remaining authenticated ancient DNA sequences could then be used to determine the correct genotype (TT). Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\nOnce all of the DNA in a sample could be analysed, it became possible to measure the true size of ancient DNA and to determine the processes of DNA degradation in order. This was first achieved in 2007 in a study of Neanderthal DNA by Adrian Briggs and colleagues at the Max Planck Institute for Evolutionary Anthropology (MPI-EVA) in Leipzig, Germany (Briggs et al. 2007). Subsequent improvements in ancient DNA recovery during DNA extraction by Jesse Dabney and colleagues at the MPI-EVA showed that ancient DNA was much shorter than previously thought (Dabney et al. 2013), and that most DNA extraction protocols were simply not capable of recovering such short DNA fragments. The biased recovery of only relatively long DNA molecules using most extraction protocols had given the false impression that ancient DNA was in low abundance but relatively long (~100-500 bp). Together, these studies instead revealed that archaeological samples actually contain much more ancient DNA than previously thought, but that this ancient DNA is highly fragmented and degraded and therefore requires specialised protocols to recover, sequence, and analyse.\nToday, we know that most ancient DNA is very short - on the order of about 30 to 50 bp long on average. Shorter DNA than this is very difficult to recover (Glocke and Meyer 2017), and also not desired since sequences &lt;30 bp cannot be unambiguously identified to a specific taxonomic group or region of the genome (Filippo, Meyer, and Prüfer 2018). The process of piecing together the evidence to understand the progression of DNA degradation was a key achievement in the history of palaeogenomics. It turns out that DNA damage is quite predictable, and this has allowed scientists to turn the problem of DNA damage into a solution. There are now a number of software tools that specifically identify and quantify DNA damage as a way of authenticating ancient DNA and distinguishing it from modern DNA contaminants (Jónsson et al. 2013; Skoglund et al. 2014). This work has been crucial for reconstructing extinct hominin genomes, and it is now used today for samples of all ages, depending on context, to separate ancient and modern DNA and to authenticate well-preserved archaeological samples.\n\n2.6.1 Authenticating the Vindija Neanderthal DNA\nLet’s take a closer look at the 2007 study on ancient DNA damage that proved to be such a game-changer in the field (Briggs et al. 2007). Adrian Briggs and colleagues were trying to solve a problem - how do you distinguish Neanderthal DNA from the DNA of all of the humans who have touched the Neanderthal bone? And how do you recognise real Neanderthal DNA once you have it? It was a pressing problem because the first Neanderthal DNA sequences, published just a year before (R. E. Green et al. 2006; Noonan et al. 2006), had undergone intense scrutiny (Wall and Kim 2007) and were shown to be compromised by contamination. Using data generated by a newly available NGS technique developed by 454 Life Sciences known as pyrosequencing, the team analysed the patterns of DNA damage present in the DNA of the Vindija Neanderthal. First, they examined the DNA sequence context around strand breaks in ancient DNA, something that could not be done using data generated using conventional targeted PCR. They aligned the 454-sequenced ancient DNA fragments to a reference genome and found that there was a bias towards G and A nucleotides in the -1 position, meaning one position upstream of the ancient DNA sequence in the alignment. In other words, the ancient DNA seemed to have disproportionately broken right after a purine, either G or A (Figure 2.13). A different pattern was observed at the 3’ end of the ancient DNA molecule, where the +1 position showed an overrepresentation of C and T nucleotides. This can be attributed to repair steps occurring during the construction of the NGS-sequencing library, such that the observed pattern at the 3’ +1 position is simply the reverse complement of the 5’ -1 position. We’ll return to this topic later in the chapter.\nThe observations of Briggs and colleagues matched predictions previously made by Lindahl that purines were particularly susceptible to hydrolytic attack and depurination, after which the sugar phosphate backbone undergoes hydrolysis 3’ to the depurinated site (Lindahl 1993). This pattern observed in the Vindija Neanderthal was then confirmed to be present in mammoth and cave bear DNA datasets, suggesting it was a generalised pattern characteristic of ancient DNA fragmentation.\n\n\n\n\n\n\nFigure 2.13: Empirically observed base frequencies in the DNA sequence adjacent to strand breaks in ancient DNA from the Vindija Neanderthal. Alignment to the reference genome showed that strands breaks disproportionately follow purines in the reference sequence, a pattern predicted by Lindahl’s model of DNA degradation (Lindahl 1993). Figure adapted from (Briggs et al. 2007).\n\n\n\nBriggs and colleagues also described a second damage-related pattern present in the Vindija Neanderthal DNA sequences: an excess of errors or differences from the reference at the ends of each DNA sequence (Figure 2.14). Specifically, they observed a high substitution frequency of C-&gt;T at the 5’ end and of G-&gt;A at the 3’ end of the DNA molecule. Corresponding again to Lindahl’s predictions, the 5’ overrepresentation of T could be explained as accelerated cytosine deamination in the single-stranded overhang at the sequence 5’ terminus, while the 3’ overrepresentation of A was simply the reverse complement. The deamination pattern was strikingly clear, and the elevated 5’ C-&gt;T substitution frequency declined to baseline values by approximately the 10th bp of the DNA molecule, suggesting that the single-stranded overhangs were mostly less than 10 bp long - a pattern in agreement with melting temperature predictions. The plot was nicknamed the smile plot because it resembles the shape of a smile.\n\n\n\n\n\n\nFigure 2.14: Damage plot of DNA from the Vindija Neanderthal. An elevated substitution frequency of C-&gt;T at the 5’ end of the DNA molecules is consistent with accelerated cytosine deamination in the single stranded overhangs at the ends of ancient DNA molecules. Note that due to repair steps during NGS library construction, the 3’ G-&gt;A substitutions are simply the reverse complement of the 5’ C-&gt;T substitutions. Figure adapted from (Briggs et al. 2007).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#dna-damage-from-problem-to-solution",
    "href": "introduction-to-ancient-dna.html#dna-damage-from-problem-to-solution",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.7 DNA damage: from problem to solution",
    "text": "2.7 DNA damage: from problem to solution\nWith the advent of NGS sequencing and the revelation that ancient DNA undergoes empirically predictable patterns of DNA degradation, DNA damage went from being an annoying and intractable problem to a powerful solution for DNA authentication. Next we will review the smile plot in detail, including how it is influenced by NGS library construction methods and how software tools designed to characterise deamination patterns are used to authenticate ancient DNA today.\n\n2.7.1 The ‘smile plot’\nWhen interpreting ancient DNA damage patterns, it is critical to know that the characteristic shape of the ancient DNA smile plot is the product of two distinct processes. The first is the DNA degradation process itself, whereby depurination leads to nicking and melting, which produces single-stranded overhangs at the ends of DNA molecules where cytosines deaminate at a rate &gt;100 times faster than in double stranded DNA. The second process relates to the asymmetric behaviour of repair enzymes used during NGS library construction, which we will discuss now.\nUp until now, we have focused on the DNA damage patterns present on the 5’ ends of the DNA fragments, and we have mostly ignored the DNA damage on the 3’ ends. Why? It is because the damage observed on the 3’ ends of ancient DNA molecules depends on the NGS library construction method. In order to understand this, it is important to recall that each strand of DNA has an orientation, or reading order, that runs from the 5’ end of the molecule to 3’ end of the molecule, and that the orientation of the two strands that make up the DNA double helix run in opposite directions. The 5’ to 3’ nomenclature refers to the position of the carbon atom in the 2-deoxyribose sugar that connects to the phosphate backbone (Figure 2.15). The 5’ to 3’ orientation of DNA is important because most enzymes involved in DNA copying and repair only work in one direction of the orientation. So, for the enzymes we use to manipulate DNA during DNA amplification and sequencing it matters very much which side of the DNA fragment is 5’ and which side is 3’.\n\n\n\n\n\n\nFigure 2.15: Model of DNA orientation. Carbon atoms within the 2-deoxyribose sugar component of each nucleotide are numbered to show the opposing 5’ to 3’ orientation of the DNA strands in the double helix. Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\nLet’s next examine a hypothetical pool of double-stranded DNA as it degrades into ancient DNA (Figure 2.16, panels A-C). In this pool, each ancient DNA molecule can exhibit one or more forms of damage in different combinations. For example, two nicks may occur on the same DNA strand, resulting in one strand having both 5’ and 3’ overhangs and the other strand having none; or nicks may occur on opposite DNA strands, resulting in both strands having a 5’ overhang or both strands having a 3’ overhang (Figure 2.16, panel B). Over time, cytosines then deaminate to form uracils on these single stranded overhangs (Figure 2.16, panel C). The presence of these overhangs interferes with the steps needed to prepare the DNA for sequencing, so the first step of DNA library construction is to end-repair the DNA to make the strands fully double-stranded with blunt ends (Figure 2.16, panels D-E). This makes the DNA suitable for ligating on adapters that make the DNA “readable” by the sequencing instrument. DNA repair is performed with T4 polymerase, a type of polymerase that originally came from the T4 virus, a bacteriophage that infects Escherichia coli. T4 polymerase has important properties that must be kept in mind in order to understand how its repair function will affect downstream sequences. When encountering DNA with single-stranded overhangs, T4 polymerase will trim back any 3’ overhangs (Figure 2.16, panel D), and fill in any 5’ overhangs (Figure 2.16, panel E). This removes approximately half of the uracils in the pool of ancient DNA, and it adds an adenine opposite the remaining uracils (recall that polymerases treat uracils like thymines). The result is that uracils only remain on the 5’ ends of the molecules, while the complementary adenines are found only on the 3’ prime ends of the molecule - a pattern that is made even more visible if we imagine rearranging all the strands in the same orientation (Figure 2.16, panel F). This asymmetric behaviour of T4 polymerase is what produces the asymmetric appearance of the ancient DNA smile plot, with an excess of C→T substitutions on the 5’ (left) side of the plot, and an excess of G→A substitutions on the 3’ (right) side of the plot (Figure 2.14).\n\n\n\n\n\n\nFigure 2.16: Processes of DNA decay and DNA repair that contribute to the DNA damage patterns observed in an ancient DNA “smile plot”. As intact DNA (A) decays, it accumulates single stranded nicks that produce fragments with different patterns of single-stranded overhangs (B). Cytosine deamination along the overhangs leads to the accumulation of uracils at the 5’ and 3’ ends of the ancient DNA molecules (C). In the lab, T4 polymerase is used to repair the ancient DNA, which it does by trimming off 3’ overhangs (D) and filling in 5’ overhangs (E). This removes uracils from the 3’ ends of the molecules and adds a complementary adenine opposite the remaining 5’ uracils, introducing 3’ adenines. This asymmetric repair behaviour of the T4 polymerase produces the characteristic asymmetric appearance of a “smile plot”, in which C→T substitutions are observed on the 5’ ends of ancient DNA molecules, and complementary G→A substitutions are observed on the 3’ ends (F). Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\nTo review, the end result of the T4 polymerase damage repair process of ancient DNA is a set of fully double-stranded DNA with blunt ends (no overhangs) and an asymmetrical removal of half of the damage present in the original ancient DNA molecules. Some DNA molecules (those with damage only on their 3’ overhangs) even end up with no damage at all after repair. An important consequence of this asymmetric loss of DNA damage on 3’ overhangs means that ancient DNA prepared using a double-stranded preparation method using T4 polymerase will never reach a terminal substitution rate of 1.0, and instead the maximum substitution rate observed under normal conditions of DNA degradation is 0.5.\n\n\n2.7.2 DNA damage tools\nThe predictable patterns of DNA decay combined with knowledge of how DNA repair mechanisms modify these patterns allow for the development of software to authenticate ancient DNA sequences. The first such publicly available automated tool was mapDamage (Ginolhac et al., 2011), which was later updated to mapDamage 2.0 (Jónsson et al. 2013). Developed in the group of Ludovic Orlando, the tool aligned and compared ancient DNA sequences to reference genomes to visualise patterns of depurination and cytosine deamination, much like those originally presented in the study of the Vindija Neanderthal (Briggs et al. 2007). The same principles were incorporated into PMDtools (Skoglund et al. 2014), a suite of software tools developed by Pontus Skoglund to enable the separation of damaged and non-damaged DNA sequences within a mixture, an application that is helpful in removing modern DNA sequences from heavily contaminated ancient DNA datasets. More recently, the tool DamageProfiler (Neukamm, Peltzer, and Nieselt 2021) was developed to allow a similar level of damage characterization, but in a more computationally efficient manner that decreases runtime and uses less memory. However, because each of these tools was designed with the goal of characterizing DNA damage for a single species of interest, they can be difficult to apply to large microbial metagenomics datasets, such as those generated from ancient microbiomes or sediments. The tool PyDamage (Borry et al. 2021), which allows damage profiling for multiple genomes within a sample, was developed to solve this problem, and the tool can also provide reference-free estimates of DNA damage when combined with de novo assembly.\nCollectively, DNA damage tools are an essential resource in ancient DNA analysis, as they allow researchers to authenticate ancient DNA sequences, estimate sample preservation, and troubleshoot potential problems (Figure 2.17). For example, the absence of DNA damage observed in Figure 2.17 (panel A) indicates that the DNA present in this dataset is likely modern. The spiky damage pattern in Figure 2.17 (panel B) indicates that too few sequences were used to perform the analysis. In general, damage analysis works best with &gt;1,000 aligned sequences, so observing this pattern simply means that more sequencing is needed. A similar pattern can also result if ultrashort DNA sequences (&lt;30 bp) are not removed from the dataset prior to analysis; because ultrashort DNA sequences of this length are taxonomically non-specific, they can misalign to the reference genome, introducing noise. The data in Figure 2.17 (panel C) exhibits an ancient DNA damage pattern at sequence termini but it also has an elevated damage baseline, which suggests that the wrong reference genome was used. This is a common problem in studies of ancient bacteria in which the correct reference genome might not be known or available. For example, imagine that a sample of archaeological dental calculus contains Streptococcus sanguinis, but you align the data to the reference genome for Streptococcus mitis, a different but related species. The reference genome is close enough to produce a damage plot, but the elevated baseline alerts you to the fact that the selected reference genome is incorrect. To remedy this, a closer reference genome should be selected. However, if not available, the aDNA can also be assembled into contiguous sequences, or contigs, and the reads mapped back to these contigs to provide a damage estimate (Borry et al. 2021); this method requires deep metagenomic sequencing, but it is the most reliable way to estimate damage for ancient microbial taxa with inadequate genomic representation in public data repositories. Finally, Figure 2.17 (panel D) shows the expected pattern of DNA damage for a dataset that contains authentic ancient DNA, is sufficiently powered (has enough DNA sequences for analysis), and is aligned to the correct reference genome.\n\n\n\n\n\n\nFigure 2.17: Common patterns of nucleotide misincorporation observed for double-stranded DNA libraries made from archaeological samples. (A) modern DNA; (B) too few sequences for analysis; (C) incorrect reference genome; (D) authentic ancient DNA. Figure modified from https://nf-co.re/eager/2.5.0, by Zandra Fagernäs under CC-BY 4.0 License, (Fellows Yates, Lamnidis, et al. 2021).\n\n\n\nAt this point, you may be wondering whether it is possible to use DNA damage as a clock. If so, that would make DNA damage very useful for dating ancient remains; however, the answer seems to be sort of, but not really. DNA damage is more like a clock that just says “today” or “a while ago”. The relationship between time and DNA damage is not linear, but rather is highly dependent on environmental factors like local temperature and humidity that speed up or slow down the processes of hydrolytic attack and depurination that cause DNA damage. Examining real examples of ancient DNA damage plots makes this clear. Figure 2.18, for example, shows cytosine deamination patterns that have been reported for Neanderthal and human remains from Europe. The Neanderthal remains are &gt;40,000 years old and they have a 0.45 terminal substitution rate, indicating that ~90% of the original DNA fragments contained damaged cytosines. Younger remains from Neolithic Scandinavia dating to around 5,000 years ago had a lower terminal substitution rate of ~ 0.25-0.35 (Skoglund et al. 2014). In contrast, far younger samples in Costa Rica dating to only around 1,000 years ago have terminal substitution rates of &gt;0.45 (Morales-Arce et al. 2017), indicating that as much or more deamination damage has accumulated in these tropical individuals in just one thousand years as in Neanderthals who are &gt;40,000 years older (Figure 2.18). Thus, while DNA damage accumulates over time, it is heavily influenced by temperature and humidity in a way that prevents it from being used as a simple clock.\n\n\n\n\n\n\nFigure 2.18: C to T substitution frequency observed in ancient DNA from different regions and periods. Cytosine deamination accumulates over time, but the rate of this hydrolytic damage is highly dependent on local environmental factors like temperature and humidity. In temperate Europe, 40,000 year-old Neanderthal DNA will have more damage than 5,000 year-old Scandinavian DNA from temperate Europe, but may have less damage than even very young samples from tropical regions like Costa Rica. For these reasons, aDNA damage cannot be used as a simple molecular clock. Adapted from (Morales-Arce et al. 2017; Skoglund et al. 2014).\n\n\n\nEfforts have been made to model these factors in order to better predict DNA damage rates in different locations and under different conditions (Allentoft et al. 2012; Kistler et al. 2017; Smith et al. 2003), and while some broad patterns are clear, such as DNA degrading faster in hot and humid environments than in permafrost and dry caves, accurately predicting specific rates of DNA decay is notoriously difficult (Bokelmann, Glocke, and Meyer 2020). Many environmental factors operate at hyperlocal scales and can cause variation in DNA preservation within a single burial context (Jeong et al. 2016), across a single sediment block (Massilani et al. 2022), among bones and teeth within a single skeleton (Gamba et al. 2014; Parker et al. 2020; Hansen et al. 2017), or even within a single bone (Prüfer et al. 2017; Hajdinjak et al. 2018).\nAnother important factor to keep in mind is that not all organisms undergo DNA degradation and fragmentation at the same rate, even if they originate from the same sample. This can be readily seen by examining the DNA fragment rate 𝝺 (Kistler et al. 2017) in archaeological samples that are known to contain multiple species (Warinner et al. 2017). For example, in an examination of a 19th century herbarium specimen of potato (Solanum tuberosum) infected with fungal pathogen known as potato blight (Phytophthora infestans), a similar DNA fragmentation rate was observed for both species (Figure 2.19, panel A). However, in a human tooth infected with the plague pathogen Yersinia pestis, Y. pestis DNA was found to fragment at a faster rate than human DNA (Figure 2.19, panel B). In dental calculus, which contains the genomes of many species, it has been observed that DNA from oral bacteria such as Streptococcus spp. fragments at a faster rate than DNA from oral archaea such as Methanobrevibacter spp. (Figure 2.19, panel C), suggesting that differences in cell membrane stereochemistry between bacteria and archaea may influence the rate of DNA decay. However, in the same dental calculus samples no relationship was observed between the DNA fragmentation rates of Methanobrevibacter spp. and human host DNA (Figure 2.19, panel D). Thus, while DNA damage does accumulate over time, the rate of degradation can vary across time and space, and even among different organisms within the same sample (Warinner et al. 2017). For these reasons, patterns of DNA damage are typically only compared in a relative manner and not used to predict absolute sample age.\n\n\n\n\n\n\nFigure 2.19: Comparison of DNA fragmentation rates (𝝺) among pairs of organisms within the same sample. (A) Herbarium sample of potato (S. tuberosum) infected with potato blight (P. infestans); (B) Archaeological human teeth (H. sapiens) infected with plague (Y. pestis); Archaeological human dental calculus, showing comparisons between (C) common oral microbes of archaeal (Methanobrevibacter spp.) and bacterial (Streptococcus spp.) origin; and between (D) host (H. sapiens) and bacteria (Streptococcus spp.). Adapted from (Warinner et al. 2017).\n\n\n\n\n\n2.7.3 Removing DNA damage\nUp to this point, we have primarily discussed how molecular damage can be useful for authenticating ancient DNA. However, once the authenticity of the DNA sequences has been established, it may be useful or even necessary to remove some of this damage - specifically cytosine deamination - before proceeding to downstream analyses like taxonomic classification, metagenomic assembly, strain separation, genome phasing, genotyping, or tree-building. Fortunately, there are multiple laboratory-based and bioinformatic-based strategies available to remove molecular damage from your ancient DNA sequences.\nLaboratory-based strategies focus primarily on removing or reducing cytosine deamination from the physical ancient DNA fragments themselves. In 2010, Adrian Briggs and colleagues developed the first enzymatic process for removing uracil-bearing DNA from ancient DNA fragments, focusing initially on ancient DNA from mammoths and Neanderthals (Briggs et al. 2010). In this approach, ancient DNA fragments are phosphorylated using polynucleotide kinase (PNK) and then a two enzyme cocktail named Uracil-Specific Excision Reagent (USER) is applied. The first enzyme, uracil-DNA glycosylase (UDG), locates and removes uracils from the DNA fragments, leaving abasic sites. The second enzyme, endonuclease-VIII, then clips the phosphate backbone at these sites. Because most uracils in ancient DNA are found on single-stranded overhangs, this effectively clips off these damaged ends of the molecules. USER treatment can be advantageous because it removes all of the uracils from a DNA extract, but it also shortens the molecules at the same time. After USER treatment, only the DNA sequences between the innermost uracils of the original aDNA template molecules are likely to be built into a successful DNA library (Figure 2.20, panel A). Damage profiling after USER treatment results in plots with no visible enrichment of base misincorporations (Figure 2.20, panel B).\nMore recently, a modified form of USER treatment known as partial-USER treatment was developed to remove most, but not all, of the uracils in an ancient DNA extract (Rohland et al. 2015). This is achieved by not adding PNK prior to USER treatment and also using a shorter USER incubation time. UDG efficiently excises uracils at most positions, but experimental studies have shown that it will not excise the 5’ terminal uracil if it is unphosphorylated (Varshney and Sande 1991). By excluding PNK from the USER treatment, these terminal uracils are selectively retained in the ancient DNA fragment, whereas most other uracils are removed (Figure 2.20, panel C). Partial USER treatment also fails to remove terminal 3’ uracils, but because 3’ overhangs are removed by T4 polymerase during double-stranded library end repair, they are not observed in downstream damage plots. Within ancient DNA, most 5’ terminal uracils are believed to be phosphorylated, even without PNK treatment, and thus are excised during partial USER treatment. However, a minority of 5’ terminal uracils are unphosphorylated and thus are shielded from excision during partial USER treatment. Through this selective uracil excision, the partial UDG approach provides the advantage of retaining a damage signal in the sequences (for authentication), but limiting it to a predictable location where it can be easily bioinformatically masked or removed for downstream data analysis. As a result, the same DNA library can be used for both authentication and data analysis. Damage profiling after partial UDG treatment results in a plot where only the terminal base at each end of the DNA molecule shows evidence of misincorporation (Figure 2.20, panel D). For ancient DNA, partial UDG-treated libraries will always have a terminal misincorporation rate and an average fragment length that is intermediate between USER-treated and non-USER treated libraries. Typically, the 5’ terminal misincorporation rate of a partial USER-treated library is approximately one third that of a non-USER treated library (Rohland et al. 2015).\n\n\n\n\n\n\nFigure 2.20: Effects of USER and partial USER treatments on ancient DNA. (A) Steps of USER treatment uracil excision; (B) nucleotide misincorporation plot for USER-treated libraries made using a double-stranded preparation method, showing no observable misincorporations; (C) steps of partial-USER treatment uracil excision; (D) nucleotide misincorporation plot for partial USER-treated libraries made using a double-stranded preparation method, showing an excess of misincorporations at a subset of terminal positions only. (B) and (D) are modified from https://nf-co.re/eager/2.5.0, by Zandra Fagernäs under CC-BY 4.0 License, (Fellows Yates, Lamnidis, et al. 2021).\n\n\n\nIn addition to removing DNA damage, USER treatment also provides an additional benefit in ancient DNA studies by allowing states of epigenetic transcription regulation, such as gene silencing, to be identified. Within mammals, cytosine-guanine dinucleotide repeats are abundant within gene promoters, and 70-80% of the cytosines in these pairs are methylated in the form of 5-methylcytosine (Blackledge and Klose 2011). Concentrations of these repeats, known as CpG islands, play important roles in transcription regulation, and the presence of many 5-methylcytosines within these islands is associated with gene silencing (Newell-Price, Clark, and King 2000). As shown in Figure 2.10, while age-related deamination of cytosine produces uracil, deamination of 5-methylcytosine produces thymine. Thus, if USER treatment is applied to ancient DNA, C→T miscoding lesions due to cytosine deamination are removed, but C–&gt;T miscoding lesions due to 5-methylcytosine deamination remain, allowing patterns of gene silencing to be explored in ancient genomes (Briggs et al. 2010; Hanghøj and Orlando 2019). If necessary, one can even reconstruct damage patterns from USER-treated libraries by quantifying 5-methylcytosine deamination at CpG sites; however, this generally requires many aligned sequences to be feasible.\n\n\n2.7.4 Single-stranded library preparation\nUntil now, we have focused our discussion on ancient DNA libraries constructed using a double-stranded preparation method, the standard approach for building DNA libraries for high-throughput sequencing. This approach was adapted into a protocol specifically for ancient DNA by Matthias Meyer and Martin Kircher in 2010 (Meyer and Kircher 2010), and today there are several double-stranded library protocols suitable for ancient DNA in use (Carøe et al. 2018; Fellows Yates, Aron, et al. 2021; Harkins et al. 2020). These approaches are very similar to those used for modern DNA but with a few key differences: first, they generally employ blunt-end ligation because T/A ligation has been shown to induce undesired effects such as GC bias and other artifacts in library construction for ancient DNA (Seguin-Orlando et al. 2013); and second, they typically do not employ a mechanical or enzymatic shearing step (Meyer and Kircher 2010). Shearing is generally unnecessary for ancient DNA because it is already fragmented to sizes within the optimal range for short read sequencing (i.e., &lt;600 bp), and the avoidance of shearing further prevents high molecular weight contaminant DNA from being successfully built into libraries, thereby reducing modern contamination in the libraries. Other protocol steps sometimes applied during modern DNA library construction (e.g., enzymatic tagmentation, size-selective purification, use of uracil-bearing hairpin adapters, commercial single-tube protocols) are not suitable for ancient DNA as they are likely to fail, introduce biases, or enrich for contamination when applied to highly fragmented, low molecular weight ancient DNA (Carøe et al. 2018; Kapp, Green, and Shapiro 2021).\nWhile double-stranded preparation methods can be used to build successful DNA libraries from ancient DNA, they include several steps that disadvantage ancient DNA and can lead to reduced library construction efficiency. For example, because the blunt-end ligation step is non-directional, approximately half of the library will be improperly constructed and consist of DNA molecules containing either only a P5 (IS1/IS3) or only P7 (IS2/IS3) adapter configuration instead of the desired combination of one P5 (IS1/IS3) and one P7 (IS2/IS3) adapter2 (Figure 2.21). Such improperly constructed molecules will not be sequenced. While such a loss is generally trivial for modern DNA, it can be highly consequential for ancient DNA, particularly for samples with very low DNA abundance or low molecular complexity. Further losses occur for ancient DNA molecules containing intra-strand nicks, as these will also fail to amplify properly during the library amplification stage, resulting in further library loss. Finally, the many purification steps required during double-stranded library preparation methods lead to yet more DNA losses, as short DNA fragments are only inefficiently retained by binding to silica or carboxylated beads during these cleaning steps. The development of new single-tube protocols substituting heat denaturation for washing during double-stranded library preparation (Carøe et al. 2018) mitigates some but not all of these losses.\n\n\n\n\n\n\nFigure 2.21: Sources of DNA loss during double-stranded library preparation methods. (A) Ancient DNA is end-repaired, resulting in the removal of 3’ overhangs and the filling in of 5’ overhangs. (B) P5 and P7 adapters and the now blunt-ended ancient DNA molecules are ligated together, resulting in (B) a mixture of incorrect (P5-P5 or P7-P7) and correct (P5-P7 or P7-P5) dsDNA adapter constructs. Of the correctly formed adapter constructs, only those without single-stranded nicks will successfully amplify to produce viable DNA libraries for sequencing (C). Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\nTo address such issues, a single-stranded library preparation method was developed in 2012 for use on highly degraded Denisova remains (Meyer et al. 2012), and then formalised as a protocol by Marie-Theres Gansauge and Matthias Meyer in 2013 (Gansauge and Meyer 2013), and later refined into protocol ssDNA2.0 in 2017 (Gansauge et al. 2017) and automated for robotics in 2020 (Gansauge et al. 2020). This method first heat denatures ancient DNA into single-stranded fragments and then tightly binds these fragments to streptavidin beads, allowing a substantially more controlled and efficient library construction (Figure 2.22). Optionally, ancient DNA can be pretreated with Endonuclease VIII to create nicks at abasic sites prior to single-stranded library preparation in order to avoid polymerase stalling during library amplification (Gansauge and Meyer 2013; Psonis, Vassou, and Kafetzopoulos 2021), but this step generally provides little empirical benefit and so it is no longer implemented in current protocols (Gansauge et al. 2017). In studies of a wide variety of ancient DNA sources (Bennett et al. 2014; Gansauge and Meyer 2013; Wales et al. 2015), the single-stranded library preparation method was found to result in major improvements in DNA recovery, and in some cases an improved endogenous DNA recovery. However, single-stranded library preparation methods are also known to produce libraries with shorter median DNA fragment lengths (Bennett et al. 2014), likely due to the improved retention of very short DNA using streptavidin binding during washing steps and the recovery of nicked DNA that was otherwise lost during double-stranded DNA library preparation methods. Libraries produced by single-stranded library preparation methods are typically 10-40 nucleotides shorter than libraries prepared using double-stranded library preparation methods, and while this poses few disadvantages for studies of large-bodied eukaryotes with well characterised reference genomes, it can introduce challenges for studies of microorganisms where DNA length is important for species deconvolution and de novo genome assembly from large metagenomics datasets (Koren and Phillippy 2015; Kingsford, Schatz, and Pop 2010; Jiang et al. 2012). Currently, multiple single-stranded library preparation protocols are in use for ancient DNA (Gansauge et al. 2020; Gansauge and Meyer 2019; Kapp, Green, and Shapiro 2021).\n\n\n\n\n\n\nFigure 2.22: Improved retention of ancient DNA during single-stranded library preparations. (A) Biotinylated adapters and a pool of ancient DNA molecules are ligated together at high temperature, resulting in denaturation of the ancient DNA strands and (B) correct ssDNA adapter constructs for each single-stranded DNA molecule, including those formed by nicks. All resulting adapter constructs are suitable for downstream library steps, amplification, and sequencing (C). Image by Christina Warinner under a CC-BY-SA 4.0.\n\n\n\n\n\n2.7.5 Damage patterns in DNA prepared using single-stranded library protocols\nAn important feature of single-stranded library preparations is that they do not involve an end-repair step, and consequently damage on the 3’ end of the DNA molecule is retained. As a result, uracils can be present on both the 5’ and 3’ ends of the DNA molecule and adenines are not misincorporated on the 3’ end. Thus, damage plots for ancient DNA prepared using single-stranded library protocols will show an enrichment of C to T substitutions on both the 5’ and 3’ ends (Figure 2.23). Optionally, USER treatment or partial-USER treatment can be applied to ancient DNA prior to single-stranded library preparation (Psonis, Vassou, and Kafetzopoulos 2021; Gansauge and Meyer 2013), although this is less commonly performed for single-stranded library preparations than for double-stranded library preparations, in part because it can result in libraries with high proportions of DNA &lt;30 bp long.\n\n\n\n\n\n\nFigure 2.23: DNA damage pattern for DNA prepared using single-stranded library protocols. C to T substitutions are observed on both the 5’ and 3’ ends of the DNA molecule. Modified from https://nf-co.re/eager/2.5.0, by Zandra Fagernäs under CC-BY 4.0 License\n\n\n\n\n\n2.7.6 Damage plot review\nHaving now examined DNA damage in detail, let’s now review again the most common damage patterns likely to be encountered in ancient DNA studies and what they signify (Figure 2.24). The first plot (Figure 2.24, panel A) shows no misincorporation; this is the pattern expected for modern DNA or ancient DNA that has been USER-treated. The second plot (Figure 2.24, panel B) shows a spiky profile indicating that the analysis is likely underpowered and requires more DNA sequences to make an accurate damage plot. Alternatively, there may be residual ultrashort DNA sequences (&lt;30 bp) in the dataset causing noise; if present, these should be removed and the analysis repeated. The third plot (Figure 2.24, panel C) looks like an expected ancient DNA profile for a library prepared using a double-stranded library preparation method, except that it has an elevated baseline. This indicates that the reference genome used for the analysis is incorrect. To remedy this, a closer reference genome should be selected, or may be created through metagenomic assembly. The fourth plot (Figure 2.24, panel D) shows an expected ancient DNA profile for a library made with a double-stranded library preparation method and using an appropriate reference genome. The fifth plot (Figure 2.24, panel E) shows a typical DNA damage pattern for ancient DNA treated with the partial-USER protocol, followed by a double-stranded library preparation. Finally, the sixth plot (Figure 2.24, panel F) shows an expected DNA damage profile for ancient DNA that has been prepared using a single-stranded library preparation method.\n\n\n\n\n\n\nFigure 2.24: Expected DNA damage profiles for different DNA treatment methods and library preparation protocols. (A) Modern DNA or USER-treated ancient DNA; (B) insufficient DNA for analysis or inclusion of DNA &lt;30 bp; (C) ancient DNA prepared using a double-stranded library preparation method, but aligned to the wrong reference genome; (D) ancient DNA prepared using a double-stranded library preparation method; (E) partial USER-treated ancient DNA prepared using a double-stranded library preparation method; (F) ancient DNA prepared using a single-stranded library preparation method. Model plots are adapted from (https://nf-co.re/eager/2.5.0), by Zandra Fagernäs under CC-BY 4.0 License, (Fellows Yates, Lamnidis, et al. 2021).\n\n\n\n\n\n2.7.7 Enzyme alert\nAt this point, we have only briefly discussed the role of DNA polymerases in DNA library preparation methods, but selection of appropriate DNA polymerases is a critical step in all ancient DNA studies. This is in large part because DNA polymerases greatly differ in how they respond to uracil, a key form of DNA damage present in ancient DNA (Heyn et al. 2010). In life, uracils are an essential component of RNA, where they are one of the four bases that make up the RNA code. Its counterpart in DNA is thymine, a chemically similar base that differs only in a single methyl group (Figure 2.10). Although not typical of DNA, uracil can spontaneously form in DNA through various enzymatic errors and chemical damage, but such uracils are rapidly removed by cellular repair enzymes, including uracil-DNA glycosylases (UDG) (Kavli et al. 2007; Krokan, Drabløs, and Slupphaug 2002; Visnes et al. 2009).\nBecause of the close chemical similarity between uracil and thymine, most DNA polymerases are unable to distinguish between the two bases, treating both as a thymine. Such DNA polymerases are called non-proofreading enzymes. T4 polymerase is one such non-proofreading enzyme, and this is why T4 polymerase incorporates a complementary adenine for each uracil it encounters when repairing 5’ overhangs, thus preserving the C to T damage signal. However, not all DNA polymerases treat thymines and uracils as equivalent. Some DNA polymerases simply stall at uracil positions, either stopping or disengaging from the DNA strand. Such enzymes belong to a group known as proofreading enzymes. Proofreading enzymes are characterised by their ability to recognise misincorporated nucleotides during DNA replication. Only archaea produce the proofreading DNA polymerases capable of recognizing uracil (Wardle et al. 2008), and most proofreading enzymes used in molecular biology are derived from the archaeal species Pyrococcus furiosus (the source of Pfu and Phusion enzymes).\nIt is important to know that many commercial library preparation kits developed for modern DNA cannot be used for ancient DNA because they incorporate proofreading enzymes that stall at uracil sites (Kapp, Green, and Shapiro 2021), causing ancient DNA libraries to become depleted in damaged sequences and resulting in library failure, skewed damage profiles, or a bias towards undamaged contaminant sequences. It is essential to ensure that only non-proofreading DNA polymerases are used during the end-repair and subsequent PCR steps of ancient DNA library preparation, and it is only after the damage signal has been “locked in” by placing complementary adenines opposite each uracil during these repair/replication steps that proofreading enzymes can be used for subsequent steps. USER-treated ancient DNA represents a special case; if uracils are fully removed during USER treatment, USER-treated ancient DNA is compatible with the use of proofreading DNA polymerases during all stages of library preparation. Knowing how different DNA polymerases react to uracil and choosing the appropriate enzyme to use at each step of DNA library preparation is critical for the successful recovery and sequencing of ancient DNA.\nAfter learning of the dangers in using proofreading enzymes during ancient DNA library preparation, one might wonder why proofreading enzymes are used at all. The reason is because they are more accurate, and when used judiciously they can improve the sequence fidelity of your DNA datasets. In a typical ancient DNA workflow, end-repair is performed using the non-proofreading enzyme T4 polymerase, and the indexing PCR is performed using a non-proofreading DNA polymerase, such as Platinum Taq or Pfu Turbo Cx (an archaeal enzyme that has been modified to remove its uracil recognition domain). Once these steps are complete, the resulting DNA library can be subsequently treated like modern DNA and high fidelity proofreading enzymes, such as Herculase II Fusion or Phusion HS II, can be used for all subsequent amplifications, reamplifications, and reconditioning steps in preparation for downstream analyses, such as DNA-capture enrichment or DNA sequencing.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#ancient-microbes-a-60000-piece-jigsaw-puzzle",
    "href": "introduction-to-ancient-dna.html#ancient-microbes-a-60000-piece-jigsaw-puzzle",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.8 Ancient microbes: a 60,000 piece jigsaw puzzle",
    "text": "2.8 Ancient microbes: a 60,000 piece jigsaw puzzle\nEarlier, we discussed how ancient DNA is like the world’s worst jigsaw puzzle and that reconstructing human chromosome 1 from ancient DNA is like trying to assemble a puzzle from 5 million pieces. The problem is not quite so immense for microbes because their genomes are much smaller. So rather than having a 5-million-piece puzzle, reconstructing a typical bacterial genome is more like working with a 60,000-piece puzzle. However, this is admittedly somewhat misleading because ancient microbial species are almost never found in isolation; instead ancient microbes are usually found communities comprising tens to thousands of distinct species, so a more realistic metaphor would be one in which you attempt to reconstruct a hundred mixed-up 60,000-piece puzzles simultaneously. And, making things even worse, some of these puzzles are present more than once.\nTaking a step back, let’s examine the important role of DNA damage in studies of ancient DNA. Why does DNA damage matter so much? DNA damage is useful because it allows for the authentication of ancient DNA at the level of species (Jónsson et al. 2013), contigs (Borry et al. 2021), and even individual sequences (Skoglund et al. 2014). In studies of plague, for example, DNA damage has been used to support the ancient origin and authenticity of reconstructed Yersinia pestis genomes (Andrades Valtueña et al. 2022; Rascovan et al. 2019; Rasmussen et al. 2015; Spyrou et al. 2018). Likewise, DNA damage has been used to support the metagenomic assemblies of fecal bacteria in ancient palaeofaeces (Borry et al. 2020; Maixner et al. 2021; Wibowo et al. 2021), and to authenticate atypical taxa of Pleistocene origin in ancient dental calculus (Klapper et al. 2023). DNA damage can also aid in the correct identification of low abundance pathogens by confirming that damaged sequences show appropriate edit distance profiles for the taxon of interest (Hübler et al. 2019).\nNevertheless, DNA damage also poses taxonomic challenges for studies of ancient microbes, especially as it relates to the correct identification of specific taxa, the accurate mapping of microbial genomes, and successful reconstruction of metagenomically assembled genomes (MAGs). However, it has become apparent that the greatest challenge for reconstructing ancient microbes and their communities is not cytosine deamination, but rather DNA fragmentation. Cytosine deamination has been shown to only minimally impact taxonomic assignment (Velsko et al. 2018), and some de novo assemblers (e.g., MEGAHIT) are relatively robust to substitution errors (Klapper et al. 2023). Moreover, cytosine deamination can be removed or mitigated through USER-treatment, or identified and corrected bioinformatically through deeper sequencing and high coverage genome generation. DNA fragmentation, however, remains a serious and insoluble problem. Ancient DNA is simply very short, and this makes ancient metagenomics more difficult.\nVery short sequences often cannot be uniquely assigned to specific strains or species, leading to ambiguous or false taxonomic assignments in read-based analyses, and sequences less than 250 bp typically produce complex assembly graphs with short contigs and a low N50 (length of the shortest contig needed to account for at least 50% of the total assembly). DNA sequence length is also one of the most important factors affecting ancient DNA assembly success. While some sources of ancient microbial DNA, such as palaeofaeces in very dry caves, have been found to have long median DNA fragment lengths of ~175 bp (Wibowo et al. 2021), most ancient microbial DNA is very short and similar to other sources of ancient DNA. The tipping point for ancient DNA studies occurs when DNA fragments become shorter than 30 bp. Such ultrashort fragments lack sufficient specificity for taxonomic or even genetic placement as they align to too many genomes with no phylogenetic coherence. Thus, DNA degradation imposes some hard limits on the study of ancient microbial DNA.\n\n2.8.1 The limits of ancient DNA\nWhat are the theoretical limits of ancient DNA? DNA decay modeling has predicted an approximately 3-million-year limit for ancient DNA studies – not because DNA does not survive beyond 3 million years, but rather because it is unlikely to survive in fragments of &gt;30 bp beyond 3 million years, even under ideal conditions. In a study of DNA decay rates, Morten Allentoft and colleagues estimated the half-life of a 30 bp DNA fragment to be 158,000 years at -5˚C (Allentoft et al. 2012). For a genetic sample with a starting amount of 1 million DNA molecules, this would mean that no DNA fragments of this length would be left after a period of ca. 3.3 million years. Moreover, they estimate that after 6.8 million years, the average length of ancient DNA would be 1 bp. Such models place deep time palaeobiology outside the realm of palaeogenomics, at least using current in-solution methods in which any spatial information still retained by DNA molecules in its parent substrate is lost during extraction, library preparation, and sequencing.\nThree recent ancient DNA studies, each focusing on remains in permafrost, seem to confirm these estimates. The first, a study of mammoths dating to the Early and Middle Pleistocene (Valk et al. 2021), reported that the recovered DNA was exceptionally short (mostly &lt;35 bp) and nearly saturated with cytosine deamination at the terminal 5’ position. The two oldest mammoths in the study, dating to ca. 1.1 Mya, are the currently oldest eukaryotic genomes to be successfully reconstructed using ancient DNA. The two other studies focused on ca. 2 Mya eDNA recovered from deep subsurface permafrost sediments in Greenland, and they identified diverse ancient plant and animal (Kjær et al. 2022) and microbial and viral (Fernandez-Guerra et al. 2023) DNA sequences. Analysis of the ancient plant and animal DNA has allowed the reconstruction of Late Pliocene and Early Pleistocene environments in Greenland, while the recovered microbial and viral DNA provides insights into the soil microbial ecology of this period. As with the mammoth study, eukaryotic DNA from the sediment cores was found to be mostly &lt;35 bp in length and nearly saturated with cytosine deamination at the terminal 5’ position, suggesting the ancient DNA in these samples falls near the limit of sufficient DNA preservation for palaeogenomic analysis. The microbial DNA was slightly more intact, likely reflecting an extended period of microbial survival in subsurface habitats, as is expected for environmental microbes in geological contexts.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#summary",
    "href": "introduction-to-ancient-dna.html#summary",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThe study of ancient DNA has changed enormously since its beginnings in the 1980s. Gone are the days of electrophoretic gels and rulers for ancient DNA sequencing. Today we have instruments capable of churning out more than 50 billion ancient DNA sequences in a single 48-hour run. This means that aspiring archaeogeneticists need to have a solid foundation in computer science and a proficiency in coding and scripting in languages such as R and Python. During life, genomes are large high molecular weight molecules, but they fragment into thousands or millions of pieces once an organism dies. The very short and ultrashort fragment lengths of ancient DNA makes taxonomic identification, genome mapping, and metagenomic assembly more challenging than for modern DNA, and ancient DNA analysis generally requires the use of specialised software or parameter modifications to account for these differences. Robust and powerful tools now exist to characterise the chemical damage that accumulates in ancient DNA, and factors such as fragment length and cytosine deamination can be used to authenticate ancient DNA, but not provide a precise age estimate. Studies of ancient DNA require specialised laboratories and library protocols in order to appropriately manipulate and handle ancient DNA, and we now have options to remove or mitigate some forms of DNA damage through USER-treatment, or we can choose to maximise recovery of DNA damage through the use of single-stranded DNA library protocols. DNA fragmentation remains the biggest challenge in ancient metagenomics, but tremendous strides in ancient DNA laboratory and bioinformatics methods, in parallel with massive gains in sequencing technology, have broadened the scope of palaeogenomics studies to an enormous diversity of sample types, environments, and applications over the past 3 million years.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#references",
    "href": "introduction-to-ancient-dna.html#references",
    "title": "2  Introduction to Ancient DNA",
    "section": "2.10 References",
    "text": "2.10 References\n\n\n\n\nAllentoft, Morten E, Matthew Collins, David Harker, James Haile, Charlotte L Oskam, Marie L Hale, Paula F Campos, et al. 2012. “The Half-Life of DNA in Bone: Measuring Decay Kinetics in 158 Dated Fossils.” Proc. Biol. Sci. 279 (1748): 4724–33.\n\n\nAndrades Valtueña, Aida, Gunnar U Neumann, Maria A Spyrou, Lyazzat Musralina, Franziska Aron, Arman Beisenov, Andrey B Belinskiy, et al. 2022. “Stone Age Yersinia Pestis Genomes Shed Light on the Early Evolution, Diversity, and Ecology of Plague.” Proc. Natl. Acad. Sci. U. S. A. 119 (17): e2116722119.\n\n\nArmbrecht, Linda H. 2020. “The Potential of Sedimentary Ancient DNA to Reconstruct Past Ocean Ecosystems.” Oceanography 33 (2): 116–23.\n\n\nAustin, J J, A J Ross, A B Smith, R A Fortey, and R H Thomas. 1997. “Problems of Reproducibility–Does Geologically Ancient DNA Survive in Amber-Preserved Insects?” Proc. Biol. Sci. 264 (1381): 467–74.\n\n\nAustin, R M, M Zuckerman, T P Honap, H Lee, G K Ward, C Warinner, K Sankaranarayanan, and C A Hofman. 2022. “Remembering St. Louis Individual—Structural Violence and Acute Bacterial Infections in a Historical Anatomical Collection.” Communications Biology In press.\n\n\nBartlett, John M S, and David Stirling. 2003. “A Short History of the Polymerase Chain Reaction.” Methods Mol. Biol. 226: 3–6.\n\n\nBendich, A J. 1987. “Why Do Chloroplasts and Mitochondria Contain so Many Copies of Their Genome?” Bioessays 6 (6): 279–82.\n\n\nBendich, Arnold J. 2004. “Circular Chloroplast Chromosomes: The Grand Illusion.” Plant Cell 16 (7): 1661–66.\n\n\nBennett, E Andrew, Diyendo Massilani, Giulia Lizzo, Julien Daligault, Eva-Maria Geigl, and Thierry Grange. 2014. “Library Construction for Ancient Genomics: Single Strand or Double Strand?” Biotechniques 56 (6): 289–90, 292–96, 298, passim.\n\n\nBlackledge, Neil P, and Robert Klose. 2011. “CpG Island Chromatin: A Platform for Gene Regulation.” Epigenetics 6 (2): 147–52.\n\n\nBlevins, Kelly E, Adele E Crane, Christopher Lum, Kanako Furuta, Keolu Fox, and Anne C Stone. 2020. “Evolutionary History of Mycobacterium Leprae in the Pacific Islands.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 375 (1812): 20190582.\n\n\nBobay, Louis-Marie, and Howard Ochman. 2017. “The Evolution of Bacterial Genome Architecture.” Front. Genet. 8 (May): 72.\n\n\nBokelmann, Lukas, Isabelle Glocke, and Matthias Meyer. 2020. “Reconstructing Double-Stranded DNA Fragments on a Single-Molecule Level Reveals Patterns of Degradation in Ancient Samples.” Genome Res. 30 (10): 1449–57.\n\n\nBoore, J L. 1999. “Animal Mitochondrial Genomes.” Nucleic Acids Res. 27 (8): 1767–80.\n\n\nBorry, Maxime, Bryan Cordova, Angela Perri, Marsha Wibowo, Tanvi Prasad Honap, Jada Ko, Jie Yu, et al. 2020. “CoproID Predicts the Source of Coprolites and Paleofeces Using Microbiome Composition and Host DNA Content.” PeerJ 8 (April): e9001.\n\n\nBorry, Maxime, Alexander Hübner, Adam B Rohrlach, and Christina Warinner. 2021. “PyDamage: Automated Ancient Damage Identification and Estimation for Contigs in Ancient DNA de Novo Assembly.” PeerJ 9 (July): e11845.\n\n\nBos, Kirsten I, Kelly M Harkins, Alexander Herbig, Mireia Coscolla, Nico Weber, Iñaki Comas, Stephen A Forrest, et al. 2014. “Pre-Columbian Mycobacterial Genomes Reveal Seals as a Source of New World Human Tuberculosis.” Nature 514 (7523): 494–97.\n\n\nBriggs, Adrian W, Udo Stenzel, Philip L F Johnson, Richard E Green, Janet Kelso, Kay Prüfer, Matthias Meyer, et al. 2007. “Patterns of Damage in Genomic DNA Sequences from a Neandertal.” Proc. Natl. Acad. Sci. U. S. A. 104 (37): 14616–21.\n\n\nBriggs, Adrian W, Udo Stenzel, Matthias Meyer, Johannes Krause, Martin Kircher, and Svante Pääbo. 2010. “Removal of Deaminated Cytosines and Detection of in Vivo Methylation in Ancient DNA.” Nucleic Acids Res. 38 (6): e87.\n\n\nCampillo-Balderas, José A, Antonio Lazcano, and Arturo Becerra. 2015. “Viral Genome Size Distribution Does Not Correlate with the Antiquity of the Host Lineages.” Frontiers in Ecology and Evolution 3.\n\n\nCapo, Eric, Marie-Eve Monchamp, Marco J L Coolen, Isabelle Domaizon, Linda Armbrecht, and Stefan Bertilsson. 2022. “Environmental Paleomicrobiology: Using DNA Preserved in Aquatic Sediments to Its Full Potential.” Environ. Microbiol. 24 (5): 2201–9.\n\n\nCarøe, C, S Gopalakrishnan, L Vinner, S Mak, M Holger, S Sinding, J A Samaniego, N Wales, and Sicheritz-Pontén T Gilbert MTP. 2018. “Single‐tube Library Preparation for Degraded DNA.” Methods in Ecology and Evolution 9: 410–19.\n\n\nCecchino, Gustavo N, and Juan A Garcia-Velasco. 2019. “Mitochondrial DNA Copy Number as a Predictor of Embryo Viability.” Fertil. Steril. 111 (2): 205–11.\n\n\nChamplot, Sophie, Camille Berthelot, Mélanie Pruvost, E Andrew Bennett, Thierry Grange, and Eva-Maria Geigl. 2010. “An Efficient Multistrategy DNA Decontamination Procedure of PCR Reagents for Hypersensitive PCR Applications.” Edited by Carles Lalueza-Fox. PLoS One 5 (9): e13042.\n\n\nCooper, A, and H N Poinar. 2000. “Ancient DNA: Do It Right or Not at All.” Science 289 (5482): 1139.\n\n\nDabney, Jesse, Michael Knapp, Isabelle Glocke, Marie-Theres Gansauge, Antje Weihmann, Birgit Nickel, Cristina Valdiosera, et al. 2013. “Complete Mitochondrial Genome Sequence of a Middle Pleistocene Cave Bear Reconstructed from Ultrashort DNA Fragments.” Proc. Natl. Acad. Sci. U. S. A. 110 (39): 15758–63.\n\n\nDidenko, V V, and P J Hornsby. 1996. “Presence of Double-Strand Breaks with Single-Base 3’ Overhangs in Cells Undergoing Apoptosis but Not Necrosis.” J. Cell Biol. 135 (5): 1369–76.\n\n\nDidenko, Vladimir V. 2008. In Situ Detection of DNA Damage: Methods and Protocols. Springer Science & Business Media.\n\n\nDidenko, Vladimir V, Hop Ngo, and David S Baskin. 2003. “Early Necrotic DNA Degradation: Presence of Blunt-Ended DNA Breaks, 3’ and 5’ Overhangs in Apoptosis, but Only 5’ Overhangs in Early Necrosis.” Am. J. Pathol. 162 (5): 1571–78.\n\n\nDuggan, Ana T, Jennifer Klunk, Ashleigh F Porter, Anna N Dhody, Robert Hicks, Geoffrey L Smith, Margaret Humphreys, et al. 2020. “The Origins and Genomic Diversity of American Civil War Era Smallpox Vaccine Strains.” Genome Biol. 21 (1): 175.\n\n\nDyall, Sabrina D, Mark T Brown, and Patricia J Johnson. 2004. “Ancient Invasions: From Endosymbionts to Organelles.” Science 304 (5668): 253–57.\n\n\nEllegaard, Marianne, Martha R J Clokie, Till Czypionka, Dagmar Frisch, Anna Godhe, Anke Kremp, Andrey Letarov, Terry J McGenity, Sofia Ribeiro, and N John Anderson. 2020. “Dead or Alive: Sediment DNA Archives as Tools for Tracking Aquatic Evolution and Adaptation.” Commun Biol 3 (1): 169.\n\n\nFagernäs, Zandra, and Christina Warinner. 2023. “Dental Calculus.” Wiley.\n\n\nFellows Yates, James A, Franziska Aron, Gunnar U Neumann, I Velsko, E Skourtanioti, Eleftheria Orfanou, Zandra Fagernäs, et al. 2021. “A-Z of Ancient DNA Protocols for Shotgun Illumina Next Generation Sequencing V2.” Protocols.io, June.\n\n\nFellows Yates, James A, Thiseas C Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947.\n\n\nFellows Yates, James A, Irina M Velsko, Franziska Aron, Cosimo Posth, Courtney A Hofman, Rita M Austin, Cody E Parker, et al. 2021. “The Evolution and Changing Ecology of the African Hominid Oral Microbiome.” Proc. Natl. Acad. Sci. U. S. A. 118 (20).\n\n\nFernandez-Guerra, Antonio, Guillaume Borrel, Tom O Delmont, Bo Elberling, A Murat Eren, Simonetta Gribaldo, Annika Jochheim, et al. 2023. “A 2-Million-Year-Old Microbial and Viral Communities from the Kap københavn Formation in North Greenland.” bioRxiv.\n\n\nFilippo, Cesare de, Matthias Meyer, and Kay Prüfer. 2018. “Quantifying and Reducing Spurious Alignments for the Analysis of Ultra-Short Ancient DNA Sequences.” BMC Biol. 16 (1): 121.\n\n\nForootan, Amin, Robert Sjöback, Jens Björkman, Björn Sjögreen, Lucas Linz, and Mikael Kubista. 2017. “Methods to Determine Limit of Detection and Limit of Quantification in Quantitative Real-Time PCR (qPCR).” Biomol Detect Quantif 12 (June): 1–6.\n\n\nFotakis, Anna K, Sean D Denham, Meaghan Mackie, Miren Iraeta Orbegozo, Dorothea Mylopotamitaki, Shyam Gopalakrishnan, Thomas Sicheritz-Pontén, et al. 2020. “Multi-Omic Detection of Mycobacterium Leprae in Archaeological Human Dental Calculus.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 375 (1812): 20190584.\n\n\nFrederico, L A, T A Kunkel, and B R Shaw. 1993. “Cytosine Deamination in Mismatched Base Pairs.” Biochemistry 32 (26): 6523–30.\n\n\nGaillard, C, and F Strauss. 1990. “Ethanol Precipitation of DNA with Linear Polyacrylamide as Carrier.” Nucleic Acids Res. 18 (2): 378.\n\n\nGalperin, Michael Y. 2007. “Linear Chromosomes in Bacteria: No Straight Edge Advantage?” Environ. Microbiol. 9 (6): 1357–62.\n\n\nGamba, Cristina, Eppie R Jones, Matthew D Teasdale, Russell L McLaughlin, Gloria Gonzalez-Fortes, Valeria Mattiangeli, László Domboróczki, et al. 2014. “Genome Flux and Stasis in a Five Millennium Transect of European Prehistory.” Nat. Commun. 5 (October): 5257.\n\n\nGansauge, Marie-Theres, Ayinuer Aximu-Petri, Sarah Nagel, and Matthias Meyer. 2020. “Manual and Automated Preparation of Single-Stranded DNA Libraries for the Sequencing of DNA from Ancient Biological Remains and Other Sources of Highly Degraded DNA.” Nat. Protoc. 15 (8): 2279–2300.\n\n\nGansauge, Marie-Theres, Tobias Gerber, Isabelle Glocke, Petra Korlevic, Laurin Lippik, Sarah Nagel, Lara Maria Riehl, Anna Schmidt, and Matthias Meyer. 2017. “Single-Stranded DNA Library Preparation from Highly Degraded DNA Using T4 DNA Ligase.” Nucleic Acids Res. 45 (10): e79.\n\n\nGansauge, Marie-Theres, and Matthias Meyer. 2013. “Single-Stranded DNA Library Preparation for the Sequencing of Ancient or Damaged DNA.” Nat. Protoc. 8 (4): 737–48.\n\n\n———. 2019. “A Method for Single-Stranded Ancient DNA Library Preparation.” Methods Mol. Biol. 1963: 75–83.\n\n\nGilbert, M Thomas P, Eske Willerslev, Anders J Hansen, Ian Barnes, Lars Rudbeck, Niels Lynnerup, and Alan Cooper. 2003. “Distribution Patterns of Postmortem Damage in Human Mitochondrial DNA.” Am. J. Hum. Genet. 72 (1): 32–47.\n\n\nGlocke, Isabelle, and Matthias Meyer. 2017. “Extending the Spectrum of DNA Sequences Retrieved from Ancient Bones and Teeth.” Genome Res. 27 (7): 1230–37.\n\n\nGokhman, David, Eitan Lavi, Kay Prüfer, Mario F Fraga, José A Riancho, Janet Kelso, Svante Pääbo, Eran Meshorer, and Liran Carmel. 2014. “Reconstructing the DNA Methylation Maps of the Neandertal and the Denisovan.” Science 344 (6183): 523–27.\n\n\nGreen, Beverley R. 2011. “Chloroplast Genomes of Photosynthetic Eukaryotes.” Plant J. 66 (1): 34–44.\n\n\nGreen, Richard E, Johannes Krause, Susan E Ptak, Adrian W Briggs, Michael T Ronan, Jan F Simons, Lei Du, et al. 2006. “Analysis of One Million Base Pairs of Neanderthal DNA.” Nature 444 (7117): 330–36.\n\n\nGregory, T R. 2005. “The c-Value Enigma in Plants and Animals: A Review of Parallels and an Appeal for Partnership.” Annals of Botany 95: 133–46.\n\n\nGryseels, Sophie, Thomas D Watts, Jean-Marie Kabongo Mpolesha, Brendan B Larsen, Philippe Lemey, Jean-Jacques Muyembe-Tamfum, Dirk E Teuwen, and Michael Worobey. 2020. “A Near Full-Length HIV-1 Genome from 1966 Recovered from Formalin-Fixed Paraffin-Embedded Tissue.” Proc. Natl. Acad. Sci. U. S. A. 117 (22): 12222–29.\n\n\nHagelberg, E, and J B Clegg. 1991. “Isolation and Characterization of DNA from Archaeological Bone.” Proc. Biol. Sci. 244 (1309): 45–50.\n\n\nHajdinjak, Mateja, Qiaomei Fu, Alexander Hübner, Martin Petr, Fabrizio Mafessoni, Steffi Grote, Pontus Skoglund, et al. 2018. “Reconstructing the Genetic History of Late Neanderthals.” Nature 555 (7698): 652–56.\n\n\nHandt, O, M Höss, M Krings, and S Pääbo. 1994. “Ancient DNA: Methodological Challenges.” Experientia 50 (6): 524–29.\n\n\nHanghøj, Kristian, and Ludovic Orlando. 2019. “Ancient Epigenomics.” In Paleogenomics: Genome-Scale Analysis of Ancient DNA, edited by Charlotte Lindqvist and Om P Rajora, 75–111. Cham: Springer International Publishing.\n\n\nHansen, Henrik B, Peter B Damgaard, Ashot Margaryan, Jesper Stenderup, Niels Lynnerup, Eske Willerslev, and Morten E Allentoft. 2017. “Comparing Ancient DNA Preservation in Petrous Bone and Tooth Cementum.” PLoS One 12 (1): e0170940.\n\n\nHarkins, Kelly M, Nathan K Schaefer, Christopher J Troll, Varsha Rao, Joshua Kapp, Colin Naughton, Beth Shapiro, and Richard E Green. 2020. “A Novel NGS Library Preparation Method to Characterize Native Termini of Fragmented DNA.” Nucleic Acids Res. 48 (8): e47.\n\n\nHeyn, Patricia, Udo Stenzel, Adrian W Briggs, Martin Kircher, Michael Hofreiter, and Matthias Meyer. 2010. “Road Blocks on Paleogenomes—Polymerase Extension Profiling Reveals the Frequency of Blocking Lesions in Ancient DNA.” Nucleic Acids Res. 38 (16): e161–61.\n\n\nHidalgo, Oriane, Jaume Pellicer, Maarten Christenhusz, Harald Schneider, Andrew R Leitch, and Ilia J Leitch. 2017. “Is There an Upper Limit to Genome Size?” Trends Plant Sci. 22 (7): 567–73.\n\n\nHiguchi, R, B Bowman, M Freiberger, O A Ryder, and A C Wilson. 1984. “DNA Sequences from the Quagga, an Extinct Member of the Horse Family.” Nature 312 (5991): 282–84.\n\n\nHinnebusch, J, and K Tilly. 1993. “Linear Plasmids and Chromosomes in Bacteria.” Mol. Microbiol. 10 (5): 917–22.\n\n\nHofreiter, M, D Serre, H N Poinar, M Kuch, and S Pääbo. 2001. “Ancient DNA.” Nat. Rev. Genet. 2 (5): 353–59.\n\n\nHöss, M, and S Pääbo. 1993. “DNA Extraction from Pleistocene Bones by a Silica-Based Purification Method.” Nucleic Acids Res. 21 (16): 3913–14.\n\n\nHübler, Ron, Felix M Key, Christina Warinner, Kirsten I Bos, Johannes Krause, and Alexander Herbig. 2019. “HOPS: Automated Detection and Authentication of Pathogen DNA in Archaeological Remains.” Genome Biol. 20 (1): 280.\n\n\nIlhan, Judith, Anne Kupczok, Christian Woehle, Tanita Wein, Nils F Hülter, Philip Rosenstiel, Giddy Landan, Itzhak Mizrahi, and Tal Dagan. 2019. “Segregational Drift and the Interplay Between Plasmid Copy Number and Evolvability.” Mol. Biol. Evol. 36 (3): 472–86.\n\n\nJeong, Choongwon, Andrew T Ozga, David B Witonsky, Helena Malmström, Hanna Edlund, Courtney A Hofman, Richard W Hagan, et al. 2016. “Long-Term Genetic Stability and a High-Altitude East Asian Origin for the Peoples of the High Valleys of the Himalayan Arc.” Proc. Natl. Acad. Sci. U. S. A. 113 (27): 7485–90.\n\n\nJiang, Hongmei, Lingling An, Simon M Lin, Gang Feng, and Yuqing Qiu. 2012. “A Statistical Framework for Accurate Taxonomic Assignment of Metagenomic Sequencing Reads.” PLoS One 7 (10): e46450.\n\n\nJónsson, Hákon, Aurélien Ginolhac, Mikkel Schubert, Philip L F Johnson, and Ludovic Orlando. 2013. “mapDamage2.0: Fast Approximate Bayesian Estimates of Ancient DNA Damage Parameters.” Bioinformatics 29 (13): 1682–84.\n\n\nKapp, Joshua D, Richard E Green, and Beth Shapiro. 2021. “A Fast and Efficient Single-Stranded Genomic Library Preparation Method Optimized for Ancient DNA.” J. Hered. 112 (3): 241–49.\n\n\nKapusta, Aurélie, Alexander Suh, and Cédric Feschotte. 2017. “Dynamics of Genome Size Evolution in Birds and Mammals.” Proc. Natl. Acad. Sci. U. S. A. 114 (8): E1460–69.\n\n\nKavli, Bodil, Marit Otterlei, Geir Slupphaug, and Hans E Krokan. 2007. “Uracil in DNA—General Mutagen, but Normal Intermediate in Acquired Immunity.” DNA Repair 6 (4): 505–16.\n\n\nKingsford, Carl, Michael C Schatz, and Mihai Pop. 2010. “Assembly Complexity of Prokaryotic Genomes Using Short Reads.” BMC Bioinformatics 11 (January): 21.\n\n\nKistler, Logan, Roselyn Ware, Oliver Smith, Matthew Collins, and Robin G Allaby. 2017. “A New Model for Ancient DNA Decay Based on Paleogenomic Meta-Analysis.” Nucleic Acids Res. 45 (11): 6310–20.\n\n\nKjær, Kurt H, Mikkel Winther Pedersen, Bianca De Sanctis, Binia De Cahsan, Thorfinn S Korneliussen, Christian S Michelsen, Karina K Sand, et al. 2022. “A 2-Million-Year-Old Ecosystem in Greenland Uncovered by Environmental DNA.” Nature 612 (7939): 283–91.\n\n\nKlapper, Martin, Alexander Hübner, Anan Ibrahim, Ina Wasmuth, Maxime Borry, Veit G Haensch, Shuaibing Zhang, et al. 2023. “Natural Products from Reconstructed Bacterial Genomes of the Middle and Upper Paleolithic.” Science 380 (6645): 619–24.\n\n\nKocher, Arthur, Luka Papac, Rodrigo Barquera, Felix M Key, Maria A Spyrou, Ron Hübler, Adam B Rohrlach, et al. 2021. “Ten Millennia of Hepatitis B Virus Evolution.” Science 374 (6564): 182–88.\n\n\nKoonin, Eugene V, Mart Krupovic, and Vadim I Agol. 2021. “The Baltimore Classification of Viruses 50 Years Later: How Does It Stand in the Light of Virus Evolution?” Microbiol. Mol. Biol. Rev. 85 (3): e0005321.\n\n\nKoren, Sergey, and Adam M Phillippy. 2015. “One Chromosome, One Contig: Complete Microbial Genomes from Long-Read Sequencing and Assembly.” Curr. Opin. Microbiol. 23 (February): 110–20.\n\n\nKovaka, Sam, Shujun Ou, Katharine M Jenike, and Michael C Schatz. 2023. “Approaching Complete Genomes, Transcriptomes and Epi-Omes with Accurate Long-Read Sequencing.” Nat. Methods 20 (1): 12–16.\n\n\nKrokan, Hans E, Finn Drabløs, and Geir Slupphaug. 2002. “Uracil in DNA – Occurrence, Consequences and Repair.” Oncogene 21 (58): 8935–48.\n\n\nLindahl, T. 1993. “Instability and Decay of the Primary Structure of DNA.” Nature 362 (6422): 709–15.\n\n\nLindahl, T, and B Nyberg. 1974. “Heat-Induced Deamination of Cytosine Residues in Deoxyribonucleic Acid.” Biochemistry 13 (16): 3405–10.\n\n\nLinderholm, Anna. 2021. “Palaeogenetics: Dirt, What Is It Good for? Everything.” Curr. Biol. 31 (16): R993–95.\n\n\nMaixner, Frank, Mohamed S Sarhan, Kun D Huang, Adrian Tett, Alexander Schoenafinger, Stefania Zingale, Aitor Blanco-Mı́guez, et al. 2021. “Hallstatt Miners Consumed Blue Cheese and Beer During the Iron Age and Retained a Non-Westernized Gut Microbiome Until the Baroque Period.” Curr. Biol. 31 (23): 5149–5162.e6.\n\n\nMann, Allison E, Susanna Sabin, Kirsten Ziesemer, Åshild J Vågene, Hannes Schroeder, Andrew T Ozga, Krithivasan Sankaranarayanan, et al. 2018. “Differential Preservation of Endogenous Human and Microbial DNA in Dental Calculus and Dentin.” Sci. Rep. 8 (1): 9822.\n\n\nMartin, William F, Sriram Garg, and Verena Zimorski. 2015. “Endosymbiotic Theories for Eukaryote Origin.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 370 (1678): 20140330.\n\n\nMassilani, Diyendo, Mike W Morley, Susan M Mentzer, Vera Aldeias, Benjamin Vernot, Christopher Miller, Mareike Stahlschmidt, et al. 2022. “Microstratigraphic Preservation of Ancient Faunal and Hominin DNA in Pleistocene Cave Sediments.” Proc. Natl. Acad. Sci. U. S. A. 119 (1).\n\n\nMeyer, Matthias, and Martin Kircher. 2010. “Illumina Sequencing Library Preparation for Highly Multiplexed Target Capture and Sequencing.” Cold Spring Harb. Protoc. 2010 (6): db.prot5448.\n\n\nMeyer, Matthias, Martin Kircher, Marie-Theres Gansauge, Heng Li, Fernando Racimo, Swapan Mallick, Joshua G Schraiber, et al. 2012. “A High-Coverage Genome Sequence from an Archaic Denisovan Individual.” Science 338 (6104): 222–26.\n\n\nMichael, Todd P. 2014. “Plant Genome Size Variation: Bloating and Purging DNA.” Brief. Funct. Genomics 13 (4): 308–17.\n\n\nMoraes, C T. 2001. “What Regulates Mitochondrial DNA Copy Number in Animal Cells?” Trends Genet. 17 (4): 199–205.\n\n\nMorales-Arce, Ana Y, Courtney A Hofman, Ana T Duggan, Adam K Benfer, M Anne Katzenberg, Geoffrey McCafferty, and Christina Warinner. 2017. “Successful Reconstruction of Whole Mitochondrial Genomes from Ancient Central America and Mexico.” Sci. Rep. 7 (1): 18100.\n\n\nMorley, Stewart A, and Brent L Nielsen. 2017. “Plant Mitochondrial DNA.” Front. Biosci. 22 (6): 1023–32.\n\n\nMühlemann, Barbara, Lasse Vinner, Ashot Margaryan, Helene Wilhelmson, Constanza de la Fuente Castro, Morten E Allentoft, Peter de Barros Damgaard, et al. 2020. “Diverse Variola Virus (Smallpox) Strains Were Widespread in Northern Europe in the Viking Age.” Science 369 (6502).\n\n\nMullis, K, F Faloona, S Scharf, R Saiki, G Horn, and H Erlich. 1992. “Specific Enzymatic Amplification of DNA in Vitro: The Polymerase Chain Reaction. 1986.” Biotechnology 24: 17–27.\n\n\nNakabachi, Atsushi, Atsushi Yamashita, Hidehiro Toh, Hajime Ishikawa, Helen E Dunbar, Nancy A Moran, and Masahira Hattori. 2006. “The 160-Kilobase Genome of the Bacterial Endosymbiont Carsonella.” Science 314 (5797): 267.\n\n\nNeukamm, Judith, Alexander Peltzer, and Kay Nieselt. 2021. “DamageProfiler: Fast Damage Pattern Calculation for Ancient DNA.” Bioinformatics 37 (20): 3652–53.\n\n\nNewell-Price, J, A J Clark, and P King. 2000. “DNA Methylation and Silencing of Gene Expression.” Trends Endocrinol. Metab. 11 (4): 142–48.\n\n\nNoonan, James P, Graham Coop, Sridhar Kudaravalli, Doug Smith, Johannes Krause, Joe Alessi, Feng Chen, et al. 2006. “Sequencing and Analysis of Neanderthal Genomic DNA.” Science 314 (5802): 1113–18.\n\n\nPääbo, Svante, Hendrik Poinar, David Serre, Viviane Jaenicke-Després, Juliane Hebler, Nadin Rohland, Melanie Kuch, Johannes Krause, Linda Vigilant, and Michael Hofreiter. 2004. “GENETIC ANALYSES FROM ANCIENT DNA.” Annu. Rev. Genet. 38 (1): 645–79.\n\n\nParfrey, Laura Wegener, Daniel J G Lahr, and Laura A Katz. 2008. “The Dynamic Nature of Eukaryotic Genomes.” Mol. Biol. Evol. 25 (4): 787–94.\n\n\nParker, Cody, Adam B Rohrlach, Susanne Friederich, Sarah Nagel, Matthias Meyer, Johannes Krause, Kirsten I Bos, and Wolfgang Haak. 2020. “A Systematic Investigation of Human DNA Preservation in Medieval Skeletons.” Sci. Rep. 10 (1): 18225.\n\n\nPiovesan, Allison, Maria Chiara Pelleri, Francesca Antonaros, Pierluigi Strippoli, Maria Caracausi, and Lorenza Vitale. 2019. “On the Length, Weight and GC Content of the Human Genome.” BMC Res. Notes 12 (1): 106.\n\n\nPreuten, Tobias, Emilia Cincu, Jörg Fuchs, Reimo Zoschke, Karsten Liere, and Thomas Börner. 2010. “Fewer Genes Than Organelles: Extremely Low and Variable Gene Copy Numbers in Mitochondria of Somatic Plant Cells.” Plant J. 64 (6): 948–59.\n\n\nPrüfer, Kay, Cesare de Filippo, Steffi Grote, Fabrizio Mafessoni, Petra Korlević, Mateja Hajdinjak, Benjamin Vernot, et al. 2017. “A High-Coverage Neandertal Genome from Vindija Cave in Croatia.” Science 358 (6363): 655–58.\n\n\nPsonis, Nikolaos, Despoina Vassou, and Dimitris Kafetzopoulos. 2021. “Testing a Series of Modifications on Genomic Library Preparation Methods for Ancient or Degraded DNA.” Anal. Biochem. 623 (June): 114193.\n\n\nRadini, A, M Tromp, A Beach, E Tong, C Speller, M McCormick, J V Dudgeon, et al. 2019. “Medieval Women’s Early Involvement in Manuscript Production Suggested by Lapis Lazuli Identification in Dental Calculus.” Science Advances.\n\n\nRascovan, Nicolás, Karl-Göran Sjögren, Kristian Kristiansen, Rasmus Nielsen, Eske Willerslev, Christelle Desnues, and Simon Rasmussen. 2019. “Emergence and Spread of Basal Lineages of Yersinia Pestis During the Neolithic Decline.” Cell.\n\n\nRasmussen, Simon, Morten Erik Allentoft, Kasper Nielsen, Ludovic Orlando, Martin Sikora, Karl-Göran Sjögren, Anders Gorm Pedersen, et al. 2015. “Early Divergent Strains of Yersinia Pestis in Eurasia 5,000 Years Ago.” Cell.\n\n\nRohland, Nadin, Eadaoin Harney, Swapan Mallick, Susanne Nordenfelt, and David Reich. 2015. “Partial uracil–DNA–glycosylase Treatment for Screening of Ancient DNA.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 370 (1660): 20130624.\n\n\nSaiki, R K, D H Gelfand, S Stoffel, S J Scharf, R Higuchi, G T Horn, K B Mullis, and H A Erlich. 1988. “Primer-Directed Enzymatic Amplification of DNA with a Thermostable DNA Polymerase.” Science 239 (4839): 487–91.\n\n\nSaiki, R K, S Scharf, F Faloona, K B Mullis, G T Horn, H A Erlich, and N Arnheim. 1985. “Enzymatic Amplification of Beta-Globin Genomic Sequences and Restriction Site Analysis for Diagnosis of Sickle Cell Anemia.” Science 230 (4732): 1350–54.\n\n\nSanger, F, S Nicklen, and A R Coulson. 1977. “DNA Sequencing with Chain-Terminating Inhibitors.” Proc. Natl. Acad. Sci. U. S. A. 74 (12): 5463–67.\n\n\nSchneiker, Susanne, Olena Perlova, Olaf Kaiser, Klaus Gerth, Aysel Alici, Matthias O Altmeyer, Daniela Bartels, et al. 2007. “Complete Genome Sequence of the Myxobacterium Sorangium Cellulosum.” Nat. Biotechnol. 25 (11): 1281–89.\n\n\nSchuenemann, Verena J, Charlotte Avanzi, Ben Krause-Kyora, Alexander Seitz, Alexander Herbig, Sarah Inskip, Marion Bonazzi, et al. 2018. “Ancient Genomes Reveal a High Diversity of Mycobacterium Leprae in Medieval Europe.” PLoS Pathog. 14 (5): e1006997.\n\n\nSeguin-Orlando, Andaine, Mikkel Schubert, Joel Clary, Julia Stagegaard, Maria T Alberdi, José Luis Prado, Alfredo Prieto, Eske Willerslev, and Ludovic Orlando. 2013. “Ligation Bias in Illumina Next-Generation DNA Libraries: Implications for Sequencing Ancient Genomes.” PLoS One 8 (10): e78575.\n\n\nShevchenko, Anna, Yimin Yang, Andrea Knaust, Henrik Thomas, Hongen Jiang, Enguo Lu, Changsui Wang, and Andrej Shevchenko. 2014. “Proteomics Identifies the Composition and Manufacturing Recipe of the 2500-Year Old Sourdough Bread from Subeixi Cemetery in China.” J. Proteomics 105 (June): 363–71.\n\n\nShillito, Lisa-Marie, John C Blong, Eleanor J Green, and Eline Van Asperen. 2020. “The What, How and Why of Archaeological Coprolite Analysis.” Earth-Sci. Rev., 103196.\n\n\nSkoglund, Pontus, Bernd H Northoff, Michael V Shunkov, Anatoli P Derevianko, Svante Pääbo, Johannes Krause, and Mattias Jakobsson. 2014. “Separating Endogenous Ancient DNA from Modern Day Contamination in a Siberian Neandertal.” Proc. Natl. Acad. Sci. U. S. A. 111 (6): 2229–34.\n\n\nSmith, Colin I, Andrew T Chamberlain, Michael S Riley, Chris Stringer, and Matthew J Collins. 2003. “The Thermal History of Human Fossils and the Likelihood of Successful DNA Amplification.” J. Hum. Evol. 45 (3): 203–17.\n\n\nSpyrou, Maria A, Kirsten I Bos, Alexander Herbig, and Johannes Krause. 2019. “Ancient Pathogen Genomics as an Emerging Tool for Infectious Disease Research.” Nat. Rev. Genet. 20 (6): 323–40.\n\n\nSpyrou, Maria A, Rezeda I Tukhbatova, Chuan-Chao Wang, Aida Andrades Valtueña, Aditya K Lankapalli, Vitaly V Kondrashin, Victor A Tsybin, et al. 2018. “Analysis of 3800-Year-Old Yersinia Pestis Genomes Suggests Bronze Age Origin for Bubonic Plague.” Nature Communications.\n\n\nStiller, Mathias, Antje Sucker, Klaus Griewank, Daniela Aust, Gustavo Bruno Baretton, Dirk Schadendorf, and Susanne Horn. 2016. “Single-Strand DNA Library Preparation Improves Sequencing of Formalin-Fixed and Paraffin-Embedded (FFPE) Cancer DNA.” Oncotarget 7 (37): 59115–28.\n\n\nThomas, Christopher M, and David Summers. 2020. “Bacterial Plasmids.” eLS. Wiley.\n\n\nVågene, Åshild J, Alexander Herbig, Michael G Campana, Nelly M Robles Garcı́a, Christina Warinner, Susanna Sabin, Maria A Spyrou, et al. 2018. “Salmonella Enterica Genomes from Victims of a Major Sixteenth-Century Epidemic in Mexico.” Nat Ecol Evol 2 (3): 520–28.\n\n\nValk, Tom van der, Patrı́cia Pečnerová, David Dı́ez-Del-Molino, Anders Bergström, Jonas Oppenheimer, Stefanie Hartmann, Georgios Xenikoudakis, et al. 2021. “Million-Year-Old DNA Sheds Light on the Genomic History of Mammoths.” Nature 591 (7849): 265–69.\n\n\nVarshney, U, and J H van de Sande. 1991. “Specificities and Kinetics of Uracil Excision from Uracil-Containing DNA Oligomers by Escherichia Coli Uracil DNA Glycosylase.” Biochemistry 30 (16): 4055–61.\n\n\nVelsko, Irina M, Laurent A F Frantz, Alexander Herbig, Greger Larson, and Christina Warinner. 2018. “Selection of Appropriate Metagenome Taxonomic Classifiers for Ancient Microbiome Research.” mSystems.\n\n\nVisnes, Torkild, Berit Doseth, Henrik Sahlin Pettersen, Lars Hagen, Mirta M L Sousa, Mansour Akbari, Marit Otterlei, Bodil Kavli, Geir Slupphaug, and Hans E Krokan. 2009. “Uracil in DNA and Its Processing by Different DNA Glycosylases.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 364 (1517): 563–68.\n\n\nWales, Nathan, Christian Carøe, Marcela Sandoval-Velasco, Cristina Gamba, Ross Barnett, José Alfredo Samaniego, Jazmı́n Ramos Madrigal, Ludovic Orlando, and M Thomas P Gilbert. 2015. “New Insights on Single-Stranded Versus Double-Stranded DNA Library Preparation for Ancient DNA.” Biotechniques 59 (6): 368–71.\n\n\nWall, Jeffrey D, and Sung K Kim. 2007. “Inconsistencies in Neanderthal Genomic DNA Sequences.” PLoS Genet. 3 (10): 1862–66.\n\n\nWardle, Josephine, Peter M J Burgers, Isaac K O Cann, Kate Darley, Pauline Heslop, Erik Johansson, Li-Jung Lin, et al. 2008. “Uracil Recognition by Replicative DNA Polymerases Is Limited to the Archaea, Not Occurring with Bacteria and Eukarya.” Nucleic Acids Res. 36 (3): 705–11.\n\n\nWarinner, Christina. 2022. “An Archaeology of Microbes.” J. Anthropol. Res., December, 000–000.\n\n\nWarinner, Christina, Alexander Herbig, Allison Mann, James A Fellows Yates, Clemens L Weiß, Hernán A Burbano, Ludovic Orlando, and Johannes Krause. 2017. “A Robust Framework for Microbial Archaeology.” Annu. Rev. Genomics Hum. Genet. 18 (August): 321–56.\n\n\nWarinner, Christina, João F Matias Rodrigues, Rounak Vyas, Christian Trachsel, Natallia Shved, Jonas Grossmann, Anita Radini, et al. 2014. “Pathogens and Host Immunity in the Ancient Human Oral Cavity.” Nat. Genet. 46 (4): 336–44.\n\n\nWarinner, Christina, Camilla Speller, and Matthew J Collins. 2015. “A New Era in Palaeomicrobiology: Prospects for Ancient Dental Calculus as a Long-Term Record of the Human Oral Microbiome.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 370 (1660): 20130376.\n\n\nWegner, Carl-Eric, Raphaela Stahl, Irina Velsko, Alex Hübner, Zandra Fagernäs, Christina Warinner, Robert Lehmann, Thomas Ritschel, Kai U Totsche, and Kirsten Küsel. 2023. “A Glimpse of the Paleome in Endolithic Microbial Communities.” Microbiome 11 (1): 210.\n\n\nWestoby, Mark, Daniel Aagren Nielsen, Michael R Gillings, Elena Litchman, Joshua S Madin, Ian T Paulsen, and Sasha G Tetu. 2021. “Cell Size, Genome Size, and Maximum Growth Rate Are Near-Independent Dimensions of Ecological Variation Across Bacteria and Archaea.” Ecol. Evol. 11 (9): 3956–76.\n\n\nWibowo, Marsha C, Zhen Yang, Maxime Borry, Alexander Hübner, Kun D Huang, Braden T Tierney, Samuel Zimmerman, et al. 2021. “Reconstruction of Ancient Microbial Genomes from the Human Gut.” Nature 594 (7862): 234–39.\n\n\nWorobey, Michael, Marlea Gemmel, Dirk E Teuwen, Tamara Haselkorn, Kevin Kunstman, Michael Bunce, Jean-Jacques Muyembe, et al. 2008. “Direct Evidence of Extensive Diversity of HIV-1 in Kinshasa by 1960.” Nature 455 (7213): 661–64.\n\n\nXie, Mingsi, Anna Shevchenko, Binghua Wang, Andrej Shevchenko, Changsui Wang, and Yimin Yang. 2016. “Identification of a Dairy Product in the Grass Woven Basket from Gumugou Cemetery (3800 BP, Northwestern China).” Quat. Int. 426 (December): 158–65.\n\n\nZavala, Elena I, Zenobia Jacobs, Benjamin Vernot, Michael V Shunkov, Maxim B Kozlikin, Anatoly P Derevianko, Elena Essel, et al. 2021. “Pleistocene Sediment DNA Reveals Hominin and Faunal Turnovers at Denisova Cave.” Nature 595 (7867): 399–403.\n\n\nZhu, T, B T Korber, A J Nahmias, E Hooper, P M Sharp, and D D Ho. 1998. “An African HIV-1 Sequence from 1959 and Implications for the Origin of the Epidemic.” Nature 391 (6667): 594–97.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-ancient-dna.html#footnotes",
    "href": "introduction-to-ancient-dna.html#footnotes",
    "title": "2  Introduction to Ancient DNA",
    "section": "",
    "text": "Temperatures calculated using the IDT Oligoanalyser tool (https://eu.idtdna.com/calc/analyser).↩︎\nSee (Meyer and Kircher 2010) for an explanation of adapter and primer nomenclature for double-stranded library preparation.↩︎",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ancient DNA</span>"
    ]
  },
  {
    "objectID": "introduction-to-metagenomics.html",
    "href": "introduction-to-metagenomics.html",
    "title": "3  Introduction to Metagenomics",
    "section": "",
    "text": "Important\n\n\n\n🚧 This page is still under construction 🚧\n\n\nThis chapter is still under construction. For the slides and recorded lecture version of this session, please see the Werner Siemens-Stiftung funded SPAAM Summer School: Introduction to Ancient Metagenomics website.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Metagenomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html",
    "href": "introduction-to-microbial-genomics.html",
    "title": "4  Introduction to Microbial Genomics",
    "section": "",
    "text": "4.1 Introduction\nMicrobial species come from diverse groups of organisms but are generally defined as organisms which are not visible to the naked eye. The most prominent amongst them are bacteria, which are single-celled prokaryotes with either circular and linear dsDNA genomes (up to ~14 Mbp). Archaea are mostly mutualistic or commensal single-celled prokaryotes with circular dsDNA genomes (~0.5 to ~5.8 Mbp). The often references group of protozoa, is a not phylogenetically defined grouping (often used in databases). They are a wide variety of free-feeding single-celled eukaryotes from the protists group with larger genomes (~2.9 to ~160 Mbp).\nFinally, viruses are understood as microbial organisms, even though they are not technically considered as “organisms”. Viruses lack the ability to live or replicate independently of host cells. They are mostly defined as infectious agents and have smaller linear or circular ssDNA, dsDNA & RNA genomes (2 kb to over 1 Mb). Another category of viruses are retroviruses. These will always be RNA viruses which can integrate into the host genome by converting to DNA. But there are also DNA viruses, which integrate into the human genome. There the virus can either be latent and be later triggered to activate, continuously produce virions or lose the ability to produce virions and become part of the host genome. An integrated genome is called a provirus and vertically inherited proviral sequences are called endogenous viral elements (EVEs).\nThere are several types of microbial organisms found within ancient DNA samples of animal hosts:\nHowever, it should be noted that not all cases are quite as clear-cut, as there are wide varieties of microbial lifestyles. Some organisms can be considered pathogenic in one sampling location and commensal in the next (pathobionts). Some organisms can also be considered harmful, when they are represented in very large amounts or in combination with other organisms. The sampling location can therefore be important information when studying the health of hosts.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#introduction",
    "href": "introduction-to-microbial-genomics.html#introduction",
    "title": "4  Introduction to Microbial Genomics",
    "section": "",
    "text": "Pathogens: infectious microorganisms or agents which cause disease in the infected host. Usually specialised organisms with delimited ecological niches.\nCommensals: non-infectious microorganisms or agents which live within a host/environment without causing harm and can be beneficial.\nEnvironmental microorganisms: Environmental microbial organisms not endogenous to the host ante-mortem, which e.g. stem from the depositional environment, the storage environment or the lab.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#larger-genomic-elements",
    "href": "introduction-to-microbial-genomics.html#larger-genomic-elements",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.2 Larger Genomic Elements",
    "text": "4.2 Larger Genomic Elements\nWhen using reference sequences from public depositories, such as NCBI, you will often be confronted with multiple sequences which represent the chromosome(s) and additional extrachromosomal sequences associated with a species or strain. A chromosome is understood as the main genetic element. It contains the core genome with all essential genetic elements, which usually remain the same across a species.\nPlasmids, on the other hand, are extra-chromosomal DNA replicons. They are replicating and acquiring/losing genetic material independently of the chromosome and can be present in high copy numbers. Plasmids can be vertically or horizontally transferred. The number of plasmids of a bacterium varies a lot (0 to 30+), and can vary within a species. They can carry virulence genes and genes which can give selective advantages. Plasmids generally have much smaller circular genomes (1 to over 400 Kbp). It should be noted that they are under-represented in databases, meaning that they are often ignored during the assembly/sequencing process and thus not present in all reference sequences, even if they could otherwise have been there.\nBesides plasmids, bacteriophages (or phages) are also important extrachromosomal (and sometimes chromosomally integrated) genetic material associated with bacterial genomes. Although they are not considered as part of the genome, and therefore part of reference sequences, if they are not integrated in the host cell genome. Bacteriophages are dsDNA viruses of small size that infect bacteria. As phages have mostly dsDNA genomes, they are also well suited for the recovery using standard aDNA techniques. They can be found everywhere and carry gene sets of diverse size and composition. They enter the cytoplasm of bacteria, where they can then replicate. There are a huge number of phages, and some are specific to certain bacterial genera or species, which can be useful for identifying taxa or phylogenetic clades. When bacteriophage genomes integrate into the host cell chromosomal genome (e.g. through horizontal gene transfer) or exist as a plasmid within the bacterial cell, they are called prophages. They can make up a large fraction of the pan-genome of plastic species.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#sampling-sequencing",
    "href": "introduction-to-microbial-genomics.html#sampling-sequencing",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.3 Sampling & Sequencing",
    "text": "4.3 Sampling & Sequencing\nFor ancient DNA samples, screening for organisms of interest is usually performed on “shotgun” sequencing data, which can then be either further genomically analysed following species identification or the library will be enriched for organisms identified during screening. Prior to sequencing, laboratory workflows, and particularly library building setups, can have a major impact on your final output (e.g. if you are interested in RNA/ssDNA viruses but have dsDNA libraries).\nEach pathogen is subject to tissue tropism, meaning that each species will have a range of cells or tissue types it will preferentially or exclusively proliferate and replicate in. This phenomenon is called tissue tropism. In some instances, the pathogen can also become latent within said tissues, meaning it will remain within the host tissue without causing disease (but could still reactivate later). Organisms which cause bacteremia or viremia, meaning they are present in the bloodstream, will not be as restricted and are generally considered easier to detect. Although this can be limited by the disease phenotype or the stage of infection (e.g. Haemophilus influenzae).\nAccordingly, what organism you can find in your data is highly dependent on the sample and where it was taken from. For example, if the organism of interest enters the bloodstream, teeth, which are vascularised and are excellent DNA archives, will probably be a good choice for sampling. Some pathogens (e.g. Mycobacterium leprae) are present in higher quantities within lesions caused by their infections, making those lesions better suited for sampling. In the case of infections, which are unlikely to be retrievable from hard tissue, calcified nodules can be an interesting type of sample. Bone will generally be less well suited, with some exceptions. Integrated viruses and retroviruses will be likelier to be found in samples types with higher host DNA, such as the petrous bone or the ossicles. And early childhood infection can also be found within low remodelling cortical bone.\n\n\n\n\n\n\nSome Myths to Dispel…\n\n\n\n\nMicrobial content and pathogen content is NOT directly correlated to human DNA content. Human DNA content is not a measure for overall sample preservation. You can have really bad samples for human DNA but get a full microbial genome from the same sample!\nYou can also find microbial signatures in petrous bones etc. it will just be a different range of organisms. Everything is worth getting screened!\n\n\n\nThe typical number of sequences recovered for an organism will depend on a range of factors which can only rarely be predicted, even in samples for which osteological/historical records show a clear association with a pathogen. Major factors are: overall sample preservation, depth of sequencing, abundance of the organism peri-mortem, disease phenotype, genome size & composition, “noise” from other organism, etc.\nOverall, it is considered a good result if your target organism makes out around 0.1% of shotgun datasets, but in many cases it will be way less. However, depending on the organism, the amount of data you need to confidently classify it will be very different, mostly due to sequence conservation, genome size and complexity. While a bacterium is generally easier to detect, their large genome size and sequence conservation, can make them harder to verify. On the other hand, viruses, having much smaller genomes, are harder to detect, especially at low sequencing depth, but generally easier to verify within a margin of uncertainty.\nFor some applications in microbial genomics, unselective/unbiased sequencing is key, and because of this shotgun sequencing is a powerful tool at our disposal, since it allows for the indiscriminate sequencing of all DNA found within the sample. Particularly in cases of organisms with large pangenomes, where you might be interested in looking for a large variety of intervals, which would be very costly to do using target enrichment. Or in cases where novel insights and genomes become relevant following the design of a target enrichment kit. Additionally, it allows for the simultaneous analysis of both microbial and host DNA. Understanding the composition of the microbial community of your sample can also be highly relevant with regard to identifying co-infectants, reconstructing disease histories and excluding gene intervals which could also stem from commensals or contaminants.\n\n\n\n\n\n\nShotgun sequencing for microbial genomics\n\n\n\nPro(s):\n\nNon-targeted taxa can be found.\nAllows the analysis of DNA from both microbial organisms and host simultaneously.\nAllows you to detect untargeted co-infections.\nAllows you to understand the composition of the microbial community (can be relevant for genomic analysis).\nNo knowledge of the taxon genomic diversity is needed beforehand.\nEnables the long time use of the data with ever-growing databases.\n\nCon(s):\n\nCan be much more expensive (depending on the sample).\nOverall less effective at generating adequate/high coverage data.\n\n\n\nHowever, microbial species make up only very small fractions of genomic libraries. So small that it can either be impossible or very expensive to try to assemble a genome using only shotgun data in most circumstances. This is usually where target enrichment, or capture, comes into play. Often, the baits required for capture are custom designed for each study. The design of such kits necessities appropriate knowledge of the genetic diversity to be expected during analysis. In most cases, a mixture of shotgun and target enrichment can be most effective, especially with regard to authentication if enrichment is performed on UDG/Half-UDG genomic libraries.\n\n\n\n\n\n\nTarget enrichment for microbial genomics\n\n\n\nPro(s):\n\nMuch cheaper per genome, especially in samples with bad preservation.\nEffective at generating high coverage genomes.\nAllows for the generation of large number of genomes effectively.\nGreat for core-genome reconstructions.\n\nCon(s):\n\nNecessitates knowledge of the genetic diversity relevant to your research questions during the design phase.\nAny sequence not included, within a margin of variation, will not be captured.\nWill not generate any data with regard to the host or the rest of the microbial community (in fact, that is the point).\nFor some species, capturing the pangenome would be extremely expensive, provided it doesn’t fully exceed kit sizes.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#validating-the-presence-of-organisms-of-interest",
    "href": "introduction-to-microbial-genomics.html#validating-the-presence-of-organisms-of-interest",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.4 Validating the Presence of Organisms of Interest",
    "text": "4.4 Validating the Presence of Organisms of Interest\nFollowing analysis of your data using appropriate databases and taxonomic classifiers, it is time to validate your hit. The results of a classifier should not be your end result. Properly validating the presence of a species is key to any genomic analysis! An identification using taxonomic classifiers is not a sufficient validation on its own, as it can be very tricky to work around false-positive hits and missing database entries. Databases are biased towards pathogenic species, as they represent the bulk of research. Closely related non-pathogenic species will either be represented less or not at all, which leads to high read assignments to the LCA (Lowest Common Ancestor) and pathogenic taxa. You should also consider whether it makes biological sense for the species you found to be detected in the sampled tissue and based on your laboratory workflow.\nFor ancient DNA, the taxon validation will often consist of three steps:\n\nAuthentication of the data as aDNA using deamination signatures and fragment lengths.\nComparative or competitive mappings to relevant reference sequences (target organism and closely related pathogenic/environmental/commensal species).\nIdentification of intervals specific to the target species or sub-species.\n\n\n\n\n\n\n\nTip\n\n\n\nIt can also be a good idea in some cases to double-check the modern data you are using during your analysis. There are cases in which the metadata doesn’t match reality.\n\n\nIncreased coverage in selected intervals in an alignment is often caused by sequence conservation. Meaning that you could have the same number of reads mapping to a genome but in the first case all reads are mapping to 5% of the genome with high depth of coverage, whether in the second one you will see low coverage across the whole sequence. The first one is likely to be a false positive, as your detection is probably based on intervals from either a commensal or an environmental organism which have very high sequence similarity to your reference genome. This can be further worsened by low complexity intervals. These peaks will usually also be reflected in the edit distance, as such tiling will also result in more sequence mismatches. While this can be reduced with higher mapping stringency, this in turn will probably decrease your coverage significantly and in some cases this can cause a reference bias. However, in most cases, these intervals will not pass the mapping quality filter.\n\n\n\n\n\n\nDefinition\n\n\n\nConserved Intervals: Regions of genomes common to organisms from the same taxonomic units (can affect all taxonomic ranks and child taxa, and very distantly related organisms). They will show high sequence similarity and cause noise/contamination within the analysis, particularly for ancient DNA.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn many cases, running a range of comparative and/or competitive mappings to closely related species is advised in order to exclude a misidentification and the presence of multiple species with similar sets of genes in your dataset (e.g. Neisseria meningitidis). It can also be useful to use multiple reference sequences per species if it has multiple phylogenetically strongly delimited clades (e.g. H. influenzae)\n\n\nViruses are less affected, as they do not carry house-keeping genes, and only have a very small usually highly specialised set of genes, which makes them easier to validate. However, viral sequencing of non-pathogenic species has only recently started to become more popular due to metagenomic studies, i.e. non-pathogenic taxa are even more under-represented, and many viral genomes have low complexity repeats over large portions of their sequence. Additionally, some viruses are highly divergent within the same species.\nFinally, intervals or mutations specific to the target organism should be identified and investigated. This can range from plasmids, genes, and phages to specific mutations. Often a combination of these factors is required to confidently validate the presence of the organism. This step should not be overlooked as this will not always be clearly visible in a phylogeny, depending on the composition of the alignment used.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#genomes",
    "href": "introduction-to-microbial-genomics.html#genomes",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.5 Genomes",
    "text": "4.5 Genomes\n\n\n\n\n\n\nStrains VS Genomes\n\n\n\n\nA strain is a genetic variant or subtype of a species or sub-species. They usually exhibit significant genetic differences to other strains of the same taxonomic unit. The level of change required for significance can vary.\nA genome can be the exact copy of an already sequenced strain.\n\nThe case of aDNA: Since our samples are so old, most of our genome constitute new strains. However, caution is advised. Particularly with clonal species (e.g. Black Death genomes).\n\n\nThere are many different microbial species, and they all have very different evolutionary dynamics and genomes. This means that depending on which species you are working on, you might have to approach genomic analysis differently. Additionally, the research questions which could be answered with your data will/can also be rather different. This section will summarise some of the core principles underlying many possible research questions.\nMapping is an important tool in ancient DNA to investigate the presence and absence of genomic intervals. However, mapping can be impeded by the reference sequence itself. Gene duplication, low complexity sequences and GC Skew (over- or under-abundance of GC in intervals) can impact the mappability (and mapping evenness) of sequencing reads to the reference sequence and their mapping quality score. Which in turn might be problematic during analysis. This can be expressed in mappability estimates, which estimate how likely it is for short reads to map to a sequence interval based on its composition and uniqueness.\nApplying a mapping quality filter by default after a bwa aln mapping, will not only cause all bad quality reads to be removed from the alignment, but also any read which could align to multiple sections of your reference genome. This means that most likely every duplicated interval/gene, specific or conserved, and low complexity regions will also be removed! Mapping quality filters are important tools, but you should make sure you understand how much of the genome is actually covered (e.g. terminal repeats in viral genomes) and when to apply them. The same applies for competitive mappings using bwa aln!\n\n\n\n\n\n\nDefinition\n\n\n\nSequence Complexity: Defined as the observed vocabulary usage for word size. Meaning, how complex is the sequence of nucleotides within an interval. Often given as values calculated using either entropy or DUST algorithms. E.g.:\nAAAAAAAAAAAAAAAAAAAA &gt;&gt; LOWEST COMPLEXITY\nATGATGATGATGATGATGATG &gt;&gt; LOW COMPLEXITY\nATGTTTCGAGGCATGATAACCGTATG &gt;&gt; COMPLEX SEQUENCE\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nGC Content: Is usually given as a percentage of guanine and cytosine bases within the sequence. Can impact DNA stability, amplification, sequencing, and capture if the GC content is too high or too low. Too high or too low GC-content will also lead to a loss in sequence complexity. E.g.:\nGCCCCCCCGGAATGGACCCGCGCCT &gt;&gt; 80% GC\nATTTTGAACCTAATTTATATAGCAA &gt;&gt; 20% GC\n\n\n\n4.5.1 Recombination\nBacteria and viruses are haploid and have small genomes, making some things much easier, but… They like to mix and match! Microbial organisms will exchange genetic material using a range of biological mechanisms (conjugation, transduction, and/or transformation) this leads to increased genetic diversity via horizontal gene transfer. Some parts of the genome will be more heavily affected by this exchange than others. This constant exchange of genetic material can happen while maintaining genome size, meaning that while genes are gained, others are lost. How much recombination a genome will undergo, is highly dependent on the species or sometimes the phylogenetic clade. These dynamics cause recombination breakpoints in reference based alignments where SNP counts will increase, which can impede phylogenetic analysis (e.g.: by causing elongated branches).\nHowever, some species do not recombine at all or do so only very rarely. These species have clonal genomes, meaning they transfer their entire genome vertically, most the of time, without significant changes to their pangenome or the sequence itself. Actually, many of the species which have been extensively studied using aDNA, fall within this group (e.g. Y. pestis, M. leprae).\nMost microbial organisms change their genomes via recombination and gene loss/gain, while maintaining genome size and retaining their core genome. Virulence is then often defined by gaining or losing virulence factors. However, some increase their virulence by reducing their genome and becoming highly specialised (e.g. Borrelia recurrentis) in what is called reductive evolution. In these cases, it is of interest to include closely related phylogenetically “basal” species (pathogenic or not), which might inform you on genes/plasmids the modern strains have lost, but the ancestral genomes might still have retained.\n\n\n4.5.2 Pangenomes\nAs mentioned, microbial organisms can be very plastic and exchange genomic material. This can lead to a wide variety of genes being represented within a single species. Pangenomes represent the sum of all genomic intervals represented within the genomes of one species. And thus, characterising the genome of a new strain can be a lot more complex than sticking to a single reference sequence.\nPangenomes are made up of three groups:\n\nCore Genes: a set of essential genes common to all strains of a species.\nAccessory/Shell Genes: a set of genes common amongst some strains of a species which encode for supplementary or modified biochemical functions. In some cases these will also be virulence genes.\nSingleton/Unique/Cloud Genes: genes, which are specific to single strains of a species. Unlikely to be recovered and identified using ancient DNA.\n\nWe also differentiate between open and closed pangenomes. In open pangenomes, the gene set available to the bacterium constantly expands by acquisition and loss of genes via horizontal gene transfer from its environment, for the purpose of adaptation (e.g. environmental adaptation, metabolism, virulence and antibiotic resistance). All while maintaining genome size and vertical transmission of the core genome. Closed pangenome have limited exchanges of genetic material. This is often the case in highly specialised organisms. So while they are much more plastic than strictly clonal species, the available genetic pool for exchange will be smaller.\n\n\n4.5.3 SNP Effects\nSingle nucleotide polymorphisms (SNPs) can heavily impact the function of genes and the phenotype of an organism when they happen to affect and change the amino acid sequence of coding elements of a genome. This effect is frequently used in ancient DNA to predict the presence/absence of significant mutation, which could have changed the phenotype of a disease.\n\n\n\n\n\n\nDefinitions\n\n\n\nNon-synonymous (missense) mutation: SNPs that alters the amino acid sequence of a protein by changing one base of a triplet group. E.g.: GAT&gt;GGT == Asp(D)&gt;Gly(G)\nFrameshift: Changes in the amino acid sequence caused by insertions or deletions of nucleotides in coding sequences which are not multiples of three.\nStart/Stop Codon: Nucleotide triplet which signals the beginning and end of translation.\nStop/Start-gain mutation: Mutation which leads to a change in the amino acid sequence and leads to a premature stop/start of translation.\nStart/Stop-loss mutation: Mutation which leads to a change in the amino acid sequence and leads to the loss of a start/stop codon. Leads to the reduction or elimination of protein production.\n\n\nThe impact of such changes is predicted by tracking changes in the translation of coding sequences using an annotated reference sequence and SNP/MNP/Indel calls. The problem when using ancient DNA, is that genome coverage is often uneven, and it isn’t rare to lack full coverage of intervals of interest or lack resolution to correctly call indels across the reference. Larger genomic insertions and recombination are also not accounted for when a genome is reconstructed solely using a reference based approach. This means that while SNP effects can indeed be very interesting and potentially highly significant, they should be understood as prediction based on the available coverage. It is therefore important to report coverage over the entire coding and promoter intervals, when discussing SNP effect, and consider potential issues.\nImportant exception are known and heavily conserved mutations, which for example are involved in pseudogenisation. Pseudogenisation is the process through which genes lose their function due to mutations but remain in the genome without being expressed or without having a function. It is a mechanism underlying gene loss. They are significant in aDNA research because we can capture “active” versions of pseudogenes and potentially date their loss of function. It is a critical part of the genome reduction process of highly specialised bacteria.\n\n\n4.5.4 Virulence Associated Intervals\nOne of the questions ancient DNA research is interested in is the evolution of virulence in pathogenic species. The location and nature of virulence factors can vary. An increase in virulence can be caused by gene gain (on chromosomes or plasmids), plasmids, mutations, or changes to complex gene mechanisms (e.g. immune evasion). To understand the underlying processes of virulence adaptation has been one of the recurring questions in the field. Virulence intervals vary, but can be found within the literature and in some cases in curated databases.\n\n\n\n\n\n\nTip\n\n\n\n\nAre there loci associated with increased virulence known to the literature?\nAre there phylogenetic groups associated with increased virulence?\n\n\n\nNot all species have evolutionary dynamics which allow us to detect a temporal signal or recognise geographic structure within their phylogenies. However, phylogenetic clades can also inform us on other aspects of their evolution (virulence, ecological niche etc.). When investigating virulence, you should be checking for the presence of chromosomal virulence factors (genes, mutations). Currently, this is mostly done using either as a Presence/Absence matrix or a cluster analysis. As well as investigating the presence/absence of plasmids and plasmid mediated virulence factors and functional mechanisms, and relevant SNPs.\nBeyond virulence, some genes, and gene combinations can also inform us on changes in disease phenotype and microbial adaptation to the host or the taxon’s ecological niche.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#coinfections-multi-strain-infections-etc.",
    "href": "introduction-to-microbial-genomics.html#coinfections-multi-strain-infections-etc.",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.6 Coinfections, Multi-strain Infections etc.",
    "text": "4.6 Coinfections, Multi-strain Infections etc.\nThe detection and study of co-infection should also be considered. While it may not always be possible to reconstruct full genomes for each pathogen, the knowledge of the presence of multiple additional infectious agents alone can be very informative. Additionally, the detection of multi-strain infections (in high coverage genomes), should also be considered, especially in the case of multi-focal infections for which multiple samples are available.\n\n\n\n\n\n\nReconstructing disease phenotype & aetiopathology:\n\n\n\n\nWhere was the genome isolated from?\nHow can this relate to the disease phenotype?\nDo we have knowledge of potential coinfection? Strain differences?\nDo we have knowledge of the extent of affected tissues within the host body?\nCan we conclude whether it is likely to be a chronic or an acute infection?\nIn what demographic cohort would the individual(s) belong to?\nWhat could this tell you about the course of the disease or the likelihood of acute infections?\nDoes the host genomes show any clinically relevant variants which could impact how the pathogen would affect them?\n\nFinally: Integrate the data in a synthesis of all available data types. E.g.: osteology, archaeology, isotopes etc.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#resources",
    "href": "introduction-to-microbial-genomics.html#resources",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.7 Resources",
    "text": "4.7 Resources\nSome useful websites:\n\nGenBank NCBI Database: https://www.ncbi.nlm.nih.gov/genbank/\nThe European Nucleotide Archive (ENA): https://www.ebi.ac.uk/ena/browser/home\nThe virulence factor database (VFDB): http://www.mgc.ac.cn/VFs/main.htm\nViral Neighbour Genomes In The Assembly Resource (NCBI): https://www.ncbi.nlm.nih.gov/genome/viruses/about/assemblies/\nNCBI Taxonomic Browser: https://www.ncbi.nlm.nih.gov/taxonomy\nBacDiv: https://bacdive.dsmz.de/\nBacterial And Viral Bioinformatics Resource Center: https://www.bv-brc.org/",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "href": "introduction-to-microbial-genomics.html#questions-to-think-about",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.8 Questions to think about",
    "text": "4.8 Questions to think about\nGet to know your genome!\n\nIs the genome circular or linear?\nDoes the species carry any plasmids?\nHow genomically diverse are the genomes of the species?\nDoes the species share large portions of its genome with closely related environmental or commensal microbial organisms? If yes, could they also be present in the data?\n\nGet to know your species!\n\nIs the species clonal or heavily recombinant? Overall? Within clades?\nIs the pangenome open or closed? How large is it?\nHas the genome undergone a reductive evolution?\nIs the genome plastic but maintains genome size?\nAre there significant genomic/phenotypic differences across clades?\nIs your sample grouping within modern diversity or basal too it?\n\nDepending on the organism, available data and databases will be very different…\n\nWhat data is available?\nWhat type of data is available?\nWhat metadata is available?\n\nReconstructing disease phenotype & aetiopathology:\n\nWhere was the genome isolated from?    - How can this relate to the disease phenotype?\nDo we have knowledge of potential coinfection? Strain differences?\nDo we have knowledge of the extent of affected tissues within the host body?\nCan we conclude whether it is likely to be a chronic or an acute infection?\nIn what demographic cohort would the individual(s) belong to?    - What could this tell you about the course of the disease or the likelihood of acute infections?\nDoes the host genomes show any clinically relevant variants which could impact how the pathogen would affect them?",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-microbial-genomics.html#summary",
    "href": "introduction-to-microbial-genomics.html#summary",
    "title": "4  Introduction to Microbial Genomics",
    "section": "4.9 Summary",
    "text": "4.9 Summary\n\nThere are many different types of microbial organisms with distinct genomic structure and evolutionary dynamics!\nSpecies need to be carefully validated before genomic analysis\nPotential research questions will depend on the taxon (its genomic structure and evolutionary dynamics) and the available modern data",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Microbial Genomics</span>"
    ]
  },
  {
    "objectID": "introduction-to-evolutionary-biology.html",
    "href": "introduction-to-evolutionary-biology.html",
    "title": "5  Introduction to Evolutionary Biology",
    "section": "",
    "text": "Important\n\n\n\n🚧 This page is still under construction 🚧\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis chapter has not been updated since the 2022 edition of this book.\n\n\nThis chapter is still under construction. For the slides and recorded lecture version of this session, please see the Werner Siemens-Stiftung funded SPAAM Summer School: Introduction to Ancient Metagenomics website.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Evolutionary Biology</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html",
    "href": "bare-bones-bash.html",
    "title": "6  Introduction to the Command Line",
    "section": "",
    "text": "6.1 Preamble\nThis chapter is a condensed version of the both ‘Basic’ and ‘Boosted’ Bare Bones Bash courses that can be found in the Bare Bones Bash website. The material is displayed here under a CC-BY-SA 4.0 license.\nThe original Bare Bones Bash material was created by Aida Andrades Valtueña, James Fellows Yates, and Thiseas C. Lamnidis.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#preamble",
    "href": "bare-bones-bash.html#preamble",
    "title": "6  Introduction to the Command Line",
    "section": "",
    "text": "Logo of BareBonesBash, a computer monitor with a command prompt dollar symbol, BBB and a bone on the screen.\n\n\n\n\n\n\n\nCartoony pictures of the three authors of the BareBonesBash course. Designed by Zandra Fagernäs.\n\n\n\n\n\n\n\n\nTL;DR: uncollapse me!\n\n\n\n\n\nBelow is a quick reference guide to the commands discussed in this tutorial. To understand actually what each command does, carry on reading below! For a complete run of all these commands AND MORE(!!), consider following the full Bare Bones Bash walkthroughs here.\n\n\n\ncommand\ndescription\nexample\ncommon flags or arguments\n\n\n\n\npwd\nprint working directory\npwd\n\n\n\nls\nlist contents of directory\nls\n-l (long info)\n\n\nmkdir\nmake directory\nmkdir pen\n\n\n\ncd\nchange directory\ncd ~/pen\n~ (home dir), - (previous dir)\n\n\nssh\nlog into a remote server\nssh @.com\n-Y (allows graphical windows)\n\n\nmv\nmove something to a new location (& rename if needed)\nmv pen pineapple\n\n\n\nrmdir\nremove a directory\nrmdir pineapple\n\n\n\nwget\ndownload something from an URL\nwget www.pineapple.com/pen.txt\n-i (use input file)\n\n\ncat\nprint contents of a file to screen\ncat pen.txt\n\n\n\ngzip\na tool for dealing with gzip files\ngzip pen.txt\n-l (show info)\n\n\nzcat\nprint contents of a gzipped file to screen\nzcat pen.txt.gz\n\n\n\nwhatis\nget a short description of a program\nwhatis zcat\n\n\n\nman\nprint the man(ual) page of a command\nman zcat\n\n\n\nhead\nprint first X number of lines of a file to screen\nhead -n 20 pineapple.txt\n-n (number of lines to show)\n\n\n|\npipe, a way to pass output of one command to another\ncat pineapple.txt | head\n\n\n\ntail\nprint last X number of lines of a file to screen\ntail -n 20 pineapple.txt\n-n (number of lines to show)\n\n\nless\nprint file to screen, but allow scrolling\nless pineapple.txt\n\n\n\nwc\ntool to count words, lines or bytes of a files\nwc -l pineapple.txt\n-l (number of lines not words)\n\n\ngrep\nprint to screen lines in a file matching a pattern\ngrep pineapple.txt | grep pen\n\n\n\nln\nmake a (sym)link between a file and a new location\nln -s pineapple.txt pineapple_pen.txt\n-s (make symbolic link)\n\n\nnano\nuser-friendly terminal-based text editor\nnano pineapple_pen.txt\n\n\n\nrm\nmore general ‘remove’ command, including files\nrm pineapple_pen.txt\n-r (to remove directories)\n\n\n$VAR\nDollar sign + text indicates the name of a variable\n$PPAP\n\n\n\necho\nprints string to screen\necho “$PPAP”\n\n\n\nfor\nbegins ‘for’ loop, requires ‘in’, ‘do’ and ‘done’\nfor p in apple pineapple; do  echo “$p$PPAP”; done  applePen pineapplePen\n\n\n\nfind\nsearch for files or directories\nfind -name ‘pen’\n-type f (search only for files) -name ’*JPG’ (search for file names matching the pattern)\n\n\n“$var”\nuse double quotes to use contents of variable\npen=apple && echo “$pen”\n\n\n\n&lt;&gt;2&gt;\nredirects the standard input/output/error stream respectively into a file\ncat &lt;file.txt &gt;file_copy.txt 2&gt;cat_file.err",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#introduction",
    "href": "bare-bones-bash.html#introduction",
    "title": "6  Introduction to the Command Line",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThe aim of this tutorial is to make you familiar with using bash everyday… for the rest of your life! More specifically, we want to do this in the context of bioinformatics. We will start with how to navigate around a filesystem in the terminal, download sequencing files, and then to manipulate these. Within these sections we will also show you simple tips and tricks to make your life generally easier.\nThis tutorial is designed so you follow along on any machine with a UNIX terminal (no warranty provided).",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#the-5-commandments-of-bare-bones-bash",
    "href": "bare-bones-bash.html#the-5-commandments-of-bare-bones-bash",
    "title": "6  Introduction to the Command Line",
    "section": "6.3 The 5 commandments of Bare Bones Bash",
    "text": "6.3 The 5 commandments of Bare Bones Bash\nThe Bare Bones Bash philosophy of learning to code follows five simple commandments:\n\n\n\n\n\n\n\n1) Be lazy!\nDesire for shortcuts motivates you to explore more!\n\n\n2) Google The Hive-Mind knows everything!\n99% of the time, someone else has already had the same issue.\n\n\n3) Document everything you do!\nMake future you happy!\n\n\n4) There will ALWAYS be a typo!\nDon’t get disheartened, even best programmers make mistakes!\n\n\n5) Don’t be afraid of you freedom!\nExplore! Try out things!\n\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nRemember: No one writes code that works first time, or without looking at StackOverflow sooner or later.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#what-is-a-terminal",
    "href": "bare-bones-bash.html#what-is-a-terminal",
    "title": "6  Introduction to the Command Line",
    "section": "6.4 What is a terminal?",
    "text": "6.4 What is a terminal?\nA terminal is simply a fancy window that allows you to access the command-line interface of a computer or server (Figure 6.1).\nThe command-line itself is how you can work on the computer with just text.\nbash (bourne again shell) is one of the most popular languages used in the terminal.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#understanding-the-command-prompt",
    "href": "bare-bones-bash.html#understanding-the-command-prompt",
    "title": "6  Introduction to the Command Line",
    "section": "6.5 Understanding the command prompt",
    "text": "6.5 Understanding the command prompt\n\n\n\n\n\n\nFigure 6.1: An example command prompt\n\n\n\nAfter opening the terminal what you will normally see is a blank screen with a ‘command prompt’, like the one shown above. This typically consists of your username, the device name, a colon, a directory path and ends with a dollar symbol. Like so:\n&lt;username&gt;@&lt;device_name&gt;:~$\nThe command prompt is never involved in any command, it is just there to ensure you know who and where you are. When copying a command you should NOT copy the command prompt.\nOften times, when looking for commands online, the commands ran will pe prefaced with a $. This is a stand-in for the command prompt. When adding multi-line commands, it is also common to preface the additional lines with a &gt;. When copying such commands it is therefore important to remove these characters from the start of each line (if present).\nFinally, in this tutorial, the symbols&lt;&gt; are used to show things that will/should be replaced by another value. For example, in Thiseas’ command prompt &lt;username&gt; will be replaced by lamnidis, as that is his username.\nNow back to your prompt: It tells you that you are in the directory ~. The directory ~, stands for your home directory. Note that this shorthand will point to a different place, depending on the machine and the user.\nIf you want to know what the shorthand means, (here comes your first command!) you can type in pwd, which stands for “print working directory”. Your “working directory” is whichever directory you are currently in.\npwd\n/home/&lt;YOUR_USERNAME&gt;\n\n\n\n\n\n\nWarning\n\n\n\nIn programming documentation, it is very common to use &lt;ALL_CAPS&gt; notation to indicate something that doesn’t exist, or will vary depending on the user.\nWhenever you see such a notation (i.e., triangular brackets and all caps), you must always replace that whole section (including the &lt;&gt;) with whatever is on your system! You should never copy and paste this blindly!\n\n\nThis prints the entire “filepath” of the directory i.e. the route from the “root” (the deepest directory of the machine), through every subdirectory, leading to your particular working directory.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re not in your home directory (i.e., the output isn’t exactly the same as the output above), please run the command\ncd $HOME\nYou’ll learn about this commands in a bit!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#absolute-vs-relative-paths",
    "href": "bare-bones-bash.html#absolute-vs-relative-paths",
    "title": "6  Introduction to the Command Line",
    "section": "6.6 Absolute vs Relative paths",
    "text": "6.6 Absolute vs Relative paths\nFilepaths (a.k.a. “paths”), come in two flavours. Let’s talk a bit about them!\n\nAn absolute path will start with the deepest directory in the machine shown as a /. Paths starting with ~ are also absolute paths, since ~ translates to an absolute path of your specific home directory. That is the directory path you see in the output of the pwd command you just ran.\nAlternatively a relative path always begins from your working directory (i.e. your current directory). Often this type of path will begin with one (./) or two (../) dots followed by a forward slash, but not always. In the syntax of relative pathways . means “the current directory” and .. means “the parent directory” (or the ‘one above’).\n\n\n6.6.1 A real life analogy for paths\nYou have just arrived to Leipzig for a summer school that is taking place at MPI-EVA. After some questionable navigation, you find yourself at the Bayerische Bahnhof. Tired and disheartened, you decide to ask for help.\nYou see a friendly-looking metalhead (Figure 6.2), and decide to ask them for directions!\n\n\n\n\n\n\nFigure 6.2: A friendly-looking metalhead.\n\n\n\n\nI’m Happy to help, but I only give directions in absolute paths!\nFrom Leipzig Main Station, you should take Querstraße southward.\nContinue straight and take Nürnberger Str. southward until you reach Str. des 18 Oktober.\nFinally take Str. des 18 Oktober. moving southeast until you reach MPI-EVA!\n\n\n\n\n\n\n\nAbsolute paths\n\n\n\nThe directions above are equivalent to an absolute path, because they will ALWAYS take you to MPI-EVA, but you can only apply these directions if you start from Leipzig Main Station!\nExamples of absolute paths:\n/home/&lt;PATH&gt;/&lt;TO&gt;/\n/Leipzig_Main_Station/Querstraße/Nürnberger_Str/Str_18_Oktober/Deutscher_Platz/MPI-EVA\n\n\nNot sure how to get back to Leipzig Main Station to apply those directions, you decide to ask someone else for directions…\nLucky for you, a friendly looking local is passing by (Figure 6.3)!\n\n\n\n\n\n\nFigure 6.3: A friendly-looking local.\n\n\n\n\nYou’re currently on Str. des 18 Oktober. Walk straight that way, past the tram tracks, and you will find Deutscher Platz. You will see MPI-EVA to your right!\n\n\n\n\n\n\n\nRelative paths\n\n\n\nThese directions are equivalent to a relative path! They are easy to follow, but only work when you happen to start at the position you were in when you first got the directions!\nExamples of relative paths:\n./&lt;PATH&gt;/&lt;TO&gt;/my_file.txt\n../Str_18_Oktober/Deutscher_Platz/MPI-EVA",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#basic-commands",
    "href": "bare-bones-bash.html#basic-commands",
    "title": "6  Introduction to the Command Line",
    "section": "6.7 Basic commands",
    "text": "6.7 Basic commands\nWe will now explore some basic commands that you will use to explore folders and interact with files:\n\nlist directory contents:\n\nls\nOutput should look like:\nDesktop    Downloads  Pictures  Templates  bin    snap\nDocuments  Music      Public    Videos     cache  thinclient_drives\n\n\n\n\n\n\nNote\n\n\n\nWe will use this format to show you commands and their corresponding output in the terminal (if any) for the rest of this chapter.\n\n\n\nmake a directory:\n\n## ⚠ this will be your working directory for the rest of this chapter! Do not move out of it unless asked to! ⚠\nmkdir barebonesbash\n\nmove (or rename) files and directories\n\nmv barebonesbash BareBonesBash\n\nchange directories\n\ncd BareBonesBash\n\nDownload (www get) a remote file to your computer\n\nwget git.io/Boosted-BBB-meta\n\ncopy a file or directory to a new location\n\ncp Boosted-BBB-meta Boosted-BBB-meta.tsv\n\nremove (delete) files\n\nrm Boosted-BBB-meta\n\nConcatenate file contents to screen\n\ncat Boosted-BBB-meta.tsv\n\nSee only the first/last 10 lines of a file\n\nhead -n 10 Boosted-BBB-meta.tsv\ntail -n 10 Boosted-BBB-meta.tsv\n\n\n\n\n\n\nNote\n\n\n\nThis is because the start of a cat is its head and the end of the cat is its tail (The great humour of computer scientists)\n\n\n\nLook at the contents of a file interactively (less than the complete file, press q to quit)\n\nless Boosted-BBB-meta.tsv\n\nword count the number of lines (-l) in a file\n\nwc -l Boosted-BBB-meta.tsv\n15 Boosted-BBB-meta.tsv",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#datastreams-piping-and-redirects",
    "href": "bare-bones-bash.html#datastreams-piping-and-redirects",
    "title": "6  Introduction to the Command Line",
    "section": "6.8 Datastreams, piping, and redirects",
    "text": "6.8 Datastreams, piping, and redirects\nEach of the commands you learned above is a small program with a very specialised functionality. Programs come in many forms and can be written in various programming languages, but most of them share some features. Specifically, most programs take some data in and spit some data out! Here’s how that works, conceptually:\n\n6.8.1 Datastreams\nComputer programs can take in and spit out data from different streams (Figure 6.4). By default there are 3 such data streams.\n\nstdin : the standard input\nstdout: the standard output\nstderr: the standard error\n\n\n\n\n\n\n\nFigure 6.4: Diagram showing stdin going into the program, and two output streams from the program: stderr and stdout.\n\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nEach programme also has an ‘exit code’, which can tell you if execution completed with/without errors. You will rarely see these in the wild.\n\n\nTypically, the stdin is where the input data comes in.\nThe stdout is the actual output of the command. In some cases this gets printed to the screen, but most often this is the information that needs to be saved in an output file.\nThe stderr is the datastream where errors and warnings go. This gets printed to your terminal to let you know when something is not going according to plan (Figure 6.5)!\n\n\n\n\n\n\nFigure 6.5: This program (in the form of a bash script executed in the terminal) takes no input, and prints one line to the stdout and one line to the stderr.\n\n\n\n\n\n6.8.2 Piping\nA “pipe” (|) is a really useful feature of bash that lets you chain together multiple commands! When two commands are chained together with a pipe, the stdout of the first command becomes the stdin of the second (Figure 6.6)! The stderr is still printed on your screen, so you can always know when things fail.\n\n\n\n\n\n\nFigure 6.6: Diagram showing stdin going into the program, and two output streams from the program: stderr and stdout, with the stdout becoming the stdin of a second program.\n\n\n\nExample:\nhead -n 10 Boosted-BBB-meta.tsv | tail -n1\nnetsukeJapan    C       Artwork\nThe above command will only show the 10th line of Boosted-BBB-meta.tsv. The way it works is that head will take the first 10 lines of the file. These lines are then passed on to tail which will keep only the last of those lines.\n\n\n6.8.3 Redirects\nMuch like streams of water in the real world, datastreams can be redirected.\nThis way you can save the stdout of a program (or even the stderr) into a file for later!\n\nstdin can be redirected with &lt;.\n\nAn arrow pointing TO your program name!\n\nstdout can be redirected with &gt;.\n\nAn arrow pointing AWAY your program name!\n\nstderr can be redirected with 2&gt;.\n\nBecause it is the secondary output stream.\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIt is also possible to combine streams, but we won’t get into that here.\n\n\nExample:\nhead -n 10 Boosted-BBB-meta.tsv | tail -n1 &gt; line10.txt\nThis will create a new file called line10.txt within your work directory. Using cat on this file will BLOW YOUR MIND - (GONE WRONG)!\n\n(Don’t forget to like and subscribe!)\n\ncat line10.txt\nnetsukeJapan    C       Artwork\n\n\n\n\n\n\nFigure 6.7: Here you can see an example of redirecting the output of the datastreams_demo.sh program from before. Redirecting the stdout with &gt; only prints the stderr to the screen, and saves the stdout into output.txt. Additionally, we can redirect the stderr with 2&gt; into runtime.log, and then nothing is printed onto the screen.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#help-text",
    "href": "bare-bones-bash.html#help-text",
    "title": "6  Introduction to the Command Line",
    "section": "6.9 Help text",
    "text": "6.9 Help text\nYou don’t always have to google for documentation! Many programs come with in-built help text, or access to online manuals right from your terminal!\n\nYou can get a one sentence summary of what a tool does with whatis\nwhatis cat\ncat(1)  - concatenate files and print on the standard \n        output\nWhile man gives you access to online manuals for each tool (exit with q)\nman cat",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#variables",
    "href": "bare-bones-bash.html#variables",
    "title": "6  Introduction to the Command Line",
    "section": "6.10 Variables",
    "text": "6.10 Variables\nVariables are a central concept of all programming. In short, a variable is a named container whose contents you can expand at will or change.\nYou can assign variables (tell the computer what do you want it to contain) with = and pull their contents with $\nThe easiest way to see the contents of a variable is using echo!\necho \"This is my home directory: $HOME\"\nThis is my home directory: /home/ubuntu\n$HOME is a variable of the type called environment variables, which are set the moment you open your terminal or log into a server, they ensure the system works as intended and should not be change unless you are very sure of why.\n\n\n\n\n\n\nEnvironment Variables\n\n\n\nEnvironment variables in bash are typically named in all capital letters. It is a good idea to avoid using only capital letters for your variable names, so you avoid accidentally overwriting any environment variables.\n\n\nBut as mentioned, you can store in a variable anything you want, so let’s see a few examples:\nFirst let’s try to store a number:\nGreekFood=4            #Here, 'GreekFood' is a number.\necho \"Greek food is $GreekFood people who want to know what heaven tastes like.\"\nGreek food is 4 people who want to know what heaven tastes like.\n\n\n\n\n\n\nNote\n\n\n\nThe # is used to add comments to your code. Comments are annotations that you write in your code to understand what it is doing but that the computer does not run. Very useful for when your future self or another person looks at your code\n\n\nNow let’s store a word (“string”):\nGreekFood=delicious   #We overwrite that number with a word (i.e. a 'string').\necho \"Everyone says that Greek food is $GreekFood.\"\nEveryone says that Greek food is delicious.\nYou can also store more than a single word (that is still a “string”):\nGreekFood=\"Greek wine\" #We can overwrite 'GreekFood' again, \n## but when there is a space in our string, we need quotations.\necho \"The only thing better than Greek food is $GreekFood!\"\nThe only thing better than Greek food is Greek wine!\nSince variables can be reset to whatever you want, you can also store a number again:\nGreekFood=7 #And, of course, we can overwrite with a number again too.\necho \"I have been to Greece $GreekFood times already this year, for the food and wine!\"\nI have been to Greece 7 times already this year, for the food and wine!\n\n\n\n\n\n\nOverwriting variables\n\n\n\nIn these examples you have seen how the same variable has been overwritten, this means that you can only access the last content that you stored in the variable. All the previous contents that a variable may have had are inaccessible as soon as the same variable is given a new value.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#quotes-matter",
    "href": "bare-bones-bash.html#quotes-matter",
    "title": "6  Introduction to the Command Line",
    "section": "6.11 Quotes matter!",
    "text": "6.11 Quotes matter!\nIn bash, there is a big difference between a single quote ' and a double quote \"!\n\nThe contents of single quotes, are passed on as they are.\nInside double quotes, contents are interpreted!\n\nIn some cases the difference doesn’t matter:\necho \"I like Greek Food\"\necho 'I like Greek Food'\nI like Greek Food\nI like Greek Food\nIn other cases it makes all the difference:\nArr=\"Banana\"\necho 'Pirates say $Arr'\necho \"Minions say $Arr\"\nPirates say $Arr\nMinions say Banana\nWhy does it make a difference in the second example?\nThis is because in the second example we are using a variable. We have assigned Banana to the variable $Arr. As mentioned above, when single (') quotes are used the computer just prints what it receives without caring that $Arr is a variable.\nIn the echo with the double (\") quotes we are telling the computer to extract the value from the variable $Arr and that is why we see the store value (Banana) in the printed output in the terminal.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#find",
    "href": "bare-bones-bash.html#find",
    "title": "6  Introduction to the Command Line",
    "section": "6.12 Find",
    "text": "6.12 Find\nYou can also ask your computer where you have put your files, in case you forgot. To do this you can use find! The find command has the following syntax:\n## ⚠ Don't run! Fake example ⚠\nfind /&lt;PATH&gt;/&lt;TO&gt;/ -type f -name 'your_file.txt'\n\nFirst part of the find command: the place to look from\n\ne.g. . to indicate ‘here’\nCould also use ~/\nCould use absolute path e.g. /home/james/\n\nSecond part of the find command: what type of things to look for?\n\nUse -type to define the filetype:\n\nfile\ndirectory\n\n\nThird part of the find command: what to look in?\n\nUse -name to say ‘look in names of things’\n\nFinally after -name we give the the ‘strings’ to search for\n\nUse wildcards (*) for maximum laziness!\n\n\nNow let’s put into practise what you have learnt about find.\nFor that you will download a messy folder from a collaborator, remember to check you are in the BareBonesBash folder!:\nwget git.io/Boosted-BBB-images -O Boosted-BBB.zip\nWe realise that this is a compressed file, and more precisely is a zip file (extension .zip). In order to access to its content we will need to “unzip” it first. For that we can use the command unzip:\nunzip Boosted-BBB.zip\nWe know that our collaborator has shared with us some pictures from animals that we need to use for our research, and according to your collaborator they are marked with JPG. We first try to check the contents of the directory to find them quickly.\nls Boosted-BBB\nAnd        Digging       Friday     Leave      Only     Where    Young\nAnybody    Everything    Getting    Looking    Ooh      With     Youre\nDancing    Feel          Having     Night      Watch    You\nWow, what a mess! How would you retrieve all the files? Thanks to your wonderful teachers you have learnt how to use find and can simply run:\nfind Boosted-BBB -type f -name '*JPG*' \nBoosted-BBB/Having/the/time/of/your/life/bubobubo.JPG.MP3.TXT\nBoosted-BBB/With/a/bit/of/rock/music/exhibitRoyal.JPG.MP3.TXT\nBoosted-BBB/Friday/night/and/the/lights/are/low/fanta.JPG.MP3.TXT\nBoosted-BBB/Everything/is/fine/nomnom.JPG.MP3.TXT\nBoosted-BBB/Getting/in/the/swing/giacomo.JPG.MP3.TXT\nBoosted-BBB/Youre/in/the/mood/for/a/dance/snore.JPG.MP3.TXT\nBoosted-BBB/Digging/the/dancing/queen/excited.JPG.MP3.TXT\nBoosted-BBB/Anybody/could/be/that/guy/alopochenaegyptiacaArnhem.JPG.MP3.TXT\nBoosted-BBB/And/when/you/get/the/chance/stretch.JPG.MP3.TXT\nBoosted-BBB/Looking/out/for/angry.JPG.MP3.TXT\nBoosted-BBB/Feel/the/beat/from/the/tambourine/oh/yeah/netsukeJapan.JPG.MP3.TXT\nBoosted-BBB/Watch/that/scene/licorne.JPG.MP3.TXT\nBoosted-BBB/You/can/weimanarer.JPG.MP3.TXT\nBoosted-BBB/Night/is/young/and/the/musics/high/bydgoszczForest.JPG.MP3.TXT\nBoosted-BBB/Ooh/see/that/girl/pompeii.JPG.MP3.TXT\nAfter -name we have written '*JPG*', this tells to find to search for any file that contains JPG in any part of its name, indicated by the *. The * are what are known as wildcards. To learn more on how to use them, please refer to the more complete material for this tutorial.\nNow you have all the paths of the files that you will need!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#for-loops",
    "href": "bare-bones-bash.html#for-loops",
    "title": "6  Introduction to the Command Line",
    "section": "6.13 For loops",
    "text": "6.13 For loops\nUntil now we have seen how to run single commands on a file. But, what about when you need to repeat a command multiple times on a list of things, for example a list of files?\nTo repeat an action (command) for a set of things (list, e.g. files) one needs to employ the concept of a loop. One of the most commonly used loops, is the for loop.\nA for loop allows us to go through a list of things and perform some actions. Let’s see an example:\nVariable=Yes\nfor i in Greece Spain Britain; do\n  echo \"Does $i have lovely food? $Variable\"\ndone\nDoes Greece have lovely food? Yes\nDoes Spain have lovely food? Yes\nDoes Britain have lovely food? Yes\nThe for loop went through the list Greece Spain Britain and printed a statement with each item in the list. What happens if we change the order of the list to Britain Greece Spain?\nVariable=Yes\nfor i in Britain Greece Spain; do\n  echo \"Does $i have lovely food? $Variable\"\ndone\nDoes Britain have lovely food? Yes\nDoes Greece have lovely food? Yes\nDoes Spain have lovely food? Yes\nWe see that changing the order of the list will affect the output, this is because the for loop will go through the list in a sequential manner.\nWe can also add more elements to the list, and the for loop will continue until it reaches the end of the list.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#how-to-google-like-a-pro",
    "href": "bare-bones-bash.html#how-to-google-like-a-pro",
    "title": "6  Introduction to the Command Line",
    "section": "6.14 How to Google like a pro",
    "text": "6.14 How to Google like a pro\nOne of the most important skills you develop when coding and/or using the command line is how to phrase your questions so you can get relevant answers out of your search engine.\nAs Deep Thought put it in the Hitchhiker’s Guide to the Galaxy:\n\nOnly when you know the question will you know what the answer means.\n\nHere are some quick tips to get you started:\n\nALWAYS include the name of the language in your query.\n\nBAD: “How to cat” (Figure 6.8)\nGOOD: “How to cat bash” (Figure 6.9)\n\nBROADEN your question!\n\nBAD: “How to set X to 4 in bash?”\nGOOD: “How to set a variable to an integer in bash?”\n\nWhen you are more familiar, use fancy programmer lingo to make google think you know what you are talking about.\n\n\n\n\n\n\n\nAll the cool hackers say:\n\n\n\n\nstring and not text.\n\nfloat and not decimal.\n\nNote: some of these terms can be language specific.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.8: How to cat the wrong way (i.e., don’t include the language in your Google search and get loads of cat pictures)\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.9: How to cat the BASH way (i.e., include the the language in your Google search and you get lots of terminal pictures! Much better, right? …right?)",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#optional-clean-up",
    "href": "bare-bones-bash.html#optional-clean-up",
    "title": "6  Introduction to the Command Line",
    "section": "6.15 (Optional) clean-up",
    "text": "6.15 (Optional) clean-up\nIt is extremely important to ALWAYS keep your directories clean from random clutter. This lowers the chances you will get lost in your directories, but also ensures you can stay lazy, since tab completion will not keep finding similarly named files. So let’s clean up your working directory by removing all the clutter we downloaded and worked with today. The command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/BareBonesBash directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\ncd ~     ## We shouldn't delete a directory while we are still in it. (It is possible though).\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/BareBonesBash*\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name bare-bones-bash --all -y",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "bare-bones-bash.html#summary",
    "href": "bare-bones-bash.html#summary",
    "title": "6  Introduction to the Command Line",
    "section": "6.16 Summary",
    "text": "6.16 Summary\nYou should now know the basics of working on the command line, like:\n\nWhat a command prompt is\nHow to navigate around the filesystem via the command line\nHow to view the contents of a file\nHow to remove files and directories\nWhat a datastream is, and how they can be redirected\nHow to chain commands together\nWhat a variable is, how to assign them and how to unpack them\nHow to construct a simple for loop\nHow to google more efficiently!\n\nIf you would like to know more about the magic of bash, you can find more commands as well as and more advanced bash concepts in the BareBonesBash website.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to the Command Line</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html",
    "href": "r-tidyverse.html",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "",
    "text": "7.1 R, RStudio, the tidyverse and penguins\nThis chapter introduces the statistical programming environment R and how to use it with the RStudio editor. It is structured as self-study material with examples and little exercises to be completed in one to four hours. A larger exercise at the end pulls the individual units together.\nThe didactic idea behind this tutorial is to get as fast as possible to tangible, useful output, namely data visualisation. So we will first learn about reading and plotting data, and only later go to some common operations like conditional queries, data structure transformation and joins. We will focus exclusively on tabular data and how to handle it with the packages in the tidyverse framework. The example data used here is an ecological dataset about penguins.\nSo here is what you need to know for the beginning:",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#r-rstudio-the-tidyverse-and-penguins",
    "href": "r-tidyverse.html#r-rstudio-the-tidyverse-and-penguins",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "",
    "text": "R (R Core Team 2023) is a fully featured programming language, but it excels as an environment for (statistical) data analysis (https://www.r-project.org)\nRStudio (RStudio Team 2020) is an integrated development environment (IDE) for R (and other languages) (https://www.rstudio.com/products/rstudio)\nThe tidyverse (Wickham et al. 2019) is a powerful collection of R packages with well-designed and consistent interfaces for the main steps of data analysis: loading, transforming and plotting data (https://www.tidyverse.org). This tutorial works with tidyverse ~v2.0. We will learn about the packages readr, tibble, ggplot2, dplyr, magrittr and tidyr. forcats will be briefly mentioned, but purrr and stringr are left out.\nThe palmerpenguins package (Horst, Hill, and Gorman 2020) provides a neat example dataset to learn data exploration and visualisation in R (https://allisonhorst.github.io/palmerpenguins)",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#loading-r-studio-and-preparing-a-project",
    "href": "r-tidyverse.html#loading-r-studio-and-preparing-a-project",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.2 Loading R Studio and preparing a project",
    "text": "7.2 Loading R Studio and preparing a project\nBefore we begin, we can load RStudio from within your conda environment, by running the following.\nrstudio\n\n\n\n\n\n\nCaution\n\n\n\nIt is not recommended to download and update Rstudio if asked to on loading while following this textbook or during the summer school. You do so at your own risk. We recommend pressing ‘Remind later’ or ‘Ignore’.\n\n\nThe RStudio window should then open.\nOpen RStudio and create a new project by going to the top tool bar, and selecting File -&gt; New Project....\nWhen asked, create the new directory in an ‘Existing directory’ and select the r-tidyverse/ directory.\nOnce created, add new R script file so that you can copy the relevant code from this textbook into it to run them by pressing in the top tool bar File -&gt; New File -&gt; New Rscript.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#loading-data-into-tibbles",
    "href": "r-tidyverse.html#loading-data-into-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.3 Loading data into tibbles",
    "text": "7.3 Loading data into tibbles\n\n7.3.1 Reading tabular data with readr\nWith R we usually operate on data in our computer’s memory. The tidyverse provides the package readr to read data from text files into memory, both from our file system or the internet. It provides functions to read data in almost any (text) format.\n\nreadr::read_csv() # .csv files (comma-separated) -&gt; see penguins.csv\nreadr::read_tsv() # .tsv files (tab-separated)\nreadr::read_delim() # tabular files with arbitrary separator\nreadr::read_fwf() # fixed width files (each column with a set number of tokens)\nreadr::read_lines() # files with any content per line for self-parsing\n\n\n\n7.3.2 How does the interface of read_csv work?\nWe can learn more about any R function with the ? operator: To open a help file for a specific function run ?&lt;function_name&gt; (e.g. ?readr::read_csv) in the R console.\nreadr::read_csv has many options to specify how to read a text file.\n\nread_csv(\n    file, # The path to the file we want to read\n    col_names = TRUE, # Are there column names?\n    col_types = NULL, # Which types do the columns have? NULL -&gt; auto\n    locale = default_locale(), # How is information encoded in this file?\n    na = c(\"\", \"NA\"), # Which values mean \"no data\"\n    trim_ws = TRUE, # Should superfluous white-spaces be removed?\n    skip = 0, # Skip X lines at the beginning of the file\n    n_max = Inf, # Only read X lines\n    skip_empty_rows = TRUE, # Should empty lines be ignored?\n    comment = \"\", # Should comment lines be ignored?\n    name_repair = \"unique\", # How should \"broken\" column names be fixed\n    ...\n)\n\n\n\n7.3.3 What does readr produce? The tibble!\nTo read a .csv file (here \"penguins.csv\") into a variable (here peng_auto) run the following.\n\npeng_auto &lt;- readr::read_csv(\"penguins.csv\")\n\n\n\nRows: 300 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (4): id, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs a by-product of reading the file readr also prints some information on the number and type of rows and columns it discovered in the file.\nIt automatically detects column types - but you can also define them manually.\n\npeng &lt;- readr::read_csv(\n    \"penguins.csv\",\n    col_types = \"iccddcc\" # this string encodes the desired types for each column\n)\n\nThe col_types argument takes a string with a list of characters, where each character denotes one columns types. Possible types are c = character, i = integer, d = double, l = logical, etc. Remember that you can check ?readr::read_csv for more.\nreadr finally returns an in-memory representation of the data in the file, a tibble. A tibble is a “data frame”, a tabular data structure with rows and columns. Unlike a simple array, each column can have another data type.\n\n\n7.3.4 How to look at a tibble?\nTyping the name of any object into the R console will print an overview of it to the console.\n\npeng\n\n# A tibble: 300 × 7\n      id species island    flipper_length_mm body_mass_g sex    year \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 Adelie  Torgersen               181        3750 male   2007 \n 2     2 Adelie  Torgersen               186        3800 female 2007 \n 3     4 Adelie  Torgersen               193        3450 female 2007 \n 4     5 Adelie  Torgersen               190        3650 male   2007 \n 5     6 Adelie  Torgersen               181        3625 female 2007 \n 6     7 Adelie  Torgersen               195        4675 male   2007 \n 7     9 Adelie  Torgersen               191        3800 male   2007 \n 8    10 Adelie  Torgersen               198        4400 male   2007 \n 9    11 Adelie  Torgersen               185        3700 female 2007 \n10    12 Adelie  Torgersen               195        3450 female 2007 \n# ℹ 290 more rows\n\n\nBut there are various other ways to inspect the content of a tibble\n\nstr(peng) # A structural overview of an R object\nsummary(peng) # A human-readable summary of an R object\nView(peng) # Open RStudio's interactive data browser",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#plotting-data-in-tibbles",
    "href": "r-tidyverse.html#plotting-data-in-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.4 Plotting data in tibbles",
    "text": "7.4 Plotting data in tibbles\n\n7.4.1 ggplot2 and the “grammar of graphics”\nTo understand and present data, we usually have to visualise it.\nggplot2 is an R package that offers a slightly unusual, but powerful and logical interface for this task (Wickham 2016). The following example describes a stacked bar chart.\n\nlibrary(ggplot2) # Loading a library to use its functions without ::\n\n\nggplot( # Every plot starts with a call to the ggplot() function\n    data = peng # This function can also take the input tibble in the data argument\n) + # The plot consists of individual functions linked with \"+\"\n    geom_bar( # \"geoms\" define the plot layers we want to draw,\n        # so in this case a bar-chart\n        mapping = aes( # The aes() function maps variables to visual properties\n            x = island, # publication_year -&gt; x-axis\n            fill = species # community_type   -&gt; fill color\n        )\n    )\n\n\n\n\n\n\n\n\nA geom_* combines data (here peng), a geometry (here vertical, stacked bars) and a statistical transformation (here counting the number of penguins per island and species). ggplot2 features many such geoms: A good overview is provided by this cheatsheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html.\nBeyond geoms, a ggplot2 plot can be further specified with (among others) scales, facets and themes.\n\n\n7.4.2 scales control the behaviour of visual elements\nHere is another plot to demonstrate this: Boxplots of penguin weight per species.\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g))\n\n\n\n\n\n\n\n\nLet’s assume we had some extreme outliers in this dataset. To simulate this, we replace some random weights with extreme values.\n\nset.seed(1234) # we set a seed for reproducible randomness\npeng_out &lt;- peng\npeng_out$body_mass_g[sample(1:nrow(peng_out), 10)] &lt;- 50000 + 50000 * runif(10)\n\nNow we plot the dataset with these “outliers”.\n\nggplot(peng_out) +\n    geom_boxplot(aes(x = species, y = body_mass_g))\n\n\n\n\n\n\n\n\nThis is not well readable, because the extreme outliers dictate the scale of the y-axis. A 50+kg penguin is a scary thought and we would probably remove these outliers, but let’s assume they were valid observation we want to include in the plot.\nTo mitigate the visualisation issue we can change the scale of different visual elements - e.g. the y-axis.\n\nggplot(peng_out) +\n    geom_boxplot(aes(x = species, y = body_mass_g)) +\n    scale_y_log10() # adding the log-scale improves readability\n\n\n\n\n\n\n\n\nWe will now go back to the normal dataset without the artificial outliers.\n\n\n7.4.3 Colour scales\n(Fill) colour is a visual element of a plot and its scaling can be adjusted as well.\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g, fill = species)) +\n    scale_fill_viridis_d(option = \"C\")\n\n\n\n\n\n\n\n\nWe use the scale_* function to select one of the visually appealing (and robust to colourblindness) viridis colour palettes (https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html).\n\n\n7.4.4 More variables! Defining plot matrices via facets\nIn the previous example we didn’t add additional information with the fill colour, as the plot already distinguished by species on the x-axis.\nWe can instead use colour to encode more information, for example by mapping the variable sex to it.\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g, fill = sex))\n\n\n\n\n\n\n\n\nNote how mapping another variable to the fill colour automatically splits the dataset and how this is reflected in the number of boxplots per species.\nAnother way to visualise more variables in one plot is to split the plot by categories into facets, so sub-plots per category. Here we split by sex, which is already mapped to the fill colour:\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g, fill = sex)) +\n    facet_wrap(~sex)\n\n\n\n\n\n\n\n\nThe fill colour is therefore free again to show yet another variable, for example the year a given penguin was examined.\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g, fill = year)) +\n    facet_wrap(~sex)\n\n\n\n\n\n\n\n\nThis plot already visualises the relationship of four variables: species, body mass, sex and year of observation.\n\n\n7.4.5 Setting purely aesthetic settings with theme\nAesthetic changes can be applied as part of the theme, which allows for very detailed configuration (see ?theme).\nHere we rotate the x-axis labels by 45°, which often helps to resolve over-plotting.\n\nggplot(peng) +\n    geom_boxplot(aes(x = species, y = body_mass_g, fill = year)) +\n    facet_wrap(~sex) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\n\n\n7.4.6 Ordering elements in a plot with factors\nR supports defining ordinal data with factors. This can be used to set the order of elements in a plot, e.g. the order of bars in a bar chart.\nWe do not cover factors beyond the following example here, although the tidyverse includes a package (forcats) specifically for handling them.\nElements based on character columns are by default ordered alphabetically.\n\nggplot(peng) +\n    geom_bar(aes(x = species)) # bars are alphabetically ordered\n\n\n\n\n\n\n\n\nWith forcats::fct_reorder we can transform an input vector to a factor, ordered by a summary statistic (even based on another vector).\n\npeng2 &lt;- peng\npeng2$species_ordered &lt;- forcats::fct_reorder(\n    peng2$species,\n    peng2$species, length\n)\n\nWith this change, the plot will be ordered according to the intrinsic order defined for species_ordered.\n\nggplot(peng2) +\n    geom_bar(aes(x = species_ordered)) # bars are ordered by size\n\n\n\n\n\n\n\n\n\n\n7.4.7 Exercise\n\nLook at the mtcars dataset and read up on the meaning of its variables with the help operator ?. mtcars is a test dataset integrated in R and can always be accessed just by typing mtcars in the console.\nVisualise the relationship between Gross horsepower and 1/4 mile time.\n\n\nIntegrate the Number of cylinders into your plot as an additional variable.\n\n\n\n\n\n\n\nPossible solutions\n\n\n\n\n\n\n?mtcars\n\n[, 1] mpg     Miles/(US) gallon\n[, 2] cyl     Number of cylinders\n[, 3] disp    Displacement (cu.in.)\n[, 4] hp      Gross horsepower\n[, 5] drat    Rear axle ratio\n[, 6] wt      Weight (1000 lbs)\n[, 7] qsec    1/4 mile time\n[, 8] vs      Engine (0 = V-shaped, 1 = straight)\n[, 9] am      Transmission (0 = automatic, 1 = manual)\n[,10] gear    Number of forward gears\n[,11] carb    Number of carburetors\n\nggplot(mtcars) +\n    geom_point(aes(x = hp, y = qsec))\n\n\n\n\n\n\n\n\n\nggplot(mtcars) +\n    geom_point(aes(x = hp, y = qsec, color = as.factor(cyl)))",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#conditional-queries-on-tibbles",
    "href": "r-tidyverse.html#conditional-queries-on-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.5 Conditional queries on tibbles",
    "text": "7.5 Conditional queries on tibbles\n\n7.5.1 Selecting columns and filtering rows with select and filter\nAmong the most basic tabular data transformation operations is the conditional selection of columns and rows. The dplyr package includes powerful functions to subset data in tibbles.\ndplyr::select allows to select columns:\n\ndplyr::select(peng, id, flipper_length_mm) # select two columns\n\n# A tibble: 300 × 2\n     id flipper_length_mm\n  &lt;int&gt;             &lt;dbl&gt;\n1     1               181\n2     2               186\n3     4               193\n4     5               190\n5     6               181\n# ℹ 295 more rows\n\ndplyr::select(peng, -island, -flipper_length_mm) # remove two columns\n\n# A tibble: 300 × 5\n     id species body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie         3750 male   2007 \n2     2 Adelie         3800 female 2007 \n3     4 Adelie         3450 female 2007 \n4     5 Adelie         3650 male   2007 \n5     6 Adelie         3625 female 2007 \n# ℹ 295 more rows\n\n\ndplyr::filter allows for conditional filtering of rows:\n\ndplyr::filter(peng, year == 2007) # penguins examined in 2007\n\n# A tibble: 93 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007 \n2     2 Adelie  Torgersen               186        3800 female 2007 \n3     4 Adelie  Torgersen               193        3450 female 2007 \n4     5 Adelie  Torgersen               190        3650 male   2007 \n5     6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 88 more rows\n\n# penguins examined in 2007 OR 2009\ndplyr::filter(peng, year == 2007 | year == 2009)\n\n# A tibble: 198 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007 \n2     2 Adelie  Torgersen               186        3800 female 2007 \n3     4 Adelie  Torgersen               193        3450 female 2007 \n4     5 Adelie  Torgersen               190        3650 male   2007 \n5     6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 193 more rows\n\n# an alternative way to express OR with the match operator \"%in%\"\ndplyr::filter(peng, year %in% c(2007, 2009))\n\n# A tibble: 198 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007 \n2     2 Adelie  Torgersen               186        3800 female 2007 \n3     4 Adelie  Torgersen               193        3450 female 2007 \n4     5 Adelie  Torgersen               190        3650 male   2007 \n5     6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 193 more rows\n\n# Adelie penguins heavier than 4kg\ndplyr::filter(peng, species == \"Adelie\" & body_mass_g &gt;= 4000)\n\n# A tibble: 29 × 7\n     id species island    flipper_length_mm body_mass_g sex   year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     7 Adelie  Torgersen               195        4675 male  2007 \n2    10 Adelie  Torgersen               198        4400 male  2007 \n3    13 Adelie  Torgersen               197        4500 male  2007 \n4    31 Adelie  Dream                   196        4150 male  2007 \n5    39 Adelie  Dream                   196        4400 male  2007 \n# ℹ 24 more rows\n\n\nNote how each function here takes peng as a first argument. This invites a more elegant syntax.\n\n\n7.5.2 Chaining functions together with the pipe %&gt;%\nA core feature of the tidyverse is the pipe %&gt;% in the magrittr package. This ‘infix’ operator allows to chain data and operations for concise and clear data analysis syntax.\n\nlibrary(magrittr)\npeng %&gt;% dplyr::filter(year == 2007)\n\n# A tibble: 93 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007 \n2     2 Adelie  Torgersen               186        3800 female 2007 \n3     4 Adelie  Torgersen               193        3450 female 2007 \n4     5 Adelie  Torgersen               190        3650 male   2007 \n5     6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 88 more rows\n\n\nIt forwards the LHS (left-hand side) of %&gt;% as the first argument of the function appearing on the RHS (right-hand side) to enable sequences of function calls (“tidyverse style”).\n\npeng %&gt;%\n    dplyr::select(id, species, body_mass_g) %&gt;%\n    dplyr::filter(species == \"Adelie\" & body_mass_g &gt;= 4000) %&gt;%\n    nrow() # count the resulting rows\n\n[1] 29\n\n\nmagrittr also offers some more operators, among which the extraction operator %$% is particularly useful to easily extract individual variables from a tibble.\n\npeng %&gt;%\n    dplyr::filter(island == \"Biscoe\") %$%\n    species %&gt;% # extract the species column as a vector\n    unique() # get the unique elements of said vector\n\n[1] \"Adelie\" \"Gentoo\"\n\n\nHere we already use the base R summary function unique.\n\n\n7.5.3 Summary statistics in base R\nSummarising and counting data is indispensable and R offers a variety of basic operations in its base package. Many of them operate on vectors, so lists of values of one type. Individual columns are vectors.\n\n# we extract a single variable as a vector of values\nchinstraps_weights &lt;- peng %&gt;%\n    dplyr::filter(species == \"Chinstrap\") %$%\n    body_mass_g\nchinstraps_weights\n\n [1] 3500 3900 3650 3525 3725 3950 3250 3750 4150 3700 3800 3775 3700 4050 4050\n[16] 3300 3450 4400 3400 2900 3800 3300 4150 3400 3800 3700 4550 3200 4300 3350\n[31] 4100 3600 3900 3850 4800 2700 4500 3950 3650 3550 3500 3675 4450 3400 4300\n[46] 3675 3325 3950 3600 4050 3350 3450 3250 4050 3800 3525 3950 3650 3650 4000\n[61] 3775 4100 3775\n\nlength(chinstraps_weights) # length/size of a vector\n\n[1] 63\n\nunique(chinstraps_weights) # unique elements of a vector\n\n [1] 3500 3900 3650 3525 3725 3950 3250 3750 4150 3700 3800 3775 4050 3300 3450\n[16] 4400 3400 2900 4550 3200 4300 3350 4100 3600 3850 4800 2700 4500 3550 3675\n[31] 4450 3325 4000\n\nmin(chinstraps_weights) # minimum\n\n[1] 2700\n\nmax(chinstraps_weights) # maximum\n\n[1] 4800\n\nmean(chinstraps_weights) # mean\n\n[1] 3751.19\n\nmedian(chinstraps_weights) # median\n\n[1] 3725\n\nvar(chinstraps_weights) # variance\n\n[1] 153032.8\n\nsd(chinstraps_weights) # standard deviation\n\n[1] 391.1941\n\n# quantiles for the given probabilities\nquantile(chinstraps_weights, probs = c(0.25, 0.75))\n\n 25%  75% \n3500 3975 \n\n\nMany of these functions can ignore missing values (so NA values) with the option na.rm = TRUE.\n\n\n7.5.4 Group-wise summaries with group_by and summarise\nThese vector summary statistics are particular useful when applied to conditional subsets of a dataset.\ndplyr allows such summary operations with a combination of the functions group_by and summarise, where the former tags a tibble with categories based on its variables and the latter reduces it to these groups while simultaneously creating new columns.\n\npeng %&gt;%\n    # group the tibble by the material column\n    dplyr::group_by(species) %&gt;%\n    dplyr::summarise(\n        # new col: min weight for each group\n        min_weight = min(body_mass_g),\n        # new col: median weight for each group\n        median_weight = median(body_mass_g),\n        # new col: max weight for each group\n        max_weight = max(body_mass_g)\n    )\n\n# A tibble: 3 × 4\n  species   min_weight median_weight max_weight\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 Adelie          2850          3650       4775\n2 Chinstrap       2700          3725       4800\n3 Gentoo          3950          5050       6300\n\n\nGrouping can also be applied across multiple columns at once.\n\npeng %&gt;%\n    # group by species and year\n    dplyr::group_by(species, year) %&gt;%\n    dplyr::summarise(\n        # new col: number of penguins for each group\n        n = dplyr::n(),\n        # drop the grouping after this summary operation\n        .groups = \"drop\"\n    )\n\n# A tibble: 9 × 3\n  species   year      n\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n1 Adelie    2007     38\n2 Adelie    2008     45\n3 Adelie    2009     43\n4 Chinstrap 2007     23\n5 Chinstrap 2008     18\n# ℹ 4 more rows\n\n\nIf we group by more than one variable, then summarise will not entirely remove the group tagging when generating the result dataset. We can force this with .groups = \"drop\" to avoid undesired behaviour with this dataset later on.\n\n\n7.5.5 Sorting and slicing tibbles with arrange and slice\ndplyr allows to arrange tibbles by one or multiple columns.\n\npeng %&gt;% dplyr::arrange(sex) # sort by sex\n\n# A tibble: 300 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     2 Adelie  Torgersen               186        3800 female 2007 \n2     4 Adelie  Torgersen               193        3450 female 2007 \n3     6 Adelie  Torgersen               181        3625 female 2007 \n4    11 Adelie  Torgersen               185        3700 female 2007 \n5    12 Adelie  Torgersen               195        3450 female 2007 \n# ℹ 295 more rows\n\npeng %&gt;% dplyr::arrange(sex, body_mass_g) # sort by sex and weight\n\n# A tibble: 300 × 7\n     id species   island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1   304 Chinstrap Dream                   192        2700 female 2008 \n2    53 Adelie    Biscoe                  181        2850 female 2008 \n3    59 Adelie    Biscoe                  184        2850 female 2008 \n4    49 Adelie    Biscoe                  187        2900 female 2008 \n5   111 Adelie    Torgersen               188        2900 female 2009 \n# ℹ 295 more rows\n\npeng %&gt;% dplyr::arrange(dplyr::desc(body_mass_g)) # sort descending\n\n# A tibble: 300 × 7\n     id species island flipper_length_mm body_mass_g sex   year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   164 Gentoo  Biscoe               221        6300 male  2007 \n2   179 Gentoo  Biscoe               230        6050 male  2007 \n3   260 Gentoo  Biscoe               222        6000 male  2009 \n4   224 Gentoo  Biscoe               223        5950 male  2008 \n5   160 Gentoo  Biscoe               213        5850 male  2007 \n# ℹ 295 more rows\n\n\nSorting also works within groups and can be paired with slice to extract extreme values per group.\nHere we extract the heaviest individuals per species.\n\npeng %&gt;%\n    dplyr::group_by(species) %&gt;% # group by species\n    dplyr::arrange(dplyr::desc(body_mass_g)) %&gt;% # sort by weight within groups\n    dplyr::slice_head(n = 3) %&gt;% # keep the first three penguins per group\n    dplyr::ungroup() # remove the still lingering grouping\n\n# A tibble: 9 × 7\n     id species   island    flipper_length_mm body_mass_g sex   year \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   104 Adelie    Biscoe                  197        4775 male  2009 \n2    96 Adelie    Biscoe                  203        4725 male  2009 \n3    76 Adelie    Torgersen               196        4700 male  2008 \n4   303 Chinstrap Dream                   210        4800 male  2008 \n5   295 Chinstrap Dream                   205        4550 male  2008 \n# ℹ 4 more rows\n\n\nSlicing is also the relevant operation to take random samples from the observations in a tibble.\n\npeng %&gt;% dplyr::slice_sample(n = 10)\n\n# A tibble: 10 × 7\n     id species island flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1    47 Adelie  Biscoe               190        3450 female 2008 \n2   239 Gentoo  Biscoe               214        4850 female 2009 \n3   221 Gentoo  Biscoe               209        4600 female 2008 \n4   104 Adelie  Biscoe               197        4775 male   2009 \n5   138 Adelie  Dream                190        3725 male   2009 \n# ℹ 5 more rows\n\n\n\n\n7.5.6 Exercise\nFor this exercise we once more go back to the mtcars dataset. See ?mtcars for details.\n\nDetermine the number of cars with four forward gears (gear) in the mtcars dataset.\n\n\nDetermine the mean 1/4 mile time (qsec) per Number of cylinders (cyl) group.\n\n\nIdentify the least efficient cars for both transmission types (am).\n\n\n\n\n\n\n\nPossible solutions\n\n\n\n\n\n\nmtcars %&gt;%\n    dplyr::filter(gear == 4) %&gt;%\n    nrow()\n\n[1] 12\n\n\n\nmtcars %&gt;%\n    dplyr::group_by(cyl) %&gt;%\n    dplyr::summarise(\n        qsec_mean = mean(qsec)\n    )\n\n# A tibble: 3 × 2\n    cyl qsec_mean\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     4      19.1\n2     6      18.0\n3     8      16.8\n\n\n\nmtcars2 &lt;- tibble::rownames_to_column(mtcars, var = \"car\")\nmtcars2 %&gt;%\n    dplyr::group_by(am) %&gt;%\n    dplyr::arrange(mpg) %&gt;%\n    dplyr::slice_head() %$%\n    car\n\n[1] \"Cadillac Fleetwood\" \"Maserati Bora\"",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#transforming-and-manipulating-tibbles",
    "href": "r-tidyverse.html#transforming-and-manipulating-tibbles",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.6 Transforming and manipulating tibbles",
    "text": "7.6 Transforming and manipulating tibbles\n\n7.6.1 Renaming and reordering columns with rename and relocate\nColumns in tibbles can be renamed with dplyr::rename.\n\npeng %&gt;% dplyr::rename(penguin_name = id) # rename a column\n\n# A tibble: 300 × 7\n  penguin_name species island    flipper_length_mm body_mass_g sex    year \n         &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1            1 Adelie  Torgersen               181        3750 male   2007 \n2            2 Adelie  Torgersen               186        3800 female 2007 \n3            4 Adelie  Torgersen               193        3450 female 2007 \n4            5 Adelie  Torgersen               190        3650 male   2007 \n5            6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 295 more rows\n\n\nAnd with dplyr::relocate they can be reordered.\n\npeng %&gt;% dplyr::relocate(year, .before = species) # reorder columns\n\n# A tibble: 300 × 7\n     id year  species island    flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1     1 2007  Adelie  Torgersen               181        3750 male  \n2     2 2007  Adelie  Torgersen               186        3800 female\n3     4 2007  Adelie  Torgersen               193        3450 female\n4     5 2007  Adelie  Torgersen               190        3650 male  \n5     6 2007  Adelie  Torgersen               181        3625 female\n# ℹ 295 more rows\n\n\n\n\n7.6.2 Adding columns to tibbles with mutate and transmute.\nA common application of data manipulation is adding new, derived columns, that combine or modify the information in the already available columns. dplyr offers this core feature with the mutate function.\n\npeng %&gt;%\n    dplyr::mutate(\n        # add a column as a modification of an existing column\n        kg = body_mass_g / 1000\n    )\n\n# A tibble: 300 × 8\n     id species island    flipper_length_mm body_mass_g sex    year     kg\n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007   3.75\n2     2 Adelie  Torgersen               186        3800 female 2007   3.8 \n3     4 Adelie  Torgersen               193        3450 female 2007   3.45\n4     5 Adelie  Torgersen               190        3650 male   2007   3.65\n5     6 Adelie  Torgersen               181        3625 female 2007   3.62\n# ℹ 295 more rows\n\n\ndplyr::transmute has the same interface as dplyr::mutate, but it removes all columns except for the newly created ones.\n\npeng %&gt;%\n    dplyr::transmute(\n        # overwrite the id column with a modified version\n        id = paste(\"Penguin Nr.\", id), # paste() concatenates strings\n        flipper_length_mm # select this column without modifying it\n    )\n\n# A tibble: 300 × 2\n  id            flipper_length_mm\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Penguin Nr. 1               181\n2 Penguin Nr. 2               186\n3 Penguin Nr. 4               193\n4 Penguin Nr. 5               190\n5 Penguin Nr. 6               181\n# ℹ 295 more rows\n\n\ntibble::add_column behaves as dplyr::mutate, but gives more control over column position.\n\npeng %&gt;% tibble::add_column(\n    # add a modified version of a column\n    # note the . representing the LHS of the pipe\n    flipper_length_cm = .$flipper_length_mm / 10,\n    # add the columns after this particular other columns\n    .after = \"flipper_length_mm\"\n)\n\n# A tibble: 300 × 8\n     id species island    flipper_length_mm flipper_length_cm body_mass_g sex   \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n1     1 Adelie  Torgersen               181              18.1        3750 male  \n2     2 Adelie  Torgersen               186              18.6        3800 female\n3     4 Adelie  Torgersen               193              19.3        3450 female\n4     5 Adelie  Torgersen               190              19          3650 male  \n5     6 Adelie  Torgersen               181              18.1        3625 female\n# ℹ 295 more rows\n# ℹ 1 more variable: year &lt;chr&gt;\n\n\ndplyr::mutate can also be combined with dplyr::group_by (instead of dplyr::summarise) to add information on a group level. This is relevant, when a value for an individual entity should be put into context of a group-wise summary statistic.\nHere is a realistic sequence of operations that makes use of this feature:\n\npeng %&gt;%\n    dplyr::group_by(species, sex, year) %&gt;%\n    dplyr::mutate(\n        mean_weight = mean(body_mass_g, na.rm = T),\n        relation_to_mean = body_mass_g / mean_weight\n    ) %&gt;%\n    dplyr::ungroup() %&gt;%\n    # mutate does not remove rows, unlike summarise, so we use select\n    dplyr::select(id, species, sex, year, relation_to_mean) %&gt;%\n    dplyr::arrange(dplyr::desc(relation_to_mean))\n\n# A tibble: 300 × 5\n     id species   sex    year  relation_to_mean\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;\n1   104 Adelie    male   2009              1.21\n2    96 Adelie    male   2009              1.20\n3   274 Chinstrap female 2007              1.17\n4     7 Adelie    male   2007              1.17\n5   303 Chinstrap male   2008              1.16\n# ℹ 295 more rows\n\n\n\n\n7.6.3 Conditional operations with ifelse, case_when and case_match\nifelse allows to implement conditional mutate operations, that consider information from other columns.\n\npeng %&gt;% dplyr::mutate(\n    weight = ifelse(\n        # is weight below or above mean weight?\n        test = body_mass_g &gt;= 4200,\n        yes  = \"above mean\",\n        no   = \"below mean\"\n    )\n)\n\n# A tibble: 300 × 8\n     id species island    flipper_length_mm body_mass_g sex    year  weight    \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n1     1 Adelie  Torgersen               181        3750 male   2007  below mean\n2     2 Adelie  Torgersen               186        3800 female 2007  below mean\n3     4 Adelie  Torgersen               193        3450 female 2007  below mean\n4     5 Adelie  Torgersen               190        3650 male   2007  below mean\n5     6 Adelie  Torgersen               181        3625 female 2007  below mean\n# ℹ 295 more rows\n\n\nifelse gets cumbersome for more than two cases. dplyr::case_when is more readable and scales much better for this application.\n\npeng %&gt;% dplyr::mutate(\n    weight = dplyr::case_when(\n        # the number of conditions is arbitrary\n        body_mass_g &gt;= 4200 ~ \"above mean\",\n        body_mass_g &lt; 4200 ~ \"below mean\",\n        TRUE ~ \"unknown\" # TRUE catches all remaining cases\n    )\n)\n\n# A tibble: 300 × 8\n     id species island    flipper_length_mm body_mass_g sex    year  weight    \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n1     1 Adelie  Torgersen               181        3750 male   2007  below mean\n2     2 Adelie  Torgersen               186        3800 female 2007  below mean\n3     4 Adelie  Torgersen               193        3450 female 2007  below mean\n4     5 Adelie  Torgersen               190        3650 male   2007  below mean\n5     6 Adelie  Torgersen               181        3625 female 2007  below mean\n# ℹ 295 more rows\n\n\ndplyr::case_match is similar, but unlike dplyr::case_when it does not check logical expressions, but matches by value.\n\npeng %&gt;%\n    dplyr::mutate(\n        island_rating = dplyr::case_match(\n            island,\n            \"Torgersen\" ~ \"My favourite island\",\n            \"Biscoe\" ~ \"Overrated tourist trap\",\n            \"Dream\" ~ \"Lost my wallet there. 4/10\"\n        )\n    ) %&gt;%\n    # here we use group_by+summarise only to show the result\n    dplyr::group_by(island, island_rating) %&gt;%\n    dplyr::summarise(.groups = \"drop\")\n\n# A tibble: 3 × 2\n  island    island_rating             \n  &lt;chr&gt;     &lt;chr&gt;                     \n1 Biscoe    Overrated tourist trap    \n2 Dream     Lost my wallet there. 4/10\n3 Torgersen My favourite island       \n\n\n\n\n7.6.4 Switching between long and wide data with pivot_longer and pivot_wider\nTo simplify certain analysis or plotting operations data often has to be transformed from a wide to a long format or vice versa (Figure 7.1). Both data formats have useful applications and usually a given R function requires either, so we need to know how to convert between the two.\n\n\n\n\n\n\nFigure 7.1: Graphical representation of converting a table from a wide to a long and back to a wide format.\n\n\n\n\nA table in wide format has N key columns and N value columns.\nA table in long format has N key columns, one descriptor column and one value column.\n\nHere is an example of a wide dataset. It features information about the number of cars sold per year per brand at a dealership.\n\ncarsales &lt;- tibble::tribble(\n    ~brand, ~`2014`, ~`2015`, ~`2016`, ~`2017`,\n    \"BMW\", 20, 25, 30, 45,\n    \"VW\", 67, 40, 120, 55\n)\ncarsales\n\n# A tibble: 2 × 5\n  brand `2014` `2015` `2016` `2017`\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 BMW       20     25     30     45\n2 VW        67     40    120     55\n\n\nIn this wide format information is spread over many columns. Based on what we learned previously we can not easily plot it with ggplot2. Although it is often more verbose and includes more duplication, in the tidyverse we generally prefer data in long, “tidy” format – well justified by Wickham (2014).\nTo transform this dataset to a long format, we can apply tidyr::pivot_longer.\n\ncarsales_long &lt;- carsales %&gt;% tidyr::pivot_longer(\n    # define a set of columns to transform\n    cols = tidyselect::num_range(\"\", range = 2014:2017),\n    # the name of the descriptor column we want\n    names_to = \"year\",\n    # a function transform names to values\n    names_transform = as.integer,\n    # the name of the value column we want\n    values_to = \"sales\"\n)\ncarsales_long\n\n# A tibble: 8 × 3\n  brand  year sales\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 BMW    2014    20\n2 BMW    2015    25\n3 BMW    2016    30\n4 BMW    2017    45\n5 VW     2014    67\n# ℹ 3 more rows\n\n\nWide datasets are not always the wrong choice. They are well suitable for example for adjacency matrices to represent graphs, covariance matrices or other pairwise statistics. When the data gets big, then wide formats can be significantly more efficient (e.g. for spatial data).\nTo transform data from long to wide, we can use tidyr::pivot_wider\n\ncarsales_wide &lt;- carsales_long %&gt;% tidyr::pivot_wider(\n    # the set of id columns that should not be changed\n    id_cols = \"brand\",\n    # the descriptor column with the names of the new columns\n    names_from = year,\n    # the value column from which the values should be extracted\n    values_from = sales\n)\ncarsales_wide\n\n# A tibble: 2 × 5\n  brand `2014` `2015` `2016` `2017`\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 BMW       20     25     30     45\n2 VW        67     40    120     55\n\n\n\n\n7.6.5 Exercise\n\nMove the column gear to the first position of the mtcars dataset.\n\n\nMake a new dataset mtcars2 from mtcars with the column mpg and an additional column am_v, which encodes the transmission type (am) as either \"manual\" or \"automatic\".\n\n\nCount the number of cars per transmission type (am_v) and number of gears (gear) in mtcars2. Then transform the result to a wide format, with one column per transmission type.\n\n\n\n\n\n\n\nPossible solutions\n\n\n\n\n\n\nmtcars %&gt;%\n    dplyr::relocate(gear, .before = mpg) %&gt;%\n    tibble::as_tibble() # transforming the raw dataset for better printing\n\n# A tibble: 32 × 11\n   gear   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     4  21       6   160   110  3.9   2.62  16.5     0     1     4\n2     4  21       6   160   110  3.9   2.88  17.0     0     1     4\n3     4  22.8     4   108    93  3.85  2.32  18.6     1     1     1\n4     3  21.4     6   258   110  3.08  3.22  19.4     1     0     1\n5     3  18.7     8   360   175  3.15  3.44  17.0     0     0     2\n# ℹ 27 more rows\n\n\n\nmtcars2 &lt;- mtcars %&gt;%\n    dplyr::mutate(\n        gear,\n        am_v = dplyr::case_match(\n            am,\n            0 ~ \"automatic\",\n            1 ~ \"manual\"\n        )\n    ) %&gt;%\n    tibble::as_tibble()\nmtcars2\n\n# A tibble: 32 × 12\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb am_v     \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4 manual   \n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4 manual   \n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1 manual   \n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1 automatic\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2 automatic\n# ℹ 27 more rows\n\n\n\nmtcars2 %&gt;%\n    dplyr::group_by(am_v, gear) %&gt;%\n    # dplyr::tally() is identical to dplyr::summarise(n = dplyr::n())\n    # it counts the number of entities in a group\n    dplyr::tally() %&gt;%\n    tidyr::pivot_wider(\n        names_from = am_v,\n        values_from = n\n    )\n\n# A tibble: 3 × 3\n   gear automatic manual\n  &lt;dbl&gt;     &lt;int&gt;  &lt;int&gt;\n1     3        15     NA\n2     4         4      8\n3     5        NA      5",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#combining-tibbles-with-join-operations",
    "href": "r-tidyverse.html#combining-tibbles-with-join-operations",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.7 Combining tibbles with join operations",
    "text": "7.7 Combining tibbles with join operations\n\n7.7.1 Types of joins\nJoins combine two datasets x and y based on overlapping key columns. We can generally distinguish two kinds of joins:\n\nMutating joins add columns and rows of x and y:\n\nLeft join: Take observations from x and add fitting information from y.\nRight join: Take observations from y and add fitting information from x.\nInner join: Join the overlapping observations from x and y.\nFull join: Join all observations from x and y, even if information is missing.\n\nFiltering joins remove observations from x based on their presence in y.\n\nSemi join: Keep every observation in x that is in y.\nAnti join: Keep every observation in x that is not in y.\n\n\nThe following sections will introduce each join with an example.\nTo experiment with joins, we need a second dataset with complementary information. This new dataset contains additional variables for a subset of the penguins in our first dataset – both datasets feature 300 penguins, but only with a partial overlap in individuals.\n\nbills &lt;- readr::read_csv(\"penguin_bills.csv\")\n\n\n\n# A tibble: 300 × 3\n     id bill_length_mm bill_depth_mm\n  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1     1           39.1          18.7\n2     2           39.5          17.4\n3     3           40.3          18  \n4     4           36.7          19.3\n5     5           39.3          20.6\n# ℹ 295 more rows\n\n\n\n\n7.7.2 Left join with left_join\nTake observations from x and add fitting information from y (Figure 7.2).\n\n\n\n\n\n\nFigure 7.2: Graphical representation of a left join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to include the columns A B C D. As A and B have a one to one match of values, this remains the same in the joined table. The B column between the two have a different value on the third row, and thus is lost from the second table, retaining row three of the first table. Column D (from the second table) has an empty value on row three, as this row was not in row three of the second table.\n\n\n\n\ndplyr::left_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\" # the key column by which to join\n)\n\n# A tibble: 300 × 9\n     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     1 Adelie  Torger…               181        3750 male  2007            39.1\n2     2 Adelie  Torger…               186        3800 fema… 2007            39.5\n3     4 Adelie  Torger…               193        3450 fema… 2007            36.7\n4     5 Adelie  Torger…               190        3650 male  2007            39.3\n5     6 Adelie  Torger…               181        3625 fema… 2007            38.9\n# ℹ 295 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\nLeft joins are the most common join operation: Add information from y to the main dataset x.\n\n\n7.7.3 Right join with right_join\nTake observations from y and add fitting information from x (Figure 7.3).\n\n\n\n\n\n\nFigure 7.3: Graphical representation of a right join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to have columns A B C D. As A and B have a one to one match of values, this remains the same in the joined table. The B column between the two have a different value on the third row, and thus is lost from the first table, retaining row three of the second table. Column C (from the first table) has an empty value on row three, as this row was not in row three of the first table.\n\n\n\n\ndplyr::right_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\"\n) %&gt;%\n    # we arrange by id to highlight the missing observation in the peng dataset\n    dplyr::arrange(id)\n\n# A tibble: 300 × 9\n     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     1 Adelie  Torger…               181        3750 male  2007            39.1\n2     2 Adelie  Torger…               186        3800 fema… 2007            39.5\n3     3 &lt;NA&gt;    &lt;NA&gt;                   NA          NA &lt;NA&gt;  &lt;NA&gt;            40.3\n4     4 Adelie  Torger…               193        3450 fema… 2007            36.7\n5     5 Adelie  Torger…               190        3650 male  2007            39.3\n# ℹ 295 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\nRight joins are almost identical to left joins – only x and y have reversed roles.\n\n\n7.7.4 Inner join with inner_join\nJoin the overlapping observations from x and y (Figure 7.4).\n\n\n\n\n\n\nFigure 7.4: Graphical representation of an inner join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to have columns A B C D. Only rows from both tables that have exact matches on columns A and B are retained. The third rows from both tables that had a different value in column B are lost.\n\n\n\n\ndplyr::inner_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\"\n)\n\n# A tibble: 275 × 9\n     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     1 Adelie  Torger…               181        3750 male  2007            39.1\n2     2 Adelie  Torger…               186        3800 fema… 2007            39.5\n3     4 Adelie  Torger…               193        3450 fema… 2007            36.7\n4     5 Adelie  Torger…               190        3650 male  2007            39.3\n5     6 Adelie  Torger…               181        3625 fema… 2007            38.9\n# ℹ 270 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\nInner joins are a fast and easy way to check to which degree two dataset overlap.\n\n\n7.7.5 Full join with full_join\nJoin all observations from x and y, even if information is missing (Figure 7.5).\n\n\n\n\n\n\nFigure 7.5: Graphical representation of a full join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to have columns A B C D. All rows from both tables are retained, even though they do not share the same value in column B on both tables. The missing values for the two third rows (i.e., column C from the second table, and column D from the first table) are are filled with an empty cell.\n\n\n\n\ndplyr::full_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\"\n) %&gt;% dplyr::arrange(id)\n\n# A tibble: 325 × 9\n     id species island  flipper_length_mm body_mass_g sex   year  bill_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     1 Adelie  Torger…               181        3750 male  2007            39.1\n2     2 Adelie  Torger…               186        3800 fema… 2007            39.5\n3     3 &lt;NA&gt;    &lt;NA&gt;                   NA          NA &lt;NA&gt;  &lt;NA&gt;            40.3\n4     4 Adelie  Torger…               193        3450 fema… 2007            36.7\n5     5 Adelie  Torger…               190        3650 male  2007            39.3\n# ℹ 320 more rows\n# ℹ 1 more variable: bill_depth_mm &lt;dbl&gt;\n\n\nFull joins allow to preserve every bit of information.\n\n\n7.7.6 Semi join with semi_join\nKeep every observation in x that is in y (Figure 7.6).\n\n\n\n\n\n\nFigure 7.6: Graphical representation of a semi join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to have only the columns A B C. Only columns A B and C are retained in the joined table. Row three of both tables are not included as the values in columns A and B do not match.\n\n\n\n\ndplyr::semi_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\"\n)\n\n# A tibble: 275 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 Adelie  Torgersen               181        3750 male   2007 \n2     2 Adelie  Torgersen               186        3800 female 2007 \n3     4 Adelie  Torgersen               193        3450 female 2007 \n4     5 Adelie  Torgersen               190        3650 male   2007 \n5     6 Adelie  Torgersen               181        3625 female 2007 \n# ℹ 270 more rows\n\n\nSemi joins are underused (!) operations to filter datasets.\n\n\n7.7.7 Anti join with anti_join\nKeep every observation in x that is not in y (Figure 7.7).\n\n\n\n\n\n\nFigure 7.7: Graphical representation of an anti join operation. Two tables with a shared first column (A B C, and A B D respectively) are merged together to have only the columns A B C and only row three of the first table. Only row three is retained from the first table as this is the only row uniquely present in the first table.\n\n\n\n\ndplyr::anti_join(\n    x = peng, # 300 observations\n    y = bills, # 300 observations\n    by = \"id\"\n)\n\n# A tibble: 25 × 7\n     id species island    flipper_length_mm body_mass_g sex    year \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n1    22 Adelie  Biscoe                  183        3550 male   2007 \n2    34 Adelie  Dream                   181        3300 female 2007 \n3    74 Adelie  Torgersen               195        4000 male   2008 \n4    92 Adelie  Dream                   196        4350 male   2008 \n5    99 Adelie  Biscoe                  193        2925 female 2009 \n# ℹ 20 more rows\n\n\nAnti joins allow to quickly determine what information is missing in a dataset compared to an other one.\n\n\n7.7.8 Exercise\nConsider the following additional dataset with my opinions on cars with a specific number of gears:\n\ngear_opinions &lt;- tibble::tibble(\n    gear = c(3, 5),\n    opinion = c(\"boring\", \"wow\")\n)\n\n\nAdd my opinions about gears to the mtcars dataset.\n\n\nRemove all cars from the dataset for which I do not have an opinion.\n\n\n\n\n\n\n\nPossible solutions\n\n\n\n\n\n\ndplyr::left_join(mtcars, gear_opinions, by = \"gear\") %&gt;%\n    tibble::as_tibble()\n\n# A tibble: 32 × 12\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb opinion\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4 &lt;NA&gt;   \n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4 &lt;NA&gt;   \n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1 &lt;NA&gt;   \n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1 boring \n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2 boring \n# ℹ 27 more rows\n\n\n\ndplyr::anti_join(mtcars, gear_opinions, by = \"gear\") %&gt;%\n    tibble::as_tibble()\n\n# A tibble: 12 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n4  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n5  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n# ℹ 7 more rows",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#optional-final-exercise",
    "href": "r-tidyverse.html#optional-final-exercise",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.8 (Optional) Final exercise",
    "text": "7.8 (Optional) Final exercise\nIn this final exercise we reiterate many of the concepts introduced above. We also leave penguins and cars behind and finally start working with a dataset relevant to the topic of this book: The environmental samples table of the AncientMetagenomeDir.\nHere’s the URL to the table for v24.06 of the AncientMetagenomeDir:\n\n“https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/e29eb729e4b5d32b3afb872a7183ff51f6b0dbb5/ancientmetagenome-environmental/samples/ancientmetagenome-environmental_samples.tsv”\n\nTo get going create a new R script where you load magrittr and ggplot2 and create a variable for this URL:\n\nlibrary(magrittr)\nlibrary(ggplot2)\nurl_to_samples_table &lt;- \"https://raw.githubusercontent.com/SPAAM-community/AncientMetagenomeDir/e29eb729e4b5d32b3afb872a7183ff51f6b0dbb5/ancientmetagenome-environmental/samples/ancientmetagenome-environmental_samples.tsv\"\n\n\n1: Load the samples table as a tibble in R, into a variable “samples”. The readr package can read directly from URLs.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples &lt;- readr::read_tsv(url_to_samples_table)\n\nRows: 702 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (13): project_name, publication_doi, site_name, geo_loc_name, study_prim...\ndbl  (6): publication_year, latitude, longitude, depth, sample_age, sampling...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nA naive assumption about this dataset might be that there is a correlation of the variables depth and sample_age. Here is a definition of these variables taken from the meta-data specification:\n\ndepth: “Depth of sample taken from top of sequence in centimeters. In case of ranges use midpoint”\nsample_age: “Age of the sample in year before present (BP 1950), to the closest century”\n\n2: Of course we could only detect this for samples with both depth and age information. Filter the dataset to only include samples with it. And also remove samples without an archaeological site name ((!= \"Unknown\")).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_with_depth_and_age &lt;- samples %&gt;%\n    dplyr::filter(\n        !is.na(depth) & !is.na(sample_age),\n        site_name != \"Unknown\"\n    )\n\n\n\n\n3: Now plot depth against sample_age in a scatterplot to see if there is a potential signal.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_with_depth_and_age %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age))\n\n\n\n\n\n\n\n\n\n\n\nWe can’t see much here, because samples with very large ages dominate the y-scale.\n4: Recreate this plot with a log-scaled axis.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_with_depth_and_age %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age)) +\n    scale_y_log10(labels = scales::label_comma()) +\n    geom_hline(yintercept = 20000, color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nThis is more interesting. There may be a signal for samples, specifically below a certain age, maybe 20000 years BP.\n5: Filter the dataset to remove all samples that are older than this threshold. Store the result in a variable samples_young.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young &lt;- samples_with_depth_and_age %&gt;%\n    dplyr::filter(\n        sample_age &lt; 20000\n    )\n\n\n\n\n6: Recreate the plot from above. The log-scaling can be turned off now.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age))\n\n\n\n\n\n\n\n\n\n\n\nWith the old samples removed, there indeed seems to be some correlation. Pearson’s correlation coefficient is not that strong though.\n\ncor(samples_young$depth, samples_young$sample_age, method = \"pearson\")\n\n[1] 0.5850941\n\n\nMaybe what we see is mostly driven by individual sites. How many sites are there actually?\n7: Determine the number of sites in the filtered dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young$site_name %&gt;%\n    unique() %&gt;%\n    length()\n\n[1] 28\n\n\n\n\n\nAnd how many samples are there per site?\n8: Calculate the number of samples per site with group_by and summarize. Sort the result table by the number of samples with arrange.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsample_count_per_site &lt;- samples_young %&gt;%\n    dplyr::group_by(site_name) %&gt;%\n    dplyr::summarise(n = dplyr::n()) %&gt;%\n    dplyr::arrange(n)\n\n\n\n\n9: Prepare a bar-plot that shows this information, with the sites on the x-axis and the number of samples per site on the y-axis. The bars should be ordered by the number of samples.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsample_count_per_site$site_name &lt;- forcats::fct_reorder(\n    sample_count_per_site$site_name,\n    sample_count_per_site$n\n)\n\nsample_count_per_site %&gt;%\n    ggplot() +\n    geom_bar(aes(x = site_name, y = n), stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\n\nOh no - the x-axis labels are not well readable in this version of the plot.\n10: Create a version where they are slightly rotated.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsample_count_per_site %&gt;%\n    ggplot() +\n    geom_bar(aes(x = site_name, y = n), stat = \"identity\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nWhat the oldest and youngest samples for each site?\n11: Use group_by, arrange and dplyr::slice(1, dplyr::n()) to get the oldest and youngest sample for each site in the filtered dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsites_oldest_youngest &lt;- samples_young %&gt;%\n    dplyr::group_by(site_name) %&gt;%\n    dplyr::arrange(sample_age) %&gt;%\n    dplyr::slice(1, dplyr::n()) %&gt;%\n    dplyr::ungroup()\n\n\n\n\nThe result is a bit hard to read because it includes all columns of the input table.\n12: Select only the columns site_name and sample_age and show all rows with print(n = Inf).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsites_oldest_youngest &lt;- sites_oldest_youngest %&gt;%\n    dplyr::select(site_name, sample_age) %&gt;%\n    print(n = Inf)\n\n# A tibble: 56 × 2\n   site_name                                  sample_age\n   &lt;chr&gt;                                           &lt;dbl&gt;\n 1 Bouldnor Cliff                                   8000\n 2 Bouldnor Cliff                                   8000\n 3 Bushman Rock Shelter                              500\n 4 Bushman Rock Shelter                              500\n 5 Charlie Lake                                    11300\n 6 Charlie Lake                                    12800\n 7 Hässeldala Port                                 11000\n 8 Hässeldala Port                                 13900\n 9 Kamchatka                                        1080\n10 Kamchatka                                       19900\n11 Lake CH12                                        1900\n12 Lake CH12                                        6700\n13 Lake Chalco                                       100\n14 Lake Chalco                                     11500\n15 Lake Hill                                        3400\n16 Lake Hill                                       17500\n17 Lake King                                         100\n18 Lake King                                         400\n19 Lake Naleng                                       100\n20 Lake Naleng                                     17700\n21 Lake Slone                                        200\n22 Lake Slone                                       2000\n23 Lake Victoria                                     100\n24 Lake Victoria                                     400\n25 Lake Øvre Æråsvatnet                            17700\n26 Lake Øvre Æråsvatnet                            19500\n27 Lirima hydrothermal system                        500\n28 Lirima hydrothermal system                       1100\n29 Lucky Lady II                                   13300\n30 Lucky Lady II                                   15500\n31 Maria Island                                      100\n32 Maria Island                                      100\n33 Mubwindi Swamp, Bwindi Impenetrable Forest        100\n34 Mubwindi Swamp, Bwindi Impenetrable Forest       2200\n35 Piazza della Vittoria, Palmero, Sicily            900\n36 Piazza della Vittoria, Palmero, Sicily            900\n37 Salar de Huasco wetlands                          800\n38 Salar de Huasco wetlands                          800\n39 Santa Barbara Basin                               800\n40 Santa Barbara Basin                             11000\n41 Shirshov Ridge                                   1800\n42 Shirshov Ridge                                  18800\n43 Spring Lake                                      1400\n44 Spring Lake                                     11800\n45 Tiefer See                                        100\n46 Tiefer See                                      11300\n47 U1534C (Falkland Plateau)                         700\n48 U1534C (Falkland Plateau)                       19000\n49 U1536B (Dove Basin)                               100\n50 U1536B (Dove Basin)                              4000\n51 U1538 (Pirie Basin)                               100\n52 U1538 (Pirie Basin)                             14500\n53 Upper Goldbottom                                 9300\n54 Upper Goldbottom                                 9300\n55 Yukechi Yedoma lake                               100\n56 Yukechi Yedoma lake                               100\n\n\n\n\n\n13: Further simplify this dataset to only one row per site (group_by) and add a column (summarize) that shows the distance between min and max age, so the age range per site.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsites_age_range &lt;- sites_oldest_youngest %&gt;%\n    dplyr::group_by(site_name) %&gt;%\n    dplyr::summarise(age_range = max(sample_age) - min(sample_age)) %&gt;%\n    print(n = Inf)\n\n# A tibble: 28 × 2\n   site_name                                  age_range\n   &lt;chr&gt;                                          &lt;dbl&gt;\n 1 Bouldnor Cliff                                     0\n 2 Bushman Rock Shelter                               0\n 3 Charlie Lake                                    1500\n 4 Hässeldala Port                                 2900\n 5 Kamchatka                                      18820\n 6 Lake CH12                                       4800\n 7 Lake Chalco                                    11400\n 8 Lake Hill                                      14100\n 9 Lake King                                        300\n10 Lake Naleng                                    17600\n11 Lake Slone                                      1800\n12 Lake Victoria                                    300\n13 Lake Øvre Æråsvatnet                            1800\n14 Lirima hydrothermal system                       600\n15 Lucky Lady II                                   2200\n16 Maria Island                                       0\n17 Mubwindi Swamp, Bwindi Impenetrable Forest      2100\n18 Piazza della Vittoria, Palmero, Sicily             0\n19 Salar de Huasco wetlands                           0\n20 Santa Barbara Basin                            10200\n21 Shirshov Ridge                                 17000\n22 Spring Lake                                    10400\n23 Tiefer See                                     11200\n24 U1534C (Falkland Plateau)                      18300\n25 U1536B (Dove Basin)                             3900\n26 U1538 (Pirie Basin)                            14400\n27 Upper Goldbottom                                   0\n28 Yukechi Yedoma lake                                0\n\n\n\n\n\nSo some sites have a huge age range of thousands of years, and others do not. This information is not really meaningful without the number of samples per site, though.\n14: Join the sample count per site (as computed above) with the age range per site to get a table with both variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsites_joined &lt;- dplyr::left_join(\n    sites_age_range,\n    sample_count_per_site,\n    by = \"site_name\"\n) %&gt;%\n    print(n = Inf)\n\n# A tibble: 28 × 3\n   site_name                                  age_range     n\n   &lt;chr&gt;                                          &lt;dbl&gt; &lt;int&gt;\n 1 Bouldnor Cliff                                     0     4\n 2 Bushman Rock Shelter                               0     2\n 3 Charlie Lake                                    1500    14\n 4 Hässeldala Port                                 2900    12\n 5 Kamchatka                                      18820    25\n 6 Lake CH12                                       4800     3\n 7 Lake Chalco                                    11400    12\n 8 Lake Hill                                      14100    58\n 9 Lake King                                        300    15\n10 Lake Naleng                                    17600    40\n11 Lake Slone                                      1800    16\n12 Lake Victoria                                    300    15\n13 Lake Øvre Æråsvatnet                            1800     2\n14 Lirima hydrothermal system                       600     3\n15 Lucky Lady II                                   2200     4\n16 Maria Island                                       0     3\n17 Mubwindi Swamp, Bwindi Impenetrable Forest      2100    29\n18 Piazza della Vittoria, Palmero, Sicily             0     1\n19 Salar de Huasco wetlands                           0     1\n20 Santa Barbara Basin                            10200     5\n21 Shirshov Ridge                                 17000    17\n22 Spring Lake                                    10400    18\n23 Tiefer See                                     11200    11\n24 U1534C (Falkland Plateau)                      18300     5\n25 U1536B (Dove Basin)                             3900     3\n26 U1538 (Pirie Basin)                            14400    11\n27 Upper Goldbottom                                   0     2\n28 Yukechi Yedoma lake                                0     1\n\n\n\n\n\n15: Calculate the mean sampling interval by dividing the age range by the number of samples and add this information in a new column with mutate.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsites_joined %&gt;%\n    dplyr::mutate(\n        sampling_interval = age_range / n\n    ) %&gt;%\n    print(n = Inf)\n\n# A tibble: 28 × 4\n   site_name                                  age_range     n sampling_interval\n   &lt;chr&gt;                                          &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;\n 1 Bouldnor Cliff                                     0     4               0  \n 2 Bushman Rock Shelter                               0     2               0  \n 3 Charlie Lake                                    1500    14             107. \n 4 Hässeldala Port                                 2900    12             242. \n 5 Kamchatka                                      18820    25             753. \n 6 Lake CH12                                       4800     3            1600  \n 7 Lake Chalco                                    11400    12             950  \n 8 Lake Hill                                      14100    58             243. \n 9 Lake King                                        300    15              20  \n10 Lake Naleng                                    17600    40             440  \n11 Lake Slone                                      1800    16             112. \n12 Lake Victoria                                    300    15              20  \n13 Lake Øvre Æråsvatnet                            1800     2             900  \n14 Lirima hydrothermal system                       600     3             200  \n15 Lucky Lady II                                   2200     4             550  \n16 Maria Island                                       0     3               0  \n17 Mubwindi Swamp, Bwindi Impenetrable Forest      2100    29              72.4\n18 Piazza della Vittoria, Palmero, Sicily             0     1               0  \n19 Salar de Huasco wetlands                           0     1               0  \n20 Santa Barbara Basin                            10200     5            2040  \n21 Shirshov Ridge                                 17000    17            1000  \n22 Spring Lake                                    10400    18             578. \n23 Tiefer See                                     11200    11            1018. \n24 U1534C (Falkland Plateau)                      18300     5            3660  \n25 U1536B (Dove Basin)                             3900     3            1300  \n26 U1538 (Pirie Basin)                            14400    11            1309. \n27 Upper Goldbottom                                   0     2               0  \n28 Yukechi Yedoma lake                                0     1               0  \n\n\n\n\n\nAfter this never-ending digression we can go back to the initial question: Is there a global relationship between sample_age and depth?\n16: Take the samples_young dataset and recreate the simple scatter plot from above. But now map the site_name to the point colour.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age, color = site_name))\n\n\n\n\n\n\n\n\n\n\n\nThere are a lot of sites, so the legend for the colour space is annoyingly large.\n17: Turn it off with + guides(color = guide_none()).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age, color = site_name)) +\n    guides(color = guide_none())\n\n\n\n\n\n\n\n\n\n\n\nIt might be helpful to look at the sites separately to make sense of this data.\n18: Use faceting to split the plot into per-site subplots.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age, color = site_name)) +\n    guides(color = guide_none()) +\n    facet_wrap(~site_name)\n\n\n\n\n\n\n\n\n\n\n\nIt is not exactly surprising that the sites operate on different scales regarding age and depth.\n19: Add the scales = \"free\" option to facet_wrap to dynamically adjust the scaling of the subplots.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age, color = site_name)) +\n    guides(color = guide_none()) +\n    facet_wrap(~site_name, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\nSome sites have too few samples to contribute meaningfully to our main question.\n20: As a final exercise remove these “single-dot” sites and recreate the plot. There are many possible ways to do this. One way may be to filter by the standard deviation (sd) along the age or the depth axis.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsamples_young %&gt;%\n    dplyr::group_by(site_name) %&gt;%\n    dplyr::filter(sd(depth) &gt; 5) %&gt;%\n    dplyr::ungroup() %&gt;%\n    ggplot() +\n    geom_point(aes(x = depth, y = sample_age, color = site_name)) +\n    guides(color = guide_none()) +\n    facet_wrap(~site_name, scales = \"free\")\n\n\n\n\n\n\n\n\nUnsurprisingly this facetted plot visually confirms that depth and sample_age are often correlated for the sites in the environmental samples table of the AncientMetagenomeDir. It also shows a number of notable exceptions that clearly stand out in this plot.\nWe conclude the analysis at this point.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#optional-clean-up",
    "href": "r-tidyverse.html#optional-clean-up",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.9 (Optional) clean-up",
    "text": "7.9 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nWhen closing rstudio, say no to saving any additional files.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/r-tidyverse directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/r-tidyverse*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name r-tidyverse --all -y",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "r-tidyverse.html#references",
    "href": "r-tidyverse.html#references",
    "title": "7  Introduction to R and the Tidyverse",
    "section": "7.10 References",
    "text": "7.10 References\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org.\n\n\nRStudio Team. 2020. RStudio: Integrated Development Environment for r. Boston, MA: RStudio, PBC. http://www.rstudio.com/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R and the Tidyverse</span>"
    ]
  },
  {
    "objectID": "python-pandas.html",
    "href": "python-pandas.html",
    "title": "8  Introduction to Python and Pandas",
    "section": "",
    "text": "8.1 Working in a Jupyter environment\nThis tutorial/walkthrough is using a Jupyter Notebook (https://jupyter.org) for writing and executing Python code and for annotating.\nJupyter notebooks have two types of cells: Markdown and Code. The Markdown cell syntax is very similar to R markdown. The markdown cells are used for annotating code, which is important for sharing work with collaborators, reproducibility, and documentation.\nChange the directory to the the working directory of this tutorial/walkthrough.\nTo launch jupyter, run the following command in the terminal. This will open a browser window with jupyter running.\nJupyter Notebook should have a file structure with all the files from the working directory. Open the student-notebook.ipynb notebook by clicking on it. This notebook has exactly the same code as written in this book chapter and is only a support so that it is not necessary to copy and paste the code. It is of course also possible to copy the code from this chapter into a fresh notebook file by clicking on: File &gt; New &gt; Notebook.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#working-in-a-jupyter-environment",
    "href": "python-pandas.html#working-in-a-jupyter-environment",
    "title": "8  Introduction to Python and Pandas",
    "section": "",
    "text": "cd python-pandas_lecture/\n\njupyter notebook\n\n\n\n\n\n\n\nNote If the notebook is not there\n\n\n\n\n\nIf you cannot find student-notebook.ipynb, it is possible the working directory is not correct. Make sure that pwd returns /&lt;path&gt;/&lt;to&gt;/python-pandas/python-pandas_lecture.\n\n\n\n\n8.1.1 Creating and running cells\nThere are multiple ways of making a new cells in jupyter, such as typing the letter b, or using the cursor on the bottom of the page that says click to add cell. The cells can be assigned to code or markdown using the drop down menu at the top. Code cells are always in edit mode. Code can be run with pressing Shift + Enter or click on the ▶ botton. To make an markdown cell active, double-click on a markdown cell, it switches from display mode to edit mode. To leave the editing mode by running the cell.\n\n\n\n\n\n\nClear your code cells\n\n\n\n\n\nBefore starting it might be nice to clear the output of all code cells, by clicking on:\nedit &gt; Clear outputs of All Cells\n\n\n\n\n\n8.1.2 Markdown cell syntax\nHere a few examples of the syntax for the Markdown cells are shown, such as making words bold, or italics. For a more comprehensive list with syntax check out this Jupyter Notebook cheat-sheet (https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet).\nList of markdown cell examples:\n\n**bold** : bold\n_italics_ : italics\n\nCode\n\n`inline code` : inline code\n\nLaTeX maths\n\n$ x = \\frac{\\pi}{42} $ : \\[ x = \\frac{\\pi}{42} \\]\n\nURL links\n\n[link](https://www.python.org/) : link\n\nImages\n\n![](https://www.spaam-community.org/assets/media/SPAAM-Logo-Full-Colour_ShortName.svg) \n\n\n\n\n\n\n\nAll roads lead to Rome\n\n\n\n\n\nIn many cases, there are multiple syntaxes, or ‘ways of doing things,’ that will give the same results. For each section in this tutorial/walkthrough, one way is presented.\n\n\n\n\n\n8.1.3 code cell syntax\nThe code cells can interpret many different coding languages including Python and Bash. The syntax of the code cells is the same as the syntax of the coding languages, in our case python.\nBelow are some examples of Python code cells with some useful basic python functions:\n\n\n\n\n\n\nPython function print()\n\n\n\n\n\nprint() is a python function for printing lines in the terminal\nprint() is the same as echo in bash\n\n\n\nprint(\"Hello World from Python!\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nHello World from Python!\n\n\n\n\n\n\n\n\n\nRunning bash code in Jupyter\n\n\n\n\n\nIt is also possible to run bash commands in Jupyter, by adding a ! at the start of the line.\n! echo \"Hello World from bash!\"\nHello World from bash!\n\n\n\nStings or numbers can be stored as a variable by using the = sign.\ni = 0\nOnes a variable is set in one code cell they are stored and can be accessed in other downstream code cells.\nTo see what value a variable contains, the print() function can be used.\nprint(i)\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n0\n\n\n\nYou can also print multiple things together in one print statement such as a number and a string.\nprint(\"The number is\", i, \"Wow!\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nThe number is, 0, Wow!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#pandas",
    "href": "python-pandas.html#pandas",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.2 Pandas",
    "text": "8.2 Pandas\n\n8.2.1 Getting started\nPandas is a Python library used for data manipulation and analysis.\nWe can import the library like this.\nimport pandas as pd\n\n\n\n\n\n\nWhy import as pd?\n\n\n\n\n\nWe set pandas to the alias pd because we are lazy and do not want to write the full word too many times.\n\n\n\nNow that Pandas is imported, we can check if it worked correctly, and check which version is running by runing .__version__.\npd.__version__\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n‘2.2.2’\n\n\n\n\n\n8.2.2 Pandas data structures\nThe primary data structures in Pandas are the Series and the DataFrame. A Series is a one-dimensional array-like object containing a value of the same type and can be imagined as one column in a table Figure 8.1. Each element in the series is associated with an index from 0 to the number of elements, but these can be changed to labels. A DataFrame is two-dimensional, and can change in size after it is created by adding and removing rows and columns, which can hold different types of data such as numbers and strings Figure 8.2. The columns and rows are labelled. By default, rows are unnamed and are indexed similarly to a series.\n\n\n\n\n\n\nFigure 8.1: A single row or column (1-dimensional data) is a Series. The dark grey squares are the index or row names, and the light grey squares are the elements.\n\n\n\n\n\n\n\n\n\nFigure 8.2: A dataframe with columns and rows. The dark grey squares are the index/row names and the column names. The light grey squares are the values.\n\n\n\n\n\n\n\n\n\nMore details on pandas\n\n\n\n\n\nFor a more in detail pandas getting started tutorial click here (https://pandas.pydata.org/docs/getting_started/index.html#)",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#reading-data-with-pandas",
    "href": "python-pandas.html#reading-data-with-pandas",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.3 Reading data with Pandas",
    "text": "8.3 Reading data with Pandas\nPandas can read in csv (comma separated values) files, which are tables in text format. It is called _c_sv because each value is separated from the others through a comma.\nA,B\n5,6\n8,4\nAnother common tabular separator are tsv, where each value is separated by a tab \\t.\nA\\tB\n5\\t6\n8\\t4\nThe dataset that is used in this tutorial/walkthrough is called \"all_data.tsv\", and is tab-separated. Pandas by default assume that the file is comma delimited, but this can be change by using the sep= argument.\n\n\n\n\n\n\nPandas function pd.read_csv()\n\n\n\n\n\npd.read_csv() is the pandas function to read in tabular tables. The sep= can be specified argument, sep=, is the default.\n\n\n\ndf = pd.read_csv(\"../all_data.tsv\", sep=\"\\t\")\ndf\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n3\n11\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\n11\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n3\n11\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646\n1\n0\n11\n4\n20\n10\n3\n5\n2\n0\n4\n6\n0\n3\n11\n\n\n4\n7446\n1967\nMaster\nTogether\n62513\n0\n1\n520\n42\n98\n0\n42\n14\n6\n4\n10\n6\n0\n3\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1749\n9432\n1977\nGraduation\nTogether\n666666\n1\n0\n9\n14\n18\n8\n1\n12\n3\n1\n3\n6\n0\n3\n11\n\n\n1750\n8372\n1974\nGraduation\nMarried\n34421\n1\n0\n3\n3\n7\n6\n2\n9\n1\n0\n2\n7\n0\n3\n11\n\n\n1751\n10870\n1967\nGraduation\nMarried\n61223\n0\n1\n709\n43\n182\n42\n118\n247\n9\n3\n4\n5\n0\n3\n11\n\n\n1752\n7270\n1981\nGraduation\nDivorced\n56981\n0\n0\n908\n48\n217\n32\n12\n24\n2\n3\n13\n6\n0\n3\n11\n\n\n1753\n8235\n1956\nMaster\nTogether\n69245\n0\n1\n428\n30\n214\n80\n30\n61\n6\n5\n10\n3\n0\n3\n11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelp\n\n\n\n\n\nWhen you are unsure what arguments a function can take, it is possible to get a help documentation using help(pd.read_csv)\n\n\n\nIn most cases, data will be read in with the pd.read_csv() function, however, internal Python data structures can also be transformed into a pandas data frame. For example using a nested list, were each row in the datafram is a list [].\ndf = pd.Dataframe([[5,6], [8,4]], colums=[\"A\", \"B\"])\ndf\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n5\n6\n\n\n1\n8\n4\n\n\n\n\n\n\nAnother usful transformation is from a dictionary (https://docs.python.org/3/tutorial/datastructures.html#dictionaries) to pd.Dataframe.\ntable_data = {'A' : [5, 6]\n              'B' : [8, 4]}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n5\n6\n\n\n1\n8\n4\n\n\n\n\n\n\nThere are many ways to turn a DataFrame back into a dictonary (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html#pandas.DataFrame.to_dict), which might be very handy for certain purposes.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#data-exploration",
    "href": "python-pandas.html#data-exploration",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.4 Data exploration",
    "text": "8.4 Data exploration\nThe data for this tutorial/walkthrough is from a customer personality analysis of a company trying to better understand how to modify their product catalogue. Here is the link to the original source (https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis) for more information.\n\n8.4.1 Columns\nTo display all the column names from the imported DataFrame, the attribute columns can be called.\ndf.columns\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nIndex(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n       'Teenhome', 'MntWines', 'MntFruits', 'MntMeatProducts',\n       'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n       'NumWebVisitsMonth', 'Complain', 'Z_CostContact', 'Z_Revenue'],\n      dtype='object')\n\n\n\nEach column has its own data types which are highly optimised. A column with only integers has the data type int64. Columns with decimal numbers are called float64. A column with only strings, or a combination of strings and integers or floats is called an object.\ndf.dtypes\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nID                       int64\nYear_Birth               int64\nEducation               object\nMarital_Status          object\nIncome                 float64\nKidhome                  int64\nTeenhome                 int64\nMntWines                 int64\nMntFruits                int64\nMntMeatProducts          int64\nMntFishProducts          int64\nMntSweetProducts         int64\nMntGoldProds             int64\nNumWebPurchases          int64\nNumCatalogPurchases      int64\nNumStorePurchases        int64\nNumWebVisitsMonth        int64\nComplain                 int64\nZ_CostContact            int64\nZ_Revenue                int64\ndtype: object\n\n\n\n\n\n\n\n\n\nWhat does 64 stand for?\n\n\n\n\n\nThe 64 indicates the number of bits the integers are stored in. 64 bits is the largest pandas handels. When it is known that a value is in a certain range, it is possible to change the bits to 8, 16, or 32. This chosing the correct range might reduce memory usage, to be save, 64 range is so large it will incorporate most user cases.\ndf['Kidhome'] = df['Kidhome'].astype('int8')\n\n\n\n\n\n8.4.2 Inspecting the DataFrame\nTo quickly check how many rows and columns the DataFrame has, we can access the shape attribute.\ndf.shape\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n(1754, 20)\n\n\n\nThe .shape attribute of a DataFrame provides a tuple representing its dimensions. A tuple is a Python data structure that is used to store ordered items. In the case of shape, the first item is always the row, and the second item is the columns. To print, or access the rows or columns the index can be used. .shape[0] gives the number of rows, and .shape[1] gives the number of columns.\ndf.shape[0]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n1754\n\n\n\ndf.shape[1]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is often useful to have a quick look at the first rows, to get, for example, an idea of the data was read correctly. This can be done with the head() function.\n\n\ndf.head()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n3\n11\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\n11\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n3\n11\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646\n1\n0\n11\n4\n20\n10\n3\n5\n2\n0\n4\n6\n0\n3\n11\n\n\n4\n7446\n1967\nMaster\nTogether\n62513\n0\n1\n520\n42\n98\n0\n42\n14\n6\n4\n10\n6\n0\n3\n11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuctions and attributes\n\n\n\n\n\nThe difference between calling a function and an atribute is the (). .head() is a function and will perform an action. While .shape is an attribute and will return a value that is already stored in the DataFrame.\n\n\n\nWhat we can see it that, unlike R, Python and in extension Pandas is 0-indexed instead of 1-indexed.\n\n\n8.4.3 Accessing rows and columns\nIt is possible to access parts of the data in DataFrames in different ways. The first method is sub-setting rows using the row name and column name. This can be done with the .loc, which loc ates row(s) by providing the row name and column name [row, column]. When the rows are not named, the row index can be used instead. To print the second row, this would be index 1 since the index in Python starts at 0. To print the all the columns, the : is used.\ndf.loc[1, :]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nID                           2174\nYear_Birth                   1954\nEducation              Graduation\nMarital_Status             Single\nIncome                    46344.0\nKidhome                         1\nTeenhome                        1\nMntWines                       11\nMntFruits                       1\nMntMeatProducts                 6\nMntFishProducts                 2\nMntSweetProducts                1\nMntGoldProds                    6\nNumWebPurchases                 1\nNumCatalogPurchases             1\nNumStorePurchases               2\nNumWebVisitsMonth               5\nComplain                        0\nZ_CostContact                   3\nZ_Revenue                      11\nName: 1, dtype: object\n\n\n\nTo print a range of rows, the first and last index can be written with a :. To print the second and third row, this would be [1:2, :].\ndf.loc[1:2]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\n11\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n3\n11\n\n\n\n\n\n\n\n\nTo print all the rows with a certain column name works the same as subsetting rows, but then adding the column name after the comma.\ndf[:, \"Year_Birth\"]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n0       1957\n1       1954\n2       1965\n3       1984\n4       1967\n        ... \n1749    1977\n1750    1974\n1751    1967\n1752    1981\n1753    1956\n\n\n\nIt is important to notice that almost all operations on DataFrames are not in place, meaning that the DataFrame is not modified. To keep the changes, the DataFrame has to be actively stored using the same name or a new variable.\nTo save the changes, a new DataFrame has to be created, or the existing DataFrame has to be overwritten. This can be done by directing the output to a variable with =. To make a new DataFrame with only the “Education” and “Marital_Status” columns, the column names have to be placed in a list ['colname1', 'colname2'].\ndf.head()\nnew_df = df.loc[:, [\"Education\", \"Marital_Status\"]]\nnew_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\nEducation\nMarital_Status\n\n\n\n\n0\nGraduation\nSingle\n\n\n1\nGraduation\nSingle\n\n\n2\nGraduation\nTogether\n\n\n3\nGraduation\nTogether\n\n\n4\nMaster\nTogether\n\n\n…\n…\n…\n\n\n1749\nGraduation\nTogether\n\n\n1750\nGraduation\nMarried\n\n\n1751\nGraduation\nMarried\n\n\n1752\nGraduation\nDivorced\n\n\n1753\nMaster\nTogether\n\n\n1754 rows × 2 columns\n\n\n\n\n\n\n\n\nIt is also possible to remove rows and columns from a DataFrame. This can be done with the function drop(). To remove the columns Z_CostContact and Z_Revenue and keep those changes, it is necessary to overwrite the DataFrame. To make sure Pandas understands that the its columns that need to be removed, the axis can be specified. Rows are called axis=0, and columns are called axis=1. In most cases Pandas will guess correctly without specifying the axis, since in this case the no row is called Z_CostContact or Z_Revenue. It is however good practice to add the axis to make sure Pandas is operating as expected.\ndf = df.drop(\"Z_CostContact\", axis=1)\ndf = df.drop(\"Z_Revenue\", axis=1)\n\n\n\n\n\n\nCan be done in one go\n\n\n\n\n\ndf = df.drop([\"Z_CostContact\", \"Z_Revenue\"], axis=1)\n\n\n\n\n\n8.4.4 Conditional subsetting\nSo far, all the subsetting has been based on row names and column names. However, in many cases, it is more helpful to look only at data that contain certain items. This can be done using conditional subsetting, which is based on Boolean values True or False. pandas will interpret a series of True and False values by printing only the rows or columns where a True is present and ignoring all rows or columns with a False.\nFor example, if we are only interested in individuals in the table who graduated, we can test each string in the column Education to see if it is equal (==) to Graduation. This will return a series with Boolean values True or False.\neducation_is_grad = (df[\"Education\"] == \"Graduation\")\neducation_is_grad\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n0        True\n1        True\n2        True\n3        True\n4       False\n        ...  \n1749     True\n1750     True\n1751     True\n1752     True\n1753    False\nName: Education, Length: 1754, dtype: bool\n\n\n\nTo quicky check if the True and False values are correct, it can be useful to print out this column.\ndf[\"Education\"]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n0       Graduation\n1       Graduation\n2       Graduation\n3       Graduation\n4       Master\n…       …\n1749    Graduation\n1750    Graduation\n1751    Graduation\n1752    Graduation\n1753    Master\nName: Education, length: 1754, dtype: object\n\n\n\nIt is possible to provide pandas with multiple conditions at the same time. This can be done by combining multiple statements with &.\ntwo_at_once = (df[\"Education\"] == \"Graduation\") & (df[\"Marital_Status\"] == \"Single\")\ntwo_at_once\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n0        True\n1        True\n2       False\n3       False\n4       False\n        ...  \n1749    False\n1750    False\n1751    False\n1752    False\n1753    False\nLength: 1754, dtype: bool\n\n\n\nTo find out the total number of Graduated singles, the .sum() can be used.\nsum(two_at_once)\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n252\n\n\n\nThese Series of Booleans can be used to subset the dataframe to rows where the condition(s) are True:\ndf[two_at_once]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n3\n11\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\n11\n\n\n18\n7892\n1969\nGraduation\nSingle\n18589\n0\n0\n6\n4\n25\n15\n12\n13\n2\n1\n3\n7\n0\n3\n11\n\n\n20\n5255\n1986\nGraduation\nSingle\nNaN\n1\n0\n5\n1\n3\n3\n263\n362\n27\n0\n0\n1\n0\n3\n11\n\n\n33\n1371\n1976\nGraduation\nSingle\n79941\n0\n0\n123\n164\n266\n227\n30\n174\n2\n4\n9\n1\n0\n3\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1720\n10968\n1969\nGraduation\nSingle\n57731\n0\n1\n266\n21\n300\n65\n8\n44\n8\n8\n6\n6\n0\n3\n11\n\n\n1723\n5959\n1968\nGraduation\nSingle\n35893\n1\n1\n158\n0\n23\n0\n0\n18\n3\n1\n5\n8\n0\n3\n11\n\n\n1743\n4201\n1962\nGraduation\nSingle\n57967\n0\n1\n229\n7\n137\n4\n0\n91\n4\n2\n8\n5\n0\n3\n11\n\n\n1746\n7004\n1984\nGraduation\nSingle\n11012\n1\n0\n24\n3\n26\n7\n1\n23\n3\n1\n2\n9\n0\n3\n11\n\n\n1748\n8080\n1986\nGraduation\nSingle\n26816\n0\n0\n5\n1\n6\n3\n4\n3\n0\n0\n3\n4\n0\n3\n11\n\n\n\n\n\n252 rows × 20 columns\n\n\n\nIt is not actually necessary to create a series every time for subsetting the table and it can be done in one go by combining the conditions within the df[].\ndf[(df[\"Education\"] == \"Master\") & (df[\"Marital_Status\"] == \"Single\")]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n26\n10738\n1951\nMaster\nSingle\n49389\n1\n1\n40\n0\n19\n2\n1\n3\n2\n0\n3\n7\n0\n3\n11\n\n\n46\n6853\n1982\nMaster\nSingle\n75777\n0\n0\n712\n26\n538\n69\n13\n80\n3\n6\n11\n1\n0\n3\n11\n\n\n76\n11178\n1972\nMaster\nSingle\n42394\n1\n0\n15\n2\n10\n0\n1\n4\n1\n0\n3\n7\n0\n3\n11\n\n\n98\n6205\n1967\nMaster\nSingle\n32557\n1\n0\n34\n3\n29\n0\n4\n10\n2\n1\n3\n5\n0\n3\n11\n\n\n110\n821\n1992\nMaster\nSingle\n92859\n0\n0\n962\n61\n921\n52\n61\n20\n5\n4\n12\n2\n0\n3\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1690\n3520\n1990\nMaster\nSingle\n91172\n0\n0\n162\n28\n818\n0\n28\n56\n4\n3\n7\n3\n0\n3\n11\n\n\n1709\n4418\n1983\nMaster\nSingle\n89616\n0\n0\n671\n47\n655\n145\n111\n15\n7\n5\n12\n2\n0\n3\n11\n\n\n1714\n2980\n1952\nMaster\nSingle\n8820\n1\n1\n12\n0\n13\n4\n2\n4\n3\n0\n3\n8\n0\n3\n11\n\n\n1738\n7366\n1982\nMaster\nSingle\n75777\n0\n0\n712\n26\n538\n69\n13\n80\n3\n6\n11\n1\n0\n3\n11\n\n\n1747\n9817\n1970\nMaster\nSingle\n44802\n0\n0\n853\n10\n143\n13\n10\n20\n9\n4\n12\n8\n0\n3\n11\n\n\n\n\n\n75 rows × 20 columns\n\n\n\n\n\n8.4.5 Describing a DataFrame\nSometimes is is nice to get a quick overview of the data in a table, such as means and counts. Pandas has a native function to do just that, it will output a count, mean, standard deviation, minimum, 25th percentile (Q1), median (50th percentile or Q2), 75th percentile (Q3), and maximum for each numeric columns.\ndf.describe()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\ncount\n1754.000\n1754.00000\n1735.00\n1754.000000\n1754.000000\n1754.0000\n1754.00000\n1754.0000\n1754.00000\n1754.00000\n1754.00000\n1754.000000\n1754.000000\n1754.000000\n1754.000000\n1754.000000\n1754\n1754\n\n\nmean\n5584.696\n1969.57127\n51166.58\n0.456100\n0.480616\n276.0724\n28.03478\n166.4920\n40.51710\n28.95838\n47.26682\n3.990878\n2.576967\n5.714937\n5.332383\n0.011403\n3\n11\n\n\nstd\n3254.656\n11.87661\n26200.42\n0.537854\n0.536112\n314.6047\n41.34888\n225.5617\n57.41299\n42.83066\n53.88565\n2.708278\n2.848335\n3.231465\n2.380183\n0.106202\n0\n0\n\n\nmin\n0.000\n1893.00000\n1730.00\n0.000000\n0.000000\n0.0000\n0.00000\n0.0000\n0.00000\n0.00000\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3\n11\n\n\n25%\n2802.500\n1960.00000\n33574.50\n0.000000\n0.000000\n19.0000\n2.00000\n15.0000\n3.00000\n2.00000\n10.00000\n2.000000\n0.000000\n3.000000\n3.000000\n0.000000\n3\n11\n\n\n50%\n5468.000\n1971.00000\n49912.00\n0.000000\n0.000000\n160.5000\n9.00000\n66.0000\n13.00000\n9.00000\n27.00000\n3.000000\n1.000000\n5.000000\n6.000000\n0.000000\n3\n11\n\n\n75%\n8441.250\n1978.00000\n68130.00\n1.000000\n1.000000\n454.0000\n35.00000\n232.0000\n53.50000\n35.00000\n63.00000\n6.000000\n4.000000\n8.000000\n7.000000\n0.000000\n3\n11\n\n\nmax\n11191.000\n1996.00000\n666666.00\n2.000000\n2.000000\n1492.0000\n199.00000\n1725.0000\n259.00000\n263.00000\n362.00000\n27.000000\n28.000000\n13.000000\n20.000000\n1.000000\n3\n11\n\n\n\n\n\n8 rows × 18 columns\n\n\n\nWe can also directly calculate the relevant statistics on numberic columns or rows we are interested in using the functions max(), min(), mean(), median() etc..\ndf[\"MntWines\"].max()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n1492\n\n\n\ndf[[\"Kidhome\", \"Teenhome\"]].mean()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nKidhome     0.456100\nTeenhome    0.480616\ndtype: float64\n\n\n\nThere are also ceartin functions that are usfule for non-numeric columns. To know which stings are present in a object, the fuction unique() can be used, this will returns an array with the unique values in the column or row.\ndf[\"Education\"].unique()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\narray(['Graduation', 'Master', 'Basic', '2n Cycle'], dtype=object)\n\n\n\nTo know how often a value is present in a column or row, the function value_counts() can be used. This will print a series for all the unique values and print a count.\ndf[\"Marital_Status\"].value_counts()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nMarital_Status\nMarried     672\nTogether    463\nSingle      382\nDivorced    180\nWidow        53\nAlone         2\nAbsurd        2\nName: count, dtype: int64\n\n\n\n\n\n8.4.6 Getting summary statistics on grouped data\nPandas is equipped with lots of useful functions which make complicated tasks very easy and fast. One of these functions is .groupby() with the arguments by=..., which will group a DataFrame using a categorical column (for example Education or Marital_Status). This makes it possible to perform operations on a group directly without the need for subsetting. For example, to get a mean income value for the different Education levels in the DataFrame can be done by specifying the column name for the grouping variable by .groupby(by='Education') and specifying the column name to perform this action on [Income] followed by the sum() function.\ndf.groupby(by=\"Education\")[\"Income\"].mean()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nEducation\n2n Cycle      47633.190000\nBasic         20306.259259\nGraduation    52720.373656\nMaster        52917.534247\nName: Income, dtype: float6\n\n\n\n\n\n8.4.7 Subsetting Questions and Exercises\nHere there are several exercises to try conditional subsetting. Try to first before seeing the awnsers.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many Single people are there in the table that also greduated? And how many are single?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsum(df[\"Marital_Status\"] == \"Single\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n382\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nSubset the DataFrame with people born before 1970 and after 1970 and store both DataFrames\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndf_before = df[df[\"Year_Birth\"] &lt; 1970]\ndf_after = df[df[\"Year_Birth\"] &gt;= 1970]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many people are in the two DataFrames?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nprint(\"n(before)   =\", df_before.shape[0])\nprint(\"n(after)   =\", df_after.shape[0])\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nn(before)   = 804\nn(after)   = 950\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDo the total number of people sum up to the original DataFrame total?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n df_before.shape[0] + df_after.shape[0] == df.shape[0]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nTrue\n\n\n\nprint(\"n(sum)      =\", df_before.shape[0] + df_after.shape[0])\nprint(\"n(expected) =\", df.shape[0])\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nn(sum)      = 1754\nn(expected) = 1754\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the mean income of the two groups differ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n  print(\"income(before) =\", df_before[\"Income\"].mean())\n  print(\"income(after)  =\", df_after[\"Income\"].mean())\n  \n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nincome(before) = 55513.38113207547\nincome(after) = 47490.29255319149\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBonus: Can you find something else that differs a lot between the two groups?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis is an open ended question.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#dealing-with-missing-data",
    "href": "python-pandas.html#dealing-with-missing-data",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.5 Dealing with missing data",
    "text": "8.5 Dealing with missing data\nIn large tables, it is often important to check if there are columns or rows that have missing data. pandas represents missing data with NA (Not Available). To identify these missing values, pandas provides the .isna() function. This function checks every cell in the DataFrame and returns a DataFrame of the same shape, where each cell contains a Boolean value: True if the original cell contains NA, and False otherwise.\ndf.isna()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n0\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n1\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n2\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n3\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n4\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1749\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n1750\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n1751\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n1752\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n1753\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n\n\n\n1754 rows × 20 columns\n\n\n\nIt is very hard to see if there are any ‘True’ values in this new Boolean table. To investigate how many missing values are present in the table, the sum() function can be used. In Python, True has the 1 assigned to it and False 0.\ndf.isna().sum()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nID                      0\nYear_Birth              0\nEducation               0\nMarital_Status          0\nIncome                 19\nKidhome                 0\nTeenhome                0\nMntWines                0\nMntFruits               0\nMntMeatProducts         0\nMntFishProducts         0\nMntSweetProducts        0\nMntGoldProds            0\nNumWebPurchases         0\nNumCatalogPurchases     0\nNumStorePurchases       0\nNumWebVisitsMonth       0\nComplain                0\ndtype: int64\n\n\n\nIn this case is it not clear what a missing value represents, are these individuals who did not want to state their income, or did not have an income? For the tutorial, we are going to keep them in the data.\nTwo possible actions that can be taken to deal with missing data. One is to remove the row or column using the .dropna() function. This function will by default remove the row with the NA but with specifying the axis=1 the column will be dropped. The second course of action is filling the empty cells with a value, this can be done with the .fillna() function, which will substitute the missing value with a set value, such as 0.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#combining-data",
    "href": "python-pandas.html#combining-data",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.6 Combining data",
    "text": "8.6 Combining data\n\n8.6.1 Concatenation exercises\nData is very often present in multiple tables. Think, for example, about a taxonomy table giving count data per sample. One way to combine multiple datasets is through concatenation, which either combines all columns or rows of multiple DataFrames. The function in Pandas that does just that is called .concat. This command combines two DataFrames by appending all rows or columns: .concat([first_dataframe, second_dataframe]).\nIn the DataFrame, there are individuals with the education levels Graduation, Master, Basic, and 2n Cycle. PhD is missing; however, there is data on people with the education level PhD in another table called phd_data.tsv.\nWith everything learned so far, and basic information on the .concat()function, try to read in the data from ../phd_data.tsv and concatenate it to the existing df.\n\n\n\n\n\n\nQuestion\n\n\n\nRead the tsv “phd_data.tsv” as a new DataFrame and name the variable df2.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndf2 = pd.read_csv(\"../phd_data.tsv\", sep=\"\\t\")\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nConcatenate the “old” DataFrame df and the new df2 and name the concatenated one concat_df.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nconcat_df = pd.concat([df, df2])\nconcat_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nZ_CostContact\nZ_Revenue\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n3\n11\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\n11\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n3\n11\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646\n1\n0\n11\n4\n20\n10\n3\n5\n2\n0\n4\n6\n0\n3\n11\n\n\n4\n7446\n1967\nMaster\nTogether\n62513\n0\n1\n520\n42\n98\n0\n42\n14\n6\n4\n10\n6\n0\n3\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n481\n11133\n1973\nPhD\nYOLO\n48432\n0\n1\n322\n3\n50\n4\n3\n42\n7\n1\n6\n8\n0\n3\n11\n\n\n482\n9589\n1948\nPhD\nWidow\n82032\n0\n0\n332\n194\n377\n149\n125\n57\n4\n6\n7\n1\n0\n3\n11\n\n\n483\n4286\n1970\nPhD\nSingle\n57642\n0\n1\n580\n6\n58\n8\n0\n27\n7\n6\n6\n4\n0\n3\n11\n\n\n484\n4001\n1946\nPhD\nTogether\n64014\n2\n1\n406\n0\n30\n0\n0\n8\n8\n2\n5\n7\n0\n3\n11\n\n\n485\n9405\n1954\nPhD\nMarried\n52869\n1\n1\n84\n3\n61\n2\n1\n21\n3\n1\n4\n7\n0\n3\n11\n\n\n\n\n\n2240 rows × 20 columns\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there anything weird about the new DataFrame and can you fix that?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe previously removed the columns “Z_CostContact” and “Z_Revenue” but they are in the new data again.\nWe can remove them like before.\nconcat_df = concat_df.drop(\"Z_CostContact\", axis=1)\nconcat_df = concat_df.drop(\"Z_Revenue\", axis=1)\nconcat_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646\n1\n0\n11\n4\n20\n10\n3\n5\n2\n0\n4\n6\n0\n\n\n4\n7446\n1967\nMaster\nTogether\n62513\n0\n1\n520\n42\n98\n0\n42\n14\n6\n4\n10\n6\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n481\n11133\n1973\nPhD\nYOLO\n48432\n0\n1\n322\n3\n50\n4\n3\n42\n7\n1\n6\n8\n0\n\n\n482\n9589\n1948\nPhD\nWidow\n82032\n0\n0\n332\n194\n377\n149\n125\n57\n4\n6\n7\n1\n0\n\n\n483\n4286\n1970\nPhD\nSingle\n57642\n0\n1\n580\n6\n58\n8\n0\n27\n7\n6\n6\n4\n0\n\n\n484\n4001\n1946\nPhD\nTogether\n64014\n2\n1\n406\n0\n30\n0\n0\n8\n8\n2\n5\n7\n0\n\n\n485\n9405\n1954\nPhD\nMarried\n52869\n1\n1\n84\n3\n61\n2\n1\n21\n3\n1\n4\n7\n0\n\n\n\n\n\n2240 rows × 18 columns\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there something interesting about the marital status of some people that have a PhD?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nconcat_df[concat_df[\"Education\"]==\"PhD\"][\"Marital_Status\"].value_counts()\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nMarital_Status\nMarried     192\nTogether    117\nSingle       98\nDivorced     52\nWidow        24\nYOLO          2\nAlone         1\nName: count, dtype: int64\n\n\n\nThere are two people that have “YOLO” as their Marital Status …\n\n\n\n\n\n8.6.2 Merging\nBesides concatenating two DataFrames, there is another powerful function for combining data from multiple sources: .merge(). This function is especially useful when we have different types of related data in separate tables. For example, we might have a taxonomy table with count data per sample and a metadata table in another DataFrame.\nThe pandas function .merge() allows us to combine these DataFrames based on a common column. This column must exist in both DataFrames and contain similar values.\nTo illustrate the .merge() function, we will create a new DataFrame and merge it with the existing one. Let’s rank the different education levels from 1 to 5 in a new DataFrame and merge this with the existing DataFrame.\nAs shown before, there are multiple ways of making a new DataFrame. Here, we first create a dictionary and then use the from_dict() function to transform this into a DataFrame.\nThe from_dict() function will by default use the keys() of the dictionary as column names. A dictionary is made up of {'key':'value'} pairs. To specify that we want the keys() as rows, the orient= argument has to be set to index. This means that the row names are the dictionary keys().\neducation_dictionary = {\n    \"Basic\": 1,\n    \"2n Cycle\": 2,\n    \"Graduation\": 3,\n    \"Master\": 4,\n    \"PhD\": 5\n}\n\neducation_df = pd.DataFrame.from_dict(education_dictionary, orient=\"index\")\neducation_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\nBasic\n1\n\n\n2n Cycle\n2\n\n\nGraduation\n3\n\n\nMaster\n4\n\n\nPhD\n5\n\n\n\n\n\n\nThe resulting DataFrame has the Education level as index “row names” and the column name is 0.\nThe 0 is not a particular useful name for our new column, so we can use the .rename() function to change this.\neducation_df = education_df.rename(columns={0: \"Level\"})\neducation_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\nLevel\n\n\n\n\nBasic\n1\n\n\n2n Cycle\n2\n\n\nGraduation\n3\n\n\nMaster\n4\n\n\nPhD\n5\n\n\n\n\n\n\nNow that there is a new DataFrame with all the needed information, we can merge it with our previous concat_df on the Education column. The .merge() function requires several arguments: left=, which is the DataFrame that will be on the left side of the merge, in our case concat_df; right=, which is the DataFrame that will be on the right side of the merge, which is education_df; left_on=, specifying the column to merge on from the left DataFrame concat_df, which is Education; and right_index=True, indicating that the right DataFrame education_df should be merged using its index. If the values were in a column instead of the index, we would use right_on= instead.\nThe right one is education_df and the information is in the index.\nmerged_df = pd.merge(left=concat_df, right=education_df, left_on=\"Education\", right_index=True)\nmerged_df\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n\n\n\n\n\n\n...1\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nMntWines\nMntFruits\nMntMeatProducts\nMntFishProducts\nMntSweetProducts\nMntGoldProds\nNumWebPurchases\nNumCatalogPurchases\nNumStorePurchases\nNumWebVisitsMonth\nComplain\nLevel\n...21\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138\n0\n0\n635\n88\n546\n172\n88\n88\n8\n10\n4\n7\n0\n3\nNA\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344\n1\n1\n11\n1\n6\n2\n1\n6\n1\n1\n2\n5\n0\n3\nNA\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613\n0\n0\n426\n49\n127\n111\n21\n42\n8\n2\n10\n4\n0\n3\nNA\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646\n1\n0\n11\n4\n20\n10\n3\n5\n2\n0\n4\n6\n0\n3\nNA\n\n\n5\n965\n1971\nGraduation\nDivorced\n55635\n0\n1\n235\n65\n164\n50\n49\n27\n7\n3\n7\n6\n0\n3\nNA\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\nNA\n\n\n481\n11133\n1973\nPhD\nYOLO\n48432\n0\n1\n322\n3\n50\n4\n3\n42\n7\n1\n6\n8\n0\n5\nNA\n\n\n482\n9589\n1948\nPhD\nWidow\n82032\n0\n0\n332\n194\n377\n149\n125\n57\n4\n6\n7\n1\n0\n5\nNA\n\n\n483\n4286\n1970\nPhD\nSingle\n57642\n0\n1\n580\n6\n58\n8\n0\n27\n7\n6\n6\n4\n0\n5\nNA\n\n\n484\n4001\n1946\nPhD\nTogether\n64014\n2\n1\n406\n0\n30\n0\n0\n8\n8\n2\n5\n7\n0\n5\nNA\n\n\n485\n9405\n1954\nPhD\nMarried\n52869\n1\n1\n84\n3\n61\n2\n1\n21\n3\n1\n4\n7\n0\n5\nNA\n\n\n\n\n\n2240 rows × 19 columns",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#data-visualisation",
    "href": "python-pandas.html#data-visualisation",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.7 Data visualisation",
    "text": "8.7 Data visualisation\nJust looking at DataFrames is nice and useful, but in many cases, it is easier to look at data in graphs. The function that can create plots directly from DataFrames is .plot(). The .plot() function uses the plotting library matplotlib by default in the background. There are other plotting libraries such as Plotnine which will be shown further in the tutorial.\nThe only arguments .plot() requires are kind=..., and the plot axis x=... and y=.... The kind argument specifies the type of plot, such as hist for histogram, bar for bar plot, and scatter for scatter plot. Check out the Pandas documentation (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) for more plot kinds and useful syntax. There are many aesthetic functions that can help to create beautiful plots. These functions such as .set_xlabel() or ,set_title() are added to the plot, as shown in the examples below.\n\n8.7.1 Histogram\nax = merged_df.plot(kind=\"hist\", y=\"Income\")\nax.set_xlabel(\"Income\")\nax.set_title(\"Histogram of income\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.3.\n\n\n\n\n\n\nFigure 8.3: This is the histogram of income that should appear with we run the code above.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThis does not look very good because the x-axis extends so much! Looking at the data, can you figure out what might cause this?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhen we look at the highest earners, we see that somebody put 666666 as their income. This is much higher than any other income, which makes the histogram very draged out to include this person.\nmerged_df[merged_df[\"Income\"].sort_values(ascending=False)]\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\n1749    666666.0\n1006    157733.0\n1290    157146.0\n512     153924.0\n504     105471.0\n        ...\n1615    NaN\n1616    NaN\n1616    NaN\n1621    NaN\n1744    NaN\nName : Income, Length: 1754, dtype: float63\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse conditional subsetting to make the histogram look nicer.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nax = merged_df[merged_df[\"Income\"] != 666666].plot(kind=\"hist\",y=\"Income\")\nax.set_xlabel(\"Income\")\nax.set_title(\"Fixed Histogram of income\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.4.\n\n\n\n\n\n\nFigure 8.4: To “fix” the histogram, the one person with the income of 666666 is removed, making the plot look a lot neater.\n\n\n\n\n\n\n\n\n\n\n\n8.7.2 Bar plot\nInstead of making a plot from the original DataFrame we can use the groupby and mean methods to make a plot with summary statistics.\ngrouped_by_education = merged_df.groupby(by=\"Education\")[\"Income\"].mean()\ngrouped_by_education\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nEducation\n2n Cycle      47633.190000\nBasic         20306.259259\nGraduation    52720.373656\nMaster        52917.534247\nPhD           56145.313929\nName: Income, dtype: float64\n\n\n\nax = grouped_by_education.plot(kind=\"bar\")\nax.set_ylabel(\"Mean income\")\nax.set_title(\"Mean income for each education level\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.5.\n\n\n\n\n\n\nFigure 8.5: Barplot of the mean income for each education level.\n\n\n\n\n\n\n\n\n8.7.3 Scatter plot\nAnother kind of plot is the scatter plot, which needs two columns for the x and y axis.\nax = df.plot(kind=\"scatter\", x=\"MntWines\", y=\"MntFruits\")\nax.set_title(\"Wine purchases and Fruit purchases\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.6.\n\n\n\n\n\n\nFigure 8.6: A scatter plot with wine purchases on the x-axis and fruit purchases on the y-axis.\n\n\n\n\n\n\nWe can also specify whether the axes should be on the log scale or not.\nax = df.plot(kind=\"scatter\", x=\"MntWines\", y=\"MntFruits\", logy=True, logx=True)\nax.set_title(\"Wine purchases and Fruit purchases, on log scale\")\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nText(0.5, 1.0, ‘Wine purchases and Fruit purchases, on log scale’)\n\n\n\nThe scatter plot with wine purchases on the x-axis and fruit purchases on the y-axis, with on a log scale.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#plotnine",
    "href": "python-pandas.html#plotnine",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.8 Plotnine",
    "text": "8.8 Plotnine\nPlotnine is the Python clone of ggplot2, which is very powerful and is great if we are already familiar with the ggplot2 syntax!\nfrom plotnine import *\n(ggplot(merged_df, aes(\"Education\", \"MntWines\", fill=\"Education\"))\n + geom_boxplot(alpha=0.8))\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.7.\n\n\n\n\n\n\nFigure 8.7: Boxplot with the amount spent on wine per education.\n\n\n\n\n\n\n(ggplot(merged_df[(merged_df[\"Year_Birth\"]&gt;1900) & (merged_df[\"Income\"]!=666666)],\n        aes(\"Year_Birth\", \"Income\", fill=\"Education\"))\n + geom_point(alpha=0.5, stroke=0)\n + facet_wrap(\"Marital_Status\"))\n\n\n\n\n\n\nExpand to see output\n\n\n\n\n\nResults in Figure 8.8.\n\n\n\n\n\n\nFigure 8.8: Plot of the income of people born after 1900, faceted by marital status, and filled by education level.\n\n\n\n\n\n\n\n8.8.1 Advanced Questions and Exercises\nNow that we are familiar with python, pandas, and plotting. There are two data.tables from AncientMetagenomeDir which contains metadata from metagenomes. We should, by using the code in the tutorial be able to explore the datasets and make some fancy plots.\nfile names:\nsample_table_url\nlibrary_table_url",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#summary",
    "href": "python-pandas.html#summary",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nIn this chapter, we have started exploring the basics of data analysis using Python with the versatile Pandas library. We wrote Python code and executed it in a Jupyter Notebook, with just a handful of functions such as .read_csv(), .loc[], drop(), merge(), .concat() andplot()`, we have done data manipulation, calculated summary statistics, and plotted the data.\nThe takeaway messages therefore are:\n\nPython, Pandas and Jupyter Notebook are relatively easy to use and powerful for data analysis\nIf you know R and R markdown, the syntax of Python is easy to learn\n\nThese functions in this chapter and the general idea of the Python syntax should help you get started using Python on your data.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "python-pandas.html#optional-clean-up",
    "href": "python-pandas.html#optional-clean-up",
    "title": "8  Introduction to Python and Pandas",
    "section": "8.10 (Optional) clean-up",
    "text": "8.10 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nWhen closing your jupyter notebook(s), say no to saving any additional files.\nPress ctrl + c on your terminal, and type y when requested. Once completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/python-pandas directory **as well as all of its contents*.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/python-pandas*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name python-pandas --all -y",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Python and Pandas</span>"
    ]
  },
  {
    "objectID": "git-github.html",
    "href": "git-github.html",
    "title": "9  Introduction to Git(Hub)",
    "section": "",
    "text": "9.1 Background\nWhat is a version control system? This is a general term for tools that allow us to track changes to objects - in this case files - over time. When it comes to bioinformatics, this is typically files such as scripts or notebooks, or simple text files such as CSV and FASTAs (although it can also apply to much larger binary files!). The use of a good version control system allows the restoration of old versions, modification to previous changes, tracking contributions by multiple people etc. By far the most popular version control system in bioinformatics is git - which was in fact also originally co-written by the creator of the Linux operating system!\nNowadays it is popular to include a remote hosting service for version-controlled repositories. In bioinformatics, the most popular remote hosting service for Git version controlled code repositories is GitHub. While other open-source alternatives exist (e.g. GitLab or BitBucket), the most popular in bioinformatics is GitHub. It provides a user-friendly GUI and a range of other useful tools and functionality, in addition to most bioinformatic code and tools are hosted there.\nSo why should you use a version control system, such as GitHub?",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#background",
    "href": "git-github.html#background",
    "title": "9  Introduction to Git(Hub)",
    "section": "",
    "text": "To have a (deep) backup of our work\nAllow you to revert to old versions/modify previous changes to files\nAllow multiple contributors to work simultaneously\nAllow you to test new scripts or code before updating a public version in a ‘sandbox’ area of the repository\nHelp share our data, code, and results with the world!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#basic-workflow",
    "href": "git-github.html#basic-workflow",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.2 Basic workflow",
    "text": "9.2 Basic workflow\nThe basic concepts of using git and GitHub are as shown in (Figure 9.1)\n\n\n\n\n\n\nFigure 9.1: Overview of basic operations when using git, and the pushing to a remote host such as GitHub. See chapter text for description of image. Reconstructed and modified after Chacon and Straub (2014).\n\n\n\nIn the diagram of Figure 9.1, the two dark gray boxes represent a local machine (e.g. a laptop) and a remote server. Within the local machine, an ‘untracked’ box represent files not indexed by the local git repository.\nThe arrow pointing into a light grey box (the local repository) in which three white boxes are present. These represent different ‘stages’ of an object of the repository.\nThis first arrow from the ‘untracked’ box then spans to the furthest box called ‘staged’, which is the operation when we add a file to be indexed by the repository (this only happens once).\nOnce staged, the arrow pointing from the ‘staged’ box back to the first box or ‘status’ within the local repository, called ‘unmodified’. This arrow, converting a staged file to unmodified, represents making a ‘commit’ (i.e. recording to git history a repository change). We can imagine committing to be equivalent to a permanent(!) save.\nThe next arrow represents an edit to a file, which spans the ‘unmodified’ box to the middle ‘modified’ status. Once all edits have been made, the edited file is ‘staged’ - the arrow that goes from the middle of the ‘modified’ state to the ‘staged’ state - after which a commit again would be made to record that file as ‘unmodified’ compared to the change history.\nThe arrow pointing from the local repository back to the furthest left ‘untracked’ state of the local repository represents the removal of the file from indexing/tracking in the git history.\nFinally the two arrows that span between the local machine and remote server - one going from the local repository to the a remote repository (on the server) - represent ‘push ing’ the commit history to the server, and in the reverse direction - ‘pull ing’ the commit history back to the local repository.\nThese can be imagined as backing-up our git history to a cloud server, and the retrieving the backup (albeit with changes from others)\n\n\n\n\n\n\nQuestion\n\n\n\nWhy do you think it is important to have a ‘staging’ areas of changes before committing them to the git history?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis can be useful when you are adding multiple modifications to multiple files, that all address the same ‘fix’ or ‘function’.\nBy staging the files, you can commit them all at once, and have a single entry in the git history that describes all the changes you made.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#preparation",
    "href": "git-github.html#preparation",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.3 Preparation",
    "text": "9.3 Preparation\nWe will now practise some of the git operations described above.\nHowever, before we do this, we need to set up a GitHub account so that we can communicate via the command line.\nGitHub does not allow pushing and pulling with normal passwords, but rather with a concept called ssh keys.\nssh keys are special cryptographic strings of characters and numbers. When generating a pair, we get both a ‘private’ and ‘public’ key. The former we keep privately, whereas the other we upload to other servers/people.\nWhen we want to ‘prove’ that it’s us sending changes to the repository, we securely send the private key and this gets compared with the corresponding public key that we uploaded on the remote server. If after some cryptographic maths magic they match, the server will trust us and will accept our changes.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#using-ssh-keys-for-passwordless-interaction-with-github",
    "href": "git-github.html#using-ssh-keys-for-passwordless-interaction-with-github",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.4 Using ssh keys for passwordless interaction with GitHub",
    "text": "9.4 Using ssh keys for passwordless interaction with GitHub\nSo, to begin, we will set up an SSH key to facilitate easier authentication when transferring data between local and remote repositories. In other words, follow this section of the tutorial so that we never have to type in our github password again!\n\n9.4.1 Creating SSH keys\nFirst, we can generate our own ssh key pair, replacing the email below with our own email address.\nssh-keygen -t ed25519 -C \"&lt;YOUR_EMAIL&gt;@&lt;EXAMPLE&gt;.com\"\n\n\n\n\n\n\nNote\n\n\n\nThe -t flag tells the command which cryptographic algorithm to use.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCheck for typos!\nCommon errors include:\n\nssh-keygen is a single word (no spaces)\nCheck the numbers in the -t\nThe -C flag is a capital C!\n\n\n\nWhen we type this command, we will be asked a range of questions:\n\nEnter file which to save key: here we suggest keep as default\nEnter passphrase: don’t specify one here (unless we want to be ultra secure), just press enter\nPress enter without any passphrase to confirm\n\nWe should now (hopefully!) have generated an ssh key. This is normally indicated by a ‘random art image’ being pasted to console.\nA random art image normally looks like something like:\n+--[ RSA 2048]----+\n|       o=.       |\n|    o  o++E      |\n|   + . Ooo.      |\n|    + O B..      |\n|     = *S.       |\n|      o          |\n|                 |\n|                 |\n|                 |\n+-----------------+\nTo check that it worked, we can change into the default directory where keys are stored. By default on UNIX operating system this is in our home directory under a folder called .ssh. Lets change into that directory, and check the contents.\nls ~/.ssh/\nWe should now see two files: id_ed25519, and id_ed25519.pub, amongst others.\nThe first is a private key. This we should not share this any one, and should always stay on our local machine.\nThe second file (ending in .pub), is a public key. This we can give to others, on remote servers, or websites, to allow those places to know it is us.\nSo lets try this out on GitHub!\n\n\n9.4.2 Logging the keys\nFirst, we need to tell our computer that the keys exist and should be used for validation to remote locations. The tool that does that is called ssh-agent. We can check if it is running with the following command.\neval \"$(ssh-agent -s)\"\nIf it’s running, we should get a text such as Agent pid &lt;NUMBERS&gt;.\nIf it’s not running, see the GitHub documentation for more information.\n\nAn operating system assigns each running program a unique number ID. If a program isn’t running, it won’t have a process ID!\n\nWhen the agent is running, we need to give the path to the private key file as follows.\nssh-add ~/.ssh/id_ed25519\n\n\n9.4.3 Registering keys on GitHub\nNext, GitHub needs to have our public key on record so it can compare between the public and private keys. Open our web browser and navigate to the github account settings page (typically: press the profile picture in the top bar menu, then settings).\nThen, under settings, in the side bar go to SSH & GPG Keys (Figure 9.2), then press New SSH Key (Figure 9.3).\n\n\n\n\n\n\nFigure 9.2: Screenshot of Github settings page sidebar (as of August 2023), with the ‘SSH and GPG keys’ section highlighted under the ‘Access’ section.\n\n\n\n\n\n\n\n\n\nFigure 9.3: Screenshot of the top of the Github SSH and GPG keys page (as of August 2023), with a green ‘New SSH Key’ button.\n\n\n\nWhen in the ‘new SHH key’ page:\n\nwe can give key a title (e.g. the local machine the key was generated on).\nLeave the ‘Key type’ as ‘Authentication Key’\nPaste the entire contents of public key into the main text box that we just generated on our local machine.\n\ncat ~/.ssh/id_ed25519.pub\n\n\n\n\n\n\nWarning\n\n\n\nIt’s very important to paste the whole string! This starts with ssh-ed (or whatever algorithm used) and ending in our email address.\n\n\nFinally, press the Add SSH key. To check that it worked, we run the following command on our local machine.\nssh -T git@github.com\nWe should see a message along the lines of the following\nHi &lt;YOUR_USERNAME&gt;! you that you've successfully authenticated.\n\n\n\n\n\n\nNote\n\n\n\nIf we get a message saying something such as.\nThe authenticity of host 'github.com (140.82.121.3)' can't be established.\nType yes on the keyboard and press enter.\n\n\nFor more information about setting up the SSH key, including instructions for different operating systems, check out github’s documentation.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the benefits of using SSH keys for authentication with GitHub?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWe don’t have to remember our password every time we push or pull from GitHub\nIt helps make it easier to get into the habit of regular commits, pushes, and pulls\nIt’s more secure than using a password!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#creating-a-github-repository",
    "href": "git-github.html#creating-a-github-repository",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.5 Creating a GitHub repository",
    "text": "9.5 Creating a GitHub repository\nNow that we have set up our own SSH key, we can begin working on some version controlled data!\nNavigate to your GitHub homepage (https://github.com) and create a new repository. We can normally do this by pressing the ➕ icon on the homepage (typically in right hand side of the top tool bar).\nFor this tutorial, on the new repository page (Figure 9.4):\n\nChoose any name for our new repo (including the auto-generated ‘inspired ones’)\nLeave the description as empty\nSelect that the repository is ‘public’ (the default)\nTick the ’Add a README file; checkbox\nLeave as default the .gitignore and license sections\n\nThen press the green ‘Create repository’ button.\n\n\n\n\n\n\nFigure 9.4: Screenshot of top half of GitHub’s Create repository interface for creating a new repository, showing owner, empty repository name box, radio boxes indicating whether the repository should be Public or Private\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor the remainder of the session, replace the name of my repository (vigilant-octo-journey) with your own repo name.\n\n\nChange into the directory where we would like to work, and let’s get started!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#the-only-6-commands-you-only-need-to-really-know",
    "href": "git-github.html#the-only-6-commands-you-only-need-to-really-know",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.6 The only 6 commands you only need to really know",
    "text": "9.6 The only 6 commands you only need to really know\nWe have set all our authentication keys for GitHub, and created a new repository. We can now run through the some of the concepts we learnt in the Basic Workflow section.\nThis can be boiled down to just six that we really need to work with for Git(Hub) for basic version control of all our software, scripts, and (small) data!\nTo start, make sure you’re in this session directory\nmkdir /&lt;path&gt;/&lt;to&gt;/git-github\n\n9.6.1 git clone\nFirst, we will learn to clone a remote repository onto our local machine.\nThis is actually not in our basic workflow diagram in Figure 9.1, however we only need to do it once, and is only needed when we work with a remote server.\nWith clone we are making a copy of the remote repository, and linking it so we can transmit data between the copy on the local machine with the remote repository on the server.\nTo make the ‘copy’, navigate to our new repo:\n\nSelect the green code dropdown button (Figure 9.5)\nMake sure to select SSH\nCopy the full address git@github.com&lt;...&gt; as shown below in Figure 9.5.\n\n\n\n\n\n\n\nFigure 9.5: Screenshot of the GitHub repository interface with the green ‘code’ drop down button pressed, and the menu showing the ‘clone’ information for SSH cloning (i.e., the copyable SSH address)\n\n\n\nBack at our command line, clone the repo as follows.\ngit clone git@github.com:&lt;YOUR_USERNAME&gt;/&lt;YOUR_REPO_NAME&gt;.git\n\n\n\n\n\n\nWarning\n\n\n\nIt’s important that select the ssh tab, otherwise we will not be able to push and pull with our ssh keys! If we get asked for a password, we’ve not used ssh! Press ctrl + c, to cancel, and try the command again but with the correct ssh address.\n\n\nOnce cloned, we can run ls to see there is now a directory with the same name as repository.\nls\nIf we change into it and run ls again, we should see the README.md file we specified to be generated on GitHub.\ncd &lt;NAME_OF_REPO&gt;\nls\n\n\n9.6.2 git add\nNext, let’s add a new and modified file to our ‘staging area’ on our local machine.\nThis corresponds either to the red arrow or blue arrows in (Figure 9.6).\nWe can do this in two ways\n\nStage a previously untracked file\nStage a tracked, but now modified file\n\n\n\n\n\n\n\nFigure 9.6: Overview of basic operations when using git (following Figure 9.1) but with the command git add operations highlighted . The arrows indicating the two ‘staging’ operations, carried out by the git add command, are coloured red for a staging a ‘previously untracked’, and blue for the editing of an already tracked file and staging of the edited file. Reconstructed and modified after Chacon and Straub (2014).\n\n\n\nFirst we will make a new file called file_A.txt, and also add some extra text to the end of the README.md file (i.e., just modify).\nOnce we’ve made those both, lets first only stage the new file.\necho \"test_file\" &gt; file_A.txt\necho \"Just an example repo\" &gt;&gt; README.md\ngit add file_A.txt\n\n\n9.6.3 git status\nSo hopefully we’ve staged at least one file, but how do we know exactly what the status is of the modified and unmodified files in the repository?\nAt any point we can use the command git status to give a summary of any files present in the repository directory that changed status since the last commit (i.e., the last preserved entry in the git history).\ngit status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   file_A.txt\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   README.md\nWe should see that file_A.txt is staged and ‘ready to be committed’ but README.md is NOT staged - thus the changes would not be preserved to the git history. Comparing to our diagram in Figure 9.6, we have performed the ‘red’ arrow, but for the blue arrow, we’ve only carried out the ‘Edit file’ arrow, the second ‘Stage edited file’ is not yet carried out.\n\n\n\n\n\n\nTask\n\n\n\nStage the modified README.md file and check the status again.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngit add README.md\ngit status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   README.md\n    new file:   file_A.txt\nWe should now see both README.md and file_A.txt coloured green, and in the ‘changes to be committed’ section. We can also see the README is ‘modified’ whereas file_A.txt is a new file, so will be newly indexed with git (i.e., will be the first entry in the git history for that file).\n\n\n\n\n\n9.6.4 git commit\nNow we need to package or save the changes into a commit with a message describing the changes we’ve just made. Each commit (i.e., entry in the git history) comes with a unique hash ID and will be stored forever in git history. Committing corresponds to the read arrow taking all staged modified or newly added files from the ‘Staged’ to ‘Unmodified state’ in (Figure 9.7).\n\n\n\n\n\n\nFigure 9.7: Overview of basic operations when using git (following Figure 9.1) but with the command git commit operations highlighted. The red arrow shows committing the changes to history, i.e., taking a tracked file in the staging error, writing the modifications in the file to the git history and placing the file back into the ‘unmodified’ status column. Reconstructed and modified after Chacon and Straub (2014).\n\n\n\ngit commit -m \"Add new file and modify README\"\n\n\n\n\n\n\nNote\n\n\n\nThe first time we commit, we may get a message about something like this.\nYour name and email address were configured automatically based\non your username and hostname. &lt;...&gt;\nFor the purposes of this tutorial this is fine, but it is highly recommended to follow the instructions in the message to correctly associate our commits to our GitHub account.\n\n\nThe -m part of the command corresponds to the human-readable description of the change.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we run git status again?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOn branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\nWe can see while there are no modified or staged files listed any more, our ‘branch’ is now ahead by 1 commit. This means that our local copy has an extra entry in the git history, compared to the remote server (‘origin’) copy of the repository.\n\n\n\n\n\n9.6.5 git push\nHow then do we ‘backup’ our local changes and the git history back up to the server?\nWe do that with, yes you guessed it, the git push command! This is the red dashed arrow in (Figure 9.8).\n\n\n\n\n\n\nFigure 9.8: Overview of basic operations when using git (following Figure 9.1) but with the command git push operations highlighted. The red dashed arrow between the local repository box of the local machine, to the remote repository on the server. This represents ‘pushing’ or sending the changes made on the local machine back up to remote repository, so both the local and remote repository have the same records of changes to the files. Reconstructed and modified after Chacon and Straub (2014).\n\n\n\nWe can run this as follows.\ngit push\nEnumerating objects: 6, done.\nCounting objects: 100% (6/6), done.\nDelta compression using up to 14 threads\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (4/4), 367 bytes | 367.00 KiB/s, done.\nTotal 4 (delta 0), reused 0 (delta 0), pack-reused 0\nTo github.com:&lt;USERNAME&gt;/&lt;REPOSITORY_NAME&gt;.git\n   536183b..2d252b5  main -&gt; main\nWhen we do this, we will get a bunch of lines including a set of progress information. Once we get a couple of hash strings, all our changes have been copied to the remote!\n\n\n\n\n\n\nQuestion\n\n\n\nWe mentioned earlier that as well as our human-readable commit ‘message’, we will also get a unique hash string for each commit. Where on the output are the commit hashes? What do you think the two hashes represent?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe hash line is:\n   536183b..2d252b5  main -&gt; main\nThe two hashes represent the previous (536183b) entry in the git history, and the new one (2d252b5) of the one we just made when we ran git commit.\nTip: try typing git log!\nThe main bit of the string represents the branch we pushed to. We will learn more about this later in this chapter.\n\n\n\nIf we go to our GitHub repository on the website, and refresh the page, we should see the changes - both the file in the file browser at the top, and the new text we added to the README.md file!\n\n\n9.6.6 git pull\nBut what if we worked on a different local machine, and pushed changes from there? How do we the download new commits from our remote to our local repository?\n\n\n\n\n\n\nFigure 9.9: Overview of basic operations when using git (following Figure 9.1) but with the command git pull operations highlighted. The red dashed arrow going from the remote repository on the server back to the local repository represents copying changes pushed from elsewhere back to our local copy of the repository. Reconstructed and modified after Chacon and Straub (2014).\n\n\n\nWe carry this out with the counterpart of push, pull (Figure 9.9)! Try running the command now!\ngit pull\n\n\n\n\n\n\nQuestion\n\n\n\nWhat output do we get from running git pull? Why does it say what it says?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe should get a message of Already up to date. This is because we have made no further changes to the remote version of the repository since we pushed!\n\n\n\nSo how can we make changes to the remote repository? One way would be to make a local clone elsewhere on a difference machine (or in a different folder on the same machine!), make some changes and commits there, and then push from there, and pull to our current local repository…\nBut that sounds rather convoluted, no 😉?\nInstead, another benefit of GitHub is we can actually make changes to our repository from our web browser! In our web browser, in the file browser, click on the README.md. If we’re still logged into our GitHub account, we should see a small pencil icon in the top right of the file viewer (Figure 9.10).\n\n\n\n\n\n\nFigure 9.10: Screenshot of a GitHub file viewer of a README.md file, with the pen ‘edit’ icon in the top right hand corner\n\n\n\nAfter pressing the pencil icon, add some more contents to the README.md file, then press the green ‘Commit changes’ button in the top right, write a commit message (sounds familiar?), and press the next ‘green ’Commit changes’ button (we can ignore the extended description).\nOn the resulting file browser, we should see our changes, and our commit message above the file browser with the commit hash!\nMoving back to our terminal, try running git pull again. This time we should get a bunch of progress bars and statistics again\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think the number next to the pulled file name means? What do the two colours represent?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe number represents the number of lines with changes on them. The green plus and red minus symbol represents number lines with ‘additions’ modifications, and deletion modifications accordingly!\nA description of the changes is seen in the last line of the git pull output.\n\n\n\n\n\n9.6.7 Keep practising!\nAnd that’s the basic 6 commands you need to know to work with Git and GitHub for our own work - repeat ad nauseum!\nGit is supremely powerful, but can get extremely complicated because of this. If we stick with these 6 commands to begin, and as you slowly get more comfortable with the routine, we recommend to start step-by-step broadening our git knowledge as you come across other questions and problems with git. There is no one comprehensive course or documentation, so we recommend just keep practising!",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#working-collaboratively",
    "href": "git-github.html#working-collaboratively",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.7 Working collaboratively",
    "text": "9.7 Working collaboratively\nOnce we’re comfortable with working Git for our own projects, it’s time to learn a few more things to help us work both more safely, efficiently, and also eventually collaboratively!\nGit facilitates ‘sandboxes’ potential changes and also simultaneous work by small teams through branching, forks, and pull requests.\nBranching generates an independent copy of the ‘mainline’ repository contents. Forking generates an independent copy of an entire repository (with all of it’s settings, but with it’s own git history) When working on a branch, we can make as many changes and edits as we wish without breaking or modifying the ‘master’ version. When working on a fork, we can make even more changes and edits to the code but also the repository itself, without modifying the original repository (or it’s codes).\nOnce we’re happy with the changes we’ve made in our branch or fork acting as the ‘sandbox’, we can then incorporate these changes into our ‘mainline’ repository using a pull request and a merge.\nWe can make a branch from any point in our git history, and also make as many branches as we want! Forks you can only fork at the particular point of latest state of the history when you make the fork.\n\n9.7.1 Branches\nThere are two ways we can make a branch. The first way is using the GitHub interface, as in Figure 9.11.\n\n\n\n\n\n\nFigure 9.11: Three panel screenshot of GitHub interface for switching and creating branches. Panel A: A dropdown menu appears when we press ‘main’ (or the name of the current branch). Panel B: A search bar in the dropdown allows us to search for existing branches which appears in the search results. If no branch with that name exists, the search results presents a button saying ‘Create branch  from ’main’. Panel C: Once the new branch is made, we are returned to the repository file or file browser, but with the name on the dropdown listing the name of the new branch we made.\n\n\n\nThe instructions in Figure 9.11 would result in the branch existing on the remote copy of our repository i.e., on GitHub. To get this branch on our local copy, we can simply run git pull in our terminal!\nWe can also create branches via command line.\nFrom our terminal, we can create a new branch as follows.\ngit switch -c test-branch\nSwitched to a new branch 'test-branch'\nThe -c flag indicates to create the new branch with the name we provide, in the example above this is test-branch.\n\n\n\n\n\n\nNote\n\n\n\nEarlier versions of git used a command called git checkout. Often we will see this command on many older tutorials. However git switch was created as a simpler and more intuitive command. git checkout is much more powerful command, but with great power comes great responsibility (and the risk to break things…)\n\n\nTo switch back to the original main branch we were on, run the same command but without the -c flag and with the name of the branch we switch to.\ngit switch main\n\n\n\n\n\n\nWarning\n\n\n\nNote that if we start making changes on one branch we must commit changes for them to be saved to the desired branch, before we switch to a new branch!\nUncommitted changes will ‘follow’ we to which ever branch we are on until make a commit.\n\n\n\n\n\n\n\n\nBonus Question\n\n\n\nWhat command could we run to see which branches already exist in the local repository we are on? This command has not been introduced in this tutorial! Try running\ngit --help\nor Google it to find the answer!\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngit branch\n  main\n* test-branch\nWhere the green colour and the star indicates the branch we are currently on.\n\n\n\n\n\n9.7.2 Pull requests\nA Pull request (a.k.a. PR) is the GitHub term for proposing changes to a branch from another branch.\nOthers can comment and make suggestions before our changes are merged into the main branch.\nA pull request is the safest way to check our changes before we merge them in, and to ensure we don’t make any mistakes breaking something else in our ‘receiving’ branch.\n\n\n\n\n\n\nTip\n\n\n\nGit(Hub) will tell and warn us if we are proposing changes on a line where since we branched, someone else has modified that line in the ‘receiving’ branch.\nThis is called a merge conflict, and is up to us to decide which is the correct change to retain.\n\n\nTo make a pull request, we first make sure we have a branch with some changes on it.\n\n\n\n\n\n\nTask\n\n\n\nOn our local repository, in our terminal, switch back to our test-branch, and add another new line to the end of README.md, stage the file, commit, and push. (If we get stuck, or unsure, feel free to check the Answer).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFirst change to the branch.\ngit switch test-branch\nSwitched to branch 'test-branch'\nMake the edit.\necho 'We love SPAAM!' &gt;&gt; README.md\ngit status\nSwitched to branch 'test-branch'\nAdd and commit the change to git history.\ngit add README.md\ngit commit -m 'Update README'\n[test-branch 99c4266] Update README\n 1 file changed, 1 insertion(+)\nPush the changes to the remote version of the repository.\ngit push\nfatal: The current branch test-branch has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin test-branch\n\nIn this case, our remote copy of the repository does not have the branch. Therefore the first time we push, we tell Git to tell GitHub to create the branch on the remote as well by using the command it suggests.\ngit push --set-upstream origin test-branch\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 14 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 352 bytes | 352.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'test-branch' on GitHub by visiting:\nremote:      https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY_NAME&gt;/pull/new/test-branch\nremote: \nTo github.com:&lt;USERNAME&gt;/&lt;REPOSITORY_NAME&gt;.git\n * [new branch]      test-branch -&gt; test-branch\nBranch 'test-branch' set up to track remote branch 'test-branch' from 'origin\n\n\n\nOnce we’ve pushed our changes to our test-branch branch, go to the GitHub interface for the repository.\nOpen the ‘Pull requests tab’ and press the green ‘New pull request’ button, or press the green ‘Compare & pull request’ button in the yellow box that may appear if we recently pushed (Figure 9.12).\n\n\n\n\n\n\nFigure 9.12: Screenshot of GitHub pull requests tab, with green button for ‘New pull request’ and a yellow message saying a branch had recent pushes and another green button next to it saying ‘Compare & pull request’.\n\n\n\nOnce opened, we can add a title and a description of the pull request (Figure 9.13).\n\n\n\n\n\n\nFigure 9.13: Screenshot of the opening a pull request page, with two text boxes for adding a title, a longer description, and a green ‘Create pull request’ button at the bottom.\n\n\n\nOnce we press the Create pull request button, it’ll open the unique Pull request of this branch, in which others can leave comments and suggestions (Figure 9.14). By pressing the ‘Files changed’ tab, we can see exactly what has changed (Figure 9.15).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.14: Example screenshot of conversations tab of an open GitHub pull request, with the title, multiple tabs (Conversation, commits, checks, and file changed), an empty description box, and a comment box at the bottom.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.15: Example screenshot of files tab of an open GitHub pull request with a text file of README.md being displayed. In the display a red line highlights the old state of a modified line, and a green line right below shows the changes (in this case, a number of empty spaces have been added to the end of the line).\n\n\n\n\n\n\nOnce we and our collaborators are happy with the changes (a code or pull request ‘review’), we can go back to the ‘Conversation’ tab of the unique pull request, and press the green ‘Merge pull request’ button, and confirm the merge.\nWhen back on the main branch of our repository, we should see our updated README.md in all it’s glory (Figure 9.16)!\n\n\n\n\n\n\nFigure 9.16: Screenshot of GitHub repository main branch after a pull request merge, with the text changes that were originally written on the test-branch displayed in the README.md file.\n\n\n\nFor more information on creating a pull request, see GitHub’s documentation.\n\n\n\n\n\n\nTask\n\n\n\nWhat command would we use to merge one branch into another on a local copy of our repository? Tip: you are working with yourself when you work on your local repository. In this case you don’t need to ‘request’ a pull!\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngit switch main\nSwitched to branch 'main'\nYour branch is up to date with 'origin/main'.\ngit merge test-branch\nUpdating 0ad9bb3..99c4266\nFast-forward\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n\n\n\n\n\n9.7.3 Forks\nWhen working collaboratively (particularly in big teams), or you’re not yet in the development team of a repository (but would like to be), it’s often safer to work completely on our own repository, rather than just a branch. Furthermore, sometimes we want to ‘diverge’ entirely from the original code as we want to do something very different from the original authors of the tool we’re forking from but with the same original code base.\nIn these cases, instead of using branches, we can completely isolate our sandbox using a fork. A fork is a complete copy of the entire repository into a new repository, but on our own GitHub account. This means that we can even completely mess up and destroy our entire ‘copy’ of the original repository - even deleting it - without effecting the original project.\n\n\n\n\n\n\nTip\n\n\n\nFor most scientists, forks are probably not necessary as you are working on your own projects and scripts. Forks are normally useful for larger team-based projects that require stricter control over the stability of main code.\n\n\nTo make a fork on GitHub, first go to the repository we wish to ‘go our own way’ with. For this tutorial, lets use https://github.com/SPAAM-community/summerschool-github-practise! Once there, press the ‘Fork’ button, which is in near the top right hand side of the repository, next to the ‘pin’, ‘watch’, and ‘star’ buttons (Figure 9.17).\n\n\n\n\n\n\nFigure 9.17: Screenshot of the buttons on a GitHub repository used for pinning, watching, forking, and starring the repository.\n\n\n\nOnce we press ‘Fork’, we will get a new window with a variety of options for setting up our new Fork (i.e., copy) of the entire original repository (Figure 9.18). These include things such as renaming the fork, setting a new description, and whether to copy the main branch only. Whether we want to change any of these depends entirely on what we plan to do with the fork.\nIn this case, for the tutorial we will leave it up to you.\n\n\n\n\n\n\nFigure 9.18: GitHub create Fork page, with a variety of options displayed such as setting the name of the fork, setting a new description, and whether to copy the main branch only.\n\n\n\nOnce we press ‘Create Fork’, we will normally wait a moment or two for GitHub to do it’s thing, and then we’ll see a shiny new repository!\nImportantly, we will see at the top the name of new fork but next to your profile picture. Additionally, we will have an indication that indeed it is a fork with a message starting ‘forked from’ (Figure 9.19).\n\n\n\n\n\n\nFigure 9.19: The top of a newly forked repository, with a message below the ‘summerschool-github-practise’ repository name saying ‘forked from ’SPAAM-community/summerschool-github-practise’.\n\n\n\nNow we can once again practise doing a pull request! However, instead of doing this from a branch, we can can also do a pull request across repositories, i.e., from a fork to the original repository (or even a fork to a fork!) - as long as they share the same git history!\nFor our tutorial, we will add our name to the end of the list of people on the README of our fork of ‘summerschool-github-practise’, and then open a pull request.\nTo make the edit, we will just the GitHub interface for doing so. Press the pencil icon on the top right of the rendered README.md file (Figure 9.20).\n\n\n\n\n\n\nFigure 9.20: Screenshot of GitHub rendered repository README, with a pencil icon in the top right.\n\n\n\nOnce the edit window is opened, add your name and GitHub user name to the list (Figure 9.21).\n\n\n\n\n\n\nFigure 9.21: Screenshot of GitHub file edit window, with a name added to a bullet point list at the bottom.\n\n\n\nMake our commit to record the change to Git history (Figure 9.22) and double check we’ve made the change (Figure 9.23).\n\n\n\n\n\n\nFigure 9.22: A commit message being written describing the addition of a new name in the GitHub commit interface.\n\n\n\n\n\n\n\n\n\nFigure 9.23: The rendered README with the newly added name at the bottom of the list.\n\n\n\nBack on our forked repository, we can navigate back to the original repo by pressing the link below our Fork’s name (Figure 9.24).\n\n\n\n\n\n\nFigure 9.24: Clicking on the link of the original repository below the name of the fork shows a mini-summary of the original repository’s information.\n\n\n\nWe can then go to the Pull Request Tab, and press the ‘New Pull Request’ button (Figure 9.25)\n\n\n\n\n\n\nFigure 9.25: Pressing the green ‘new pull request’ button on the original repository’s Pull Request tab\n\n\n\nOnce in the open pull request interface, we almost do the same thing as we did when opening pull requests from branches within the same repository. A critical difference when dealing with Forks, however, is we must specify which fork our changes are coming from. We can do this by pressing ‘compare across forks’, and selecting our fork from the drop down menu (Figure 9.26).\n\n\n\n\n\n\nFigure 9.26: The Pull Request creation window, with the ‘compare across forks’ link pressed, and the name of the edited fork in the dropdown menu as the ‘source of the changes’ (head) being proposed to go into the ‘main’ branch of the name of the original repository (base).\n\n\n\nOnce selected, as with branch pull requests, we can check the changes we are proposing, and if happy, press the ‘Open Pull Request’ to see our Pull Request being ready for review ((ig-gitgithub-fork-openedpr?))\n\n\n\n\n\n\nFigure 9.27: An opened Pull Request on the GitHub interface of the SPAAM-community/summerschool-github-practise repository\n\n\n\nWith that you can request reviews from the curators of the original repository. Alternatively, if they don’t like your changes - you can simply keep your repository and use the code there instead (assuming the original repository had an open source license, of course 😉).\nRemember that for most scientists, forks are likely not necessary when working on your own projects or very small teams. However if you branch further into bioinformatics, and want to contribute documentation, typo fixes, or even code into existing tools - forks are likely your best friend.\n\n\n\n\n\n\nTask\n\n\n\nOther than further ‘sandboxing’ your changes, why else would you want to fork a code repository?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBecause you want to use the same original code base to make a new or changed tool for a different purpose outside that of the one of the original authors, and/or it doesn’t make sense to include it in the original tool.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#summary",
    "href": "git-github.html#summary",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.8 Summary",
    "text": "9.8 Summary\nIn this chapter, we have gone over the fundamental concepts of Git.\nWe’ve gone through setting up a GitHub account to allow passwordless interaction between the GitHub remote repository, and making a local copy on our machine with SSH keys.\nThrough the GitHub website interface we made a new repository and gone through the 6 basic commands we need for using Git:\n\ngit clone\ngit add\ngit status\ngit commit\ngit push\ngit pull\n\nWe finally covered how to work in collaboratively with:\n\nBranches: code ‘sandboxes’ within the same repository to prevent editing the ‘main’ branch\nPull requests: how to propose changes from your branch (or fork) to your ‘main’ branch\nForks: code ‘sandbox’ clones of entire repositories in a different user’s space\n\nAs you continue to use Git and GitHub, always keep in mind the two questions:\n\nWhy is using a version control software for tracking data and code important?\nHow can using Git(Hub) help me to collaborate on group projects?",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#optional-clean-up",
    "href": "git-github.html#optional-clean-up",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.9 (Optional) clean-up",
    "text": "9.9 (Optional) clean-up\nLet’s clean up our working directory by removing all the data and output from this chapter.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/git-github as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/git-github*\nOnce deleted we can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with.\nconda deactivate\nThen to delete the conda environment.\nconda remove --name git-github --all -y",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "git-github.html#references",
    "href": "git-github.html#references",
    "title": "9  Introduction to Git(Hub)",
    "section": "9.10 References",
    "text": "9.10 References\n\n\n\n\nChacon, Scott, and Ben Straub. 2014. Pro Git. 2nd ed. Berlin, Germany: APress. https://library.oapen.org/bitstream/handle/20.500.12657/28155/1/1001839.pdf.",
    "crumbs": [
      "Useful Skills",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Git(Hub)</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html",
    "href": "taxonomic-profiling.html",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "",
    "text": "10.1 Introduction\nIn this chapter, we’re going to look at taxonomic profiling, or in other words, how to get the microbial composition of a sample from the DNA sequencing data.\nThough there are many algorithms, and even more different tools available to perform taxonomic profiling, the general idea remains the same (Figure 10.1).\nAfter cleaning up the sequencing data, generally saved as FASTQ files, a taxonomic profiler is used to compare the sequenced DNA to a reference database of sequences from known organisms, in order to generate a taxonomic profile of all organisms identified in a sample (Figure 10.1)\nIf you prefer text instead of pictograms, the workflow we’re going to cover today is outlined in Figure 10.2, adapted from Sharpton (2014)\nBecause different organisms can possess the same DNA, especially when looking at shorter sequences, taxonomic profilers need to have a way to resolve the ambiguity in the taxonomic assignation (Figure 10.3).\nBy leveraging an algorithm known as the Lowest Common Ancestor (LCA), and the taxonomic tree of all known species, ambiguities are going to be resolved by assigning a higher, less precise, taxonomic rank to ambiguous matches (Figure 10.4).",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#introduction",
    "href": "taxonomic-profiling.html#introduction",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "",
    "text": "Figure 10.1: General overview of a taxonomic profiling analysis workflow\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: A typical metagenomics analysis workflow, adapted from Sharpton (2014)\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Different species can share the same DNA sequence\n\n\n\n\n\n\n\n\n\n\nFigure 10.4: A diagram of the LCA algorithm, a way to resolve these ambiguities",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#chapter-overview",
    "href": "taxonomic-profiling.html#chapter-overview",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.2 Chapter Overview",
    "text": "10.2 Chapter Overview\nToday, we’re going to use the following tools:\n\nfastp (Chen et al. 2018) for sequencing data cleaning\nMetaPhlAn (Segata et al. 2012), for taxonomic profiling\nKrona (Brian D. Ondov, Bergman, and Phillippy 2011a) and Pavian (Florian P. Breitwieser and Salzberg 2016) for the interactive exploration of the taxonomic profiles\ncuratedMetagenomicData (Pasolli et al. 2017) for retrieving modern comparison data\nPython, pandas (Reback et al. 2022), plotnine (https://plotnine.readthedocs.io/en/stable/), and scikit-bio (scikit-bio 2022) to perform exploratory data analysis and a bit of microbial ecology\n\nto explore a toy dataset that has already been prepared for you.\n\n\n\n\n\n\nPreamble: what has been done to generate this toy dataset\n\n\n\n\n\n\n10.2.1 Download and Subsample\nimport subprocess\nimport glob\nfrom pathlib import Path\nFor this tutorial, we will be using the ERR5766177 library from the sample 2612 published by (Maixner et al. 2021)\n\n`\n\n\n10.2.2 Subsampling the sequencing files to make the analysis quicker for this tutorial\nThis Python code defines a function called subsample that takes in a FASTQ file name, an output directory, and a depth value (defaulting to 1000000). The function uses the seqtk command-line tool to subsample the input FASTQ file to the desired depth and saves the output to a new file in the specified output directory. The function prints the constructed command string to the console for debugging purposes.\n\ndef subsample(filename, outdir, depth=1000000):\n    basename = Path(filename).stem\n    cmd = f\"seqtk sample -s42 {filename} {depth} &gt; {outdir}/{basename}_subsample_{depth}.fastq\"\n    print(cmd)\n    subprocess.check_output(cmd, shell=True)\nThis Python code uses a for loop to iterate over all the files in the ../data/raw/ directory that match the pattern *, and calls the subsample (defined above) function on each file in the directory ../data/subsampled.\n\nfor f in glob.glob(\"../data/raw/*\"):\n    outdir = \"../data/subsampled\"\n    subsample(f, outdir)\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\nseqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.fwd.fq.gz 1000000 &gt;\n../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq\nseqtk sample -s42 ../data/raw/ERR5766177_PE.mapped.hostremoved.rev.fq.gz 1000000 &gt;\n../data/subsampled/ERR5766177_PE.mapped.hostremoved.rev.fq_subsample_1000000.fastq\n\n\n\nFinally, we compress all files to gzip format\ngzip -f ../data/subsampled/*.fastq",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#working-in-a-jupyter-environment",
    "href": "taxonomic-profiling.html#working-in-a-jupyter-environment",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.3 Working in a jupyter environment",
    "text": "10.3 Working in a jupyter environment\nThis tutorial run-through is using a Jupyter Notebook (https://jupyter.org) for writing & executing Python code and for annotating.\nJupyter notebooks are convenient and have two types of cells: Markdown and Code. The markup cell syntax is very similar to R markdown. The markdown cells are used for annotating, which is important for sharing code with collaborators, reproducibility, and documentation.\nTo load, please run the following command from within the chapter’s directory.\ncd notebooks/\njupyter notebook analysis.ipynb\nYou can then follow that notebook, which should mirror the contents of this chapter! Otherwise try making a new notebook within Jupyter File &gt; New &gt; Notebook!\n\n\n\n\n\n\nWarning\n\n\n\nIf you wish to run all commands manually (i.e., without the notebook), you must make sure you run all commands while within the notebook directory of this chapter.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#data-pre-processing",
    "href": "taxonomic-profiling.html#data-pre-processing",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.4 Data pre-processing",
    "text": "10.4 Data pre-processing\nBefore starting to analyze our data, we will need to pre-process them to remove reads mapping to the host genome, here, Homo sapiens.\nTo do so, I’ve used the first steps of the nf-core/eager (Fellows Yates et al. 2020) pipeline, more information of which can be found in the Ancient Metagenomic Pipelines chapter.\nI’ve already done some pre-processed the data, and the resulting cleaned files are available in the data/eager_cleaned/.\n\n\n\n\n\n\nSelf-guided: onstructions for manual pre-preparation of data\n\n\n\n\n\nIf you wish to re-pre-prepare the data yourself, the basic eager command to do so is below, running on the output of the previous block in chapter overview.\nnextflow run nf-core/eager \\\n-r 2.4.7 \\\n-profile &lt;docker/singularity/podman/conda/institute&gt; \\\n--input '*_R{1,2}.fastq.gz' \\\n--fasta 'human_genome.fasta' \\\n--hostremoval_input_fastq",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#adapter-sequence-trimming-and-low-quality-bases-trimming",
    "href": "taxonomic-profiling.html#adapter-sequence-trimming-and-low-quality-bases-trimming",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.5 Adapter sequence trimming and low-quality bases trimming",
    "text": "10.5 Adapter sequence trimming and low-quality bases trimming\nSequencing adapters are small DNA sequences adding prior to DNA sequencing to allow the DNA fragments to attach to the sequencing flow cells (see Introduction to NGS Sequencing). Because these adapters could interfere with downstream analyses, we need to remove them before proceeding any further. Furthermore, because the quality of the sequencing is not always optimal, we need to remove bases of lower sequencing quality to might lead to spurious results in downstream analyses.\nTo perform both of these tasks, we’ll use the program fastp (https://github.com/OpenGene/fastp) by Chen et al. (2018).\nThe following command gets you the help of fastp (the --help option is a common option in command-line tools that displays a list of available options and their descriptions).\nfastp -h\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\noption needs value: --html\nusage: fastp [options] ...\noptions:\n  -i, --in1                            read1 input file name (string [=])\n  -o, --out1                           read1 output file name (string [=])\n  -I, --in2                            read2 input file name (string [=])\n  -O, --out2                           read2 output file name (string [=])\n      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1.\n                                       Default is to discard it. (string [=])\n      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2.\n                                       If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be\n                                       written to this same file. (string [=])\n      --overlapped_out                 for each read pair, output the overlapped region if it has no any mismatched\n                                       base. (string [=])\n      --failed_out                     specify the file to store reads that cannot pass the filters. (string [=])\n  -m, --merge                          for paired-end input, merge each pair of reads into a single read if they are\n                                       overlapped. The merged reads will be written\n                                       to the file given by --merged_out, the unmerged reads will be written to the\n                                       files specified by --out1 and --out2. The merging mode is disabled by default.\n      --merged_out                     in the merging mode, specify the file name to store merged output, or specify\n                                       --stdout to stream the merged output (string [=])\n      --include_unmerged               in the merging mode, write the unmerged or unpaired reads to the file specified\n                                       by --merge. Disabled by default.\n  -6, --phred64                        indicate the input is using phred64 scoring (it'll be converted to phred33,\n                                       so the output will still be phred33)\n  -z, --compression                    compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 4. (int [=4])\n      --stdin                          input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in.\n      --stdout                         stream passing-filters reads to STDOUT. This option will result in interleaved\n                                       FASTQ output for paired-end output. Disabled by default.\n      --interleaved_in                 indicate that &lt;in1&gt; is an interleaved FASTQ which contains both read1 and read2.\n                                       Disabled by default.\n      --reads_to_process               specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0])\n      --dont_overwrite                 don't overwrite existing files. Overwritting is allowed by default.\n      --fix_mgi_id                     the MGI FASTQ ID format is not compatible with many BAM operation tools, enable this option to fix it.\n  -V, --verbose                        output verbose log information (i.e. when every 1M reads are processed).\n  -A, --disable_adapter_trimming       adapter trimming is enabled by default. If this option is specified, adapter trimming is disabled\n  -a, --adapter_sequence               the adapter for read1. For SE data, if not specified, the adapter will be auto-detected.\n                                       For PE data, this is used if R1/R2 are found not overlapped. (string [=auto])\n      --adapter_sequence_r2            the adapter for read2 (PE data only). This is used if R1/R2 are found not overlapped.\n                                       If not specified, it will be the same as &lt;adapter_sequence&gt; (string [=auto])\n      --adapter_fasta                  specify a FASTA file to trim both read1 and read2 (if PE) by all the sequences in this FASTA file (string [=])\n      --detect_adapter_for_pe          by default, the auto-detection for adapter is for SE data input only, turn on this\n                                    option to enable it for PE data.\n  -f, --trim_front1                    trimming how many bases in front for read1, default is 0 (int [=0])\n  -t, --trim_tail1                     trimming how many bases in tail for read1, default is 0 (int [=0])\n  -b, --max_len1                       if read1 is longer than max_len1, then trim read1 at its tail to make it as\n                                       long as max_len1. Default 0 means no limitation (int [=0])\n  -F, --trim_front2                    trimming how many bases in front for read2. If it's not specified, it will follow read1's settings (int [=0])\n  -T, --trim_tail2                     trimming how many bases in tail for read2. If it's not specified, it will follow read1's settings (int [=0])\n  -B, --max_len2                       if read2 is longer than max_len2, then trim read2 at its tail to make it as long as max_len2.\n                                       Default 0 means no limitation. If it's not specified, it will follow read1's settings (int [=0])\n  -D, --dedup                          enable deduplication to drop the duplicated reads/pairs\n      --dup_calc_accuracy              accuracy level to calculate duplication (1~6), higher level uses more memory (1G, 2G, 4G, 8G, 16G, 24G).\n                                       Default 1 for no-dedup mode, and 3 for dedup mode. (int [=0])\n      --dont_eval_duplication          don't evaluate duplication rate to save time and use less memory.\n  -g, --trim_poly_g                    force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data\n      --poly_g_min_len                 the minimum length to detect polyG in the read tail. 10 by default. (int [=10])\n  -G, --disable_trim_poly_g            disable polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data\n  -x, --trim_poly_x                    enable polyX trimming in 3' ends.\n      --poly_x_min_len                 the minimum length to detect polyX in the read tail. 10 by default. (int [=10])\n  -5, --cut_front                      move a sliding window from front (5') to tail, drop the bases in the window if\n                                       its mean quality &lt; threshold, stop otherwise.\n  -3, --cut_tail                       move a sliding window from tail (3') to front, drop the bases in the window if\n                                       its mean quality &lt; threshold, stop otherwise.\n  -r, --cut_right                      move a sliding window from front to tail, if meet one window with mean quality\n                                       &lt; threshold, drop the bases in the window and the right part, and then stop.\n  -W, --cut_window_size                the window size option shared by cut_front, cut_tail or cut_sliding. Range: 1~1000, default: 4 (int [=4])\n  -M, --cut_mean_quality               the mean quality requirement option shared by cut_front, cut_tail or cut_sliding.\n                                       Range: 1~36 default: 20 (Q20) (int [=20])\n      --cut_front_window_size          the window size option of cut_front, default to cut_window_size if not specified (int [=4])\n      --cut_front_mean_quality         the mean quality requirement option for cut_front, default to cut_mean_quality if not specified (int [=20])\n      --cut_tail_window_size           the window size option of cut_tail, default to cut_window_size if not specified (int [=4])\n      --cut_tail_mean_quality          the mean quality requirement option for cut_tail, default to cut_mean_quality if not specified (int [=20])\n      --cut_right_window_size          the window size option of cut_right, default to cut_window_size if not specified (int [=4])\n      --cut_right_mean_quality         the mean quality requirement option for cut_right, default to cut_mean_quality if not specified (int [=20])\n  -Q, --disable_quality_filtering      quality filtering is enabled by default. If this option is specified, quality filtering is disabled\n  -q, --qualified_quality_phred        the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified. (int [=15])\n  -u, --unqualified_percent_limit      how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40% (int [=40])\n  -n, --n_base_limit                   if one read's number of N base is &gt;n_base_limit, then this read/pair is discarded. Default is 5 (int [=5])\n  -e, --average_qual                   if one read's average quality score &lt;avg_qual, then this read/pair is discarded.\n                                       Default 0 means no requirement (int [=0])\n  -L, --disable_length_filtering       length filtering is enabled by default. If this option is specified, length filtering is disabled\n  -l, --length_required                reads shorter than length_required will be discarded, default is 15. (int [=15])\n      --length_limit                   reads longer than length_limit will be discarded, default 0 means no limitation. (int [=0])\n  -y, --low_complexity_filter          enable low complexity filter. The complexity is defined as the percentage of base\n                                       that is different from its next base (base[i] != base[i+1]).\n  -Y, --complexity_threshold           the threshold for low complexity filter (0~100). Default is 30, which means 30% complexity is required. (int [=30])\n      --filter_by_index1               specify a file contains a list of barcodes of index1 to be filtered out, one barcode per line (string [=])\n      --filter_by_index2               specify a file contains a list of barcodes of index2 to be filtered out, one barcode per line (string [=])\n      --filter_by_index_threshold      the allowed difference of index barcode for index filtering, default 0 means completely identical. (int [=0])\n  -c, --correction                     enable base correction in overlapped regions (only for PE data), default is disabled\n      --overlap_len_require            the minimum length to detect overlapped region of PE reads. This will affect overlap analysis based PE merge,\n                                       adapter trimming and correction. 30 by default. (int [=30])\n      --overlap_diff_limit             the maximum number of mismatched bases to detect overlapped region of PE reads.\n                                       This will affect overlap analysis based PE merge, adapter trimming and correction. 5 by default. (int [=5])\n      --overlap_diff_percent_limit     the maximum percentage of mismatched bases to detect overlapped region of PE reads.\n                                       This will affect overlap analysis based PE merge, adapter trimming and correction. Default 20 means 20%. (int [=20])\n  -U, --umi                            enable unique molecular identifier (UMI) preprocessing\n      --umi_loc                        specify the location of UMI, can be (index1/index2/read1/read2/per_index/per_read, default is none (string [=])\n      --umi_len                        if the UMI is in read1/read2, its length should be provided (int [=0])\n      --umi_prefix                     if specified, an underline will be used to connect prefix and UMI (i.e.\n                                       prefix=UMI, UMI=AATTCG, final=UMI_AATTCG). No prefix by default (string [=])\n      --umi_skip                       if the UMI is in read1/read2, fastp can skip several bases following UMI, default is 0 (int [=0])\n  -p, --overrepresentation_analysis    enable overrepresented sequence analysis.\n  -P, --overrepresentation_sampling    one in (--overrepresentation_sampling) reads will be computed for overrepresentation\n                                       analysis (1~10000), smaller is slower, default is 20. (int [=20])\n  -j, --json                           the json format report file name (string [=fastp.json])\n  -h, --html                           the html format report file name (string [=fastp.html])\n  -R, --report_title                   should be quoted with ' or \", default is \"fastp report\" (string [=fastp report])\n  -w, --thread                         worker thread number, default is 3 (int [=3])\n  -s, --split                          split output by limiting total split file number with this option (2~999), a sequential number prefix\n                                       will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (int [=0])\n  -S, --split_by_lines                 split output by limiting lines of each file with this option(&gt;=1000), a sequential number prefix will be\n                                       added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (long [=0])\n  -d, --split_prefix_digits            the digits for the sequential number padding (1~10), default is 4, so the filename will be padded as\n                                       0001.xxx, 0 to disable padding (int [=4])\n      --cut_by_quality5                DEPRECATED, use --cut_front instead.\n      --cut_by_quality3                DEPRECATED, use --cut_tail instead.\n      --cut_by_quality_aggressive      DEPRECATED, use --cut_right instead.\n      --discard_unmerged               DEPRECATED, no effect now, see the introduction for merging.\n  -?, --help                           print this message\n\n\n\nHere we use fastp to preprocess a pair of FASTQ files. The code specifies the input files, merges the paired-end reads on their overlaps, removes duplicate reads, and generates JSON and HTML reports. The output files are saved in the ../results/fastp/ directory.\nfastp \\\n    --in1 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n    --in2 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n    --merge \\\n    --merged_out ../results/fastp/ERR5766177.merged.fastq.gz \\\n    --include_unmerged \\\n    --dedup \\\n    --json ../results/fastp/ERR5766177.fastp.json \\\n    --html ../results/fastp/ERR5766177.fastp.html \\\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\nRead1 before filtering:\ntotal reads: 1000000\ntotal bases: 101000000\nQ20 bases: 99440729(98.4562%)\nQ30 bases: 94683150(93.7457%)\n\nRead2 before filtering:\ntotal reads: 1000000\ntotal bases: 101000000\nQ20 bases: 99440729(98.4562%)\nQ30 bases: 94683150(93.7457%)\n\nMerged and filtered:\ntotal reads: 1994070\ntotal bases: 201397311\nQ20 bases: 198330392(98.4772%)\nQ30 bases: 188843169(93.7665%)\n\nFiltering result:\nreads passed filter: 1999252\nreads failed due to low quality: 728\nreads failed due to too many N: 20\nreads failed due to too short: 0\nreads with adapter trimmed: 282\nbases trimmed due to adapters: 18654\nreads corrected by overlap analysis: 0\nbases corrected by overlap analysis: 0\n\nDuplication rate: 0.2479%\n\nInsert size peak (evaluated by paired-end reads): 31\n\nRead pairs merged: 228\n% of original read pairs: 0.0228%\n% in reads after filtering: 0.0114339%\n\n\nJSON report: ../results/fastp/ERR5766177.fastp.json\nHTML report: ../results/fastp/ERR5766177.fastp.html\n\nfastp --in1 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz \\\n--in2 ../data/subsampled/ERR5766177_PE.mapped.hostremoved.fwd.fq_subsample_1000000.fastq.gz --merge \\\n--merged_out ../results/fastp/ERR5766177.merged.fastq.gz --include_unmerged --dedup \\\n--json ../results/fastp/ERR5766177.fastp.json --html ../results/fastp/ERR5766177.fastp.html\nfastp v0.23.2, time used: 11 seconds\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of the number of read pairs that were merged ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHere, only 228 read pairs were merged. This is due to the length of the reads of 100bp, and length of the DNA fragments. If you would use fewer cycles, and have shorter DNA fragments, you would expect this number to go up.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#taxonomic-profiling-with-metaphlan",
    "href": "taxonomic-profiling.html#taxonomic-profiling-with-metaphlan",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.6 Taxonomic profiling with Metaphlan",
    "text": "10.6 Taxonomic profiling with Metaphlan\nMetaPhlAn is a computational tool for profiling the composition of microbial communities from metagenomic shotgun sequencing data.\nmetaphlan  --help\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\nusage: metaphlan --input_type {fastq,fasta,bowtie2out,sam} [--force]\n                 [--bowtie2db METAPHLAN_BOWTIE2_DB] [-x INDEX]\n                 [--bt2_ps BowTie2 presets] [--bowtie2_exe BOWTIE2_EXE]\n                 [--bowtie2_build BOWTIE2_BUILD] [--bowtie2out FILE_NAME]\n                 [--min_mapq_val MIN_MAPQ_VAL] [--no_map] [--tmp_dir]\n                 [--tax_lev TAXONOMIC_LEVEL] [--min_cu_len]\n                 [--min_alignment_len] [--add_viruses] [--ignore_eukaryotes]\n                 [--ignore_bacteria] [--ignore_archaea] [--stat_q]\n                 [--perc_nonzero] [--ignore_markers IGNORE_MARKERS]\n                 [--avoid_disqm] [--stat] [-t ANALYSIS TYPE]\n                 [--nreads NUMBER_OF_READS] [--pres_th PRESENCE_THRESHOLD]\n                 [--clade] [--min_ab] [-o output file] [--sample_id_key name]\n                 [--use_group_representative] [--sample_id value]\n                 [-s sam_output_file] [--legacy-output] [--CAMI_format_output]\n                 [--unknown_estimation] [--biom biom_output] [--mdelim mdelim]\n                 [--nproc N] [--install] [--force_download]\n                 [--read_min_len READ_MIN_LEN] [-v] [-h]\n                 [INPUT_FILE] [OUTPUT_FILE]\n\nDESCRIPTION\n MetaPhlAn version 3.1.0 (25 Jul 2022):\n METAgenomic PHyLogenetic ANalysis for metagenomic taxonomic profiling.\n\nAUTHORS: Francesco Beghini (francesco.beghini@unitn.it),Nicola Segata (nicola.segata@unitn.it), Duy Tin Truong,\nFrancesco Asnicar (f.asnicar@unitn.it), Aitor Blanco Miguez (aitor.blancomiguez@unitn.it)\n\nCOMMON COMMANDS\n\n We assume here that MetaPhlAn is installed using the several options available (pip, conda, PyPi)\n Also BowTie2 should be in the system path with execution and read permissions, and Perl should be installed)\n\n========== MetaPhlAn clade-abundance estimation =================\n\nThe basic usage of MetaPhlAn consists in the identification of the clades (from phyla to species )\npresent in the metagenome obtained from a microbiome sample and their\nrelative abundance. This correspond to the default analysis type (-t rel_ab).\n\n*  Profiling a metagenome from raw reads:\n$ metaphlan metagenome.fastq --input_type fastq -o profiled_metagenome.txt\n\n*  You can take advantage of multiple CPUs and save the intermediate BowTie2 output for re-running\n   MetaPhlAn extremely quickly:\n$ metaphlan metagenome.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq -o profiled_metagenome.txt\n\n*  If you already mapped your metagenome against the marker DB (using a previous MetaPhlAn run), you\n   can obtain the results in few seconds by using the previously saved --bowtie2out file and\n   specifying the input (--input_type bowtie2out):\n$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out -o profiled_metagenome.txt\n\n*  bowtie2out files generated with MetaPhlAn versions below 3 are not compatibile.\n   Starting from MetaPhlAn 3.0, the BowTie2 ouput now includes the size of the profiled metagenome and the average read length.\n   If you want to re-run MetaPhlAn using these file you should provide the metagenome s via --nreads:\n$ metaphlan metagenome.bowtie2.bz2 --nproc 5 --input_type bowtie2out --nreads 520000 -o profiled_metagenome.txt\n\n*  You can also provide an externally BowTie2-mapped SAM if you specify this format with\n   --input_type. Two steps: first apply BowTie2 and then feed MetaPhlAn with the obtained sam:\n$ bowtie2 --sam-no-hd --sam-no-sq --no-unal --very-sensitive -S metagenome.sam -x \\\n  ${mpa_dir}/metaphlan_databases/mpa_v30_CHOCOPhlAn_201901 -U metagenome.fastq\n$ metaphlan metagenome.sam --input_type sam -o profiled_metagenome.txt\n\n*  We can also natively handle paired-end metagenomes, and, more generally, metagenomes stored in\n  multiple files (but you need to specify the --bowtie2out parameter):\n$ metaphlan metagenome_1.fastq,metagenome_2.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 5 --input_type fastq\n\n-------------------------------------------------------------------\n\n\n========== Marker level analysis ============================\n\nMetaPhlAn introduces the capability of characterizing organisms at the strain level using non\naggregated marker information. Such capability comes with several slightly different flavours and\nare a way to perform strain tracking and comparison across multiple samples.\nUsually, MetaPhlAn is first ran with the default -t to profile the species present in\nthe community, and then a strain-level profiling can be performed to zoom-in into specific species\nof interest. This operation can be performed quickly as it exploits the --bowtie2out intermediate\nfile saved during the execution of the default analysis type.\n\n*  The following command will output the abundance of each marker with a RPK (reads per kilo-base)\n   higher 0.0. (we are assuming that metagenome_outfmt.bz2 has been generated before as\n   shown above).\n$ metaphlan -t marker_ab_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n   The obtained RPK can be optionally normald by the total number of reads in the metagenome\n   to guarantee fair comparisons of abundances across samples. The number of reads in the metagenome\n   needs to be passed with the '--nreads' argument\n\n*  The list of markers present in the sample can be obtained with '-t marker_pres_table'\n$ metaphlan -t marker_pres_table metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n   The --pres_th argument (default 1.0) set the minimum RPK value to consider a marker present\n\n*  The list '-t clade_profiles' analysis type reports the same information of '-t marker_ab_table'\n   but the markers are reported on a clade-by-clade basis.\n$ metaphlan -t clade_profiles metagenome_outfmt.bz2 --input_type bowtie2out -o marker_abundance_table.txt\n\n*  Finally, to obtain all markers present for a specific clade and all its subclades, the\n   '-t clade_specific_strain_tracker' should be used. For example, the following command\n   is reporting the presence/absence of the markers for the B. fragilis species and its strains\n   the optional argument --min_ab specifies the minimum clade abundance for reporting the markers\n\n$ metaphlan -t clade_specific_strain_tracker --clade s__Bacteroides_fragilis metagenome_outfmt.bz2 --input_typ\n  bowtie2out -o marker_abundance_table.txt\n\n-------------------------------------------------------------------\n\npositional arguments:\n  INPUT_FILE            the input file can be:\n                        * a fastq file containing metagenomic reads\n                        OR\n                        * a BowTie2 produced SAM file.\n                        OR\n                        * an intermediary mapping file of the metagenome generated by a previous MetaPhlAn run\n                        If the input file is missing, the script assumes that the input is provided using the standard\n                        input, or named pipes.\n                        IMPORTANT: the type of input needs to be specified with --input_type\n  OUTPUT_FILE           the tab-separated output file of the predicted taxon relative abundances\n                        [stdout if not present]\n\nRequired arguments:\n  --input_type {fastq,fasta,bowtie2out,sam}\n                        set whether the input is the FASTA file of metagenomic reads or\n                        the SAM file of the mapping of the reads against the MetaPhlAn db.\n\nMapping arguments:\n  --force               Force profiling of the input file by removing the bowtie2out file\n  --bowtie2db METAPHLAN_BOWTIE2_DB\n                        Folder containing the MetaPhlAn database. You can specify the location by exporting the\n                        DEFAULT_DB_FOLDER variable in the shell.\n                        [default /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/metaphlan/metaphlan_databases]\n  -x INDEX, --index INDEX\n                        Specify the id of the database version to use. If \"latest\", MetaPhlAn will get the latest version.\n                        If an index name is provided, MetaPhlAn will try to use it, if available, and skip the online check.\n                        If the database files are not found on the local MetaPhlAn installation they\n                        will be automatically downloaded [default latest]\n  --bt2_ps BowTie2 presets\n                        Presets options for BowTie2 (applied only when a FASTA file is provided)\n                        The choices enabled in MetaPhlAn are:\n                         * sensitive\n                         * very-sensitive\n                         * sensitive-local\n                         * very-sensitive-local\n                        [default very-sensitive]\n  --bowtie2_exe BOWTIE2_EXE\n                        Full path and name of the BowTie2 executable. This option allowsMetaPhlAn to reach the\n                        executable even when it is not in the system PATH or the system PATH is unreachable\n  --bowtie2_build BOWTIE2_BUILD\n                        Full path to the bowtie2-build command to use, deafult assumes that 'bowtie2-build is present in the system path\n  --bowtie2out FILE_NAME\n                        The file for saving the output of BowTie2\n  --min_mapq_val MIN_MAPQ_VAL\n                        Minimum mapping quality value (MAPQ) [default 5]\n  --no_map              Avoid storing the --bowtie2out map file\n  --tmp_dir             The folder used to store temporary files [default is the OS dependent tmp dir]\n\nPost-mapping arguments:\n  --tax_lev TAXONOMIC_LEVEL\n                        The taxonomic level for the relative abundance output:\n                        'a' : all taxonomic levels\n                        'k' : kingdoms\n                        'p' : phyla only\n                        'c' : classes only\n                        'o' : orders only\n                        'f' : families only\n                        'g' : genera only\n                        's' : species only\n                        [default 'a']\n  --min_cu_len          minimum total nucleotide length for the markers in a clade for\n                        estimating the abundance without considering sub-clade abundances\n                        [default 2000]\n  --min_alignment_len   The sam records for aligned reads with the longest subalignment\n                        length smaller than this threshold will be discarded.\n                        [default None]\n  --add_viruses         Allow the profiling of viral organisms\n  --ignore_eukaryotes   Do not profile eukaryotic organisms\n  --ignore_bacteria     Do not profile bacterial organisms\n  --ignore_archaea      Do not profile archeal organisms\n  --stat_q              Quantile value for the robust average\n                        [default 0.2]\n  --perc_nonzero        Percentage of markers with a non zero relative abundance for misidentify a species\n                        [default 0.33]\n  --ignore_markers IGNORE_MARKERS\n                        File containing a list of markers to ignore.\n  --avoid_disqm         Deactivate the procedure of disambiguating the quasi-markers based on the\n                        marker abundance pattern found in the sample. It is generally recommended\n                        to keep the disambiguation procedure in order to minimize false positives\n  --stat                Statistical approach for converting marker abundances into clade abundances\n                        'avg_g'  : clade global (i.e. normalizing all markers together) average\n                        'avg_l'  : average of length-normalized marker counts\n                        'tavg_g' : truncated clade global average at --stat_q quantile\n                        'tavg_l' : truncated average of length-normalized marker counts (at --stat_q)\n                        'wavg_g' : winsorized clade global average (at --stat_q)\n                        'wavg_l' : winsorized average of length-normalized marker counts (at --stat_q)\n                        'med'    : median of length-normalized marker counts\n                        [default tavg_g]\n\nAdditional analysis types and arguments:\n  -t ANALYSIS TYPE      Type of analysis to perform:\n                         * rel_ab: profiling a metagenomes in terms of relative abundances\n                         * rel_ab_w_read_stats: profiling a metagenomes in terms of relative abundances and estimate\n                                                the number of reads coming from each clade.\n                         * reads_map: mapping from reads to clades (only reads hitting a marker)\n                         * clade_profiles: normalized marker counts for clades with at least a non-null marker\n                         * marker_ab_table: normalized marker counts (only when &gt; 0.0 and normalized by metagenome size if --nreads is specified)\n                         * marker_counts: non-normalized marker counts [use with extreme caution]\n                         * marker_pres_table: list of markers present in the sample (threshold at 1.0 if not differently specified with --pres_th\n                         * clade_specific_strain_tracker: list of markers present for a specific clade, specified with --clade, and all its subclades\n                        [default 'rel_ab']\n  --nreads NUMBER_OF_READS\n                        The total number of reads in the original metagenome. It is used only when\n                        -t marker_table is specified for normalizing the length-normalized counts\n                        with the metagenome size as well. No normalization applied if --nreads is not\n                        specified\n  --pres_th PRESENCE_THRESHOLD\n                        Threshold for calling a marker present by the -t marker_pres_table option\n  --clade               The clade for clade_specific_strain_tracker analysis\n  --min_ab              The minimum percentage abundance for the clade in the clade_specific_strain_tracker analysis\n\nOutput arguments:\n  -o output file, --output_file output file\n                        The output file (if not specified as positional argument)\n  --sample_id_key name  Specify the sample ID key for this analysis. Defaults to 'SampleID'.\n  --use_group_representative\n                        Use a species as representative for species groups.\n  --sample_id value     Specify the sample ID for this analysis. Defaults to 'Metaphlan_Analysis'.\n  -s sam_output_file, --samout sam_output_file\n                        The sam output file\n  --legacy-output       Old MetaPhlAn2 two columns output\n  --CAMI_format_output  Report the profiling using the CAMI output format\n  --unknown_estimation  Scale relative abundances to the number of reads mapping to known clades in order to estimate unknowness\n  --biom biom_output, --biom_output_file biom_output\n                        If requesting biom file output: The name of the output file in biom format\n  --mdelim mdelim, --metadata_delimiter_char mdelim\n                        Delimiter for bug metadata: - defaults to pipe. e.g. the pipe in k__Bacteria|p__Proteobacteria\n\nOther arguments:\n  --nproc N             The number of CPUs to use for parallelizing the mapping [default 4]\n  --install             Only checks if the MetaPhlAn DB is installed and installs it if not. All other parameters are ignored.\n  --force_download      Force the re-download of the latest MetaPhlAn database.\n  --read_min_len READ_MIN_LEN\n                        Specify the minimum length of the reads to be considered when parsing the input file with\n                        'read_fastx.py' script, default value is 70\n  -v, --version         Prints the current MetaPhlAn version and exit\n  -h, --help            show this help message and exit\n\n\n\nThe following command uses MetaPhlAn to profile the taxonomic composition of the ERR5766177 metagenomic sample. The input file is specified as a merged FASTQ file, and the output is saved as a text file containing the taxonomic profile. The --bowtie2out option is used to specify the output file for the Bowtie2 alignment, and the –nproc option is used to specify the number of CPUs to use for the analysis.\nmetaphlan ../results/fastp/ERR5766177.merged.fastq.gz  \\\n    --input_type fastq \\\n    --bowtie2out ../results/metaphlan/ERR5766177.bt2.out  \\\n    --nproc 4 \\\n    &gt; ../results/metaphlan/ERR5766177.metaphlan_profile.txt\nThe main results files that we’re interested in is located at ../results/metaphlan/ERR5766177.metaphlan_profile.txt\nIt’s a tab separated file, with taxons in rows, with their relative abundance in the sample\nhead ../results/metaphlan/ERR5766177.metaphlan_profile.txt\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\n#mpa_v30_CHOCOPhlAn_201901\n#/home/maxime_borry/.conda/envs/maxime/envs/summer_school_microbiome/bin/metaphlan ../results/fastp/ERR5766177.merged.fastq.gz \\\n--input_type fastq --bowtie2out ../results/metaphlan/ERR5766177.bt2.out --nproc 8\n#SampleID   Metaphlan_Analysis\n#clade_name NCBI_tax_id relative_abundance  additional_species\nk__Bacteria 2   82.23198\nk__Archaea  2157    17.76802\nk__Bacteria|p__Firmicutes   2|1239  33.47957\nk__Bacteria|p__Bacteroidetes    2|976   28.4209\nk__Bacteria|p__Actinobacteria   2|201174    20.33151\nk__Archaea|p__Euryarchaeota 2157|28890  17.76802\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the letter preceding the __ before every clade name ? Eg. k__Bacteria\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis letter corresponds to the rank of the clade. - k for Kingdom - p for Phylum - […] - f for Family - g for Genus - s for Species",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#visualizing-the-taxonomic-profile",
    "href": "taxonomic-profiling.html#visualizing-the-taxonomic-profile",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.7 Visualizing the taxonomic profile",
    "text": "10.7 Visualizing the taxonomic profile\n\n10.7.1 Visualizing metaphlan taxonomic profile with Pavian\nPavian (https://github.com/fbreitwieser/pavian) by Breitweiser et al. (2020) is a web-based tool for interactive visualization and analysis of metagenomics data. It provides a user-friendly interface for exploring taxonomic and functional profiles of microbial communities, and allows users to generate interactive plots and tables that can be customised and shared (Figure 10.5).\nYou can open Pavian in your browser by visiting https://fbreitwieser.shinyapps.io/pavian.\n\n\n\n\n\n\nFigure 10.5: Screenshot of the pavian metagenomics visualisation interface, with menus on the left, a select sample and filter taxa search bar at the top, and a Sankey visualisation of the example metagenome sample\n\n\n\n\n\n\n\n\n\nExpand for instructions for running yourself\n\n\n\n\n\nThere are different ways to run it:\n\nIf you have docker (https://www.docker.com/) installed\ndocker pull 'florianbw/pavian'\ndocker run --rm -p 5000:80 florianbw/pavian\n\nThen open your browser and visit localhost:5000\n\nIf you are familiar with R (https://www.r-project.org/)\n\nif (!require(remotes)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"fbreitwieser/pavian\")\n\npavian::runApp(port = 5000)\n\n\nThen open your browser and visit localhost:5000\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the relative abundance of the phylum Firmicutes ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this example, the relative abundance of the Firmicutes (officially renamed Bacillota since 2021) is 33.5%. You can verify this number either in the Sankey plot (section “Sample”), or in the comparison table (section “Comparison”) by selecting the Phylum tab.\n\n\n\n\n\n10.7.2 Visualizing metaphlan taxonomic profile with Krona\nKrona (https://github.com/marbl/Krona/wiki) by Ondov et al. (Brian D. Ondov, Bergman, and Phillippy 2011b) is a software tool for interactive visualization of hierarchical data, such as taxonomic profiles generated by metagenomics tools like MetaPhlAn. Krona allows users to explore the taxonomic composition of microbial communities in a hierarchical manner, from the highest taxonomic level down to the species level.\nThe metaphlan2krona.py script is used to convert the MetaPhlAn taxonomic profile output to a format that can be visualised by Krona. The output of the script is a text file that contains the taxonomic profile in a hierarchical format that can be read by Krona. The ktImportText command is then used to generate an interactive HTML file that displays the taxonomic profile in a hierarchical manner using Krona.\n\npython ../scripts/metaphlan2krona.py -p ../results/metaphlan/ERR5766177.metaphlan_profile.txt -k ../results/krona/ERR5766177_krona.out\nktImportText -o ../results/krona/ERR5766177_krona.html ../results/krona/ERR5766177_krona.out\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\nWriting ../results/krona/ERR5766177_krona.html...\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich proportion of the Firmicutes is made of Clostridiales ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Clostridiales represent 63% of the Firmicutes in this sample. You can verify this by clicking in the Clostridiales and looking at the pie chart for Firmicutes on the top right of the screen.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#getting-modern-comparative-reference-data",
    "href": "taxonomic-profiling.html#getting-modern-comparative-reference-data",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.8 Getting modern comparative reference data",
    "text": "10.8 Getting modern comparative reference data\nIn order to compare our sample with modern reference samples, I used the curatedMetagenomicsData package, which provides both curated metadata, and pre-computed metaphlan taxonomic profiles for published modern human samples.\nThe full R code to get these data is available in curatedMetagenomics/get_sources.Rmd.\nI pre-selected 200 gut microbiome samples from non-westernised (100) and westernised (100) from healthy, non-antibiotic users donors.\n\n# Load required packages\nlibrary(curatedMetagenomicData)\nlibrary(tidyverse)\n\n# Filter samples based on specific criteria\nsampleMetadata %&gt;%\n    filter(body_site == \"stool\" & antibiotics_current_use == \"no\" & disease == \"healthy\") %&gt;%\n    group_by(non_westernized) %&gt;%\n    sample_n(100) %&gt;%\n    ungroup() -&gt; selected_samples\n\n# Extract relative abundance data for selected samples\nselected_samples %&gt;%\n    returnSamples(\"relative_abundance\") -&gt; rel_ab\n\n# Split relative abundance data by taxonomic rank and write to CSV files\ndata_ranks &lt;- splitByRanks(rel_ab)\n\nfor (r in names(data_ranks)) {\n    # Print taxonomic rank and output file name\n    print(r)\n    output_file &lt;- paste0(\"../../data/curated_metagenomics/modern_sources_\", tolower(r), \".csv\")\n    print(output_file)\n\n    # Write relative abundance data to CSV file\n    assay_rank &lt;- as.data.frame(assay(data_ranks[[r]]))\n    write.csv(assay_rank, output_file)\n}\n\n\nThe resulting pre-computed metaphlan taxonomic profiles (split by taxonomic ranks) are available in data/curated_metagenomics\nThe associated metadata is available at data/metadata/curated_metagenomics_modern_sources.csv",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#loading-the-ancient-sample-taxonomic-profile",
    "href": "taxonomic-profiling.html#loading-the-ancient-sample-taxonomic-profile",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.9 Loading the ancient sample taxonomic profile",
    "text": "10.9 Loading the ancient sample taxonomic profile\nThis is the moment where we will use the Pandas (https://pandas.pydata.org) Python library (https://www.python.org/) to perform some data manipulation. We will also use the Taxopy (https://github.com/apcamargo/taxopy) library to work with taxonomic information.\nIn python we need to import necessary libraries, i.e. pandas and taxopy, and a couple of other utility libraries.\nimport pandas as pd\nimport taxopy\nimport pickle\nimport gzip\nAnd we then create an instance of the taxopy taxonomy database. This will take a few seconds/minutes, as it needs to download the entire NCBI taxonomy before storing in a local database.\ntaxdb = taxopy.TaxDb()\nLet’s read the metaphlan profile table with pandas (a python package with a similar concept to tidyverse dyplyr, tidyr packa). It’s a tab separated file, so we need to specify the delimiter as \\t, and skip the comment lines of the files that start with #.\nancient_data = pd.read_csv(\"../results/metaphlan/ERR5766177.metaphlan_profile.txt\",\n                            comment=\"#\",\n                            delimiter=\"\\t\",\n                            names=['clade_name','NCBI_tax_id','relative_abundance','additional_species'])\nTo look at the head of a dataframe (Table 10.1) with pandas\nancient_data.head()\n\n\n\nTable 10.1: Top few lines of a MetaPhlAn taxonomic profile\n\n\n\n\n\n\n\n\n\n\n\n\n\nclade_name\nNCBI_tax_id\nrelative_abundance\nadditional_species\n\n\n\n\n0\nk__Bacteria\n2\n82.23198\nNaN\n\n\n1\nk__Archaea\n2157\n17.76802\nNaN\n\n\n2\nk__Bacteria|p__Firmicutes\n2|1239\n33.47957\nNaN\n\n\n3\nk__Bacteria|p__Bacteroidetes\n2|976\n28.42090\nNaN\n\n\n4\nk__Bacteria|p__Actinobacteria\n2|201174\n20.33151\nNaN\n\n\n\n\n\n\nWe can also specify more rows by randomly picking 10 rows to display (Table 10.2).\nancient_data.sample(10)\n\n\n\nTable 10.2: Ten randomly selected lines of a MetaPhlAn taxonomic profile\n\n\n\n\n\n\n\n\n\n\n\n\n\nclade_name\nNCBI_tax_id\nrelative_abundance\nadditional_species\n\n\n\n\n1\nk__Archaea\n2157\n17.76802\nNaN\n\n\n46\nk__Bacteria|p__Bacteroidetes|c_Bacteroidia|o…\n2|976|200643|171549|171552|838|165179\n25.75544\nk__Bacteria|p__Bacteroidetes|c_Bacteroidia|o…\n\n\n55\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n2|1239|186801|186802|186803|189330|88431\n0.91178\nNaN\n\n\n18\nk__Archaea|p__Euryarchaeota|c_Halobacteria|o…\n2157|28890|183963|2235\n0.71177\nNaN\n\n\n36\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n2|201174|1760|85004|31953|1678\n9.39377\nNaN\n\n\n65\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n2|201174|1760|85004|31953|1678|216816\n0.05447\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n\n\n37\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n2|1239|186801|186802|186803|\n2.16125\nNaN\n\n\n38\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n2|1239|186801|186802|541000|216851\n1.24537\nNaN\n\n\n26\nk__Bacteria|p__Actinobacteria|c__Actinobacteri…\n2|201174|1760|85004|31953\n9.39377\nNaN\n\n\n48\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n2|1239|186801|186802|541000|1263|40518\n14.96816\nk__Bacteria|p__Firmicutes|c__Clostridia|o__Clo…\n\n\n\n\n\n\nBecause for this analysis, we’re only going to look at the relative abundance, we’ll only use this column, and the Taxonomic ID (TAXID) (https://www.ncbi.nlm.nih.gov/taxonomy) information, so we can drop (get rid of) the unnecessary columns.\nancient_data = (\n    ancient_data\n    .rename(columns={'NCBI_tax_id': 'TAXID'})\n    .drop(['clade_name','additional_species'], axis=1)\n)\n\n\n\n\n\n\nImportant\n\n\n\nAlways investigate your data at first !\nancient_data.relative_abundance.sum()\n700.00007\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think of a 700% relative abundance ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s proceed further and try to understand what’s happening (Table 10.3).\nancient_data.head()\n\n\n\nTable 10.3: A two column table of TAXIDs and the organisms corresponding relative abundance\n\n\n\n\n\n\nTAXID\nrelative_abundance\n\n\n\n\n0\n2\n82.23198\n\n\n1\n2157\n17.76802\n\n\n2\n2|1239\n33.47957\n\n\n3\n2|976\n28.42090\n\n\n4\n2|201174\n20.33151\n\n\n\n\n\n\nTo make sense of the TAXID, we will use taxopy to get all the taxonomic related informations such as (Table 10.4):\n\nName of the taxon\nRank of the taxon\nLineage of the taxon\n\n## This function is here to help us get the taxon information\n## from the metaphlan taxonomic ID lineage, of the following form\n## 2|976|200643|171549|171552|838|165179\n\ndef to_taxopy(taxid_entry, taxo_db):\n    \"\"\"Returns a taxopy taxon object\n    Args:\n        taxid_entry(str): metaphlan TAXID taxonomic lineage\n        taxo_db(taxopy database)\n    Returns:\n        (bool): Returns a taxopy taxon object\n    \"\"\"\n    taxid = taxid_entry.split(\"|\")[-1] # get the last element\n    try:\n        if len(taxid) &gt; 0:\n            return taxopy.Taxon(int(taxid), taxo_db) # if it's not empty, get the taxon corresponding to the taxid\n        else:\n            return taxopy.Taxon(12908, taxo_db) # otherwise, return the taxon associated with unclassified sequences\n    except taxopy.exceptions.TaxidError as e:\n        return taxopy.Taxon(12908, taxo_db)\nancient_data['taxopy'] = ancient_data['TAXID'].apply(to_taxopy, taxo_db=taxo_db)\n\nancient_data.head()\n\n\n\nTable 10.4: A three column table of TAXIDs and the organisms corresponding relative abundance, and the attached taxonomic path associated with the TAXID\n\n\n\n\n\n\n\n\n\n\n\n\nTAXID\nrelative_abundance\ntaxopy\n\n\n\n\n0\n2\n82.23198\ns__Bacteria\n\n\n1\n2157\n17.76802\ns__Archaea\n\n\n2\n2|1239\n33.47957\ns__Bacteria;c__Terrabacteria group;p__Firmicutes\n\n\n3\n2|976\n28.42090\ns__Bacteria;c__FCB group;p__Bacteroidetes\n\n\n4\n2|201174\n20.33151\ns__Bacteria;c__Terrabacteria group;p__Actinoba…\n\n\n\n\n\n\nancient_data = ancient_data.assign(\n    rank = ancient_data.taxopy.apply(lambda x: x.rank),\n    name = ancient_data.taxopy.apply(lambda x: x.name),\n    lineage = ancient_data.taxopy.apply(lambda x: x.name_lineage),\n)\n\nancient_data\n\n\n\nTable 10.5: A five column table of TAXIDs and the organisms corresponding relative abundance, and the attached taxonomic path associated with the TAXID, but also the rank and name of the particular taxonomic ID\n\n\n\n\n\n\n\n\n...1\nTAXID\nrelative_abundance\ntaxopy\nrank\nname\nlineage\n\n\n\n\n0\n2\n82.23198\ns__Bacteria\nsuperkingdom\nBacteria\n[Bacteria, cellular organisms, root]\n\n\n1\n2157\n17.76802\ns__Archaea\nsuperkingdom\nArchaea\n[Archaea, cellular organisms, root]\n\n\n2\n2\\|1239\n33.47957\ns__Bacteria;c__Terrabacteria group;p__Firmicutes\nphylum\nFirmicutes\n[Firmicutes, Terrabacteria group, Bacteria, ce...\n\n\n3\n2\\|976\n28.4209\ns__Bacteria;c__FCB group;p__Bacteroidetes\nphylum\nBacteroidetes\n[Bacteroidetes, Bacteroidetes/Chlorobi group, ...\n\n\n4\n2\\|201174\n20.33151\ns__Bacteria;c__Terrabacteria group;p__Actinoba...\nphylum\nActinobacteria\n[Actinobacteria, Terrabacteria group, Bacteria...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n62\n2\\|1239\\|186801\\|186802\\|186803\\|572511\\|33039\n0.2491\ns__Bacteria;c__Terrabacteria group;p__Firmicut...\nspecies\n[Ruminococcus] torques\n[[Ruminococcus] torques, Mediterraneibacter, L...\n\n\n63\n2\\|201174\\|84998\\|84999\\|84107\\|1472762\\|1232426\n0.17084\ns__Bacteria;c__Terrabacteria group;p__Actinoba...\nspecies\n[Collinsella] massiliensis\n[[Collinsella] massiliensis, Enorma, Coriobact...\n\n\n64\n2\\|1239\\|186801\\|186802\\|186803\\|189330\\|39486\n0.0769\ns__Bacteria;c__Terrabacteria group;p__Firmicut...\nspecies\nDorea formicigenerans\n[Dorea formicigenerans, Dorea, Lachnospiraceae...\n\n\n65\n2\\|201174\\|1760\\|85004\\|31953\\|1678\\|216816\n0.05447\ns__Bacteria;c__Terrabacteria group;p__Actinoba...\nspecies\nBifidobacterium longum\n[Bifidobacterium longum, Bifidobacterium, Bifi...\n\n\n66\n2\\|1239\\|186801\\|186802\\|541000\\|1263\\|1262959\n0.0144\ns__Bacteria;c__Terrabacteria group;p__Firmicut...\nspecies\nRuminococcus sp. CAG:488\n[Ruminococcus sp. CAG:488, environmental sampl...\n\n\n\n\n\n\n\n\nBecause our modern data are split by ranks, we’ll first split our ancient sample by rank\nWhich of the entries are at the species rank level?\nancient_species = ancient_data.query(\"rank == 'species'\")\n\nancient_species.head()\n\n\n\nTable 10.6: A five column table of TAXIDs and the organisms corresponding relative abundance, and the attached taxonomic path associated with the TAXID, but also the rank and name of the particular taxonomic ID, filtered to only species\n\n\n\n\n\n\n\n\n...1\nTAXID\nrelative_abundance\ntaxopy\nrank\nname\nlineage\n\n\n\n\n46\n2\\|976\\|200643\\|171549\\|171552\\|838\\|165179\n25.75544\ns__Bacteria;c__FCB group;p__Bacteroidetes;c__B...\nspecies\nPrevotella copri\n[Prevotella copri, Prevotella, Prevotellaceae,...\n\n\n47\n2157\\|28890\\|183925\\|2158\\|2159\\|2172\\|2173\n17.05626\ns__Archaea;p__Euryarchaeota;c__Methanomada gro...\nspecies\nMethanobrevibacter smithii\n[Methanobrevibacter smithii, Methanobrevibacte...\n\n\n48\n2\\|1239\\|186801\\|186802\\|541000\\|1263\\|40518\n14.96816\ns__Bacteria;c__Terrabacteria group;p__Firmicut...\nspecies\nRuminococcus bromii\n[Ruminococcus bromii, Ruminococcus, Oscillospi...\n\n\n49\n2\\|1239\\|186801\\|186802\\|186803\\|841\\|301302\n13.57908\ns__Bacteria;c__Terrabacteria group;p__Firmicut...\nspecies\nRoseburia faecis\n[Roseburia faecis, Roseburia, Lachnospiraceae,...\n\n\n50\n2\\|201174\\|84998\\|84999\\|84107\\|102106\\|74426\n9.49165\ns__Bacteria;c__Terrabacteria group;p__Actinoba...\nspecies\nCollinsella aerofaciens\n[Collinsella aerofaciens, Collinsella, Corioba...\n\n\n\n\n\n\n\n\nLet’s do a bit of renaming to prepare for what’s coming next\nancient_species = ancient_species[['relative_abundance','name']].set_index('name').rename(columns={'relative_abundance':'ERR5766177'})\n\nancient_species.head()\n\n\n\nTable 10.7: Reconstruction of the first two column taxonomic profile but with species-level organism names rather than TAXIDs\n\n\n\n\n\nname\nERR5766177\n\n\n\n\nPrevotella copri\n25.75544\n\n\nMethanobrevibacter smithii\n17.05626\n\n\nRuminococcus bromii\n14.96816\n\n\nRoseburia faecis\n13.57908\n\n\nCollinsella aerofaciens\n9.49165\n\n\n\n\n\n\nancient_phylums = ancient_data.query(\"rank == 'phylum'\")\n\nancient_phylums = ancient_phylums[['relative_abundance','name']].set_index('name').rename(columns={'relative_abundance':'ERR5766177'})\n\nancient_phylums\n\n\n\nTable 10.8: Reconstruction of the first two column taxonomic profile but with phylum-level organism names rather than TAXIDs\n\n\n\n\n\nname\nERR5766177\n\n\n\n\nFirmicutes\n33.47957\n\n\nBacteroidetes\n28.42090\n\n\nActinobacteria\n20.33151\n\n\nEuryarchaeota\n17.76802\n\n\n\n\n\n\nNow, let’s go back to the 700% relative abundance issue…\nancient_data.groupby('rank')['relative_abundance'].sum()\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\n    rank\n    class            99.72648\n    family           83.49854\n    genus            97.56524\n    no rank          19.48331\n    order            99.72648\n    phylum          100.00000\n    species         100.00002\n    superkingdom    100.00000\n    Name: relative_abundance, dtype: float64\n\n\n\nSeems better, right ?\nPause and think: why don’t we get exactly 100% ?",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#bringing-together-ancient-and-modern-samples",
    "href": "taxonomic-profiling.html#bringing-together-ancient-and-modern-samples",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.10 Bringing together ancient and modern samples",
    "text": "10.10 Bringing together ancient and modern samples\nNow let’s load our modern reference samples\nFirst at the phylum level (Table 10.9)\nmodern_phylums = pd.read_csv(\"../data/curated_metagenomics/modern_sources_phylum.csv\", index_col=0)\nmodern_phylums.head()\n\n\n\nTable 10.9: Taxonomic profiles at phylum level of multiple modern samples\n\n\n\n\n\n\n\n\n...1\nde028ad4-7ae6-11e9-a106-68b59976a384\nPNP_Main_283\nPNP_Validation_55\nG80275\nPNP_Main_363\nSAMEA7045572\nSAMEA7045355\nHD-13\nEGAR00001420773_9002000001423910\nSID5428-4\n...12\nA46_02_1FE\nTZ_87532\nA94_01_1FE\nKHG_7\nLDK_4\nKHG_9\nA48_01_1FE\nKHG_1\nTZ_81781\nA09_01_1FE\n\n\n\n\nBacteroidetes\n0.00000\n17.44332\n82.86400\n69.99087\n31.93081\n51.76204\n53.32801\n74.59667\n8.81074\n26.39694\n...\n1.97760\n1.49601\n67.21410\n4.29848\n68.16890\n38.59709\n14.81828\n10.13908\n57.14031\n11.61544\n\n\nFirmicutes\n95.24231\n60.47031\n16.53946\n22.81977\n65.23075\n41.96928\n45.77661\n23.51065\n54.35341\n62.23094\n...\n76.68499\n78.13269\n29.72394\n33.51772\n19.11149\n46.87139\n72.68136\n35.43789\n40.57101\n24.72113\n\n\nProteobacteria\n4.49959\n0.77098\n0.05697\n4.07757\n0.27316\n3.33972\n0.02001\n1.72865\n0.00000\n1.81016\n...\n16.57250\n0.76159\n2.35058\n9.83772\n5.32392\n0.19699\n3.64655\n17.64151\n0.30580\n56.20177\n\n\nActinobacteria\n0.25809\n10.27631\n0.45187\n1.11902\n2.31075\n2.92715\n0.77667\n0.16403\n36.55138\n1.19951\n...\n3.01814\n19.20468\n0.69913\n46.99479\n7.39093\n14.26365\n5.47750\n36.77145\n1.16426\n7.40894\n\n\nVerrucomicrobia\n0.00000\n0.00784\n0.00000\n1.99276\n0.25451\n0.00000\n0.00000\n0.00000\n0.09940\n3.29795\n...\n0.05011\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n\n\n\n\n\n\n\n\nThen at the species level\nmodern_species = pd.read_csv(\"../data/curated_metagenomics/modern_sources_species.csv\", index_col=0)\nAs usual, we always check if our data has been loaded correctly (Table 10.10)\nmodern_species.head()\n\n\n\nTable 10.10: Taxonomic profiles at species level of multiple modern samples\n\n\n\n\n\n\n\n\n...1\nde028ad4-7ae6-11e9-a106-68b59976a384\nPNP_Main_283\nPNP_Validation_55\nG80275\nPNP_Main_363\nSAMEA7045572\nSAMEA7045355\nHD-13\nEGAR00001420773_9002000001423910\nSID5428-4\n...12\nA46_02_1FE\nTZ_87532\nA94_01_1FE\nKHG_7\nLDK_4\nKHG_9\nA48_01_1FE\nKHG_1\nTZ_81781\nA09_01_1FE\n\n\n\n\nBacteroides vulgatus\n0\n0.60446\n1.59911\n4.39085\n0.04494\n4.66505\n2.99431\n29.30325\n1.48560\n0.98818\n...\n0.20717\n0\n0.00309\n0.48891\n0.00000\n0.02230\n0.00000\n0.15112\n0\n0.00836\n\n\nBacteroides stercoris\n0\n0.00546\n0.00000\n0.00000\n2.50789\n0.00000\n20.57498\n8.28443\n1.23261\n0.00000\n...\n0.00000\n0\n0.00000\n0.00693\n0.00000\n0.02603\n0.00000\n0.19318\n0\n0.00000\n\n\nAcidaminococcus intestini\n0\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.32822\n0.00000\n...\n0.00000\n0\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0.00000\n0\n0.00000\n\n\nEubacterium sp CAG 38\n0\n0.06712\n0.81149\n0.05247\n0.26027\n0.00000\n0.00000\n2.62415\n0.46585\n0.23372\n...\n0.78140\n0\n0.00000\n0.00499\n0.00000\n0.02446\n0.00000\n0.00000\n0\n0.00000\n\n\nParabacteroides distasonis\n0\n1.34931\n2.00672\n5.85067\n0.59019\n7.00027\n1.28075\n0.61758\n0.07383\n2.80355\n...\n0.11423\n0\n0.01181\n0.01386\n0.03111\n0.07463\n0.15597\n0.07541\n0\n0.01932\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would we get a random sample of your species table ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmodern_species.sample()\n\n\n\n\n10.10.1 Time to merge !\nNow, let’s merge our ancient sample with the modern data in one single table. For that, we’ll use the pandas merge function which will merge the two tables together, using the index as the merge key.\nall_species = ancient_species.merge(modern_species, left_index=True, right_index=True, how='outer').fillna(0)\nall_phylums = ancient_phylums.merge(modern_phylums, left_index=True, right_index=True, how='outer').fillna(0)\nFinally, let’s load the metadata, which contains the information about the modern samples (Table 10.11).\nmetadata = pd.read_csv(\"../data/metadata/curated_metagenomics_modern_sources.csv\")\n\nmetadata.head()\n\n\n\nTable 10.11: Various metadata information about the samples in this example\n\n\n\n\n\n\n\n\n...1\nstudy_name\nsample_id\nsubject_id\nbody_site\nantibiotics_current_use\nstudy_condition\ndisease\nage\ninfant_age\nage_category\n...12\nhla_drb11\nbirth_order\nage_twins_started_to_live_apart\nzigosity\nbrinkman_index\nalcohol_numeric\nbreastfeeding_duration\nformula_first_day\nALT\neGFR\n\n\n\n\n0\nShaoY_2019\nde028ad4-7ae6-11e9-a106-68b59976a384\nC01528_ba\nstool\nno\ncontrol\nhealthy\n0\n4\nnewborn\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nZeeviD_2015\nPNP_Main_283\nPNP_Main_283\nstool\nno\ncontrol\nhealthy\nNaN\nNaN\nadult\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nZeeviD_2015\nPNP_Validation_55\nPNP_Validation_55\nstool\nno\ncontrol\nhealthy\nNaN\nNaN\nadult\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nVatanenT_2016\nG80275\nT014806\nstool\nno\ncontrol\nhealthy\n1\nNaN\nchild\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nZeeviD_2015\nPNP_Main_363\nPNP_Main_363\nstool\nno\ncontrol\nhealthy\nNaN\nNaN\nadult\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy did we use an “outer” join when merging the ancient and modern taxonomic profiles ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAn outer join will include entries from both left (ancient) and right (modern) table, even if they’re not present in both. This ensures that taxons present in the ancient sample are not excluded when merging with the modern samples, and vice versa. Missing entries in one table will be replaced with NaN which we then replaced by zeros (with .fillna(0)).",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#comparing-ancient-and-modern-samples",
    "href": "taxonomic-profiling.html#comparing-ancient-and-modern-samples",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.11 Comparing ancient and modern samples",
    "text": "10.11 Comparing ancient and modern samples\n\n10.11.1 Taxonomic composition\nOne common plot in microbiome papers in a stacked barplot, often at the phylum or family level.\nFirst, we’ll do some renaming, to make the value of the metadata variables a bit easier to understand (Table 10.12)\ngroup_info = pd.concat(\n    [\n        (\n        metadata['non_westernized']\n        .map({'no':'westernized','yes':'non_westernized'}) # for the non_westernized in the modern sample metadata, rename the value levels\n        .to_frame(name='group').set_index(metadata['sample_id']) # rename the column to group\n        .reset_index()\n        ),\n        (\n        pd.Series({'sample_id':'ERR5766177', 'group':'ancient'}).to_frame().transpose()\n        )\n    ],\n    axis=0, ignore_index=True\n)\ngroup_info\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\n    /var/folders/1c/l1qb09f15jddsh65f6xv1n_r0000gp/T/ipykernel_40830/27419655.py:2:\n    FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version.\n    Use pandas.concat instead.\n      metadata['non_westernized']\n\n\n\n\n\n\nTable 10.12: Table of samples and their group\n\n\n\n\n\n\nsample_id\ngroup\n\n\n\n\n0\nde028ad4-7ae6-11e9-a106-68b59976a384\nwesternized\n\n\n1\nPNP_Main_283\nwesternized\n\n\n2\nPNP_Validation_55\nwesternized\n\n\n3\nG80275\nwesternized\n\n\n4\nPNP_Main_363\nwesternized\n\n\n…\n…\n…\n\n\n196\nA48_01_1FE\nnon_westernized\n\n\n197\nKHG_1\nnon_westernized\n\n\n198\nTZ_81781\nnon_westernized\n\n\n199\nA09_01_1FE\nnon_westernized\n\n\n200\nERR5766177\nancient\n\n\n\n\n\n\nWe need transform our data in ‘tidy format’ (https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to plot with plotnine (https://plotnine.readthedocs.io/en/stable/), a python clone of ggplot (https://ggplot2.tidyverse.org/index.html).\n\n\n\nTable 10.13: Table of the raw multi-sample taxonomic table\n\n\n\n\n\n\n\n\n...1\nActinobacteria\nApicomplexa\nAscomycota\nBacteroidetes\nBasidiomycota\nCandidatus Melainabacteria\nChlamydiae\nChloroflexi\nCyanobacteria\nDeferribacteres\n...12\nFusobacteria\nLentisphaerae\nPlanctomycetes\nProteobacteria\nSpirochaetes\nSynergistetes\nTenericutes\nVerrucomicrobia\nsample_id\ngroup\n\n\n\n\n200\n20.33151\n0\n0\n28.4209\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\nERR5766177\nancient\n\n\n0\n0.25809\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n4.49959\n0\n0\n0\n0\nde028ad4-7ae6-11e9-a106-68b59976a384\nwesternized\n\n\n1\n10.27631\n0\n0\n17.44332\n0\n0\n0\n0\n0\n0\n...\n0\n0.01486\n0\n0.77098\n0\n0\n0\n0.00784\nPNP_Main_283\nwesternized\n\n\n2\n0.45187\n0\n0\n82.864\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0.05697\n0\n0\n0\n0\nPNP_Validation_55\nwesternized\n\n\n3\n1.11902\n0\n0\n69.99087\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n4.07757\n0\n0\n0\n1.99276\nG80275\nwesternized\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n195\n14.26365\n0\n0\n38.59709\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0.19699\n0\n0\n0\n0\nKHG_9\nnon_westernized\n\n\n196\n5.4775\n0\n0\n14.81828\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n3.64655\n0.09964\n0\n0\n0\nA48_01_1FE\nnon_westernized\n\n\n197\n36.77145\n0\n0\n10.13908\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n17.64151\n0\n0\n0\n0\nKHG_1\nnon_westernized\n\n\n198\n1.16426\n0\n0\n57.14031\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0.3058\n0.70467\n0\n0\n0\nTZ_81781\nnon_westernized\n\n\n199\n7.40894\n0\n0\n11.61544\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n56.20177\n0\n0\n0\n0\nA09_01_1FE\nnon_westernized\n\n\n\n\n\n\n\n\nNow, we need transform this (Table 10.13) in the tidy format, with the melt function.\ntidy_phylums = (\n    all_phylums\n    .transpose()\n    .merge(group_info, left_index=True, right_on='sample_id')\n    .melt(id_vars=['sample_id', 'group'], value_name='relative_abundance', var_name='Phylum', ignore_index=True)\n)\nFinally, we only want to keep the mean relative abundance for each phylum. To do so, we will compute the mean relative abundance, for each phylum, for each group (ancient, westernized, and non_westernized).\ntidy_phylums = tidy_phylums.groupby(['group', 'Phylum']).mean().reset_index()\nWe then verify that the sum of the mean relative abundance is still ~100%, as an extra sanity check.\ntidy_phylums.groupby('group')['relative_abundance'].sum()\ngroup\nancient            100.000000\nnon_westernized     99.710255\nwesternized         99.905089\nName: relative_abundance, dtype: float64",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#lets-make-some-plots",
    "href": "taxonomic-profiling.html#lets-make-some-plots",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.12 Let’s make some plots",
    "text": "10.12 Let’s make some plots\nWe first import plotnine\nfrom plotnine import *\nAnd then run plotnine to a barplot of the mean abundance per group (Figure 10.6).\nggplot(tidy_phylums, aes(x='group', y='relative_abundance', fill='Phylum')) \\\n+ geom_bar(position='stack', stat='identity') \\\n+ ylab('mean abundance') \\\n+ xlab(\"\") \\\n+ theme_classic()\n\n\n\n\n\n\nFigure 10.6: Stacked bar chart of ancient, non-westernised, and westernised sample groups on the X axis columns, and mean abundance percentage on the Y-axis. The legend and stacks of the bar represent different phyla each with a different colour\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the most abundant Phylum in all samples\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhether ancient, non westernised, or westernised gut microbiome sample, the phylum Firmicutes (officially renamed Bacillota since 2021) is the most abundant.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#ecological-diversity",
    "href": "taxonomic-profiling.html#ecological-diversity",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.13 Ecological diversity",
    "text": "10.13 Ecological diversity\n\n10.13.1 Alpha diversity\nAlpha diversity is the measure of diversity withing each sample. It is used to estimate how many species are present in a sample, and how diverse they are. We’ll use the python library scikit-bio (http://scikit-bio.org/) to compute it, and the plotnine (https://plotnine.readthedocs.io/) library (a python port of ggplot2 (https://ggplot2.tidyverse.org/reference/ggplot.html) to visualise the results).\nimport skbio\nLet’s compute the species richness, the Shannon index, and Simpson index of diversity (Table 10.14)\nshannon = skbio.diversity.alpha_diversity(\n    metric=\"shannon\", counts=all_species.transpose(), ids=all_species.columns\n)\nsimpson = skbio.diversity.alpha_diversity(\n    metric=\"simpson\", counts=all_species.transpose(), ids=all_species.columns\n)\nrichness = (all_species != 0).astype(int).sum(axis=0)\nalpha_diversity = (\n    shannon.to_frame(name=\"shannon\")\n    .merge(simpson.to_frame(name=\"simpson\"), left_index=True, right_index=True)\n    .merge(richness.to_frame(name=\"richness\"), left_index=True, right_index=True)\n)\nalpha_diversity\n\n\n\nTable 10.14: Table of the shannon, simpson, and richness alpha diversity indicies for a subset of samples\n\n\n\n\n\n\n\n\n\n\n\n\nshannon\nsimpson\nrichness\n\n\n\n\nERR5766177\n3.032945\n0.844769\n21\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n0.798112\n0.251280\n11\n\n\nPNP_Main_283\n5.092878\n0.954159\n118\n\n\nPNP_Validation_55\n3.670162\n0.812438\n72\n\n\nG80275\n3.831358\n0.876712\n66\n\n\n…\n…\n…\n…\n\n\nKHG_9\n3.884285\n0.861683\n87\n\n\nA48_01_1FE\n4.377755\n0.930024\n53\n\n\nKHG_1\n3.733834\n0.875335\n108\n\n\nTZ_81781\n2.881856\n0.719491\n44\n\n\nA09_01_1FE\n2.982322\n0.719962\n75\n\n\n\n\n\n\nLet’s load the group information from the metadata. To do so, we merge the alpha diversity dataframe that we compute previously, with the metadata dataframe, using the sample_id as a merge key. Finally, we do a bit of renaming to re-encode yes/no as non_westernized/westernized.\nalpha_diversity = (\n    alpha_diversity\n    .merge(metadata[['sample_id', 'non_westernized']], left_index=True, right_on='sample_id', how='outer')\n    .set_index('sample_id')\n    .rename(columns={'non_westernized':'group'})\n)\nalpha_diversity['group'] = alpha_diversity['group'].replace({'yes':'non_westernized','no':'westernized', pd.NA:'ERR5766177'})\n\nalpha_diversity\n\n\n\nTable 10.15: Table of the shannon, simpson, and richness alpha diversity indicies for a subset of samples but with the group metadata\n\n\n\n\n\n\n\n\n\n\n\n\n\nshannon\nsimpson\nrichness\ngroup\n\n\n\n\nsample_id\n\n\n\n\n\n\nERR5766177\n3.032945\n0.844769\n21\nERR5766177\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n0.798112\n0.251280\n11\nwesternized\n\n\nPNP_Main_283\n5.092878\n0.954159\n118\nwesternized\n\n\nPNP_Validation_55\n3.670162\n0.812438\n72\nwesternized\n\n\nG80275\n3.831358\n0.876712\n66\nwesternized\n\n\n…\n…\n…\n…\n…\n\n\nKHG_9\n3.884285\n0.861683\n87\nnon_westernized\n\n\nA48_01_1FE\n4.377755\n0.930024\n53\nnon_westernized\n\n\nKHG_1\n3.733834\n0.875335\n108\nnon_westernized\n\n\nTZ_81781\n2.881856\n0.719491\n44\nnon_westernized\n\n\nA09_01_1FE\n2.982322\n0.719962\n75\nnon_westernized\n\n\n\n\n\n\nAnd as always, we need it in tidy format (Table 10.16) for plotnine.\nalpha_diversity = alpha_diversity.melt(id_vars='group', value_name='alpha diversity', var_name='diversity_index', ignore_index=False)\n\nalpha_diversity\n\n\n\nTable 10.16: Table of the shannon, simpson, and richness alpha diversity indicies for a subset of samples but with the group metadata but in long-form tidy format\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\ndiversity_index\nalpha diversity\n\n\n\n\nsample_id\n\n\n\n\n\nERR5766177\nERR5766177\nshannon\n3.032945\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\nwesternized\nshannon\n0.798112\n\n\nPNP_Main_283\nwesternized\nshannon\n5.092878\n\n\nPNP_Validation_55\nwesternized\nshannon\n3.670162\n\n\nG80275\nwesternized\nshannon\n3.831358\n\n\n…\n…\n…\n…\n\n\nKHG_9\nnon_westernized\nrichness\n87.000000\n\n\nA48_01_1FE\nnon_westernized\nrichness\n53.000000\n\n\nKHG_1\nnon_westernized\nrichness\n108.000000\n\n\nTZ_81781\nnon_westernized\nrichness\n44.000000\n\n\nA09_01_1FE\nnon_westernized\nrichness\n75.000000\n\n\n\n\n\n\nWe now make a violin plot to compare the alpha diversity for each group, faceted by the type of alpha diversity index (Figure 10.7).\ng = ggplot(alpha_diversity, aes(x='group', y='alpha diversity', color='group'))\ng += geom_violin()\ng += geom_jitter()\ng += theme_classic()\ng += facet_wrap('~diversity_index', scales = 'free')\ng += theme(axis_text_x=element_text(rotation=45, hjust=1))\ng += scale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\ng += theme(subplots_adjust={'wspace': 0.15})\ng\n\n\n\n\n\n\nFigure 10.7: Three groups of violin plots of an ancient sample, westernised samples and non-westernised samples (x-axis) of the alpha diversity (y-axis) calculated for richness, shannon and simpson alpha indicies\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy do we observe a smaller species richness and diversity in our sample ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are different possibilies: - First, our ancient sample might genuilely have a lower diversity. - Second, our ancient sample diversity might have been lost through the degradation of the sample. - And third, the most likely explanation here: to make this tutorial fast enough, the reads were downsampled by quite a bit, which artifically lowered the diversity of this ancient sample.\n\n\n\n\n\n10.13.2 Beta diversity\nThe Beta diversity is the measure of diversity between a pair of samples. It is used to compare the diversity between samples and see how they relate.\nWe will compute the beta diversity using the bray-curtis dissimilarity\nbeta_diversity = skbio.diversity.beta_diversity(metric='braycurtis', counts=all_species.transpose(), ids=all_species.columns, validate=True)\nWe get a distance matrix\nprint(beta_diversity)\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\n    201x201 distance matrix\n    IDs:\n    'ERR5766177', 'de028ad4-7ae6-11e9-a106-68b59976a384', 'PNP_Main_283', ...\n    Data:\n    [[0.         1.         0.81508134 ... 0.85716612 0.69790092 0.8303726 ]\n     [1.         0.         0.99988327 ... 0.99853413 0.994116   0.99877258]\n     [0.81508134 0.99988327 0.         ... 0.82311942 0.87202543 0.91363156]\n     ...\n     [0.85716612 0.99853413 0.82311942 ... 0.         0.84253376 0.76616679]\n     [0.69790092 0.994116   0.87202543 ... 0.84253376 0.         0.82409272]\n     [0.8303726  0.99877258 0.91363156 ... 0.76616679 0.82409272 0.        ]]\n\n\n\nTo visualise this distance matrix in a lower dimensional space, we’ll use a PCoA, which is is a method very similar to a PCA, but taking a distance matrix as input.\npcoa = skbio.stats.ordination.pcoa(beta_diversity)\n\n\n\n\n\n\nExpand to see command output\n\n\n\n\n\n    /Users/maxime/mambaforge/envs/summer_school_microbiome/lib/python3.9/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:143: RuntimeWarning:\n    The result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues.\n    If the negative ones are smaller, it's probably safe to ignore them, but if they are large in magnitude, the results won't be useful.\n    See the Notes section for more details. The smallest eigenvalue is -0.25334842745723996 and the largest is 10.204440747987945.\n\n\n\npcoa.samples\n\n\n\nTable 10.17: Table of principal coordinates (columns) for each of the samples (rows)\n\n\n\n\n\n\n\n\n...1\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n...12\nPC192\nPC193\nPC194\nPC195\nPC196\nPC197\nPC198\nPC199\nPC200\nPC201\n\n\n\n\nERR5766177\n0.216901\n-0.039778\n0.107412\n0.273272\n0.02054\n0.114876\n-0.256332\n-0.151069\n0.097451\n0.060211\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nde028ad4-7ae6-11e9-a106-68b59976a384\n-0.099355\n0.145224\n-0.191676\n0.127626\n0.119754\n-0.132209\n-0.097382\n0.036728\n0.081294\n-0.056686\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nPNP_Main_283\n-0.214108\n-0.147466\n0.116027\n0.090059\n0.076644\n0.111536\n0.092115\n0.026477\n-0.00646\n-0.018592\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nPNP_Validation_55\n0.244827\n-0.173996\n-0.311197\n-0.012836\n0.031759\n0.117548\n0.148715\n-0.135641\n0.03473\n-0.009395\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nG80275\n-0.261358\n-0.077147\n-0.254374\n-0.065932\n0.088538\n0.16597\n-0.00526\n-0.028739\n-0.002016\n0.015719\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nKHG_9\n0.296057\n-0.1503\n0.013941\n0.032649\n-0.147692\n0.019663\n-0.06312\n-0.034453\n-0.073514\n0.070085\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nA48_01_1FE\n0.110621\n0.030971\n0.154231\n-0.185961\n-0.008512\n-0.10342\n0.028169\n-0.04453\n0.041902\n0.068597\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nKHG_1\n-0.100009\n0.167885\n0.009915\n0.076842\n-0.405582\n-0.039111\n-0.006421\n-0.009774\n-0.072252\n0.15\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nTZ_81781\n0.405716\n-0.139297\n-0.075026\n-0.079716\n-0.053264\n-0.119271\n0.068261\n-0.018821\n0.198152\n-0.012792\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nA09_01_1FE\n0.089101\n0.471135\n0.069629\n-0.125644\n-0.036793\n0.115151\n0.060507\n-0.000912\n-0.027239\n-0.138436\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nLet’s look at the variance explained by the first axes by using a scree plot (Figure 10.8).\nvar_explained = pcoa.proportion_explained[:9].to_frame(name='variance explained').reset_index().rename(columns={'index':'PC'})\n\nggplot(var_explained, aes(x='PC', y='variance explained', group=1)) \\\n+ geom_point() \\\n+ geom_line() \\\n+ theme_classic()\n\n\n\n\n\n\nFigure 10.8: Scree plot describing the variance explained (Y-axis), for each Principal Componanent (X-axis), with a curved line from PC1 having highest variance to lowest on PC9.\n\n\n\nIn this scree plot, we’re looking for the “elbow”, where there is a drop in the slope. Here, it seems that most of the variance is captures by the 3 first principal components\npcoa_embed = pcoa.samples[['PC1','PC2','PC3']].rename_axis('sample').reset_index()\n\npcoa_embed = (\n    pcoa_embed\n    .merge(metadata[['sample_id', 'non_westernized']], left_on='sample', right_on='sample_id', how='outer')\n    .drop('sample_id', axis=1)\n    .rename(columns={'non_westernized':'group'})\n)\n\npcoa_embed['group'] = pcoa_embed['group'].replace({'yes':'non_westernized','no':'westernized', pd.NA:'ERR5766177'})\nLet’s first look at these components with 2D plots (Figure 10.9, Figure 10.10)\nggplot(pcoa_embed, aes(x='PC1', y='PC2', color='group')) \\\n+ geom_point() \\\n+ theme_classic() \\\n+ scale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\n\n\n\n\n\n\nFigure 10.9: Principal Coordinate Analysis plot of PC1 (X-axis) and PC2 (Y-axis), with three groups of points in the scatter plot - blue circles of westernised data points in the bottom left, overlapping with green circles of non-westernised datapoints in the top right, and the single ancient sample as a red circle falling in between the two on the right of the overlap\n\n\n\nggplot(pcoa_embed, aes(x='PC1', y='PC3', color='group')) +\ngeom_point() +\ntheme_classic() +\nscale_color_manual({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\n\n\n\n\n\n\nFigure 10.10: Principal Coordinate Analysis plot of PC1 (X-axis) and PC3 (Y-axis), a similar overlap between westernised/non-westernised individuals and position of the ancient sample as in the PC1-PC2 PCoA, however this time in a horseshoe shape from bottom left for the westernised data points, curving up to the top of PC3 at a peak, and then falling again at the top of PC1\n\n\n\n\n\n\n\n\n\nExpand to see additional visualisations\n\n\n\n\n\nYou can also plot the data above as a with a 3d plot if you were to run the following command\nimport plotly.express as px\n\nfig = px.scatter_3d(pcoa_embed, x=\"PC1\", y=\"PC2\", z=\"PC3\",\n                  color = \"group\",\n                  color_discrete_map={'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'},\n                  hover_name=\"sample\")\nfig.show()\n\n\n\nFinally, we can also visualise this distance matrix using a clustered heatmap, where pairs of sample with a small beta diversity are clustered together (Figure 10.11).\nimport seaborn as sns\nimport scipy.spatial as sp, scipy.cluster.hierarchy as hc\nWe set the color in seaborn to match the color palette we’ve used so far.\npcoa_embed['colour'] = pcoa_embed['group'].map({'ERR5766177':'#DB5F57','westernized':'#5F57DB','non_westernized':'#57DB5E'})\n\nlinkage = hc.linkage(sp.distance.squareform(beta_diversity.to_data_frame()), method='average')\n\nsns.clustermap(\n    beta_diversity.to_data_frame(),\n    row_linkage=linkage,\n    col_linkage=linkage,\n    row_colors = pcoa_embed['colour'].to_list()\n)\n\n\n\n\n\n\nFigure 10.11: Sample-by-sample clustered heatmap, with tree representation of the clustering on the left and top of the heatmap\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBased on the PCoA and heatmap, how you think our ancient sample relates to modern ones ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe sample we looked at in this tutorial is actually coming from the (Maixner et al. 2021) article, looking at a human gut microbiome found in a salt mine in nowadays Austria. This sample is actually for the Baroque period (150 years BP). Reassuringly for our analysis, the title of this article is Hallstatt miners consumed blue cheese and beer during the Iron Age and retained a non-Westernized gut microbiome until the Baroque period.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#optional-clean-up",
    "href": "taxonomic-profiling.html#optional-clean-up",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.14 (Optional) clean-up",
    "text": "10.14 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nWhen closing your jupyter notebook(s), say no to saving any additional files.\nPress ctrl + c on your terminal, and type y when requested. Once completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/taxonomic-profiling directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/taxonomic-profiling*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name taxonomic-profiling --all -y",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#summary",
    "href": "taxonomic-profiling.html#summary",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.15 Summary",
    "text": "10.15 Summary\nIn this practical session we\n\nLooked at how to process the raw sequencing data to focus only on the non-human reads\nProfiled these reads with the Metaphlan3 taxonomic profiler\nLooked at phylum composition, and alpha, and beta diversity with the help of PCoA and abundance heatmaps\n\n\n\n\n\n\n\nCautionary note - sequencing depth\n\n\n\nWe worked on relative abundances provided by metaphlan. But beware that not all taxonomic profilers will output relative abundances. To perform quantitative analysis of taxonomic profiles, sequencing data need to be normalised for sequencing depth. There are many different ways to normalise sequencing data, but this out of the scope of this session. Just to name a few, the most commonly used are RLE, TSS, rarefaction, CLR, or GMPR.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "taxonomic-profiling.html#references",
    "href": "taxonomic-profiling.html#references",
    "title": "10  Taxonomic Profiling, OTU Tables and Visualisation",
    "section": "10.16 References",
    "text": "10.16 References\n\n\n\n\nBreitwieser, Florian P., and Steven L. Salzberg. 2016. “Pavian: Interactive Analysis of Metagenomics Data for Microbiomics and Pathogen Identification.” bioRxiv, October, 084715.\n\n\nBreitwieser, Florian P, and Steven L Salzberg. 2020. “Pavian: Interactive Analysis of Metagenomics Data for Microbiome Studies and Pathogen Identification.” Bioinformatics (Oxford, England) 36 (4): 1303–4. https://doi.org/10.1093/bioinformatics/btz715.\n\n\nChen, Shifu, Yanqing Zhou, Yaru Chen, and Jia Gu. 2018. “Fastp: An Ultra-Fast All-in-One FASTQ Preprocessor.” Bioinformatics 34 (17): i884–90.\n\n\nFellows Yates, James A, T C Lamnidis, M Borry, A A Valtueña, Zandra Fagernäs, S Clayton, M Garcia, Judith Neukamm, and Alexander Peltzer. 2020. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” bioRxiv 9 (June): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nMaixner, Frank, Mohamed S Sarhan, Kun D Huang, Adrian Tett, Alexander Schoenafinger, Stefania Zingale, Aitor Blanco-Mı́guez, et al. 2021. “Hallstatt Miners Consumed Blue Cheese and Beer During the Iron Age and Retained a Non-Westernized Gut Microbiome Until the Baroque Period.” Current Biology 31 (23): 5149–62.\n\n\nOndov, Brian D., Nicholas H. Bergman, and Adam M. Phillippy. 2011a. “Interactive Metagenomic Visualization in a Web Browser.” BMC Bioinformatics 12 (1): 385.\n\n\nOndov, Brian D, Nicholas H Bergman, and Adam M Phillippy. 2011b. “Interactive Metagenomic Visualization in a Web Browser.” BMC Bioinformatics 12 (1): 385. https://doi.org/10.1186/1471-2105-12-385.\n\n\nPasolli, Edoardo, Lucas Schiffer, Paolo Manghi, Audrey Renson, Valerie Obenchain, Duy Tin Truong, Francesco Beghini, et al. 2017. “Accessible, Curated Metagenomic Data Through ExperimentHub.” Nature Methods 14 (11): 1023–24.\n\n\nReback, Jeff, jbrockmendel, Wes McKinney, Joris Van den Bossche, Tom Augspurger, Matthew Roeschke, Simon Hawkins, et al. 2022. “Pandas-Dev/Pandas: Pandas 1.4.2.” Zenodo.\n\n\nscikit-bio, developers. 2022. “Scikit-Bio: A Bioinformatics Library for Data Scientists, Students, and Developers.”\n\n\nSegata, Nicola, Levi Waldron, Annalisa Ballarini, Vagheesh Narasimhan, Olivier Jousson, and Curtis Huttenhower. 2012. “Metagenomic Microbial Community Profiling Using Unique Clade-Specific Marker Genes.” Nature Methods 9 (8): 811–14.\n\n\nSharpton, Thomas J. 2014. “An Introduction to the Analysis of Shotgun Metagenomic Data.” Frontiers in Plant Science 5 (June).",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling, OTU Tables and Visualisation</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html",
    "href": "denovo-assembly.html",
    "title": "11  De novo Genome Assembly",
    "section": "",
    "text": "11.1 Introduction\nFirst of all, what is a metagenomic sample? Metagenomic sample is a sample that consists of DNA from more than one source. The number and the type of sources might vary widely between different samples. Typical sources for ancient remains are e.g. the host organism and the microbial species. The important part is that we generally do not know the origin of a DNA molecule prior to analysing the sequencing data generated from the DNA library of this sample. In the example presented in (Figure 11.1), our metagenomic sample has DNA from three different sources, here coloured in blue, red, and yellow.\nHow can we determine the sources of the DNA that we have in our metagenomic sample? There are three main options whose pros and cons are summarised in (Table 11.1).\nTable 11.1: Pros and cons of the major methods for determining the sources of metagenomic DNA.\n\n\n\n\n\n\n\n\n\n\nmethod\npros\ncons\n\n\n\n\nreference-based alignment\nhighly sensitive, applicable to aDNA samples\nrequires all sources to be represented in database\n\n\nsingle-genome assembly\nhigh-quality genomes from cultivated bacteria\nnot available for ancient DNA samples\n\n\nmetagenome assembly\nable to recover unknown diversity present in sample\nhighly dependent on preservation of ancient DNA\nUntil recently, the only option for ancient DNA samples was to take the short-read sequencing data and align them against some known reference genomes. However, this approach is heavily relying on whether all sources of our samples are represented in the available reference genomes. If a source is missing in the reference database - in our toy example, this is the case for the yellow source (Figure 11.1) -, then we won’t be able to detect it using this reference database.\nWhile there is a potential workaround for modern metagenomic samples, single-genome assembly relies on being able to cultivate a microbial species to obtain an isolate. This is unfeasible for ancient metagenomic samples because there are no more viable microbial cells available that could be cultivated.\nAround 2015, a technical revolution started when the first programs, e.g. MEGAHIT (Li et al. 2015) and metaSPAdes (Nurk et al. 2017), were published that could successfully perform de novo assembly from metagenomic data. Since then, tens of thousands metagenomic samples have been assembled and it was revealed that even well studied environments, such as the human gut microbiome, have a lot of additional microbial diversity that has not been observed previously via culturing and isolation (Almeida et al. 2021).\nThe technical advancement of being able to perform de novo assembly on metagenomic samples led to an explosion of studies that analysed samples that were considered almost impossible to study beforehand. For researchers that are exposed to ancient DNA, the imminent question arises: can we apply the same methods to ancient DNA data? In this practical course, we will walk through all required steps that are necessary to successfully perform de novo assembly from ancient DNA metagenomic sequencing data and show you what you can do once you have obtained the data.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#introduction",
    "href": "denovo-assembly.html#introduction",
    "title": "11  De novo Genome Assembly",
    "section": "",
    "text": "Figure 11.1: Overview of the ways how to analyse metagenomic sequencing data.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#sample-overview",
    "href": "denovo-assembly.html#sample-overview",
    "title": "11  De novo Genome Assembly",
    "section": "11.2 Sample overview",
    "text": "11.2 Sample overview\nFor this practical course, I selected a palaeofaeces sample from the study by Maixner et al. (2021), who generated deep metagenomic sequencing data for four palaeofaeces samples that were excavated from an Austrian salt mine in Hallstatt and were associated with the Celtic Iron Age. We will focus on the youngest sample, 2612, which was dated to be just a few hundred years old (Figure 11.2).\n\n\n\n\n\n\nFigure 11.2: The graphical abstract of Maixner et al. (2021).\n\n\n\nHowever, because the sample was very deeply sequenced for more than 250 million paired-end reads and we do not want to wait for days for the analysis to finish, we will not use all data but just a sub-sample of them.\nYou can access the sub-sampled data by making a symbolic link of the chapter pre-made data into the main chapter folder.\nln -s /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files/2612_R1.fastq.gz 2612_R1.fastq.gz\nln -s /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files/2612_R2.fastq.gz 2612_R2.fastq.gz\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are in each FastQ file?\nHint: You can run either bioawk.\nbioawk -c fastx 'END{print NR}' &lt;FastQ file&gt;\nor (if installed) you can use seqtk.\nseqtk size &lt;FastQ file&gt;\nto find this out.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are about 3.25 million paired-end sequences in these files.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#preparing-the-sequencing-data-for-de-novo-assembly",
    "href": "denovo-assembly.html#preparing-the-sequencing-data-for-de-novo-assembly",
    "title": "11  De novo Genome Assembly",
    "section": "11.3 Preparing the sequencing data for de novo assembly",
    "text": "11.3 Preparing the sequencing data for de novo assembly\nBefore running the actual assembly, we need to pre-process our sequencing data. Typical pre-processing steps include the trimming of adapter sequences and barcodes from the sequencing data and the removal of host or contaminant sequences, such as the bacteriophage PhiX, which is commonly sequenced as a quality control.\nMany assembly pipelines, such as nf-core/mag, have these steps automatically included, however, these steps can be performed prior to it, too. For this practical course, I have performed these steps for us and we could directly continue with the de novo assembly.\n\n\n\n\n\n\nImportant\n\n\n\nThe characteristic of ancient DNA samples that pre-determines the success of the de novo assembly is the distribution of the DNA molecule length. Determine this distribution prior to running the de novo assembly to be able to predict the results of the de novo assembly.\n\n\nHowever, since the average length of the insert size of sequencing data (Figure 11.3) is highly correlated with the success of the assembly, we want to first evaluate it. For this we can make use of the program fastp (Chen et al. 2018).\n\n\n\n\n\n\nFigure 11.3: Scheme of a read pair of Illumina sequencing data.\n\n\n\nFastp will try to overlap the two mates of a read pair and, if this is possible, return the length of the merged sequence, which is identical to insert size or the DNA molecule length. If the two mates cannot be overlapped, we will not be able to know the exact DNA molecule length but only know that it is longer than 290 bp (each read has a length of 150 bp and FastP requires a 11 bp overlap between the reads).\nThe final histogram of the insert sizes that is returned by FastP can tell us how well preserved the DNA of an ancient sample is (Figure 11.4). The more the distribution is skewed to the right, i.e. the longer the DNA molecules are, the more likely we are to obtain long contiguous DNA sequences from the de novo assembly. A distribution that is skewed to the left means that the DNA molecules are more highly degraded and this lowers our chances for obtaining long continuous sequences.\n\n\n\n\n\n\nFigure 11.4: Example of a DNA molecule length distribution of a well-preserved ancient DNA sample. This histogram belongs to a 700,000-year-old horse that was preserved in permafrost, as reported in Orlando et al. (2013), Fig. S4.\n\n\n\nTo infer the distribution of the DNA molecules, we can run the command\nfastp --in1 2612_R1.fastq.gz \\\n      --in2 2612_R2.fastq.gz \\\n      --stdout --merge -A -G -Q -L --json /dev/null --html overlaps.html \\\n&gt; /dev/null\n\n\n\n\n\n\nQuestion\n\n\n\nInfer the distribution of the DNA molecule length of the sequencing data. Is this sample well-preserved?\nHint: You can easily inspect the distribution by opening the HTML report overlaps.html in your web browser.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHere is the histogram of the insert sizes determined by fastp (Figure 11.5). By default, fastp will only keep reads that are longer than 30 bp and requires an overlap between the read mates of 30 bp. The maximum read length is 150 bp, therefore, the histogram only spreads from 31 to 271 bp in total.\n\n\n\n\n\n\nFigure 11.5: Histogram of the insert sizes determined by fastp\n\n\n\nThe sequencing data for the sample 2612 were generated across eight different sequencing runs, which differed in their nominal length. Some sequencing runs were 2x 100 bp, while others were 2x 150 bp. This is the reason why we observe two peaks just short of 100 and 150 bp. The difference to the nominal length is caused by the quality trimming of the data.\nOverall, we have almost no short DNA molecules (&lt; 50 bp) but most DNA molecules are longer than 80 bp. Additionally, there were &gt; 200,000 read pairs that could not be overlapped. Therefore, we can conclude that the sample 2612 is moderately degraded ancient DNA sample and has many long DNA molecules.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#de-novo-assembly",
    "href": "denovo-assembly.html#de-novo-assembly",
    "title": "11  De novo Genome Assembly",
    "section": "11.4 De novo assembly",
    "text": "11.4 De novo assembly\nNow, we will actual perform the de novo assembly on the sequencing data. For this, we will use the program MEGAHIT (Li et al. 2015), a de Bruijn-graph assembler.\n\n\n\n\n\n\nFigure 11.6: Overview of inferring k-mers from a DNA sequence. Credit: https://medium.com/swlh/bioinformatics-1-k-mer-counting-8c1283a07e29\n\n\n\nDe Bruijn graph assemblers are together with overlap assemblers the predominant group of assemblers. They use k-mers (see Figure 11.6 for an example of 4-mers) extracted from the short-read sequencing data to build the graph. For this, they connect each k-mer to adjacent k-mer using a directional edge. By walking along the edges and visiting each k-mer or node once, longer continuous sequences are reconstructed. This is a very rough explanation and I would advise you to watch this excerpt of a lecture by Rob Edwards from San Diego State University and a Coursera lecture by Ben Langmead from Johns Hopkins University, if you would like to learn more about it.\nAll de Bruijn graph assemblers work in a similar way so the question is why do we use MEGAHIT and not other programs, such as metaSPAdes?\n\n\n\n\n\n\nPros and cons of MEGAHIT\n\n\n\nPros:\n\nlow-memory footprint: can be run on computational infrastructure that does not have large memory resources\ncan assembly both single-end and paired-end sequencing data\nthe assembly algorithm can cope with the presence of high amounts of ancient DNA damage\n\nCons:\n\nlower assembly quality on modern comparative data, particularly a higher rate of mis-assemblies (CAMI II challenge)\n\n\n\nIn the Warinner group, we realised after some tests that MEGAHIT has a clear advantage when ancient DNA damage is present at higher quantities. While it produced a higher number of mis-assemblies compared to metaSPAdes when being evaluated on simulated modern metagenomic data (Critical Assessment of Metagenome Interpretation II challenge, Meyer et al. 2022), it produces more long contigs when ancient DNA damage is present.\nTo de novo assemble the short-read sequencing data of the sample 2612 using MEGAHIT, we can run the command\nmegahit -1 2612_R1.fastq.gz \\\n        -2 2612_R2.fastq.gz \\\n        -t 14 --min-contig-len 500 \\\n        --out-dir megahit\nThis will use the paired-end sequencing data as input and return all contigs that are at least 500 bp long.\n\n\n\n\n\n\nCaution\n\n\n\nWhile MEGAHIT is able to use merged sequencing data, it is advised to use the unmerged paired-end data as input. In tests using simulated data I have observed that MEGAHIT performed slightly better when using the unmerged data and it likely has something to do with its internal algorithm to infer insert sizes from the paired-end data.\n\n\nWhile we are waiting for MEGAHIT to finish, here is a question:\n\n\n\n\n\n\nQuestion\n\n\n\nWhich k-mer lengths did MEGAHIT select for the de novo assembly?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBased on the maximum read length, MEGAHIT decided to use the k-mer lengths of 21, 29, 39, 59, 79, 99, 119, and 141.\n\n\n\nNow, as MEGAHIT has finished, we want to evaluate the quality of the assembly results. MEGAHIT has written the contiguous sequences (contigs) into a single FastA file stored in the folder megahit.\nWe will process this FastA file with a small script from Heng Li (creator of the famous bwa aligner among others), calN50, which will count the number of contigs and give us an overview of their length distribution.\nThe script is already present in the chapter’s directory.\nk8 ./calN50.js megahit/final.contigs.fa\n\n\n\n\n\n\nQuestion\n\n\n\nHow many contigs were assembled? What is the sum of the lengths of all contigs? What is the maximum, the median, and the minimum contig length?\nHint: The maximum contig length is indicated by the label “N0”, the median by the label “N50”, and the minimum by the label “N100”?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMEGAHIT assembled 3,606 contigs and their lengths sum up to 11.4 Mb. The maximum contig length was 448 kb, the median length was 15.6 kb, and the minimum length was 500 bp.\n\n\n\nThere is a final caveat when assembling ancient metagenomic data with MEGAHIT: while it is able to assemble sequencing data with a high percentage of C-to-T substitutions, it tends to introduce these changes into the contig sequences, too.\n\n\n\n\n\n\nFigure 11.7: De Bruijn graph with a bubble caused by a second allele. Adapted from (Leggett et al. 2013, fig. 1a)\n\n\n\nThese C-to-T substitutions are similar to biological single-nucleotide polymorphisms in the sequencing data. Both lead the introduction of bubbles in the de Bruijn graph when two alleles are present in the k-mer sequences (Figure 11.7) and the assembler decides during its pruning steps which allele to keep in the contig sequence.\nWhile it does not really matter which allele is kept for biological polymorphisms, it does matter for technical artefacts that are introduced by the presence of ancient DNA damage. In our group we realised that the gene sequences that were annotated on the contigs of MEGAHIT tended to have a higher number of nonsense mutations compared to the known variants of the genes. After manual inspection, we observed that many of these mutations appeared because MEGAHIT chose the damage-derived T allele over the C allele or the damage-derived A allele over a G allele (see Klapper et al. 2023, fig. S1).\nTo overcome this issue, my colleagues Maxime Borry, James Fellows Yates and I developed a strategy to replace these damage-derived alleles in the contig sequences. This approach consists of aligning the short-read data against the contigs, performing genotyping along them, and finally replacing all alleles for which we have strong support for an allele that is different from the one selected by MEGAHIT.\nWe standardised this approach and added it to the Nextflow pipeline nf-core/mag (Krakau et al. 2022). It can simply be activated by providing the parameter --ancient_dna.\n\n\n\n\n\n\nCaution\n\n\n\nWhile MEGAHIT is able to assemble ancient metagenomic sequencing data with high amounts of ancient DNA damage, it tends to introduce damage-derived T and A alleles into the contig sequences instead of the true C and G alleles. This can lead to a higher number of nonsense mutations in coding sequences. We strongly advise you to correct such mutations, e.g. by using the ancient DNA workflow of the Nextflow pipeline nf-core/mag.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#aligning-the-short-read-data-against-the-contigs",
    "href": "denovo-assembly.html#aligning-the-short-read-data-against-the-contigs",
    "title": "11  De novo Genome Assembly",
    "section": "11.5 Aligning the short-read data against the contigs",
    "text": "11.5 Aligning the short-read data against the contigs\nAfter the assembly, the next detrimental step that is required for many subsequent analyses is the alignment of the short-read sequencing data back to assembled contigs.\nAnalyses that require these alignment information are for example:\n\nthe correction of the contig sequences to remove damage-derived alleles\nthe non-reference binning of contigs into MAGs for inferring the coverage along the contigs\nthe quantification of the presence of ancient DNA damage\n\nAligning the short-read data to the contigs requires multiple steps:\n\nBuild an index from the contigs for the alignment program BowTie2\nAlign the short-read data against the contigs using this index with BowTie2\nCalculate the mismatches and deletions of the short-read data against the contig sequences\nSort the aligned reads by the contig name and the coordinate they were aligned to\nIndex the resulting alignment file for faster access\n\nHowever, these steps are rather time-consuming, even when we just have so little sequencing data as we do for our course example. The alignment is rather slow because we allow a single mismatch in the seeds that are used by the aligner BowTie2 to quickly determine the position of a read along the contig sequences (parameter -N 1). This is necessary because otherwise we might not be able to align reads with ancient DNA damage present on them. Secondly, the larger the resulting alignment file is the longer it takes to sort it by coordinate.\nTo save us some time and continue with the more interesting analyses, I prepared the resulting files for us. For this, I also corrected damage-derived alleles in the contig sequences. You can access these files on the cluster by running the following commands:\nmkdir alignment\nln -s /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files/2612.sorted.calmd.bam alignment/\nln -s /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files/2612.sorted.calmd.bam.bai alignment/\nln -s /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files/2612.fa alignment/\nIf we wanted to generate the alignments ourselves, we can check the commands in the box below.\n\n\n\n\n\n\nManual alignment with Bowtie2\n\n\n\n\n\nTo execute the alignment step ourselves we could theoretically run the following commands:\nmkdir alignment\nbowtie2-build -f megahit/final.contigs.fa alignment/2612\nbowtie2 -p 14 --very-sensitive -N 1 -x alignment/2612 -1 2612_R1.fastq.gz -2 2612_R2.fastq.gz | \\\nsamtools view -Sb - | \\\nsamtools calmd -u /dev/stdin megahit/final.contigs.fa | \\\nsamtools sort -o alignment/2612.sorted.calmd.bam -\nsamtools index alignment/2612.sorted.calmd.bam",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#reconstructing-metagenome-assembled-genomes",
    "href": "denovo-assembly.html#reconstructing-metagenome-assembled-genomes",
    "title": "11  De novo Genome Assembly",
    "section": "11.6 Reconstructing metagenome-assembled genomes",
    "text": "11.6 Reconstructing metagenome-assembled genomes\nThere are typically two major approaches on how to study biological diversity of samples using the results obtained from the de novo assembly. The first one is to reconstruct metagenome-assembled genomes (MAGs) and to study the species diversity.\n“Metagenome-assembled genome” is a convoluted term that means that we reconstructed a genome from metagenomic data via de novo assembly. While these reconstructed genomes, particularly the high-quality ones, are most likely a good representation of the genomes of the organisms present in the sample, the scientific community refrains from calling them a species or a strain. The reason is that for calling a genome a species or a strain additional analyses would be necessary, of which many would include the cultivation of the organism. For many samples, this is not feasible and therefore the community stuck to the term MAG instead.\nThe most commonly applied method to obtain MAGs is the so-called “non-reference binning”. Non-reference binning means that we do not try to identify contigs by aligning them against known reference genomes, but only use the characteristics of the contigs themselves to cluster them (Figure 11.8).\n\n\n\n\n\n\nFigure 11.8: Scheme of binning de novo assembled contigs into metagenome-assembled genomes. During the binning contigs are grouped into clusters based on their characteristics, such as tetra-nucleotide frequency and the coverage along the contigs. Clusters of contigs that fulfil a minimum of quality criteria are then considered as metagenome-assembled genomes. However, depending on the sample, a number of contigs will remain unbinned.\n\n\n\nThe two most commonly used characteristics are:\n\nthe tetra-nucleotide frequency: the frequency of all 4-mers (e.g. AAAA, AAAC, AAAG etc.) in the contig sequence\nthe coverage along the contig\n\nThe idea here is that two contigs that are derived from the same bacterial genome will likely have a similar nucleotide composition and coverage.\nThis approach works very well when the contigs are longer and they strongly differ in the nucleotide composition and the coverage from each other. However, if this is not the case, e.g. there is more than one strain of the same species in the sample, these approaches will likely not be able to assign some contigs to clusters: these contigs remain unbinned. In case there is a high number of unbinned contigs, one can also employ the more sensitive reference-based binning strategies but we will not cover this in this practical course.\nThere are a number of different binning tools out there that can be used for this task. Since this number is constantly growing, there have been attempts to standardise the test data sets that these tools are run on so that their performances can be easily compared. The most well-known attempt is the Critical Assessment of Metagenome Interpretation (Critical Assessment of Metagenome Interpretation II challenge, Meyer et al. 2022), which released the latest comparison of commonly used tools for assembly, binning, and bin refinement in 2022. I recommend that you first check the performance of a new tool against the CAMI datasets before testing it to see whether it is worth using it.\nThree commonly used binners are:\n\nmetaBAT2 (Kang et al. 2019, more than 1,300 citations)\nMaxBin2 (Wu, Simmons, and Singer 2016, more than 1,200 citations)\nCONCOCT (Alneberg et al. 2014, more than 1,000 citation)\n\nEach of these three binners employs a slightly different strategy. While metaBAT2 simply uses the two previously mentioned metrics, the tetra-nucleotide frequency and the coverage along the contigs, MaxBin2 additionally uses single-copy marker genes to infer the taxonomic origin of contigs. In contrast, CONCOCT also just uses the two aforementioned metrics but first performs a Principal Component Analysis (PCA) on these metrics and uses the PCA results for the clustering.\nThe easiest way to run all these three programs is the program metaWRAP (Uritskiy, DiRuggiero, and Taylor 2018). metaWRAP is in fact a pipeline that allows you to assemble your contigs, bin them, and subsequently refine the resulting MAGs. However, the pipeline is not very well written and does not contain any strategies to deal with ancient metagenomic sequencing data. Therefore, I prefer to use different pipelines, such as nf-core/mag for the assembly, and only use metaWRAP for binning and bin refinement.\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you have followed the instructions for setting up the additional software requirements for this chapter already! See the beginning of the chapter for more information.\n\n\nTo skip the first steps of metaWRAP and start straight with the binning, we need to create the folder structure and files that metaWRAP expects:\nmkdir -p metawrap/INITIAL_BINNING/2612/work_files\nln -s $PWD/alignment/2612.sorted.calmd.bam \\\n    metawrap/INITIAL_BINNING/2612/work_files/2612.bam\nmkdir -p metawrap/faux_reads\necho \"@\" &gt; metawrap/faux_reads/2612_1.fastq\necho \"@\" &gt; metawrap/faux_reads/2612_2.fastq\nNow, we can start to run the binning. In this practical course, we will focus on metaBAT2 and MaxBin2. To bin the contigs with these binners, we execute:\nconda activate metawrap-env\nmetawrap binning -o metawrap/INITIAL_BINNING/2612 \\\n    -t 14 \\\n    -a alignment/2612.fa \\\n    --metabat2 --maxbin2 --universal \\\n    metawrap/faux_reads/2612_1.fastq metawrap/faux_reads/2612_2.fastq\n\n\n\n\n\n\nCaution\n\n\n\nMetaWRAP is not a well-written Python software and has not been update for more than three years. It still relies on the deprecated Python v2.7. This is in conflict with many other tools and therefore it requires its own conda environment, metawrap-env. Do not forget to deactivate this environment afterwards again!\n\n\nMetaWRAP will run metaBAT2 and MaxBin2 for us and store their respective output into sub-folders in the folder metawrap/INITIAL_BINNING/2612.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins did metaBAT2 and MaxBin2 reconstruct, respectively? Is there a difference in the genome sizes of these reconstructed bins?\nHint: You can use the previously introduced script k8 ./calN50.js to analyse the genome size of the individual bins.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmetaBAT2 reconstructed seven bins, while MaxBin2 reconstructed only five bins.\nWhen comparing the genome sizes of these bins, we can see that despite having reconstructed fewer bins, MaxBin2’s bins have on average larger genome size and all of them are at least 1.5 Mb. In contrast, five out of seven metaBAT2 bins are shorter than 1.5 Mb.\n\n\n\nWhile we could have run these two binning softwares manually ourselves, there is another reason why we should use metaWRAP: it has a powerful bin refinement algorithm.\nAs we just observed, binning tools come to different results when performing non-reference binning on the same contigs. So how do we know which binning tool delivered the better or even correct results?\nA standard approach is to identify single-copy marker genes that are specific for certain taxonomic lineages, e.g. to all members of the family Prevotellaceae or to all members of the kingdom archaea. If we find lineage-specific marker genes from more than one lineage in our bin, something likely went wrong. While in certain cases horizontal gene transfer could explain such a finding, it is much more common that a binning tool clustered two contigs from two different taxons.\nDuring its bin refinement, metaWRAP first combines the results of all binning tools in all combinations. So it would merge the results of metaBAT2 and MaxBin2, metaBAT2 and CONCOCT, MaxBin2 and CONCOCT, and all three together. Afterwards, it evaluates the presence of lineage-specific marker genes on the contigs of the bins for every combination and the individual binning tools themselves. In the case that it would find marker genes of more than one lineage in a bin, it would split the bin into two. After having evaluated everything, metaWRAP selects the refined bins that have the highest quality score across the whole set of bins.\n\n\n\n\n\n\nFigure 11.9: Performance of metaWRAP’s bin refinement algorithm compared to other tools. Adapted from Uritskiy, DiRuggiero, and Taylor (2018), Fig. 4\n\n\n\nUsing this approach, the authors of metaWRAP could show that they can outperform the individual binning tools and other bin refinement algorithms both regarding the completeness and contamination that was estimated for the MAGs (Figure 11.9).\nDue to the large memory requirements of the the bin refinment module, we have prepared the results of metaWRAP’s bin refinement algorithm, metawrap_50_10_bins.stats for you already, which can be found in the folder /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files.\n\n\n\n\n\n\nCaution\n\n\n\nRunning metaWRAP’s bin refinement module requires about 72 GB of memory because it has to load a larger reference database containing the lineage-specific marker genes of checkM.\nIf you have sufficient computational memory resources, you can run the following steps to run the bin refinement yourself.\n\n\n\n\n\n\nExample commands - do not run!\n\n\n\n\n\nTo apply metaWRAP’s bin refinement to the bins that we obtained from metaBAT2 and MaxBin2, we first need to install the software checkM (Parks et al. 2015) that will provide the lineage-specific marker gene catalogue:\n## Only run if you have at least 72 GB of memory!\nmkdir checkM\nwget -O checkM/checkm_data_2015_01_16.tar.gz \\\n    https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\ntar xvf checkM/checkm_data_2015_01_16.tar.gz -C checkM\n\necho checkM | checkm data setRoot checkM\nAfterwards, we can execute metaWRAP’s bin refinement module:\n## Only run if you have at least 72 GB of memory!\nmkdir -p metawrap/BIN_REFINEMENT/2612\nmetawrap bin_refinement -o metawrap/BIN_REFINEMENT/2612 \\\n    -t 14 \\\n    -c 50 \\\n    -x 10 \\\n    -A metawrap/INITIAL_BINNING/2612/maxbin2_bins \\\n    -B metawrap/INITIAL_BINNING/2612/metabat2_bins\nOnce finished you can deactivate the metawrap conda environment.\nconda deactivate\n\n\n\nThe latter step will produce a summary file, metawrap_50_10_bins.stats, that lists all retained bins and some key characteristics, such as the genome size, the completeness estimate, and the contamination estimate. The latter two can be used to assign a quality score according to the Minimum Information for MAG (MIMAG; see info box).\n\n\n\n\n\n\n\n\nThe Minimum Information for MAG (MIMAG)\n\n\n\nThe two most common metrics to evaluate the quality of MAGs are:\n\nthe completeness: how many of the expected lineage-specific single-copy marker genes were present in the MAG?\nthe contamination: how many of the expected lineage-specific single-copy marker genes were present more than once in the MAG?\n\nThese metric is usually calculated using the marker-gene catalogue of checkM (Parks et al. 2015), also if there are other estimates from other tools such as BUSCO (Manni et al. 2021), GUNC (Orakov et al. 2021) or checkM2 (Chklovski et al. 2022).\nDepending on the estimates on completeness and contamination plus the presence of RNA genes, MAGs are assigned to the quality category following the Minimum Information for MAG criteria (Bowers et al. 2017) You can find the overview here.\n\n\nAs these two steps will run rather long and need a large amount of memory and disk space, I have provided the results of metaWRAP’s bin refinement. You can find the file here: /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/metawrap_50_10_bins.stats. Be aware that these results are based on the bin refinement of the results of three binning tools and include CONCOCT.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many bins were retained after the refinement with metaWRAP? How many high-quality and medium-quality MAGs did the refinement yield following the MIMAG criteria?\nHint: You can more easily visualise tables on the terminal using the Python program visidata. You can open a table using vd -f tsv /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/metawrap_50_10_bins.stats. (press ctrl+q to exit). Next to separating the columns nicely, it allows you to perform a lot of operations like sorting conveniently. Check the cheat sheet here.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn total, metaWRAP retained five bins, similarly to MaxBin2. Of these five bins, the bins bin.3 and bin.4 had completeness and contamination estimates that would qualify them for being high-quality MAGs. However, we would need to check the presence of rRNA and tRNA genes. The other three bins are medium-quality bins because their completeness estimate was &lt; 90%.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#taxonomic-assignment-of-contigs",
    "href": "denovo-assembly.html#taxonomic-assignment-of-contigs",
    "title": "11  De novo Genome Assembly",
    "section": "11.7 Taxonomic assignment of contigs",
    "text": "11.7 Taxonomic assignment of contigs\nWhat should we do when we simply want to know to which taxon a certain contig most likely belongs to?\nReconstructing metagenome-assembled genomes requires multiple steps and might not even provide the answer in the case that the contig of interest is not binned into a MAG. Instead, it is sufficient to perform a sequence search against a reference database.\nThere are plenty of tools available for this task, such as:\n\nBLAST/DIAMOND\nKraken2\nCentrifuge\nMMSeqs2\n\nFor each tool, we can either use pre-computed reference databases or compute our own one. The two taxonomic classification systems that are most commonly used are:\n\nNCBI Taxonomy\nGTDB\n\nAs for any task that involves the alignment of sequences against a reference database, the chosen reference database should fit the sequences you are searching for. If your reference database does not capture the diversity of your samples, you will not be able to assign a subset of the contigs. There is also a trade-off between a large reference database that contains all sequences and its memory requirement. Wright, Comeau, and Langille (2023) elaborated on this quite extensively when comparing Kraken2 against MetaPhlAn.\nWhile all of these tools can do the job, I typically prefer to use the program MMSeqs2 (Steinegger and Söding 2017) because it comes along with a very fast algorithm based on amino acid sequence alignment and implements a lowest common ancestor (LCA) algorithm (Figure 11.10). Recently, they implemented a taxonomy workflow (Mirdita et al. 2021) that allows to efficiently assign contigs to taxons. Luckily, it comes with multiple pre-computed reference databases, such as the GTDB v207 reference database (Parks et al. 2020), and therefore it is even more accessible for users.\n\n\n\n\n\n\nFigure 11.10: Scheme of the taxonomy workflow implemented into MMSeqs2. Adapted from Mirdita et al. (2021), Fig. 1.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe latest version of the pre-computed GTDB reference database (r207) requires about 105 GB of harddisk space and 700 GB of memory for running.\nTherefore for those on smaller computational infrastructure, I have pre-computed the results. You can find the results 2612.mmseqs2_gtdb.tsv in the folder /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files.\nAn alternative for users with a less powerful infrastructure is the program KrakenUniq, however if your infrastructure has sufficient computational resources, or you want to see how you would run mmseqs2 see the collapsed block.\n\n\n\n\n\n\nExample commands - do not run!\n\n\n\n\n\nBefore running MMSeqs2’s taxonomy workflow against the GTDB reference database, we need to install it.\n## Only run if you have at least 105 GB of disk space!\nmkdir -p refdbs/mmseqs2/gtdb\nmmseqs databases GTDB \\\n    refdbs/mmseqs2/gtdb /tmp --threads 14\nSubsequently, we can align all the contigs of the sample 2612 against the GTDB r207 with MMSeqs2:\n## Only run if you have at least 700 GB of memory!\nmkdir mmseqs2\nmmseqs createdb alignment/2612.fa mmseqs2/2612.contigs\nmmseqs taxonomy  mmseqs2/2612.contigs \\\n    refdbs/mmseqs2/gtdb/mmseqs2_gtdb \\\n    mmseqs2/2612.mmseqs2_gtdb \\\n    /tmp \\\n    -a --tax-lineage 1 \\\n    --lca-ranks kingdom,phylum,class,order,family,genus,species \\\n    --orf-filter 1 \\\n    --remove-tmp-files 1 \\\n    --threads 14\nmmseqs createtsv mmseqs2/2612.contigs \\\n    mmseqs2/2612.mmseqs2_gtdb \\\n\n\n\n\n\nLets inspect the file 2612.mmseqs2_gtdb.tsv in the folder /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the proportion of contigs that could be assigned to the different taxonomic ranks, such as species or genus? What are the dominant taxa?\nHint: You can access this information easily by opening the file using visidata: vd seqdata/2612.mmseqs2_gtdb.tsv (press ctrl+q to exit)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the 3,606 contigs, MMSeqs2’s taxonomy workflow assigned 3,523 contigs to any taxonomy. For the rest, there was not enough information and they were discarded.\nFrom the 3,523 assigned contigs, 2,013 were assigned to the rank “species”, while 1,137 could only be assigned to the rank “genus”.\nThe most contigs were assigned the archael species Halococcus morrhuae (n=386), followed by the bacterial species Olsenella E sp003150175 (n=298) and Collinsella sp900768795 (n=186).",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#taxonomic-assignment-of-mags",
    "href": "denovo-assembly.html#taxonomic-assignment-of-mags",
    "title": "11  De novo Genome Assembly",
    "section": "11.8 Taxonomic assignment of MAGs",
    "text": "11.8 Taxonomic assignment of MAGs\nMMSeqs2’s taxonomy workflow is very useful to classify all contigs taxonomically. However, how would we determine which species we reconstructed by binning our contigs?\nThe simplest approach would be that we could summarise MMSeqs2’s taxonomic assignments of all contigs of a MAG and then determine which lineage is the most frequent one. Although this would work in general, there is another approach that is more sophisticated: GTDB toolkit (GTDBTK, Chaumeil et al. 2020).\nGTDBTK performs three steps to assign a MAG to a taxonomic lineage:\n\nLineage identification based on single-copy marker genes using Hidden Markov Models (HMMs)\nMulti-sequence alignment of the identified marker genes\nPlacement of the MAG genome into a fixed reference tree at class level\n\nThe last step is particularly clever. Based on the known diversity of a lineage present in the GTDB, it will construct a reference tree with all known taxa of this lineage. Afterwards, the tree structure is fixed and an algorithm attempts to create a new branch in the tree for placing the unknown MAG based on both the tree structure and the multi-sequence alignment.\nIn most cases, both the simple approach of taking the majority of the MMSeqs2’s taxonomy results and the GTDBTK approach lead to very similar results. However, GTDBTK performs better when determining whether a new MAG potentially belongs to a new genus or even a new family.\nTo infer which taxa our five reconstructed MAGs represent, we can run the GTDBTK.\n\n\n\n\n\n\nCaution\n\n\n\nThe latest version of the database used by GTDBTK (r207) requires about 70 GB of harddisk space and 80 GB of memory for running.\nTherefore for those on smaller computational infrastructure, I have pre-computed the results. You can find the results 2612.gtdbtk_archaea.tsv and 2612.gtdbtk_bacteria.tsv in the folder /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files.\nIf you want to see how you would run the gtdbtk classify_wf see the collapsed block.\n\n\n\n\n\n\nExample commands - do not run!\n\n\n\n\n\nFirst, we need to install the GTDB database:\n## Only run if you have at least 70 GB disk space!\nmkdir -p refdbs/gtdbtk\nwget -O refdbs/gtdbtk/gtdbtk_v2_data.tar.gz \\\n    https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_v2_data.tar.gz\ntar xvf refdbs/gtdbtk/gtdbtk_v2_data.tar.gz -C refdbs/gtdbtk\nAfterwards, we can run GTDBTK’s classify workflow:\n## Only run if you have at least 80 GB of memory!\nmkdir gtdbtk\nGTDBTK_DATA_PATH=\"$PWD/refdbs/gtdbtk/gtdbtk_r207_v2\" \\\ngtdbtk classify_wf --cpu 14 --extension fa \\\n    --genome_dir metawrap/BIN_REFINEMENT/2612/metawrap_50_10_bins \\\n    --out_dir gtdbtk\n\n\n\n\n\nLets inspect files 2612.gtdbtk_archaea.tsv and 2612.gtdbtk_bacteria.tsv in the folder /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly/seqdata_files.\n\n\n\n\n\n\nQuestion\n\n\n\nDo the classifications obtained by GTDBTK match the classifications that were assigned to the contigs using MMSeqs2? Would you expect these taxa given the archaeological context of the samples?\nHint: You can access the classification results of GTDBTK easily by opening the file using visidata: vd 2612.gtdbtk_archaea.tsv and vd 2612.gtdbtk_bacteria.tsv (press ctrl+q to exit)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe five MAGs reconstructed from the sample 2612 were assigned to the taxa:\n\nbin.1: Agathobacter rectalis\nbin.2: Halococcus morrhuae\nbin.3: Methanobrevibacter smithii\nbin.4: Ruminococcus bromii\nbin.5: Bifidobacterium longum\n\nAll of these species were among the most frequent lineages that were identified by MMSeqs2’s taxonomy workflow highlighting the large overlap between the methods.\nWe would expect all five species to be present in our sample. All MAGs but bin.2 were assigned to human gut microbiome commensal species that are typically found in healthy humans. The MAG bin.2 was assigned to a halophilic archaeal species, which is typically found in salt mines.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#evaluating-the-amount-of-ancient-dna-damage",
    "href": "denovo-assembly.html#evaluating-the-amount-of-ancient-dna-damage",
    "title": "11  De novo Genome Assembly",
    "section": "11.9 Evaluating the amount of ancient DNA damage",
    "text": "11.9 Evaluating the amount of ancient DNA damage\nOne of the common questions that remain at this point of our analysis is whether the contigs that we assembled show evidence for the presence of ancient DNA damage. If yes, we could argue that these microbes are indeed ancient, particularly when their DNA fragment length distribution is rather short, too.\nMAGs typically consist of either tens or hundreds of contigs. For this case, many of the commonly used tools for quantifying the amount of ancient DNA damage, such as damageprofiler (Neukamm, Peltzer, and Nieselt 2021) or mapDamage2 (Jónsson et al. 2013), are not very well suited because they would require us to manually check each of their generated “smiley plots”, which visualise the amount of C-to-T substitutions at the end of reads.\n\n\n\n\n\n\nFigure 11.11: Overview of the model comparison performed by pyDamage. The green line represents the null model, i.e. the absence of ancient DNA damage, while the orange line represents the alternative model, i.e. the presence of ancient DNA damage.\n\n\n\nInstead, we will use the program pyDamage (Borry et al. 2021) that was written with the particular use-case of metagenome-assembled contigs in mind. Although pyDamage can visualise the amount of C-to-T substitutions at the 5’ end of reads, it goes a step further and fits two models upon the substitution frequency (Figure 11.11). The null hypothesis is that the observed distribution of C-to-T substitutions at the 5’ end of reads reflects a flat line, i.e. a case when no ancient DNA damage is present. The alternative model assumes that the distribution resembles an exponential decay, i.e. a case when ancient DNA damage is present. By comparing the fit of these two models to the observed data for each contig, pyDamage can quickly identify contigs that are likely of ancient origin without requiring the user to inspect the plots visually.\nWe can run pyDamage directly on the alignment data in BAM format:\npydamage analyze -w 30 -p 14 alignment/2612.sorted.calmd.bam\n\n\n\n\n\n\nQuestion\n\n\n\nEvaluate the pyDamage results with respect to the amount of C-to-T substitutions observed on the contigs! How many contigs does pyDamage consider to show evidence for ancient DNA damage? How much power (prediction accuracy) does it have for this decision? Which MAGs are strongly “ancient” or “modern”?\nHint: You can access pyDamage’s results easily by opening the file using visidata: vd pydamage_results/pydamage_results.csv (press ctrl + q to exit)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the 3,606 contigs, pyDamage inferred a q-value, i.e. a p-value corrected for multiple testing, of &lt; 0.5 to 26 contigs. This is partially due to a high fraction of the contigs with no sufficient information to discriminate between the models (predicted accuracy of &lt; 0.5). However, the majority of the contigs with a prediction accuracy of &gt; 0.5 still had q-values of 0.05 and higher. This suggests that overall the sample did not show large evidence of ancient DNA damage.\nThis reflects also on the MAGs. Although four of the five MAGs were human gut microbiome taxa, they did not show strong evidence of ancient DNA damage. This suggests that the sample is too young and is well preserved.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#annotating-genomes-for-function",
    "href": "denovo-assembly.html#annotating-genomes-for-function",
    "title": "11  De novo Genome Assembly",
    "section": "11.10 Annotating genomes for function",
    "text": "11.10 Annotating genomes for function\nThe second approach on how to study biological diversity of samples using the assembly results is to compare the reconstructed genes and their functions with each other.\nWhile it is very interesting to reconstruct the metagenome-assembled genomes from contigs and speculate about the evolution of a certain taxon, this does not always help when studying the ecology of a microbiome. Studying the ecology of a microbiome means to understand which taxa are present in an environment, what type of community do they form, what kind of natural products do they produce etc.?\nWith the lack of any data from culture isolates, it is rather difficult to discriminate from which species a reconstructed gene sequence is coming from, particularly when the contigs are short. Many microbial species exchange genes among each other via horizontal gene transfer which leads to multiple copies of a gene to be present in our metagenome and increases the level of difficulty further.\nBecause of this, many researchers tend to annotate all genes of a MAGs and compare the presence and absence of these genes across other genomes that were taxonomically assigned to the same taxon. This analysis, called pan-genome analysis, allows us to estimate the diversity of a microbial species with respect to their repertoire of protein-coding genes.\nOne of the most commonly used annotation tools for MAGs is Prokka (Seemann 2014), although it has recently been challenged by Bakta (Schwengers et al. 2021). The latter provides the same functionality as Prokka but incorporates more up-to-date reference databases for the annotation. Therefore, the scientific community is slowly shifting to Bakta.\nNext to returning information on the protein-coding genes, Prokka also returns the annotation of RNA genes (tRNAs and rRNAs), which will help us to evaluate the quality of MAGs regarding the MIMAG criteria.\nFor this practical course, we will use Prokka and we will focus on annotating the pre-refined MAG bin bin.3.fa that we reconstructed from the sample 2612.\nprokka --outdir prokka \\\n    --prefix 2612_003 \\\n    --compliant --metagenome --cpus 14 \\\n    seqdata_files/metawrap_50_10_bins/bin.3.fa\n\n\n\n\n\n\nNote\n\n\n\nIf you performed your own metaWRAP bin refineent, you would find the bin.3.fa in the metawrap/BIN_REFINEMENT directory.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nProkka has annotated our MAG. What type of files does Prokka return? How many genes/tRNAs/rRNAs were detected?\nHint: Check the file prokka/2612_003.txt for the number of annotated elements.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nProkka returns the following files:\n\n.faa: the amino acid sequences of all identified coding sequences\n.ffn: the nucleotide sequences of all identified coding sequences\n.fna: all contigs in nucleotide sequence format renamed following Prokka’s naming scheme\n.gbk: all annotations and sequences in GenBank format\n.gff: all annotations and sequences in GFF format\n.tsv: the tabular overview of all annotations\n.txt: the short summary of the annotation results\n\nProkka found 1,797 coding sequences, 32 tRNAs, but no rRNAs. Finding no rRNAs is a common issues when trying to assemble MAGs without long-read sequencing data and is not just characteristic for ancient DNA samples. However, this means that we cannot technically call this MAG a high-quality MAG due to the lack of the rRNA genes.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#optional-clean-up",
    "href": "denovo-assembly.html#optional-clean-up",
    "title": "11  De novo Genome Assembly",
    "section": "11.11 (Optional) clean-up",
    "text": "11.11 (Optional) clean-up\n\n\n\n\n\n\nWarning\n\n\n\nIf you plan to run through the ancient metagenomic pipelines chapter, do not clear up the denovo assembly directory yet! The data from this chapter will be re-used.\n\n\nLet’s clean up your working directory by removing all the data and output from this chapter.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -rf /&lt;PATH&gt;/&lt;TO&gt;/denovo-assembly*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name denovo-assembly --all -y",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#summary",
    "href": "denovo-assembly.html#summary",
    "title": "11  De novo Genome Assembly",
    "section": "11.12 Summary",
    "text": "11.12 Summary\nIn this practical course you have gone through all the important steps that are necessary for de novo assembling ancient metagenomic sequencing data to obtain contiguous DNA sequences with little error. Furthermore, you have learned how to cluster these sequences into bins without using any references and how to refine them based on lineage-specific marker genes. For these refined bins, you have evaluated their quality regarding common standards set by the scientific community and assigned the MAGs to its most likely taxon. Finally, we learned how to infer the presence of ancient DNA damage and annotate them for RNA genes and protein-coding sequences.\n\n\n\n\n\n\nCautionary note - sequencing depth\n\n\n\nBe aware of the sequencing depth when you assemble your sample. This sample used in this practical course was not obtained by randomly subsampling but I subsampled the sample so that we are able to reconstruct MAGs.\nThe original sample had almost 200 million reads, however, I subsampled it to less than 5 million reads. You usually need a lot of sequencing data for de novo assembly and definitely more data than for reference-alignment based profiling. However, it also heavily depends on the complexity of the sample. So the best advice is: just give it a try!",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "denovo-assembly.html#references",
    "href": "denovo-assembly.html#references",
    "title": "11  De novo Genome Assembly",
    "section": "11.13 References",
    "text": "11.13 References\n\n\n\n\nAlmeida, Alexandre, Stephen Nayfach, Miguel Boland, Francesco Strozzi, Martin Beracochea, Zhou Jason Shi, Katherine S. Pollard, et al. 2021. “A Unified Catalog of 204,938 Reference Genomes from the Human Gut Microbiome.” Nature Biotechnology 39 (1): 105–14. https://doi.org/10.1038/s41587-020-0603-3.\n\n\nAlneberg, Johannes, Brynjar Smári Bjarnason, Ino de Bruijn, Melanie Schirmer, Joshua Quick, Umer Z. Ijaz, Leo Lahti, Nicholas J. Loman, Anders F. Andersson, and Christopher Quince. 2014. “Binning Metagenomic Contigs by Coverage and Composition.” Nature Methods 11 (11): 1144–46. https://doi.org/10.1038/nmeth.3103.\n\n\nBorry, Maxime, Alexander Hübner, Adam B. Rohrlach, and Christina Warinner. 2021. “PyDamage: Automated Ancient Damage Identification and Estimation for Contigs in Ancient DNA de Novo Assembly.” PeerJ 9: e11845. https://doi.org/10.7717/peerj.11845.\n\n\nBowers, Robert M., Nikos C. Kyrpides, Ramunas Stepanauskas, Miranda Harmon-Smith, Devin Doud, T. B. K. Reddy, Frederik Schulz, et al. 2017. “Minimum Information about a Single Amplified Genome (MISAG) and a Metagenome-Assembled Genome (MIMAG) of Bacteria and Archaea.” Nature Biotechnology 35 (8): 725–31. https://doi.org/10.1038/nbt.3893.\n\n\nChaumeil, Pierre-Alain, Aaron J Mussig, Philip Hugenholtz, and Donovan H Parks. 2020. “GTDB-Tk: A Toolkit to Classify Genomes with the Genome Taxonomy Database.” Bioinformatics 36 (6): 1925–27. https://doi.org/10.1093/bioinformatics/btz848.\n\n\nChen, Shifu, Yanqing Zhou, Yaru Chen, and Jia Gu. 2018. “Fastp: An Ultra-Fast All-in-One FASTQ Preprocessor.” Bioinformatics 34 (17): i884–90. https://doi.org/10.1093/bioinformatics/bty560.\n\n\nChklovski, Alex, Donovan H. Parks, Ben J. Woodcroft, and Gene W. Tyson. 2022. “CheckM2: A Rapid, Scalable and Accurate Tool for Assessing Microbial Genome Quality Using Machine Learning.” bioRxiv. https://doi.org/10.1101/2022.07.11.499243.\n\n\nJónsson, Hákon, Aurélien Ginolhac, Mikkel Schubert, Philip L. F. Johnson, and Ludovic Orlando. 2013. “mapDamage2.0: Fast Approximate Bayesian Estimates of Ancient DNA Damage Parameters.” Bioinformatics 29 (13): 1682–84. https://doi.org/10.1093/bioinformatics/btt193.\n\n\nKang, Dongwan D., Feng Li, Edward Kirton, Ashleigh Thomas, Rob Egan, Hong An, and Zhong Wang. 2019. “MetaBAT 2: An Adaptive Binning Algorithm for Robust and Efficient Genome Reconstruction from Metagenome Assemblies.” PeerJ 7 (July): e7359. https://doi.org/10.7717/peerj.7359.\n\n\nKlapper, Martin, Alexander Hübner, Anan Ibrahim, Ina Wasmuth, Maxime Borry, Veit G. Haensch, Shuaibing Zhang, et al. 2023. “Natural Products from Reconstructed Bacterial Genomes of the Middle and Upper Paleolithic.” Science 380 (6645): 619–24. https://doi.org/10.1126/science.adf5300.\n\n\nKrakau, Sabrina, Daniel Straub, Hadrien Gourlé, Gisela Gabernet, and Sven Nahnsen. 2022. “Nf-Core/Mag: A Best-Practice Pipeline for Metagenome Hybrid Assembly and Binning.” NAR Genomics and Bioinformatics 4 (1): lqac007. https://doi.org/10.1093/nargab/lqac007.\n\n\nLeggett, Richard M., Ricardo H. Ramirez-Gonzalez, Walter Verweij, Cintia G. Kawashima, Zamin Iqbal, Jonathan D. G. Jones, Mario Caccamo, and Daniel MacLean. 2013. “Identifying and Classifying Trait Linked Polymorphisms in Non-Reference Species by Walking Coloured de Bruijn Graphs.” PLOS ONE 8 (3): e60058. https://doi.org/10.1371/journal.pone.0060058.\n\n\nLi, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76. https://doi.org/10.1093/bioinformatics/btv033.\n\n\nMaixner, Frank, Mohamed S. Sarhan, Kun D. Huang, Adrian Tett, Alexander Schoenafinger, Stefania Zingale, Aitor Blanco-Míguez, et al. 2021. “Hallstatt Miners Consumed Blue Cheese and Beer During the Iron Age and Retained a Non-Westernized Gut Microbiome Until the Baroque Period.” Current Biology 31 (23): 5149–5162.e6. https://doi.org/10.1016/j.cub.2021.09.031.\n\n\nManni, Mosè, Matthew R Berkeley, Mathieu Seppey, Felipe A Simão, and Evgeny M Zdobnov. 2021. “BUSCO Update: Novel and Streamlined Workflows Along with Broader and Deeper Phylogenetic Coverage for Scoring of Eukaryotic, Prokaryotic, and Viral Genomes.” Molecular Biology and Evolution 38 (10): 4647–54. https://doi.org/10.1093/molbev/msab199.\n\n\nMeyer, Fernando, Adrian Fritz, Zhi-Luo Deng, David Koslicki, Till Robin Lesker, Alexey Gurevich, Gary Robertson, et al. 2022. “Critical Assessment of Metagenome Interpretation: The Second Round of Challenges.” Nature Methods 19 (4): 429–40. https://doi.org/10.1038/s41592-022-01431-4.\n\n\nMirdita, M, M Steinegger, F Breitwieser, J Söding, and E Levy Karin. 2021. “Fast and Sensitive Taxonomic Assignment to Metagenomic Contigs.” Bioinformatics 37 (18): 3029–31. https://doi.org/10.1093/bioinformatics/btab184.\n\n\nNeukamm, Judith, Alexander Peltzer, and Kay Nieselt. 2021. “DamageProfiler: Fast Damage Pattern Calculation for Ancient DNA.” Bioinformatics 37 (20): 3652–53. https://doi.org/10.1093/bioinformatics/btab190.\n\n\nNurk, Sergey, Dmitry Meleshko, Anton Korobeynikov, and Pavel A. Pevzner. 2017. “metaSPAdes: A New Versatile Metagenomic Assembler.” Genome Research 27 (5): 824–34. https://doi.org/10.1101/gr.213959.116.\n\n\nOrakov, Askarbek, Anthony Fullam, Luis Pedro Coelho, Supriya Khedkar, Damian Szklarczyk, Daniel R. Mende, Thomas S. B. Schmidt, and Peer Bork. 2021. “GUNC: Detection of Chimerism and Contamination in Prokaryotic Genomes.” Genome Biology 22 (1): 178. https://doi.org/10.1186/s13059-021-02393-0.\n\n\nOrlando, Ludovic, Aurélien Ginolhac, Guojie Zhang, Duane Froese, Anders Albrechtsen, Mathias Stiller, Mikkel Schubert, et al. 2013. “Recalibrating Equus Evolution Using the Genome Sequence of an Early Middle Pleistocene Horse.” Nature 499 (7456): 74–78. https://doi.org/10.1038/nature12323.\n\n\nParks, Donovan H., Maria Chuvochina, Pierre-Alain Chaumeil, Christian Rinke, Aaron J. Mussig, and Philip Hugenholtz. 2020. “A Complete Domain-to-Species Taxonomy for Bacteria and Archaea.” Nature Biotechnology 38 (9): 1079–86. https://doi.org/10.1038/s41587-020-0501-8.\n\n\nParks, Donovan H., Michael Imelfort, Connor T. Skennerton, Philip Hugenholtz, and Gene W. Tyson. 2015. “CheckM: Assessing the Quality of Microbial Genomes Recovered from Isolates, Single Cells, and Metagenomes.” Genome Research 25 (7): 1043–55. https://doi.org/10.1101/gr.186072.114.\n\n\nSchwengers, Oliver, Lukas Jelonek, Marius Alfred Dieckmann, Sebastian Beyvers, Jochen Blom, and AlexanderYR 2021 Goesmann. 2021. “Bakta: Rapid and Standardized Annotation of Bacterial Genomes via Alignment-Free Sequence Identification.” Microbial Genomics 7 (11): 000685. https://doi.org/10.1099/mgen.0.000685.\n\n\nSeemann, Torsten. 2014. “Prokka: Rapid Prokaryotic Genome Annotation.” Bioinformatics 30 (14): 2068–69. https://doi.org/10.1093/bioinformatics/btu153.\n\n\nSteinegger, Martin, and Johannes Söding. 2017. “MMseqs2 Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets.” Nature Biotechnology 35 (11): 1026–28. https://doi.org/10.1038/nbt.3988.\n\n\nUritskiy, Gherman V., Jocelyne DiRuggiero, and James Taylor. 2018. “MetaWRAPa Flexible Pipeline for Genome-Resolved Metagenomic Data Analysis.” Microbiome 6 (1): 158. https://doi.org/10.1186/s40168-018-0541-1.\n\n\nWright, Robyn J., Andrè M. Comeau, and Morgan G. I. Langille. 2023. “From Defaults to Databases: Parameter and Database Choice Dramatically Impact the Performance of Metagenomic Taxonomic Classification Tools.” Microbial Genomics 9 (3): 000949. https://doi.org/10.1099/mgen.0.000949.\n\n\nWu, Yu-Wei, Blake A. Simmons, and Steven W. Singer. 2016. “MaxBin 2.0: An Automated Binning Algorithm to Recover Genomes from Multiple Metagenomic Datasets.” Bioinformatics 32 (4): 605–7. https://doi.org/10.1093/bioinformatics/btv638.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>_De novo_ Genome Assembly</span>"
    ]
  },
  {
    "objectID": "authentication.html",
    "href": "authentication.html",
    "title": "12  Authentication",
    "section": "",
    "text": "12.1 Simulated ancient metagenomic data\nIn this chapter, we will use 10 pre-simulated metagenomics with gargammel ancient metagenomic samples from Pochon et al. (2022).\nIn here you will see a range of directories, each representing different parts of this tutorial. One set of trimmed ‘simulated’ reads from Pochon et al. (2022) in rawdata/.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#simulated-ancient-metagenomic-data",
    "href": "authentication.html#simulated-ancient-metagenomic-data",
    "title": "12  Authentication",
    "section": "",
    "text": "Figure 12.1: Screenshot of preprint of aMeta by Pochon et al. 2022\n\n\n\n\n\n\n\n\n\nSelf guided: data preparation\n\n\n\n\n\nThe raw simulated data can be accessed via https://doi.org/10.17044/scilifelab.21261405\nTo download the simulated ancient metagenomic data please use the following command lines:\nmkdir ameta/ && cd ameta/\nwget https://figshare.scilifelab.se/ndownloader/articles/21261405/versions/1 \\\n&& unzip 1 && rm 1\nThe DNA reads were simulated with damage, sequencing errors and Illumina adapters, therefore one will have to trim the adapters first:\nfor i in $(ls *.fastq.gz)\ndo\nsample_name=$(basename $i .fastq.gz)\ncutadapt -a AGATCGGAAGAG --minimum-length 30 -o ${sample_name}.trimmed.fastq.gz ${sample_name}.fastq.gz -j 4\ndone\nNow, after the basic data pre-processing has been done, we can proceed with validation, authentication and decontamination analyses.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#genomic-hit-confirmation",
    "href": "authentication.html#genomic-hit-confirmation",
    "title": "12  Authentication",
    "section": "12.2 Genomic hit confirmation",
    "text": "12.2 Genomic hit confirmation\nOnce an organism has been detected in a sample (via alignment, classification or de-novo assembly), one needs to take a closer look at multiple quality metrics in order to reliably confirm that the organism is not a false-positive detection and is of ancient origin. The methods used for this purpose can be divided into modern validation and ancient-specific validation criteria. Below, we will cover both of them.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#modern-genomic-hit-validation-criteria",
    "href": "authentication.html#modern-genomic-hit-validation-criteria",
    "title": "12  Authentication",
    "section": "12.3 Modern genomic hit validation criteria",
    "text": "12.3 Modern genomic hit validation criteria\nThe modern validation methods aim at confirming organism presence regardless of its ancient status. The main approaches include evenness / breadth of coverage computation, assessing alignment quality, and monitoring affinity of the DNA reads to the reference genome of the potential host.\n\n12.3.1 Depth vs breadth and evenness of coverage\nConcluding organism presence by relying solely on the numbers of assigned sequenced reads (aka depth of coverage metric) turns out to be not optimal and too permissive, which may result in a large amount of false-positive discoveries. For example, when using alignment to a reference genome, the mapped reads may demonstrate non-uniform coverage as visualised in the Integrative Genomics Viewer (IGV) below.\n\n\n\n\n\n\nFigure 12.2: Screenshot of the IGV software. The reference genome bar is shown at the top, and in the main panel tall isolated ‘towers’ of reads with lots of coloured bases representing read ‘stacking’ i.e., un-uniform distribution of reads across the whole genome (as expected of the correct reference genome), but accumulation of all reads in the same isolated places on the reference genome, with the many variants on the reads suggesting they are from different species and aligning to conserved regions.\n\n\n\nIn this case, DNA reads originating from another microbe were (mis-)aligned to Yersina pestis reference genome. It can be observed that a large number the reads align only to a few conserved genomic loci. Therefore, even if many thousands of DNA reads are capable of aligning to the reference genome, the overall uneven alignment pattern suggests no presence of Yersina pestis in the metagenomic sample. Thus, not only the number of assigned reads (proportional to depth of coverage metric) but also the breadth and evenness of coverage metrics become of particular importance for verification of metagenomic findings, i.e. hits with DNA reads uniformly aligned across the reference genome are more likely to be true-positive detections (Figure 12.3).\n\n\n\n\n\n\nFigure 12.3: Schematic diagram of the differences between a) read stacking (all reads aligned at one position fo the genome), indicating you’ve not correctly identified the organism, vs b) reads distributed across all the genome. A formula at the bottom of the image shows how both A and B have the same depth of coverage even though they have very different actual patterns on the genome.\n\n\n\nIn the next sections, we will show how to practically compute the breadth and evenness of coverage via KrakenUniq and Samtools.\n\n\n12.3.2 Breadth of coverage via KrakenUniq\nHere we are going to demonstrate that one can assess breadth of coverage information already at the taxonomic profiling step. Although taxonomic classifiers do not perform alignment, some of them, such as KrakenUniq and Kraken2 provide a way to infer breadth of coverage in addition to the number of assigned reads to a taxon. This allows for immediate filtering out a lot of false positive hits. Since Kraken-family classifiers are typically faster and less memory-demanding, i.e. can work with very large reference databases, compared to genome aligners, they provide a robust and fairly unbiased initial taxonomic profiling, which can still later be followed-up with proper alignment and computing evenness of coverage as described above.\n\n\n\n\n\n\nSelf guided: data preparation\n\n\n\n\n\n⚠️ This step will require large amounts of memory and CPUs!, so if running yourself please note this step is better suited for a server, HPC cluster, or the cloud rather than on a laptop!\nTo profile the data with KrakenUniq one needs a database, a pre-built complete microbial NCBI RefSeq database can be accessed via https://doi.org/10.17044/scilifelab.21299541.\nPlease use the following command line to download the database:\ncd krakenuniq/ ## If you've left it...\n\nwget https://figshare.scilifelab.se/ndownloader/articles/21299541/versions/1 \\\n&& unzip 1 && rm 1\nThe following example command is how you would execute KrakenUniq.\nfor i in $(ls *.trimmed.fastq.gz)\ndo\nkrakenuniq --db KRAKENUNIQ_DB --fastq-input $i --threads 20 \\\n--classified-out ${i}.classified_sequences.krakenuniq \\\n--unclassified-out ${i}.unclassified_sequences.krakenuniq \\\n--output ${i}.sequences.krakenuniq --report-file ${i}.krakenuniq.output\ndone\n\n\n\nTaxonomic k-mer-based classification of the ancient metagenomic reads can be done via KrakenUniq. However as this requires a very large database file, the results from running KrakenUniq on the 10 simulated genomes can be found in.\ncd krakenuniq/\nKrakenUniq by default delivers a proxy metric for breadth of coverage called the number of unique kmers (in the 4th column of its output table) assigned to a taxon. KrakenUniq output can be easily filtered with respect to both depth and breadth of coverage, which substantially reduces the number of false-positive hits.\n\nWe can filter the KrakenUniq output with respect to both depth (taxReads) and breadth (kmers) of coverage with the following custom Python script, which selects only species with at east 200 assigned reads and 1000 unique k-mers. After the filtering, we can see a Yersinia pestis hit in the sample 10 that possess the filtering thresholds with respect to both depth and breadth of coverage.\nRun this from within the krakenuniq/ directory.\nfor i in $(ls *.krakenuniq.output)\ndo\n../scripts/filter_krakenuniq.py $i 1000 200 ../scripts/pathogensfound.very_inclusive.tab\ndone\n\nWe can also easily produce a KrakenUniq taxonomic abundance table krakenuniq_abundance_matrix.txt using the custom R script below, which takes as argument the contents of the krakenuniq/ folder containing the KrakenUniq output files.\nRscript ../scripts/krakenuniq_abundance_matrix.R . krakenuniq_abundance_matrix/ 1000 200\nFrom the krakenuniq_abundance_matrix.txt table inside the resulting directory, it becomes clear that Yersinia pestis seems to be present in a few other samples in addition to sample 10.\n\nWhile KrakenUniq delivers information about breadth of coverage by default, you can also get this information from Kraken2.\nFor this one has to use a special flag –report-minimizer-data when running Kraken2 in order to get the breadth of coverage proxy which is called the number of distinct minimizers for the case of Kraken2. Below, we provide an example Kraken2 command line containing the distinct minimizer flag:\n\n\n\n\n\n\nExample only -, do not run!\n\n\n\nDBNAME=Kraken2_DB_directory\nKRAKEN_INPUT=sample.fastq.gz\nKRAKEN_OUTPUT=Kraken2_output_directory\nkraken2 --db $DBNAME --fastq-input $KRAKEN_INPUT --threads 20 \\\n--classified-out $KRAKEN_OUTPUT/classified_sequences.kraken2 \\\n--unclassified-out $KRAKEN_OUTPUT/unclassified_sequences.kraken2 \\\n--output $KRAKEN_OUTPUT/sequences.kraken2 \\\n--report $KRAKEN_OUTPUT/kraken2.output \\\n--use-names --report-minimizer-data\n\n\nThen the filtering of Kraken2 output with respect to breadth and depth of coverage can be done by analogy with filtering KrakenUniq output table. In case of de-novo assembly, the original DNA reads are typically aligned back to the assembled contigs, and the evenness / breadth of coverage can be computed from these alignments.\n\n\n12.3.3 Evenness of coverage via Samtools\nNow, after we have detected an interesting Y. pestis hit, we would like to follow it up, and compute multiple quality metrics (including proper breadth and evenness of coverage) from alignments (Bowtie2 aligner will be used in our case) of the DNA reads to the Y. pestis reference genome. Below, we download Yersinia pestis reference genome from NCBI, build its Bowtie2 index, and align trimmed reads against Yersinia pestis reference genome with Bowtie2. Do not forget to sort and index the alignments as it will be important for computing the evenness of coverage. It is also recommended to remove multi-mapping reads, i.e. the ones that have MAPQ = 0, at least for Bowtie and BWA aligners that are commonly used in ancient metagenomics. Samtools with -q flag can be used to extract reads with MAPQ &gt; = 1.\n\n\n\n\n\n\nSelf guided: data preparation\n\n\n\n\n\ncd /&lt;path/&lt;to&gt;/authentication/bowtie2\n\n## Download reference genome\nNCBI=https://ftp.ncbi.nlm.nih.gov; ID=GCF_000222975.1_ASM22297v1\nwget $NCBI/genomes/all/GCF/000/222/975/${ID}/${ID}_genomic.fna.gz\n\n\n\ncd /&lt;path/&lt;to&gt;/authentication/bowtie2\n\n## Prepare reference genome and build Bowtie2 index\ngunzip GCF_000222975.1_ASM22297v1_genomic.fna.gz; echo NC_017168.1 &gt; region.bed\nseqtk subseq GCF_000222975.1_ASM22297v1_genomic.fna region.bed &gt; NC_017168.1.fasta\nbowtie2-build --large-index NC_017168.1.fasta NC_017168.1.fasta --threads 10\n\n## Run alignment of raw reads against FASTQ\nbowtie2 --large-index -x NC_017168.1.fasta --end-to-end --threads 10 \\\n--very-sensitive -U ../rawdata/sample10.trimmed.fastq.gz | samtools view -bS -h -q 1 \\\n-@ 20 - &gt; Y.pestis_sample10.bam\n\n## Sort and index BAM files for rapid access in downstream commands\nsamtools sort Y.pestis_sample10.bam -@ 10 &gt; Y.pestis_sample10.sorted.bam\nsamtools index Y.pestis_sample10.sorted.bam\nNext, the breadth / evenness of coverage can be computed from the BAM-alignments via samtools depth as follows:\nsamtools depth -a Y.pestis_sample10.sorted.bam &gt; Y.pestis_sample10.sorted.boc\nand visualised using for example the following R code snippet (alternatively aDNA-BAMPlotter can be used).\nLoad R by running R in your terminal\nR\nNote the following may take a minute or so to run.\n\n# Read output of samtools depth commans\ndf &lt;- read.delim(\"Y.pestis_sample10.sorted.boc\", header = FALSE, sep = \"\\t\")\nnames(df) &lt;- c(\"Ref\", \"Pos\", \"N_reads\")\n\n# Split reference genome in tiles, compute breadth of coverage for each tile\nN_tiles &lt;- 500\nstep &lt;- (max(df$Pos) - min(df$Pos)) / N_tiles\ntiles &lt;- c(0:N_tiles) * step; boc &lt;- vector()\nfor(i in 1:length(tiles))\n{\n  df_loc &lt;- df[df$Pos &gt;= tiles[i] & df$Pos &lt; tiles[i+1], ]\n  boc &lt;- append(boc, rep(sum(df_loc$N_reads &gt; 0) / length(df_loc$N_reads),\n  dim(df_loc)[1]))\n}\n\nboc[is.na(boc)]&lt;-0; df$boc &lt;- boc\nplot(df$boc ~ df$Pos, type = \"s\", xlab = \"Genome position\", ylab = \"Coverage\")\nabline(h = 0, col = \"red\", lty = 2)\nmtext(paste0(round((sum(df$N_reads &gt; 0) / length(df$N_reads)) * 100, 2), \n\"% of genome covered\"), cex = 0.8)\n\nOnce finished examining the plot you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\nIn the R script above, we simply split the reference genome into N_tiles tiles and compute the breadth of coverage (number of reference nucleotides covered by at least one read normalised by the total length) locally in each tile. By visualising how the local breadth of coverage changes from tile to tile, we can monitor the distribution of the reads across the reference genome. In the evenness of coverage figure above, the reads seem to cover all parts of the reference genome uniformly, which is a good evidence of true-positive detection, even though the total mean breadth of coverage is low due to the low total number of reads.\n\n\n12.3.4 Alignment quality\nIn addition to evenness and breadth of coverage, it is very informative to monitor how well the metagenomic reads map to a reference genome. Here one can control for mapping quality (MAPQ field in the BAM-alignments) and the number of mismatches for each read, i.e. edit distance.\nMapping quality (MAPQ) can be extracted from the 5th column of BAM-alignments using Samtools and cut command in bash.\nsamtools view Y.pestis_sample10.sorted.bam | cut -f5 &gt; mapq.txt\nThen the 5th column of the filtered BAM-alignment can be visualised via a simple histogram in R as below for two random metagenomic samples.\nLoad R\nR\nAnd generate the histogram with.\n\nhist(as.numeric(readLines(\"mapq.txt\")), col = \"darkred\", breaks = 100)\n\n\nNote that MAPQ scores are computed slightly differently for Bowtie and BWA, so they are not directly comparable, however, for both MAPQ ~ 10-30, as in the histograms below, indicates good affinity of the DNA reads to the reference genome. here we provide some examples of how typical MAPQ histograms for Bowtie2 and BWA alignments can look like:\n\nEdit distance can be extracted by gathering information in the NM-tag inside BAM-alignments, which reports the number of mismatches for each aligned read. This can be done either in bash / awk, or using handy functions from Rsamtools R package:\n\nlibrary(\"Rsamtools\")\nparam &lt;- ScanBamParam(tag = \"NM\")\nbam &lt;- scanBam(\"Y.pestis_sample10.sorted.bam\", param = param)\nbarplot(table(bam[[1]]$tag$NM), ylab=\"Number of reads\", xlab=\"Number of mismatches\")\n\nOnce finished examining the plot you can quit R.\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\nIn the barplot above we can see that the majority of reads align either without or with very few mismatches, which is an evidence of high affinity of the aligned reads with respect to the reference genome. For a true-positive finding, the edit distance barplot typically has a decreasing profile. However, for a very degraded DNA, it can have a mode around 1 or 2, which can also be reasonable. A false-positive hit would have a mode of the edit distance barplot shifted toward higher numbers of mismatches.\n\n\n12.3.5 Affinity to reference\nVery related to edit distance is another alignment validation metric which is called percent identity. It represents a barplot demonstrating the numbers of reads that are 100% identical to the reference genome (i.e. map without a single mismatch), 99% identical, 98% identical etc. Misaligned reads originating from another related organism have typically most reads with percent identity of 93-96%. In the figure below, the panels (c–e) demonstrate different percent identity distributions. In panel c, most reads show a high similarity to the reference, which indicates a correct assignment of the reads to the reference. In panel d, most reads are highly dissimilar to the reference, which suggests that they originate from different related species. In some cases, as in panel e, a mixture of correctly assigned and misassigned reads can be observed.\n\nAnother important way to detect reads that cross-map between related species is haploidy or checking the amount of multi-allelic SNPs. Because bacteria are haploid organisms, only one allele is expected for each genomic position. Only a small number of multi-allelic sites are expected, which can result from a few mis-assigned or incorrectly aligned reads. In the figure above, panels (f–i) demonstrate histograms of SNP allele frequency distributions. Panel f demonstrates the situation when we have only a few multi-allelic sites originating from a misaligned reads. This is a preferable case scenario corresponding to correct assignment of the reads to the reference. Please also check the corresponding “Good alignments” IGV visualisation to the right in the figure above.\nIn contrast, a large number of multi-allelic sites indicates that the assigned reads originate from more than one species or strain, which can result in symmetric allele frequency distributions (e.g., if two species or strains are present in equal abundance) (panel g) or asymmetric distributions (e.g., if two species or strains are present in unequal abundance) (panel h). A large number of mis-assigned reads from closely related species can result in a large number of multi-allelic sites with low frequencies of the derived allele (panel i). The situations (g-i) correspond to incorrect assignment of the reads to the reference. Please also check the corresponding “Bad alignments” IGV visualisation to the right in the figure above.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#ancient-specific-genomic-hit-validation-criteria",
    "href": "authentication.html#ancient-specific-genomic-hit-validation-criteria",
    "title": "12  Authentication",
    "section": "12.4 Ancient-specific genomic hit validation criteria",
    "text": "12.4 Ancient-specific genomic hit validation criteria\nIn contrast to modern genomic hit validation criteria, the ancient-specific validation methods concentrate on DNA degradation and damage pattern as ultimate signs of ancient DNA. Below, we will discuss deamination profile, read length distribution and post mortem damage (PMD) scores metrics that provide good confirmation of ancient origin of the detected organism.\n\n12.4.1 Degradation patterns\nChecking evenness of coverage and alignment quality can help us to make sure that the organism we are thinking about is really present in the metagenomic sample. However, we still need to address the question “How ancinet?”. For this purpose we need to compute deamination profile and read length distribution of the aligned reads in order to prove that they demonstrate damage pattern and are sufficiently fragmented, which would be a good evidence of ancient origin of the detected organisms.\nDeamination profile of a damaged DNA demonstrate an enrichment of C / T polymorphisms at the ends of the reads compared to all other single nucleotide substitutions. There are several tools for computing deamination profile, but perhaps the most popular is mapDamage. The tool can be run using the following command line, still in the authentication/bowtie2/ directory:\nmapDamage -i Y.pestis_sample10.sorted.bam -r NC_017168.1.fasta -d mapDamage_results/ --merge-reference-sequences --no-stats\n\nmaDamage delivers a bunch of useful statistics, among other read length distribution can be checked. A typical mode of DNA reads should be within a range 30-70 base-pairs in order to be a good evidence of DNA fragmentation. Reads longer tha 100 base-pairs are more likely to originate from modern contamination.\n\nAnother useful tool that can be applied to assess how DNA is damaged is PMDtools which is a maximum-likelihood probabilistic model that calculates an ancient score, PMD score, for each read. The ability of PMDtools to infer ancient status with a single read resolution is quite unique and different from mapDamage that can only assess deamination based on a number of reads. PMD scores can be computed using the following command line, please note that Python2 is needed for this purpose.\nsamtools view -h Y.pestis_sample10.bam | pmdtools --printDS &gt; PMDscores.txt\nThe distribution of PMD scores can be visualised via a histogram in R as follows.\nLoad R.\n\nR\n\nThen generate the histogram.\n\npmd_scores &lt;- read.delim(\"PMDscores.txt\", header = FALSE, sep = \"\\t\")\nhist(pmd_scores$V4, breaks = 1000, xlab = \"PMDscores\")\n\nOnce finished examining the plot you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\nTypically, reads with PMD scores greater than 3 are considered to be reliably ancient, i.e. damaged, and can be extracted for taking a closer look. Therefore PMDtools is great for separating ancient reads from modern contaminant reads.\nAs mapDamage, PMDtools can also compute deamination profile. However, the advantage of PMDtools that it can compute deamination profile for UDG / USER treated samples (with the flag –CpG). For this purpose, PMDtools uses only CpG sites which escape the treatment, so deamination is not gone completely and there is a chance to authenticate treated samples. Computing deamination pattern with PMDtools can be achieved with the following command line (please note that the scripts pmdtools.0.60.py and plotPMD.v2.R can be downloaded from the github repository here https://github.com/pontussk/PMDtools):\nsamtools view Y.pestis_sample10.bam | pmdtools --platypus &gt; PMDtemp.txt\nWe can then run simple R commands directly from the terminal (without loading R itself with) the following.\nR CMD BATCH plotPMD\n\nWhen performing ancient status analysis on de-novo assembled contigs, it can be computationally challenging and time consuming to run mapDamage or PMDtools on all of them as there can be hundreds of thousands contigs. In addition, the outputs from mapDamage and PMDtools lacking a clear numeric quantity or a statistical test that could provide an “ancient vs. non-ancient” decision for each de-novo assembled contig. To address these limitations, pyDamage tool was recently developed. PyDamage evaluates the amount of aDNA damage and tests the hypothesis whether a model assuming presence of aDNA damage better explains the data than a null model.\n\npyDamage can be run on a sorted BAM-alignments of the microbial reads to the de-novo assembled contigs using the following command line:\n\n\n\n\n\n\nExample command - do not run!\n\n\n\npydamage analyze -w 30 -p 14 filtered.sorted.bam",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#authentication-with-taxonomic-information",
    "href": "authentication.html#authentication-with-taxonomic-information",
    "title": "12  Authentication",
    "section": "12.5 Authentication with taxonomic information",
    "text": "12.5 Authentication with taxonomic information\nDifferent tools can be utilised to assess the level of deamination at the strand termini and the most popular are mapDamage2.0 (https://academic.oup.com/bioinformatics/article/29/13/1682/184965) and PMDtools (https://github.com/pontussk/PMDtools).\nHowever, applying these to a metagenomic dataset can be computationally demanding and time-consuming due to the tens of thousands of different taxonomic entities it can include.\nmetaDMG is currently under development and it represents a fast, flexible, and efficient tool for performing taxonomic profiling (with the integrated ngsLCA algorithm) and quantifying post-mortem DNA damage. It is specially optimised for ancient metagenomic datasets where raw fastq files have been mapped against large sets of reference genomes. metaDMG should be run on a read coordinate sorted BAM/SAM-alignment file and can calculate the degree of damage from read data mapped against single and multiple genomes by analysing mismatches and deletions contained in the MD:Z tag of the input alignment file. This reference-free approach allows for a faster processing of the deamination patterns (Figure 12.4).\n\n\n\n\n\n\nFigure 12.4: Damage plots of reads assigned to Homo sapiens with post-mortem damage pattern increasing at read termini [from metagenomic samples, adapted from (Michelsen et al. 2022)]. The figure shows the damage rate f (x) = k(x)∕N(x) as a function of position x for both forward (C→T) and reverse (G→A).\n\n\n\nIn particular, three different settings can be run:\n\nSingle genome analysis with one overall global estimate of the damage patterns. Similar to mapDamage2.0 (https://academic.oup.com/bioinformatics/article/29/13/1682/184965).\n\nmetaDMG-cpp getdamage --run_mode 0\n\nMetagenomic (e.g. multiple genome alignments) analyses. This mode provides a damage estimate per reference, taxonomic name or accession number, including all alignments without any taxonomical classification\n\nmetaDMG-cpp getdamage --run_mode 1\n\nMetagenomic analyses with the integration of the taxonomy: Least Common Ancestor algorithm (ngsLCA). This allows the computation of damage estimates for alignments classified to given taxonomic levels.\n\nmetaDMG-cpp lca\nIn this section we will utilise metaDMG-cpp lca since we are interested in a more comprehensive analysis that includes the taxonomic classification of our alignments. For those interested in exploring other functionalities of metaDMG, I encourage you to visit the tool’s official GitHub (https://github.com/metaDMG-dev/metaDMG-cpp) and the ngsLCA official GitHub (https://github.com/miwipe/ngsLCA).\n\n12.5.1 Taxonomic profiling: metaDMG-cpp lca\nThe metaDMG-cpp lca function is based on the ngsLCA (next-generation sequence Lowest Common Ancestor) algorithm to collect mismatch information for all reads and generate a taxonomic profile ((Wang et al. 2022)). It counts substitutions between read and reference on internal nodes within a taxonomy (e.g. species, genus and family level). It is built upon the NCBI taxonomy (https://www.ncbi.nlm.nih.gov/taxonomy) and requires three files:\n\nnodes.dmp: taxonomic nodes and the relationships between different taxa in the tree\naccess2taxID: the “taxonomy file”, sequence accession numbers, and the corresponding taxonomic IDs.\nnames.dmp: scientific names of the taxa are associated with each taxon contained in nodes.dmp file.\n\n\n\n\n\n\n\nTip\n\n\n\nFor custom reference genomes not covered by NCBI, their accession IDs and the corresponded NCBI taxonomic IDs need to be manually attached to the NCBI access2taxID file.\n\n\nThe ngsLCA program considers a chosen similarity interval between each read and its reference in the generated bam/sam file. The similarity can be set as an edit distance [-editdist[min/max]], i.e., number of mismatches between the read to reference genome, or as a similarity distance [-simscore[low/high]], i.e., percentage of mismatches between the read to reference genome.\nThe main files produced by this command have the extensions .bdamage.gz and lca.gz. The first consists of a nucleotide misincorporation matrix (also called mismatch matrix) which represents the nucleotide substitution counts across the reads (Table 12.1). The lca file reports the sequence analysed and its taxonomic path, as well as other statistics (gc content, fragment length).\nWe report an example of the bdamage.gz file output printed using the command metaDMG-cpp print:\nmetaDMG-cpp print file.bdamage.gz -names names.dmp &gt; bdamage_table.tsv\n\n\n\nTable 12.1: Table 1. Example of the bdamage.gz mismatch matrix table for beech (Fagus sylvatica) of sample VM-14 provided in the exercise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1. Example of the bdamage.gz mismatch matrix table for beech (Fagus sylvatica) of sample VM-14 provided in the exercise.\n\n\nFunkyName\nNalignments\nDirection\nPos\nAA\nAC\nAG\nAT\nCA\nCC\nCG\nCT\nGA\nGC\nGG\nGT\nTA\nTC\nTG\nTT\n\n\n\n\nFagus sylvatica\n196960\n5'\n0\n0.989742\n0.002790\n0.006384\n0.001085\n0.001337\n0.901014\n0.003230\n0.094419\n0.029968\n0.004090\n0.963388\n0.002554\n0.003400\n0.013838\n0.001919\n0.980843\n\n\nFagus sylvatica\n196960\n5'\n1\n0.989488\n0.003044\n0.004779\n0.002689\n0.000085\n0.943479\n0.005200\n0.051235\n0.018523\n0.004122\n0.974800\n0.002556\n0.002407\n0.008830\n0.000764\n0.987998\n\n\nFagus sylvatica\n196960\n5'\n2\n0.993037\n0.000288\n0.005555\n0.001121\n0.003679\n0.980400\n0.000069\n0.015852\n0.016200\n0.002869\n0.975140\n0.005790\n0.005933\n0.014151\n0.004898\n0.975018\n\n\nFagus sylvatica\n196960\n5'\n3\n0.981054\n0.005307\n0.009159\n0.004480\n0.004385\n0.978382\n0.000910\n0.016323\n0.017552\n0.005319\n0.971735\n0.005394\n0.003053\n0.006302\n0.000173\n0.990472\n\n\nFagus sylvatica\n196960\n5'\n4\n0.994413\n0.000076\n0.005455\n0.000056\n0.000403\n0.971447\n0.003505\n0.024645\n0.007117\n0.002939\n0.989472\n0.000472\n0.000247\n0.008299\n0.000152\n0.991301\n\n\nFagus sylvatica\n196960\n5'\n5\n0.988985\n0.000061\n0.009803\n0.001151\n0.000032\n0.977904\n0.005916\n0.016148\n0.025535\n0.000138\n0.974234\n0.000094\n0.001613\n0.009086\n0.000482\n0.988820\n\n\nFagus sylvatica\n196960\n5'\n6\n0.989806\n0.000239\n0.004127\n0.005828\n0.003278\n0.975862\n0.002776\n0.018084\n0.015836\n0.004876\n0.977987\n0.001301\n0.000418\n0.015145\n0.002972\n0.981465\n\n\nFagus sylvatica\n196960\n5'\n7\n0.987598\n0.007473\n0.004898\n0.000031\n0.009555\n0.974281\n0.002914\n0.013250\n0.015450\n0.000015\n0.978637\n0.005898\n0.002356\n0.005547\n0.000231\n0.991865\n\n\nFagus sylvatica\n196960\n5'\n8\n0.983646\n0.001314\n0.013659\n0.001380\n0.001187\n0.971496\n0.000207\n0.027110\n0.011010\n0.003486\n0.985432\n0.000072\n0.000006\n0.007790\n0.001605\n0.990598\n\n\nFagus sylvatica\n196960\n5'\n9\n0.995312\n0.000119\n0.003117\n0.001452\n0.001707\n0.982706\n0.002370\n0.013217\n0.018666\n0.003942\n0.972469\n0.004922\n0.004357\n0.002576\n0.002286\n0.990781\n\n\nFagus sylvatica\n196960\n5'\n10\n0.991643\n0.003135\n0.004654\n0.000568\n0.000132\n0.980449\n0.000000\n0.019419\n0.015322\n0.002766\n0.979047\n0.002864\n0.000164\n0.010653\n0.000005\n0.989178\n\n\nFagus sylvatica\n196960\n5'\n11\n0.992977\n0.002227\n0.004369\n0.000427\n0.000010\n0.978681\n0.000303\n0.021006\n0.019069\n0.003154\n0.976644\n0.001134\n0.003626\n0.003529\n0.002571\n0.990274\n\n\nFagus sylvatica\n196960\n5'\n12\n0.986903\n0.002606\n0.006447\n0.004044\n0.008512\n0.972589\n0.000003\n0.018896\n0.003895\n0.002801\n0.992522\n0.000781\n0.002139\n0.008530\n0.000036\n0.989295\n\n\nFagus sylvatica\n196960\n5'\n13\n0.991904\n0.001482\n0.001852\n0.004761\n0.005883\n0.971748\n0.000493\n0.021876\n0.021197\n0.000126\n0.978637\n0.000040\n0.000438\n0.003696\n0.000058\n0.995809\n\n\nFagus sylvatica\n196960\n5'\n14\n0.990775\n0.000005\n0.009060\n0.000159\n0.002901\n0.976768\n0.000280\n0.020051\n0.013583\n0.000690\n0.980013\n0.005714\n0.000930\n0.006973\n0.000307\n0.991789\n\n\nFagus sylvatica\n196960\n5'\n15\n0.988845\n0.002256\n0.004008\n0.004892\n0.013058\n0.960680\n0.000246\n0.026015\n0.016849\n0.000012\n0.982107\n0.001032\n0.003212\n0.008099\n0.000052\n0.988638\n\n\n\n\n\n\n\n\n\n\n12.5.2 Deamination patterns\nmetaDMG can perform a numerical optimisation of the deamination frequencies (C→T, G→A) using the binomial or beta-binomial likelihood models, where the latter can deal with a large amount of variance (overdispersion). This function is called the metaDMG-cpp dfit or damage estimates function.\nmetaDMG-cpp dfit \nmetaDMG-cpp dfit allows us to estimate the four fit parameters of the damage model (Figure 12.5):\n\nA: the amplitude of damage on position one;\nq: the constant deamination background;\nc: relative decrease of damage per position;\nϕ: the uncertainty of the likelihood model used (binomial or beta-binomial).\n\nAnother important parameter is the Zfit or significance, which represents the number of standard deviations (“sigmas”) away from zero, or in other words, the certainty of the damage being positive.\n\n\n\n\n\n\nFigure 12.5: The damage model. The figure shows the misincorporations as circles and the damage as a solid line. The fit parameters are reported: A,c,q,ϕ. The uncertainty for a binomial model is in dark grey, while the uncertainty for a beta-binomial model is in light grey (from Michelsen et al. 2022)\n\n\n\nAccurate damage estimates are crucial for the authentication of the metagenomic dataset. The number of reads and the significance (Zfit) are additional parameters that can impact the accuracy and reliability of ancient DNA analysis.\nTests on a single-genome have shown how the accuracy of metaDMG dfit increases depending on the number of reads (Figure 12.6).\n\n\n\n\n\n\nFigure 12.6: The single-genome simulations on the Homo Sapiens genome with a mean fragment length of 60 and 10% damage. The known damage (Dknown) is shown as a dashed line. A. Estimated damage and its standard deviation (error bars) for 20 replicates (100 reads each). B. Average damage as a function of the number of reads (with the average of the standard deviations as errors) (Adapted from Michelsen et al. 2022).\n\n\n\nThe simulation on the Homo Sapiens genome, illustrated in Figure 12.6 demonstrates the individual metaDMG damage estimates for 20 selected replications (iterations 60 to 79). When damage estimates are minimal, the distribution of Dfit is limited to positive values. This limitation can result in error bars extending into negative damage values, thus producing unrealistic estimates. As shown in Figure 12.6, the damage tends to converge towards the known values as the number of reads increases.\nIn addition, the simulation reports a relationship between the amount of damage in taxa and the number of reads (Figure 12.7). As shown in Figure 12.7, low expected damage (~5 %) requires about 1000 reads to be 95% certain about its estimation, while higher levels of damage (~15-30%) require fewer reads (100-500) to reach the same level of certainty. When increasing the significance threshold (Zfit) more reads are also required.\n\n\n\n\n\n\nFigure 12.7: Relationship between the damage and the number of reads for the simulated single-genome data. The lines represent the number of reads needed to correctly infer the amount of damage: the solid line shows a certainty of 95%, and the dashed line, a certainty of 50%. Colors represent the significance (Zfit) cuts at 2 (blue), 3 (red), and 4 (green). (from Michelsen et al. 2022)\n\n\n\nSimulations using metagenomic datasets have also evaluated the relationship between the amount of damage and its significance (Figure 12.8).\n\n\n\n\n\n\nFigure 12.8: The amount of damage as a function of significance (metagenomic simulations). A. damage of the ancient taxa; B. damage of the non-ancient taxa. A larger dot size indicates a higher number of reads. (Adapted from Michelsen et al. 2022)\n\n\n\nAs shown in Figure 12.8, there is a difference in the damage estimates between the ancient and the non-ancient taxa of simulated metagenomic datasets. The non-ancient taxa (Figure 12.8) report significance values below 2, in contrast to the ancient taxa (Figure 12.8) which reach a significance of 20. A relaxed significance threshold (Zfit &gt; 2) and a minimum of 100 reads increase the accuracy of damage estimates to 90% of the dataset (Michelsen et al. 2022).\nWe also observe how the oldest samples (Cave-100 and Cave-102) which are 100 and 102 thousand years BP, show the highest amount of damage of all the metagenomes. While Pitch-6 and Cave-22 samples, which are 6 and 22 thousand years old and thus younger have almost similar levels of damage.\n\n\n12.5.3 Ancient metagenomic dataset\nIn this section, we will use 6 metagenomic libraries downsampled with eukaryotes reads from the study by (Zampirolo et al. 2023) (Figure 12.9). The libraries originate from sediment samples of the Velký Mamut’ák rock shelter located in Northern Bohemia (Czech Republic) and covering the period between the Late Neolithic (~6100-5300 cal. BP) to more recent times (800 cal BP).\n\n\n\n\n\n\nFigure 12.9: Screenshot of preprint of the source dataset by (Zampirolo et al. 2023)\n\n\n\n\n\n12.5.4 Ancient metagenomics with metaDMG-cpp: the workflow\nThis section will cover the metaDMG analysis which involve taxonomic classification of the reads starting from sorted SAM files, the damage estimation and compilation of the final metaDMG output.\nTo begin, we can find raw SAM files used as input to metaDMG we will use for the exercise are stored in metadmg.\nWe also need the taxonomy files, which are in the folder metadmg/small_taxonomy/, these include names.dmp, nodes.dmp and small_accession2taxid.txt.gz.\n\n\n\n\n\n\nWarning\n\n\n\nmetaDMG is currently under development and it is therefore important to keep it updated. The best documentation is currently found in the –help function.\n\n\nWe need to activate a dedicated environment for metaDMG as it is still under development. We candeactivate the current one with\nconda deactivate \nAnd we will work with metaDMG activating the environment with\nconda activate metaDMG\n\n\n\n\n\n\nWarning\n\n\n\nBefore you continue, make sure you’re within the authentication/ directory!\n\n\nFirst we will run metaDMG-cpp lca to get the mismatch matrix file bdamage.gz that we need to estimate the dfit.\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-3_800.merged.sort.bam --out_prefix metadmg/VM-3_800.merged.bam\n\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-11_3000.merged.sort.bam --out_prefix metadmg/VM-11_3000.merged.bam\n\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-14_3900.merged.sort.bam --out_prefix metadmg/VM-14_3900.merged.bam \n\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-15_4100.merged.sort.bam --out_prefix metadmg/VM-15_4100.merged.bam \n\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-17_5300.merged.sort.bam --out_prefix metadmg/VM-17_5300.merged.bam \n\nmetaDMG-cpp lca --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --acc2tax metadmg/small_taxonomy/small_accession2taxid.txt.gz --sim_score_low 0.95 --sim_score_high 1.0 --how_many 30 --weight_type 1 --threads 12 --bam metadmg/VM-19_6100.merged.sort.bam --out_prefix metadmg/VM-19_6100.merged.bam\nWe use the file generated from the previous command (bdamage.gz), containing the misincorporation matrix to calculate the deamination pattern. We use metaDMG-cpp dfit function to obtain a quick computation of a beta-binomial model.\nmetaDMG-cpp dfit metadmg/VM-3_800.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-3_800.merged.bam \n\nmetaDMG-cpp dfit metadmg/VM-11_3000.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-11_3000.merged.bam \n\nmetaDMG-cpp dfit metadmg/VM-14_3900.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-14_3900.merged.bam\n\nmetaDMG-cpp dfit metadmg/VM-15_4100.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-15_4100.merged.bam \n\nmetaDMG-cpp dfit metadmg/VM-17_5300.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-17_5300.merged.bam \n\nmetaDMG-cpp dfit metadmg/VM-19_6100.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2  --lib ds --out metadmg/VM-19_6100.merged.bam\n\n\n\n\n\n\nExample only -, do not run!\n\n\n\nThe metaDMG-cpp dfit function also allows for the computation of a binomial model, which includes additional statistics (such as bootstrap estimated parameters). For the exercise, we only run the quick statistics, but we provide an example of the codes for full stat:\nmetaDMG-cpp dfit metadmg/VM-11_3000.merged.bam.bdamage.gz --names metadmg/small_taxonomy/names.dmp --nodes  metadmg/small_taxonomy/nodes.dmp --showfits 2 --nopt 10 --nbootstrap 20 --doboot 1 --seed 1234  --lib ds --out metadmg/VM-11_3000.merged.bam\n\n\nWe run the metaDMG-cpp aggregate function to merge the statistics from the previous two steps and obtain a file for each sample.\nmetaDMG-cpp aggregate metadmg/VM-3_800.merged.bam.bdamage.gz -lcastat metadmg/VM-3_800.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-3_800.merged.bam.dfit.gz --out metadmg/VM-3_aggregated_results\n\nmetaDMG-cpp aggregate metadmg/VM-11_3000.merged.bam.bdamage.gz -lcastat metadmg/VM-11_3000.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-11_3000.merged.bam.dfit.gz --out metadmg/VM-11_aggregated_results\n\nmetaDMG-cpp aggregate metadmg/VM-14_3900.merged.bam.bdamage.gz -lcastat metadmg/VM-14_3900.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-14_3900.merged.bam.dfit.gz --out metadmg/VM-14_aggregated_results\n\nmetaDMG-cpp aggregate metadmg/VM-15_4100.merged.bam.bdamage.gz -lcastat metadmg/VM-15_4100.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-15_4100.merged.bam.dfit.gz --out metadmg/VM-15_aggregated_results\n\nmetaDMG-cpp aggregate metadmg/VM-17_5300.merged.bam.bdamage.gz -lcastat metadmg/VM-17_5300.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-17_5300.merged.bam.dfit.gz --out metadmg/VM-17_aggregated_results\n\nmetaDMG-cpp aggregate metadmg/VM-19_6100.merged.bam.bdamage.gz -lcastat metadmg/VM-19_6100.merged.bam.stat.gz --names metadmg/small_taxonomy/names.dmp --nodes metadmg/small_taxonomy/nodes.dmp --dfit metadmg/VM-19_6100.merged.bam.dfit.gz --out metadmg/VM-19_aggregated_results\nIn the last step we merge the header and the filenames in a unique tab-separated file (tsv).\nWe first unzip the output files\ngunzip *_aggregated_results.stat.gz\nThen we extract the header and we concatenate the content of all the output files in a unique tsv file.\n#Define header for final output table\nheader_file=\"VM-11_aggregated_results.stat\"\n\n# Get the header\nheader=$(head -n 1 \"$header_file\")\n\n# Define the output file\noutput_file=\"concatenated_metaDMGfinal.tsv\"\n\n# Add the header to the concatenated file\necho -e \"filename\\t$header\" &gt; \"$output_file\"\n\nfor file in VM-11_aggregated_results.stat \\\n        VM-14_aggregated_results.stat \\\n            VM-15_aggregated_results.stat \\\n            VM-17_aggregated_results.stat \\\n            VM-19_aggregated_results.stat \\\n            VM-3_aggregated_results.stat\ndo\n    tail -n +2 \"$file\" | while read -r line; do\n        echo -e \"$file\\t$line\" &gt;&gt; \"$output_file\"\n    done\ndone\n\n\n12.5.5 Investigating the final output with R\nWe first visualise our metaDMG output manually navigating to the folder metadmg/ and clicking on “Open folder”. We can double-click on the tsv file concatenated_metaDMGfinal.tsv and visualise it.\nWe will now investigate the tsv table produced by metaDMG to authenticate damage patterns, visualise the relationship between the damage and the significance, and the degree of damage through depth and time.\nR packages for this exercise are located in our original conda environment authentication.\nWe deactivate the current conda environment and we re-activate the environment authentication\nconda deactivate\nconda activate authentication\nWe navigate to the working directory:\ncd metadmg/figures\nWe load R by running R in your terminal\nR\nWe load the libraries\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(ggpubr)\n\n\n12.5.5.1 Deamination patterns\nWe run the damage plot to visualise the deamination patterns along forward and reverse strands, and we save the results per each taxon detected in the samples.\nWe will use the function get_dmg_decay_fit to visualise damage pattern (Figure 12.10). The function is saved in metadmg/script/, so we only need to run the following command to recall it:\n\nsource(\"metadmg/script/get_dmg_decay_fit.R\")\n\nBut if you are curious and want to know how it works, here is the function itself:\n\nget_dmg_decay_fit &lt;- function(df, orient = \"fwd\", pos = 30, p_breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), y_max = 0.7, y_min = -0.01) {\n  df_dx_fwd &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"fwdx\")) %&gt;%\n    select(-starts_with(\"fwdxConf\")) %&gt;% \n    pivot_longer(names_to = \"type\", values_to = \"Dx_fwd\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"fwdx\", \"\", type)) %&gt;%\n    select(-type)\n  \n  df_dx_rev &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"bwdx\")) %&gt;%\n    select(-starts_with(\"bwdxConf\")) %&gt;%\n    pivot_longer(names_to = \"type\", values_to = \"Dx_rev\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"bwdx\", \"\", type)) %&gt;%\n    select(-type)\n  \n  df_dx_std_fwd &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"fwdxConf\")) %&gt;%\n    pivot_longer(names_to = \"type\", values_to = \"Dx_std_fwd\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"fwdxConf\", \"\", type)) %&gt;%\n    select(-type)\n  \n  df_dx_std_rev &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"bwdxConf\")) %&gt;%\n    pivot_longer(names_to = \"type\", values_to = \"Dx_std_rev\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"bwdxConf\", \"\", type)) %&gt;%\n    select(-type)\n  \n  df_fit_fwd &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"fwf\")) %&gt;%\n    pivot_longer(names_to = \"type\", values_to = \"f_fwd\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"fwf\", \"\", type)) %&gt;%\n    select(-type)\n  \n  df_fit_rev &lt;- df %&gt;%\n    select(taxid, name, label, starts_with(\"bwf\")) %&gt;%\n    pivot_longer(names_to = \"type\", values_to = \"f_rev\", c(-taxid, -name, -label)) %&gt;%\n    mutate(x = gsub(\"bwf\", \"\", type)) %&gt;%\n    select(-type)\n  \n  dat &lt;- df_dx_fwd %&gt;%\n    inner_join(df_dx_rev, by = c(\"taxid\", \"name\", \"label\", \"x\")) %&gt;%\n    inner_join(df_dx_std_fwd, by = c(\"taxid\", \"name\", \"label\", \"x\")) %&gt;%\n    inner_join(df_dx_std_rev, by = c(\"taxid\", \"name\", \"label\", \"x\")) %&gt;%\n    inner_join(df_fit_fwd, by = c(\"taxid\", \"name\", \"label\", \"x\")) %&gt;%\n    inner_join(df_fit_rev, by = c(\"taxid\", \"name\", \"label\", \"x\")) %&gt;%\n    mutate(x = as.numeric(x)) %&gt;%\n    filter(x &lt;= pos) %&gt;%\n    rowwise() %&gt;%\n    mutate(Dx_fwd_min = Dx_fwd - Dx_std_fwd,\n           Dx_fwd_max = Dx_fwd + Dx_std_fwd,\n           Dx_rev_min = Dx_rev - Dx_std_rev,\n           Dx_rev_max = Dx_rev + Dx_std_rev)\n  \n  fwd_max &lt;- dat %&gt;%\n    group_by(as.character(x)) %&gt;%\n    summarise(val = mean(Dx_std_fwd) + sd(Dx_std_fwd)) %&gt;%\n    pull(val) %&gt;%\n    max()\n  \n  fwd_min &lt;- dat %&gt;%\n    group_by(as.character(x)) %&gt;%\n    summarise(val = mean(Dx_std_fwd) - sd(Dx_std_fwd)) %&gt;%\n    pull(val) %&gt;%\n    min()\n  \n  rev_max &lt;- dat %&gt;%\n    group_by(as.character(x)) %&gt;%\n    summarise(val = mean(Dx_std_rev) + sd(Dx_std_rev)) %&gt;%\n    pull(val) %&gt;%\n    max()\n  \n  rev_min &lt;- dat %&gt;%\n    group_by(as.character(x)) %&gt;%\n    summarise(val = mean(Dx_std_rev) - sd(Dx_std_rev)) %&gt;%\n    pull(val) %&gt;%\n    min()\n  \n  if (orient == \"fwd\") {\n    ggplot() +\n      geom_ribbon(data = dat, aes(x, ymin = Dx_fwd_min, ymax = Dx_fwd_max, group = interaction(name, taxid)), alpha = 0.6, fill = \"darkcyan\") +\n      geom_line(data = dat, aes(x, Dx_fwd, group = interaction(name, taxid)), color = \"black\") +\n      geom_point(data = dat, aes(x, f_fwd), alpha = .50, size = 2, fill = \"black\") +\n      theme_test() +\n      xlab(\"Position\") +\n      ylab(\"Frequency\") +\n      scale_y_continuous(limits = c(y_min, y_max), breaks = p_breaks) +\n      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n      facet_wrap(~label, ncol = 1)\n  } else {\n    ggplot() +\n      geom_ribbon(data = dat, aes(x, ymin = Dx_rev_min, ymax = Dx_rev_max, group = interaction(name, taxid)), alpha = 0.6, fill = \"orange\") +\n      geom_path(data = dat, aes(x, Dx_rev, group = interaction(name, taxid)), color = \"black\") +\n      geom_point(data = dat, aes(x, f_rev), alpha = .50, size = 2, fill = \"black\") +\n      theme_test() +\n      xlab(\"Position\") +\n      ylab(\"Frequency\") +\n      scale_x_continuous(trans = \"reverse\") +\n      scale_y_continuous(limits = c(y_min, y_max), position = \"right\", breaks = p_breaks) +\n      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n      facet_wrap(~label, ncol = 1)\n  }\n}\n\nWe load our metaDMG output data (tsv file) and generate the damage plots as seen in Figure 12.10 using the function get-damage.\n\ndf &lt;- read.csv(\"metadmg/concatenated_metaDMGfinal.tsv\",  sep = \"\\t\")\n\n#Rename sample column\ncolnames(df)[colnames(df) == 'filename'] &lt;- 'sample'\n\n#Modify sample name with short names\ndf$sample[df$sample == \"VM-11_aggregated_results.stat\"] &lt;- \"VM-11\"\ndf$sample[df$sample == \"VM-14_aggregated_results.stat\"] &lt;- \"VM-14\"\ndf$sample[df$sample == \"VM-15_aggregated_results.stat\"] &lt;- \"VM-15\"\ndf$sample[df$sample == \"VM-17_aggregated_results.stat\"] &lt;- \"VM-17\"\ndf$sample[df$sample == \"VM-19_aggregated_results.stat\"] &lt;- \"VM-19\"\ndf$sample[df$sample == \"VM-3_aggregated_results.stat\"] &lt;- \"VM-3\"\n\n#Setting filtering theshold for ancient reads\nminDMG = 0.02 # filter criteria, plot only taxa above set value\nzfit = 2 # minimum significance, the higher the better, 2 would mean that we estimante the damage with 95% confidence. \nMinLength = 35 # minimum mean readlength, while we set a hard filter initially while trimming, we would like the mean readlength to be 35 or higher. \nreads = 200 # number of reads required depends on the amount of damage and the significance\n\n#Subsetting only animals and plants, at the genus level, number of reads &gt; 200.\ndt1 &lt;- df %&gt;% filter(A &gt; minDMG, nreads &gt;= reads, mean_rlen &gt;= MinLength, Zfit  &gt; zfit, grepl(\"\\\\bgenus\\\\b\", rank), !grepl(\"Bacteria\",taxa_path))\n\n#deamination plot with facet wrap per each taxon in a sample\ntax_g_list &lt;- unique(dt1$name)\nnrank &lt;- \"rank\" # Replace with the actual rank column name\n\nX &lt;- tax_g_list \npurrr::map(tax_g_list, function(X, nrank) {\n  sel_tax &lt;- dt1 %&gt;%\n    rename(label = sample) %&gt;%\n    #filter(Genome_Id %in% (tax_superg %&gt;% filter(Taxa_Super_Groups == X) %&gt;% pull(Genome_Id))) %&gt;%\n    filter(name == X) %&gt;%\n    filter(rank == rank) %&gt;%\n    select(name, label) %&gt;%\n    distinct() %&gt;%\n    arrange(name)\n  if (nrow(sel_tax) &gt; 0) {\n    n_readsa &lt;- dt1 %&gt;%\n      inner_join(sel_tax) %&gt;%\n      filter(rank == rank) %&gt;%\n      pull(nreads) %&gt;%\n      sum()\n    ggpubr::ggarrange(plotlist = list(\n      get_dmg_decay_fit(df = dt1 %&gt;% rename(label = sample) %&gt;% inner_join(sel_tax) %&gt;% filter(rank == rank), orient = \"fwd\", y_max = 0.70) +\n        ggtitle(paste0(X, \" nreads=\", n_readsa, \" Forward\")),\n      get_dmg_decay_fit(df = dt1 %&gt;% rename(label = sample)  %&gt;% inner_join(sel_tax) %&gt;% filter(rank == rank), orient = \"rev\", y_max = 0.70) +\n        ggtitle(paste0(X, \" nreads=\", n_readsa, \" Reverse\"))\n    ), align = \"hv\")\n    ggsave(paste0(X, \"-dmg.pdf\"), plot = last_plot(), width = 8, height = 4)\n  }\n})\n\n\n\n\n\n\n\nFigure 12.10: Deamination patterns for sheep (Ovis) and beech (Fagus) reads.\n\n\n\n\n\n\n12.5.6 Amplitude of damage vs Significance\nWe provide an R script to investigate the main statistics.\nHere we visualise the amplitude of damage (A) and its significance (Zfit), for the full dataset but filtering it to a minimum of 100 reads and at the genus level (Figure 12.11).\n\n#Subset dataset at genus level\ndt2 &lt;- df %&gt;% filter(grepl(\"\\\\bgenus\\\\b\", rank))\n\n#plotting  amplitude of damage vs its significance \np1 &lt;- ggplot(dt2, aes(y=A, x=Zfit)) + \n  geom_point(aes(size=nreads), col =\"dark green\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust =1)) +\n  scale_size_continuous(labels = function(x) format(x, scientific = FALSE)) +\n  xlab(\"significance\") + ylab(\"damage\") + theme_minimal()\np1\n\n#Plotting only animals and plants at genus level\n#subset dataset\ndt3 &lt;- dt2 %&gt;% filter(nreads &gt; 100, grepl(\"\\\\bgenus\\\\b\", rank), grepl(\"Metazoa\", taxa_path) | grepl(\"Viridiplant\", taxa_path))\n\n#Adding factor column for Kingdom\ndt3 &lt;- dt3 %&gt;% \n  mutate(Kingdom =   # creating our new column\n           case_when(grepl(\"Viridiplant\", taxa_path) ~ \"Viridiplantae\",\n                     grepl(\"Metazoa\",taxa_path) ~ \"Metazoa\"))\n\n#Plotting  amplitude of damage vs its significance \np2 &lt;- ggplot(dt3, aes(y=A, x=Zfit)) + \n  geom_point(aes(size=nreads, col=Kingdom)) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust =1)) +\n  scale_color_manual(values = c(\"#8B1A1A\", \"#458B00\"))+\n  scale_size_continuous(labels = function(x) format(x, scientific = FALSE)) +\n  xlab(\"significance\") + ylab(\"damage\") + theme_minimal()\np2\n\n#Save the plots as a PDF file\nggsave(\"p1.pdf\", plot = p1, width = 8, height = 6)\nggsave(\"p2.pdf\", plot = p2, width = 8, height = 6)\n\n\n\n\n\n\n\nFigure 12.11: Amplitude of damage (A) vs significance (Zfit) for animals and plants.\n\n\n\n\n\n12.5.7 Amplitude of damage and mean fragment length through time\nHere we visualise the amplitude of damage (A) and the mean length of the fragments (mean_rlen) by depth and by date (BP) for the full dataset but filtering it to a minimum of 100 reads and at the genus level (Figure 12.12).\n\n#Import the metadata \ndepth_data &lt;- read.csv (\"metadmg/figures/depth_data.csv\", sep = \",\")\nView (depth_data)\n\n#Merge context_data and depth_data with dataframe (adding new column for dates BP)\ndf$new &lt;- depth_data$Date_BP[match(df$sample, depth_data$Sample_ID)]\nnames(df)[names(df) == 'new'] &lt;- 'Date_BP'\n\n# Convert Date_BP columns to factors (categorical variable) \ndf$Date_BP &lt;- as.factor(df$Date_BP)\n\n#Plotting damage (A) by period (dates BP)\np3a&lt;- dt4 %&gt;%\n  mutate(Date_BP = fct_relevel(Date_BP,\n                             \"6100\",\"5300\",\"4100\",\"3900\",\"3000\", \"800\")) %&gt;%\n  ggplot(aes(x=A, y=Date_BP))+ \n  geom_boxplot(aes(x=A, y=Date_BP, fill = sample))+\n  geom_point(aes(fill = sample), size = 3, shape = 21, color = \"black\", stroke = .5) +\n  scale_x_continuous(limits = c(0, 0.20), breaks = seq(0, 0.20, by = 0.05)) +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\np3a\n\n#Plotting mean length (mean_rlen) by period (dates BP)\np3b&lt;- dt4 %&gt;%\n  mutate(Date_BP = fct_relevel(Date_BP,\n                             \"6100\",\"5300\",\"4100\",\"3900\",\"3000\", \"800\")) %&gt;%\n  ggplot(aes(x=mean_rlen, y=Date_BP))+ \n  geom_boxplot(aes(x=mean_rlen, y=Date_BP, fill = sample)) +\n  geom_point(aes(fill = sample), size = 3, shape = 21, color = \"black\", stroke = .5) +\n  scale_x_continuous(limits = c(30, 80), breaks = seq(30, 80, by = 10)) +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\np3b\n\n#Combining the plots\np3 &lt;- grid.arrange(p3a, p3b,\n                   ncol = 2, nrow = 1)\np3\n\n#Save plot\nggsave(\"p3.pdf\", plot = p4, width = 10, height = 8)\n\n\n\n\n\n\n\nFigure 12.12: Amplitude of damage (A) and mean fragment length (mean_rlen) through time.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOnce finished examining the plots you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can manually navigate to the folder metadmg/figures/ And click “Open folder” You can double-click on the pdf files to visualise them.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#summary",
    "href": "authentication.html#summary",
    "title": "12  Authentication",
    "section": "12.6 Summary",
    "text": "12.6 Summary\nIn addition, we:\n\nProcessed bam files with metaDMG to generate taxonomic profiles and damage estimates from a metagenomic dataset\nInvestigated some primary statistics of the metaDMG output using R:\n\nDeamination patterns,\nThe amplitude of damage vs significance,\nThe amplitude of damage and\nMean fragment length through time.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#acknowledgments",
    "href": "authentication.html#acknowledgments",
    "title": "12  Authentication",
    "section": "12.7 Acknowledgments",
    "text": "12.7 Acknowledgments\nWe thank Mikkel Winther Pedersen and Antonio Fernandez Guerra for their contribution to the development of the metaDMG section.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#recommended-reading",
    "href": "authentication.html#recommended-reading",
    "title": "12  Authentication",
    "section": "12.8 Recommended Reading",
    "text": "12.8 Recommended Reading\n\nClio Der Sarkissian, Irina M. Velsko, Anna K. Fotakis, Åshild J. Vågene, Alexander Hübner, and James A. Fellows Yates, Ancient Metagenomic Studies: Considerations for the Wider Scientific Community, mSystems 2021 Volume 6 Issue 6 e01315-21.\nWarinner C, Herbig A, Mann A, Fellows Yates JA, Weiß CL, Burbano HA, Orlando L, Krause J. A Robust Framework for Microbial Archaeology. Annu Rev Genomics Hum Genet. 2017 Aug 31;18:321-356. doi: 10.1146/annurev-genom-091416-035526. Epub 2017 Apr 26. PMID: 28460196; PMCID: PMC5581243.\nOrlando, L., Allaby, R., Skoglund, P. et al. Ancient DNA analysis. Nat Rev Methods Primers 1, 14 (2021). https://doi.org/10.1038/s43586-020-00011-0",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#resources",
    "href": "authentication.html#resources",
    "title": "12  Authentication",
    "section": "12.9 Resources",
    "text": "12.9 Resources\n\nKrakenUniq: Breitwieser, F. P., Baker, D. N., & Salzberg, S. L. (2018). KrakenUniq: confident and fast metagenomics classification using unique k-mer counts. Genome Biology, vol. 19(1), p. 1–10. http://www.ec.gc.ca/education/default.asp?lang=En&n=44E5E9BB-1\nSamtools: Heng Li, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, Richard Durbin, 1000 Genome Project Data Processing Subgroup, The Sequence Alignment/Map format and SAMtools, Bioinformatics, Volume 25, Issue 16, 15 August 2009, Pages 2078–2079, https://doi.org/10.1093/bioinformatics/btp352\nPMDtools: Skoglund P, Northoff BH, Shunkov MV, Derevianko AP, Pääbo S, Krause J, Jakobsson M. Separating endogenous ancient DNA from modern day contamination in a Siberian Neandertal. Proc Natl Acad Sci U S A. 2014 Feb 11;111(6):2229-34. doi: 10.1073/pnas.1318934111. Epub 2014 Jan 27. PMID: 24469802; PMCID: PMC3926038.\npyDamage: Borry M, Hübner A, Rohrlach AB, Warinner C. PyDamage: automated ancient damage identification and estimation for contigs in ancient DNA de novo assembly. PeerJ. 2021 Jul 27;9:e11845. doi: 10.7717/peerj.11845. PMID: 34395085; PMCID: PMC8323603.\nmetaDMG-cpp: https://github.com/metaDMG-dev/metaDMG-cpp\nngsLCA: Wang, Y., Korneliussen, T. S., Holman, L. E., Manica, A., & Pedersen, M. W. (2022). ngs LCA—A toolkit for fast and flexible lowest common ancestor inference and taxonomic profiling of metagenomic data. Methods in Ecology and Evolution, 13(12), 2699-2708. (https://github.com/miwipe/ngsLCA)",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "authentication.html#references",
    "href": "authentication.html#references",
    "title": "12  Authentication",
    "section": "12.10 References",
    "text": "12.10 References\n\n\n\n\nMichelsen, Christian, Mikkel Winther Pedersen, Antonio Fernandez-Guerra, Lei Zhao, Troels C. Petersen, and Thorfinn Sand Korneliussen. 2022. “metaDMG – a Fast and Accurate Ancient DNA Damage Toolkit for Metagenomic Data,” December. https://doi.org/10.1101/2022.12.06.519264.\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579.\n\n\nWang, Yucheng, Thorfinn Sand Korneliussen, Luke E Holman, Andrea Manica, and Mikkel Winther Pedersen. 2022. “NgsLCA—a Toolkit for Fast and Flexible Lowest Common Ancestor Inference and Taxonomic Profiling of Metagenomic Data.” Methods in Ecology and Evolution 13 (12): 2699–2708. https://doi.org/10.1111/2041-210x.14006.\n\n\nZampirolo, Giulia, Luke E. Holman, Rikai Sawafuji, Michaela Ptáková, Lenka Kovačiková, Petr Šı́da, Petr Pokorný, Mikkel Winther Pedersen, and Matthew Walls. 2023. “Early Pastoralism in Central European Forests: Insights from Ancient Environmental Genomics.” bioRxiv. https://doi.org/10.1101/2023.12.01.569562.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authentication</span>"
    ]
  },
  {
    "objectID": "contamination.html",
    "href": "contamination.html",
    "title": "13  Contamination",
    "section": "",
    "text": "13.1 Decontamination\nModern contamination is one of the major problems in ancient metagenomics analysis. Large fractions of modern bacterial, animal or human DNA in metagenomic samples can lead to false biological and historical conclusions. A lot of scientific literature is dedicated to this topic, and comprehensive tables and sources of potential contamination (e.g. animal and bacterial DNA present in PCR reagents) are available.\nA good practice to discriminate between endogenous and contaminant organisms is to sequence negative controls, so-called blanks. Organisms detected on blanks, like the microbial genera reported in the table below, can substantially facilitate making more informed decision about true metagenomic profile of a sample. Nevertheless, the table below may seem rather conservative since in addition to well-known environmental contaminants as Burkholderia and Pseudomonas it includes also human oral genera as Streptococcus, which are probably less likely to be of environmental origin.\nIt is typically assumed that an organism found on a blank has a lower confidence to be endogenous to the studied metagenomic sample, and sometimes it is even excluded from the downstream analysis as an unreliable hit. Despite there are attempts to automate filtering out modern contaminants (we will discuss them below), decontamination process still remains to be a tedious manual work where each candidate should be carefully investigated from different contexts in order to prove its ancient and endogenous origin.\nIf negative control samples (blanks) are available, contaminating organisms can be detected by comparing their abundances in the negative controls with true samples. In this case, contaminant organisms stand out by their high prevalence in both types of samples if one simply plots mean across samples abundance of each detected organism in true samples and negative controls against each other as in the figure below.\nFirst move back into the KrakenUniq folder and load R.\nThen in here we can compare between true samples and negative controls\nOnce finished examining the plot you can quit R\nIn the figure above, one point indicates an organism detected in a group of metagenomic samples. The points highlighted by red have high abundance in negative control samples, and therefore they are likely contamiannts.\nIn addition to PCR reagents and lab contaminants, reference databses can also be contaminanted by various, often microbial, organisms. A typical example that when screening environmental or sedimentary ancient DNA samples, a fish Cyprinos carpio can pop up if adapter trimming procedure was not successful for some reason.\nIt was noticed that the Cyprinos carpio reference genome available at NCBI contains large fraction of Illumina sequncing adapters. Therefore, appearence of this organism in your analysis may falsely lead your conclusion toward potential lake or river present in the excavation site.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#decontamination",
    "href": "contamination.html#decontamination",
    "title": "13  Contamination",
    "section": "",
    "text": "cd /&lt;path&gt;/&lt;to&gt;/contamination/krakenuniq\nR\n\nsamples&lt;-read.delim(\"krakenuniq_abundance_matrix/krakenuniq_abundance_matrix.txt\",header=TRUE,\nrow.names = 1, check.names = FALSE, sep = \"\\t\")\n## The blanks abundance matrix has already been made for you\ncontrols&lt;-read.delim(\"blank_krakenuniq_abundance_matrix.txt\",header=TRUE,\nrow.names = 1, check.names = FALSE, sep = \"\\t\")\n\ndf &lt;- merge(samples, controls, all = TRUE, by = \"row.names\")\nrownames(df)&lt;-df$Row.names; df$Row.names &lt;- NULL; df[is.na(df)] &lt;- 0\n\ntrue_sample &lt;- subset(df,select=colnames(df)[!grepl(\"control\",colnames(df))])\nnegative_control &lt;- subset(df,select=colnames(df)[grepl(\"control\",colnames(df))])\n\nplot(log10(rowMeans(true_sample)+1) ~ log10(rowMeans(negative_control)+1),\nxlab = \"Log10 ( Negative controls )\", ylab = \"Log10 ( True samples )\",\nmain = \"Organism abundance in true samples vs. negative controls\",\npch = 19, col = \"blue\")\n\npoints(log10(rowMeans(true_sample)+1)[(log10(rowMeans(true_sample)+1) &gt; 1) &\n(log10(rowMeans(negative_control)+1)&gt;1)] ~\nlog10(rowMeans(negative_control)+1)[(log10(rowMeans(true_sample)+1) &gt; 1) &\n(log10(rowMeans(negative_control)+1)&gt;1)], pch = 19, col = \"red\")\n\ntext(log10(rowMeans(true_sample)+1)[(log10(rowMeans(true_sample)+1) &gt; 1) &\n(log10(rowMeans(negative_control)+1) &gt; 1)] ~\nlog10(rowMeans(negative_control)+1)[(log10(rowMeans(true_sample)+1) &gt; 1) &\n(log10(rowMeans(negative_control)+1)  &gt;1)],\nlabels = rownames(true_sample)[(log10(rowMeans(true_sample)+1) &gt; 1) &\n(log10(rowMeans(negative_control)+1) &gt; 1)], pos = 4)\n\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\n\n\n\n\n\n13.1.1 decontam\nLet us now discuss a few available computational approaches to decontaminate metagenomic samples. One of them is decontam R package that offers a simple statistical test for whether a detected organism is likely contaminant. This approach is useful when DNA quantitation data recording the concentration of DNA in each sample (e.g. PicoGreen fluorescent intensity measures) is available. The idea of the decontam is that contaminant DNA is expected to be present in approximately equal and low concentrations across samples, while sample DNA concentrations can vary widely. As a result, the expected frequency of contaminant DNA varies inversely with total sample DNA concentration (red line in the figure below), while the expected frequency of non-contaminant DNA does not (blue line).\n\n\n\n13.1.2 recentrifuge\nAnother popular tool for detecting contaminating microorganisms is Recentrifuge. It works as a classifier that is trained to recognize contaminant microbial organisms. In case of Recentrifuge, one has to use blanks or other negative controls and provide microbial names and abundances on the blanks in order to train Recentrifuge to recognize endogenous vs. contaminant sources.\n\n\n13.1.3 cuperdec\nIf one wants to assess the degree of contamination for each sample, there is a handy tool cuperdec, which is an R package that allows a quick comparison of microbial profiles in a query metagenomic sample against a database. The idea of cuperdec is to rank organisms in each sample by their abundance and then using an “expanding window” approach to compute their enrichment in a reference database that contains a comprehensive list of microbial organisms which are specific to a tissue / environment in question. The tool produces so-called Cumulative Percent Decay curves that aim to represent the level of endogenous content of microbiome samples, such as ancient dental calculus, to help to identify samples with low levels of preservation that should be discarded for downstream analysis.\nFirst change into the cuperdec directory, and load R\ncd /&lt;path&gt;/&lt;to&gt;/contamination/cuperdec\nThen we can load the files, generate the decay curves, set a preservation threshold cut-off, and plot the result.\nLoad R as usual.\nR\nThen load the libraries and run the typical cuperdec workflow.\nlibrary(\"cuperdec\"); library(\"magrittr\"); library(\"dplyr\")\n\n# Load database (in this case an internal 'test' oral database from the package)\ndata(cuperdec_database_ex)\ndatabase &lt;- load_database(cuperdec_database_ex, target = \"oral\") %&gt;% print()\n\n# Load abundance matrix and metadata\nraw_table &lt;- read.delim(\"../krakenuniq/krakenuniq_abundance_matrix/krakenuniq_abundance_matrix.txt\", row.names=1) %&gt;% as_tibble(rownames='Taxon')\ntaxatable &lt;- load_taxa_table(raw_table)  %&gt;% print()\nmetadata &lt;- as_tibble(data.frame(Sample = unique(taxatable$Sample),\nSample_Source = \"Oral\"))\n\n# Compute cumulative percent decay curves, filter and plot results\ncurves &lt;- calculate_curve(taxatable, database = database) %&gt;% print()\nfilter_result &lt;- simple_filter(curves, percent_threshold = 50) %&gt;% print()\nplot_cuperdec(curves, metadata = metadata, filter_result)\nOnce finished examining the plot you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\nIn the figure above, one curve represents one sample, and the red curves have a very high amount of contamination and very low amount of endogenous DNA. These samples might be considered to be dropped from the downstream analysis.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#microbial-source-tracking",
    "href": "contamination.html#microbial-source-tracking",
    "title": "13  Contamination",
    "section": "13.2 Microbial source tracking",
    "text": "13.2 Microbial source tracking\nFor the case of ancient microbiome profiling, in addition to traditional inspection of the list of detected organisms and comparing it with the ones detected on blanks, we can use tools that make a prediction on what environment the detected organisms most likely come from.\n\n13.2.1 Sourcetracker\nThe most popular and widely used tool is called SourceTracker. SourceTracker is a Bayesian version of the Gaussian Mixture Model (GMM) clustering algorithm that is trained on a user-supplied reference data called Sources, i.e. different classes such as Soil or Human Oral or Human Gut microbial communities etc., and then it can estimate proportion / contribution of each of these sources the users actual samples called Sinks.\n\nOriginally, SourceTracker was developed for 16S data, i.e. using only 16S ribosomal RNA genes, but it can be easily trained using also shotgun metagenomics data, which was demonstrated in its metagenomic extension called mSourceTracker and its faster and more scalable version FEAST. The input data for SourceTracker are metadata, i.e. each sample has to have “source” or “sink” annotation as well as environmental label (e.g. Oral, Gut, Soil etc.), and microbial abundances (OTU abundances) quantified in some way, for example through QIIME pipeline, MetaPhlan or Kraken. The SourceTracker R script can be downloaded from https://github.com/danknights/sourcetracker.\n\n\n\n\n\n\nSelf guided: data preparation\n\n\n\n\n\nIn addition to the input files (already prepared for you), you will also need to clone the Sourcetracker 1 R code repository (unfortunately it’s not available as a package)\n\ncd /&lt;path&gt;/&lt;to&gt;/contamination/sourcetracker/\ngit clone https://github.com/danknights/sourcetracker.git\n\n\n\n\n\n\nPatching sourcetracker for newer R versions\n\n\n\nUnfortunately Sourcetracker’s original R code was done a very long time ago, and has resulted in some incompatiblities with more recent versions of R.\nIf using a more recent version of R (such as in this chapter’s conda environment), you will need to perform the following ‘patching’ of the code, to prevent an error.\nRun the following command to comment out the offending lines (538, 539), where R’s behaviour in later versions of a fundamental validation function has changed.\n\nsed -i '538,539s/^/#/' sourcetracker/src/SourceTracker.r\n\n\n\n\n\nSourcetracker expects two input data frames: metadata with at least sample name, environment and source / sink labels, and abundance matrix. Note that source and sink metadata and abundances have to be merged together prior to using SourceTracker. Here we are going to use data from the Human Microbiome Project (HMP) as sources, and we are going to merge the HMP data with the sink samples into single OTU table and meta-data table.\ncd /&lt;path&gt;/&lt;to&gt;/contamination/sourcetracker/\n\n## Load R\nR\nThen in R we can load the HMP source files, and our ‘sink samples’ from the KrakenUniq matrix we made earlier, and clean them up to make them compatible for Sourcetracker.\n\n\n\n\n\n\nNote\n\n\n\nWe’ve included the Sourcetracker R script in the repository for you, as it’s not available on conda.\n\n\n## Load files\notus_hmp &lt;- read.delim(\"otus_hmp.txt\", header = TRUE, row.names = 1, sep = \"\\t\")\nmeta_hmp &lt;- read.delim(\"meta_hmp.txt\", header = TRUE, row.names = 1, sep = \"\\t\")\notus_sink&lt;-read.delim(\"../krakenuniq/krakenuniq_abundance_matrix/krakenuniq_abundance_matrix.txt\",header=T,row.names=1,sep=\"\\t\")\n\n## Join source and sink tables into one\notus &lt;- merge(otus_hmp, otus_sink, all = TRUE, by = \"row.names\")\n\n## Put OTU IDs as row names and replace NAs with 0\nrownames(otus) &lt;- otus$Row.names; otus$Row.names &lt;- NULL; otus[is.na(otus)] &lt;- 0\n\n## Set column and row names for meta\nmeta_sink &lt;- data.frame(ID = colnames(otus_sink), Env = \"Unknown\", SourceSink = \"sink\")\nrownames(meta_sink) &lt;- meta_sink$ID; meta_sink$ID&lt;-NULL\nmetadata &lt;- rbind(meta_hmp, meta_sink)\n\n## Further value cleanups and sample filtering to remove not useful HMP samples\notus &lt;- as.data.frame(t(as.matrix(otus)))\notus[otus &gt; 0] &lt;- 1; otus &lt;- otus[rowSums(otus)!=0,]\nmetadata&lt;-metadata[as.character(metadata$Env)!=\"Vaginal\",]; envs &lt;- metadata$Env\ncommon.sample.ids &lt;- intersect(rownames(metadata), rownames(otus))\notus &lt;- otus[common.sample.ids,]; metadata &lt;- metadata[common.sample.ids,]\nNext, training SourceTracker on source samples and running predictions on sink samples can be done using following commands:\n# Train SourceTracker on sources (HMP) and run predictions on sinks\nsource('sourcetracker/src/SourceTracker.r')\ntrain.ix &lt;- which(metadata$SourceSink=='source')\ntest.ix &lt;- which(metadata$SourceSink=='sink')\nst &lt;- sourcetracker(otus[train.ix,], envs[train.ix])\nresults &lt;- predict(st, otus[test.ix,], alpha1 = 0.001, alpha2 = 0.001)\nFinally, we can plot SourceTracker environment inference in the form of barcharts as follows:\n# Sort SourceTracker proportions for plotting\nprops &lt;- results$proportions\nprops &lt;- props[order(-props[,\"Oral\"]),]\nresults$proportions &lt;- props\n\n# Prepare SourceTracker output for plotting\nname &lt;- rep(rownames(results$proportions), each = 4)\nvalue &lt;- as.numeric(t(results$proportions))\nlabels &lt;- c(\"Gut\",\"Oral\",\"Skin\",\"Unknown\"); condition&lt;-rep(labels, length(test.ix))\ndata &lt;- data.frame(name, condition, value)\n\n# Plot SourceTracker inference as a barplot\nlibrary(\"ggplot2\")\nggplot(data, aes(fill=condition, y=value, x = reorder(name, seq(1:length(name))))) +\ngeom_bar(position = \"fill\", stat = \"identity\") +\ntheme(axis.text.x = element_text(angle = 90, size=5, hjust=1, vjust=0.5)) +\nxlab(\"Sample\") + ylab(\"Fraction\")\nOnce finished examining the plot you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\n\n\n\n\n\nFigure 13.1: Stacked barchart of each sample (sinks) with the estimated fraction of each source ‘sample group’ that each sink is made up of. Note that the X-axis sorting of samples may change between each person, thus your plot may not match exactly the same as the one here.\n\n\n\nIn the figure above (Figure 13.1) the SourceTracker was trained on Human Microbiome Project (HMP) data, and was capable of predicting the fractions of oral, gut, skin or other microbial composition on the query sink samples. In a similar way, environmental soil or marine microbes can be used as Sources. In this way, environmental percentage of contamination can be detected per sample.\nA drawback of SourceTracker, mSourceTracker and FEAST is that they require a microbial abundance table after a taxonomic classification with e.g. QIIME or Kraken has been performed. Such taxonomic classification can be biased since it is computed against a reference database with known taxonomic annotation. In contrast, a novel microbial source tracking tool decOM aims at moving away from database-dependent methods and using unsupervised approaches exploiting read-level sequence composition.\n\n\n\n13.2.2 deCOM\ndecOM uses kmtricks to compute a matrix of k-mer counts on raw reads (FASTQ files) from source samples, and then uses the source k-mer abundance matrix for looking up k-mer composition of sink samples. This allows decOM to calculate microbial contributions / fractions from the sources. For example, for estimating contributions from ancient Oral (aOral), modern Oral (mOral), Skin and Sediment / Soil environments one can use an already computed source matrix from here https://github.com/CamilaDuitama/decOM/ and provide it as a -p_sources parameter.\n\n\n\n\n\n\nSelf guided: data preparation\n\n\n\n\n\nIf doing a self-guided tutorial, you will need to download a rather large pre-built database.\n## In the directory\n## Note that decOM does not like dots in names so make symlinks\nfor i in ../rawdata/*trimmed.fastq.gz; do\n  ln -s \"$i\" \"${i/.trimmed/_trimmed}\"\ndone\n\n# Prepare input fof-files that have a key - value format\nfor i in {1..10}; do\n  echo \"sample${i}_trimmed : sample${i}_trimmed.fastq.gz\" &gt; sample${i}_trimmed.fof\n  echo sample${i}_trimmed &gt;&gt; FASTQ_NAMES_LIST.txt;\ndone\n\n# Download pre-built kmer-matrix of sources (aOral, mOral, Sediment/Soil, Skin)\nwget https://zenodo.org/record/6513520/files/decOM_sources.tar.gz\ntar -xf decOM_sources.tar.gz\n\n\n\nThe following example command is how you would execute deCOM. However as this requires a very large database file and thus large memory requirements, therefore we have already made available for you the output files from this step.\n\n\n\n\n\n\nExample command - do not run!\n\n\n\n# DO NOT RUN!: Run decOM predictions\ndecOM -p_sources decOM_sources/ -p_sinks FASTQ_NAMES_LIST.txt -p_keys decOM/FASTQ -mem 900GB -t 15\n\n\nYou can find the relevant output files for the rest of this chapter in the following directory already prepared for you i.e.the output of deCOM with our simulated data from the command above.\ncd /&lt;path&gt;/&lt;to&gt;/contamination/decom\n\n\n\n\n\n\nNote\n\n\n\nThe entire output from running deCOM on the simulated data was 29GB!\n\n\nIn the example command line used to generate, the -p_sinks parameter provides a list of sink samples, for example SRR13355807.\nThe sink fastq-files are placed together with keys fof-files containing the mapping between fastq file names and locations of the fastq-files, for example SRR13355807 : SRR13355807.fastq.gz. The contributions from the sources to the sink samples, which are recorded in the decOM_output.csv output file (which is in the contamination/decom directory already for you), can then be processed and plotted as follows:\nLoad R\nR\nThen we can make a plot\ndf&lt;-read.csv(\"decOM_output.csv\", check.names=FALSE)\n\nresult &lt;- subset(df, select = c(\"Sink\", \"Sediment/Soil\", \"Skin\", \"aOral\",\n\"mOral\", \"Unknown\"))\nrownames(result) &lt;- result$Sink; result$Sink &lt;- NULL\nresult &lt;- result / rowSums(result)\nresult&lt;-result[order(-result$aOral),]\n\nname &lt;- rep(rownames(result), each = 5); value &lt;- as.numeric(t(result))\ncondition &lt;- rep(c(\"Sediment/Soil\",\"Skin\",\"aOral\",\"mOral\",\"Unknown\"),\ndim(result)[1])\ndata &lt;- data.frame(name, condition, value)\n\nlibrary(\"ggplot2\"); library(\"viridis\")\nggplot(data, aes(fill=condition, y=value, x=reorder(name,seq(1:length(name))))) +\n  geom_bar(position = \"fill\", stat = \"identity\") +\n  theme(axis.text.x = element_text(angle=90, size = 5, hjust = 1, vjust = 0.5)) +\n  xlab(\"Sample\") + ylab(\"Fraction\")\nOnce finished examining the plot you can quit R\n## Press 'n' when asked if you want to save your workspace image.\nquit()\n\ndecOM has certain advantages compared to SourceTracker as its is a taxonomic classification / database free approach. However, it also appears to be very sensitive to the particular training / source data set. In the example above it can be seen that the microbial source tracking of sink samples is very much dominated by the Oral community, which was the training / source data set.",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#optional-clean-up",
    "href": "contamination.html#optional-clean-up",
    "title": "13  Contamination",
    "section": "13.3 (Optional) clean-up",
    "text": "13.3 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/contamination directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -rf /&lt;PATH&gt;/&lt;TO&gt;/contamination*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name contamination --all -y",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#summary",
    "href": "contamination.html#summary",
    "title": "13  Contamination",
    "section": "13.4 Summary",
    "text": "13.4 Summary\nIn this chapter we have learned that:\n\nNegative controls are important for disentangling ancient / endogenous from modern / exogenous contamination\nMicrobial source tracking is another layer of evidence that can facilitate interpretation of ancient metagenomic findings",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#questions-to-think-about",
    "href": "contamination.html#questions-to-think-about",
    "title": "13  Contamination",
    "section": "13.5 Questions to think about",
    "text": "13.5 Questions to think about\n\nWhat is a false-positive microbial finding and how can we recognize it?\nWhat is contamination and how can it bias ancient metagenomic analysis?\nWhat is a negative (blank) control sample and why is it useful to have?\nWhat is microbial source tracking and how can it help with decontamination?",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#resources",
    "href": "contamination.html#resources",
    "title": "13  Contamination",
    "section": "13.6 Resources",
    "text": "13.6 Resources\n\nSourceTracker: Knights D, Kuczynski J, Charlson ES, Zaneveld J, Mozer MC, Collman RG, Bushman FD, Knight R, Kelley ST. Bayesian community-wide culture-independent microbial source tracking. Nat Methods. 2011 Jul 17;8(9):761-3. doi: 10.1038/nmeth.1650. PMID: 21765408; PMCID: PMC3791591.\ndeCOM: https://www.biorxiv.org/content/10.1101/2023.01.26.525439v1, doi: https://doi.org/10.1101/2023.01.26.525439\naMeta: https://www.biorxiv.org/content/10.1101/2022.10.03.510579v1, doi: https://doi.org/10.1101/2022.10.03.510579\nBowtie2: Langmead, B., Salzberg, S. Fast gapped-read alignment with Bowtie 2. Nat Methods 9, 357–359 (2012). https://doi.org/10.1038/nmeth.1923\ncuperdec: https://cran.r-project.org/web/packages/cuperdec/index.html\ndecontam: https://www.bioconductor.org/packages/release/bioc/html/decontam.html",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "contamination.html#references",
    "href": "contamination.html#references",
    "title": "13  Contamination",
    "section": "13.7 References",
    "text": "13.7 References",
    "crumbs": [
      "Ancient Metagenomics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Contamination</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html",
    "href": "genome-mapping.html",
    "title": "14  Genome Mapping",
    "section": "",
    "text": "14.1 Introduction\nOne way of reconstructing genomic information from DNA sequencing reads is mapping/aligning them to a reference genome. This allows for identification of differences between the genome from your sample and the reference genome. This information can be used for example for comparative analyses such as in phylogenetics. For a detailed explanation of the read alignment problem and an overview of concepts for solving it, please see https://doi.org/10.1146/annurev-genom-090413-025358.\nIn this session we will map two samples to the Yersinia pestis (plague) genome using different parameter sets. We will do this “manually” in the sense that we will use all necessary commands one by one in the terminal. These commands usually run in the background when you apply DNA sequencing data processing pipelines.\nWe will be using the Burrows-Wheeler Aligner (Li et al. 2009 – http://bio-bwa.sourceforge.net). There are different algorithms implemented for different types of data (e.g. different read lengths). Here, we use BWA backtrack (bwa aln), which is suitable for Illumina sequences up to 100bp. Other algorithms are bwa mem and bwa sw for longer reads.",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#reference-genome",
    "href": "genome-mapping.html#reference-genome",
    "title": "14  Genome Mapping",
    "section": "14.2 Reference Genome",
    "text": "14.2 Reference Genome\nFor mapping we need a reference genome in FASTA format. Ideally we use a genome from the same species that our data relates to or, if not available, a closely related species. The selection of the correct reference genome is highly relevant. E.g. if the chosen genome differs too much from the organism the data relates to, it might not be possible to map most of the reads. Reference genomes can be retrieved from comprehensive databases such as NCBI.\nIn your directory, you can find 2 samples and your reference. As a first step we will index our reference genome (make sure you are inside your directory).\nThe first index we will generate is for bwa.\nbwa index YpestisCO92.fa\nThe second index will be used by the genome browser we will apply to our results later on:\nsamtools faidx YpestisCO92.fa\nWe need to build a third index that is necessary for the genotyping step, which comes later after mapping:\npicard CreateSequenceDictionary R=YpestisCO92.fa",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#mapping-parameters",
    "href": "genome-mapping.html#mapping-parameters",
    "title": "14  Genome Mapping",
    "section": "14.3 Mapping Parameters",
    "text": "14.3 Mapping Parameters\nWe will be using bwa aln, but we need to specify parameters. For now we will concentrate on the “seed length” and the “maximum edit distance”. We will use the default setting for all other parameters during this session. The choice of the right parameters depend on many factors such as the type of data and the specific use case. One aspect is the mapping sensitivity, i.e. how different a read can be from the chosen reference and still be mapped. In this context we generally differentiate between strict and lenient mapping parameters.\nAs many other mapping algorithms bwa uses a so-called “seed-and-extend” approach. I.e. it initially maps the first N nucleotides of each read to the genome with relatively few mismatches and thereby determines candidate positions for the more time-intensive full alignment.\nA short seed length will generate more such candidate positions and therefore mapping will take longer, but it will also be more sensitive, i.e. there can be more differences between the read and the genome. Long seeds are less sensitive but the mapping procedure is faster.\nIn this session we will use the following two parameter sets:\nLenient\nAllow for more mismatches → -n 0.01\nShort seed length → -l 16\nStrict\nAllow for less mismatches → -n 0.1\nLong seed length → -l 32\nWe will be working with pre-processed files (sample1.fastq.gz, sample2.fastq.gz), i.e. any quality filtering and removal of sequencing adapters is already done.\nWe will map each file once with lenient and once with strict parameters. For this, we will make 4 separate directories, to avoid mixing up files:\nmkdir sample1_lenient sample2_lenient sample1_strict sample2_strict",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#mapping-sample1",
    "href": "genome-mapping.html#mapping-sample1",
    "title": "14  Genome Mapping",
    "section": "14.4 Mapping Sample1",
    "text": "14.4 Mapping Sample1\nLet’s begin with a lenient mapping of sample1.\nGo into the corresponding folder:\ncd sample1_lenient\nPerform the bwa alignment, here for sample1, and specify lenient mapping parameters:\nbwa aln -n 0.01 -l 16 ../YpestisCO92.fa ../sample1.fastq.gz &gt; reads_file.sai\nProceed with writing the mapping in sam format (https://en.wikipedia.org/wiki/SAM_(file_format)):\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample1.fastq.gz &gt; reads_mapped.sam\nNote that we have specified the sequencing platform (Illumina) by creating a so-called “Read Group” (-r). This information is used later during the genotyping step.\nConvert SAM file to binary format (BAM file):\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\nFor processing of sam and bam files we use SAMtools (Li et al. 2009 – http://samtools.sourceforge.net/).\n-b specifies to output in BAM format. (-S specifies input is SAM, can be omitted in recent versions.)\nNow we sort the bam file → Sort alignments by leftmost coordinates:\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\nThe sorted bam file needs to be indexed → more efficient for further processing:\nsamtools index reads_mapped_sorted.bam\nDeduplication → Removal of reads from duplicated fragments:\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\nsamtools index reads_mapped_sorted_dedup.bam\nDuplicated reads are usually a consequence of amplification of the DNA fragments in the lab. Therefore, they are not biologically meaningful.\nWe have now completed the mapping procedure. Let’s have a look at our mapping results:\nsamtools view reads_mapped_sorted_dedup.bam | less -S\n(exit by pressing q)\nWe can also get a summary about the number of mapped reads. For this we use the samtools idxstats command (http://www.htslib.org/doc/samtools-idxstats.html):\nsamtools idxstats reads_mapped_sorted_dedup.bam",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#genotyping",
    "href": "genome-mapping.html#genotyping",
    "title": "14  Genome Mapping",
    "section": "14.5 Genotyping",
    "text": "14.5 Genotyping\nThe next step we need to perform is genotyping, i.e. the identification of all SNPs that differentiate the sample from the reference. For this we use the Genome Analysis Toolkit (GATK) (DePristo et al. 2011 – http://www.broadinstitute.org/gatk/)\nIt uses the reference genome and the mapping as input and produces an output in Variant Call Format (VCF) (https://en.wikipedia.org/wiki/Variant_Call_Format).\nPerform genotyping on the mapping file:\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\nLet’s have a look…\ncat mysnps.vcf | less -S\n(exit by pressing q)",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#mapping-and-genotyping-for-the-other-samplesparameters",
    "href": "genome-mapping.html#mapping-and-genotyping-for-the-other-samplesparameters",
    "title": "14  Genome Mapping",
    "section": "14.6 Mapping and Genotyping for the other Samples/Parameters",
    "text": "14.6 Mapping and Genotyping for the other Samples/Parameters\nLet’s now continue with mapping and genotyping for the other samples and parameter settings.\n\n14.6.1 Sample2 Lenient\n\n\n\n\n\n\nNote\n\n\n\nThis is a larger file and lenient mapping takes longer so this file will likely take a few minutes. If you are short on time, proceed with the other sample/parameter settings first and come back to this later if there is time.\n\n\ncd ..\ncd sample2_lenient\n\nbwa aln -n 0.01 -l 16 ../YpestisCO92.fa ../sample2.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample2.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\n\n\n14.6.2 Sample1 Strict\ncd ..\ncd sample1_strict\n\nbwa aln -n 0.1 -l 32 ../YpestisCO92.fa ../sample1.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample1.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf\n\n\n14.6.3 Sample2 Strict\ncd ..\ncd sample2_strict\n\nbwa aln -n 0.1 -l 32 ../YpestisCO92.fa ../sample2.fastq.gz &gt; reads_file.sai\n\nbwa samse -r '@RG\\tID:all\\tLB:NA\\tPL:illumina\\tPU:NA\\tSM:NA' ../YpestisCO92.fa reads_file.sai ../sample2.fastq.gz &gt; reads_mapped.sam\n\nsamtools view -b -S reads_mapped.sam &gt; reads_mapped.bam\n\nsamtools sort reads_mapped.bam &gt; reads_mapped_sorted.bam\n\nsamtools index reads_mapped_sorted.bam\n\nsamtools rmdup -s reads_mapped_sorted.bam reads_mapped_sorted_dedup.bam\n\nsamtools index reads_mapped_sorted_dedup.bam\n\ngatk3 -T UnifiedGenotyper -R ../YpestisCO92.fa -I reads_mapped_sorted_dedup.bam --output_mode EMIT_ALL_SITES -o mysnps.vcf",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#comparing-genotypes",
    "href": "genome-mapping.html#comparing-genotypes",
    "title": "14  Genome Mapping",
    "section": "14.7 Comparing Genotypes",
    "text": "14.7 Comparing Genotypes\nIn order to combine the results from multiple samples and parameter settings we need to agregate and comparatively analyse the information from all the vcf files. For this we will use the software MultiVCFAnalyzer (https://github.com/alexherbig/MultiVCFAnalyzer).\nIt produces various output files and summary statistics and can integrate gene annotations for SNP effect analysis as done by the program SnpEff (Cingolani et al. 2012 - http://snpeff.sourceforge.net/).\nRun MultiVCFAnalyzer on all 4 files at once. First cd one level up (if you type ls you should see your 4 directories, reference, etc.):\ncd ..\nThen make a new directory…\nmkdir vcf_out\n…and run the programme:\nmultivcfanalyzer NA YpestisCO92.fa NA vcf_out F 30 3 0.9 0.9 NA sample1_lenient/mysnps.vcf sample1_strict/mysnps.vcf sample2_lenient/mysnps.vcf sample2_strict/mysnps.vcf\nLet’s have a look in the ‘vcf_out’ directory (cd into it):\ncd vcf_out\nCheck the parameters we set earlier:\nless -S info.txt\n(exit by pressing q)\nCheck results:\nless -S snpStatistics.tsv\n(exit by pressing q)\nThe file content should look like this:\nSNP statistics for 4 samples.\nQuality Threshold: 30.0\nCoverage Threshold: 3\nMinimum SNP allele frequency: 0.9\nsample  SNP Calls (all) SNP Calls (het) coverage(fold)  coverage(percent)\nrefCall allPos  noCall  discardedRefCall    discardedVarCall    filteredVarCall unhandledGenotype\nsample1_lenient 213 0   16.38   92.69\n4313387 4653728 293297  46103   728 0   0\nsample1_strict  207 0   16.33   92.71\n4314060 4653728 293403  45633   425 0   0\nsample2_lenient 1274    0   9.01    83.69\n3893600 4653728 453550  297471  7829    0   4\nsample2_strict  1218    0   8.94    83.76\n3896970 4653728 455450  295275  4815    0   0\nFirst we find the most important parameter settings and then the table of results. The first column contains the dataset name and the second column the number of called SNPs. The genome coverage and the fraction of the genome covered with the used threshold can be found in columns 4 and 5, respectively. For example, sample1 had 207 SNP calls with strict parameters. The coverage is about 16-fold and about 93% of the genome are covered 3 fold or higher (The coverage threshold we set was 3).",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#exploring-the-results",
    "href": "genome-mapping.html#exploring-the-results",
    "title": "14  Genome Mapping",
    "section": "14.8 Exploring the Results",
    "text": "14.8 Exploring the Results\nFor visual exploration of mapping results so-called “Genome Browsers” are used. Here we will use the Integrative Genomics Viewer (IGV) (https://software.broadinstitute.org/software/igv/).\nTo open IGV, simply type the following command and the app will open:\nigv\nNote that you cannot use the terminal while IGV is open. If you want to use it anyways, open a second terminal via the bar on the bottom.\nLoad your reference (YpestisCO92.fa):\n→ Genomes → Load Genome from File\n\nLoad your sorted and dedupp’ed bam files (do this 4 times, once for each mapping):\n→ File → Load from File\n\nTry to explore the mapping results yourself. Here are some questions to guide you. Please also have a look at the examples below.\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat differences do you observe between the samples and parameters?\nDifferences in number of mapped reads, coverage, number of SNPs\nDo you see any global patterns?\nWhich sample is more affected by changing the parameters?\nWhich of the two samples might be ancient, which is modern?\n\n\n\nLet’s examine some SNPs. Have a look at snpTable.tsv.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you identify SNPs that were called with lenient but not with strict parameters or vice versa?\n\n\nLet’s check out some of these in IGV.\n\n\n\n\n\n\nQuestion\n\n\n\nDo you observe certain patterns in these genomic regions?",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#examples",
    "href": "genome-mapping.html#examples",
    "title": "14  Genome Mapping",
    "section": "14.9 Examples",
    "text": "14.9 Examples\nPlease find here a few examples for exploration. To get a better visualization we only loaded sample2_lenient (top track) and sample2_strict (bottom track):\n\nYou can see all aligned reads in the current genomic region as stacks of grey arrows. In the middle of the image you see brown dashes in all of the reads. This is a SNP. You also see sporadically green or red dashes in some reads but not all of them at a given position. These sporadic differences are DNA damage such as we typically find it for ancient DNA.\nFor jumping to a specific coordinate you need to enter it into the coordinate field at the top:\n\nE.g. if you enter 12326942 after the colon in the coordinate field and hit enter, you will jump to the same position as in the screenshot above.\nLet’s have a look at some positions.\nFor example position 36472:\n\nIn the middle of the image you see a SNP (T) that was called with strict parameters (bottom) but not with lenient parameters (top). But why would it not be called in the top track? It is not called because there are three reads that cover the same position, but do not contain the T. We can see that these reads have other difference to the reference at other positions. That’s why they are not mapped with strict parameters. It is quite likely that they originate from a different species. This example demonstrates that sensitive mapping parameters might actually lead to a loss of certain SNP calls.\nDoes this mean that stricter parameters will always give us a clean mapping? Let’s have a look at position 219200:\n\nYou might need to zoom out a bit using the slider in the upper right corner.\nSo, what is going on here? We see a lot of variation in most of the reads. This is reduced a bit with strict mapping parameters (bottom track) but the effect is still quite pronounced. Here, we see a region that seems to be conserved in other species as well, so we have a lot of mapping from other organisms. We can’t compensate that with stricter mapping parameters and we would have to apply some filtering on genotype level to remove this variation from our genotyping. Removing false positive SNP calls is important as it would interfere with downstream analyses such as phylogenomics.\nSuch regions can be fairly large. For example, see this 20 kb region around position 224750:",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#optional-clean-up",
    "href": "genome-mapping.html#optional-clean-up",
    "title": "14  Genome Mapping",
    "section": "14.10 (Optional) clean-up",
    "text": "14.10 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nWhen closing your jupyter notebook(s), say no to saving any additional files.\nPress ctrl + c on your terminal, and type y when requested. Once completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/genome-mapping directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/genome-mapping*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name genome-mapping --all -y",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "genome-mapping.html#summary",
    "href": "genome-mapping.html#summary",
    "title": "14  Genome Mapping",
    "section": "14.11 Summary",
    "text": "14.11 Summary\n\nMapping DNA sequencing reads to a reference genome is a complex procedure that requires multiple steps.\nMapping results are the basis for genotyping, i.e. the detection of differences to the reference.\nThe genotyping results can be aggregated from multiple samples and comparatively analysed e.g. in the context of phylogenomics.\nThe chosen mapping parameters can have a strong influence on the results of any downstream analysis.\nThis is particularly true when dealing with ancient DNA samples as they tend to contain DNA from multiple organisms. This can lead to mismapped reads and therefore incorrect genotypes, which can further influence downstream analyses.",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Genome Mapping</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html",
    "href": "phylogenomics.html",
    "title": "15  Introduction to Phylogenomics",
    "section": "",
    "text": "15.1 Preparation\nThe data and conda environment .yaml file for this practical session can be downloaded from here: https://doi.org/10.5281/zenodo.6983184. See instructions on page.\nChange into the session directory\nThe data in this folder should contain an alignment (snpAlignment_session5.fasta) and a txt file with the ages of the samples that we are going to be working with in this session (samples.ages.txt)\nLoad the conda environment.",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#preparation",
    "href": "phylogenomics.html#preparation",
    "title": "15  Introduction to Phylogenomics",
    "section": "",
    "text": "cd /&lt;path&gt;/&lt;to&gt;/phylogenomics/\n\n\nconda activate phylogenomics",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#visualize-the-sequence-alignment",
    "href": "phylogenomics.html#visualize-the-sequence-alignment",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.2 Visualize the sequence alignment",
    "text": "15.2 Visualize the sequence alignment\nIn this practical session, we will be working with an alignment produced as you learned in the practical Genome mapping.\n\n\n\n\n\n\nWhat is in the data?\n\n\n\n\nthe alignment is a SNP alignment (it contains only the variable genomic positions, not the full genomes)\nit contains 33 Yersinia pestis sequences and 1 Yersinia pseudotuberculosis sequence which can be used as an outgroup\nin this practical, we will investigate the phylogenetic position of four prehistorical Y. pestis strains that we have recently discovered: KZL002, GZL002, CHC004 and VLI092\n\n\n\nWe start by exploring the alignment in MEGA. Open the MEGA desktop application by typing mega & in the terminal.\n\n\n\n\n\n\nTip\n\n\n\nAdding “&” at the end of a command line allows to run a program in the background while letting the terminal accessible. This particularly useful when starting a graphical interface from the terminal.\n\n\nThen, load the alignment by clicking on File -&gt; Open A File/Session -&gt; Select the snpAlignment_session5.fasta (in the working directory of the session).\n\nIt will you ask what you want to do with the alignment. In MEGA you can also produce an alignment, however, since our sequences are already aligned we will press on Analyze.\nThen we will select Nucleotide Sequences since we are working with a DNA alignment. Note that MEGA can also work with Protein Sequences as well as Pairwise Distance Matrix (which we will cover shortly). In the same window, we will change the character for Missing Data to N and click in OK.\n\nA window would open up asking if our alignment contains protein encoding sequences, and we will select No.\n\n\n\n\n\n\nTip\n\n\n\nIf you had protein encoding sequences, you would have selected Yes. This will allow you to treat different positions with different evolutionary modes depending on their codon position. One can do this to take in account that the third codon position can change to different nucleotides without resulting in a different amino acid, while position one and two of the codon are more restricted.\n\n\nTo explore the alignment, you will then click on the box with TA\n\nYou will see an alignment containing sequences from the bacterial pathogen Yersinia pestis. Within the alignment, we have four sequences of interest (KZL002, GZL002, CHC004 and VLI092) that date between 5000-2000 years Before Present (BP), and we want to know how they relate to the rest of the Yersinia pestis genomes in the alignment.\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many sequences are we analysing?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe are analysing 34 sequences.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are the Ns in the sequences?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey represent positions where we have missing data. We told MEGA to encode missing positions as N\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do you think the dots represent?\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe first line is a consensus sequence: it indicates the nucleotide supported by the majority of the sequences in the alignment (90% of the sequences should agree, otherwise an N is displayed)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey represent positions that are the same as the consensus\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOnce you know this, can you already tell by looking at the alignment which sequence is the most divergent (scroll down)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can easily see that the last sequence in the alignment (Y. pseudotuberculosis) contains more disagreements to the consensus. This is normal since this is the only genome not belonging to the Y. pestis species: we will use it as an outgroup",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#distance-based-phylogeny-neighbour-joining",
    "href": "phylogenomics.html#distance-based-phylogeny-neighbour-joining",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.3 Distance-based phylogeny: Neighbour Joining",
    "text": "15.3 Distance-based phylogeny: Neighbour Joining\nThe Neighbour Joining (NJ) method is an agglomerative algorithm which can be used to derive a phylogenetic tree from a pairwise distance matrix. In essence, this method will be grouping taxa that have the shortest distance together first, and will be doing this iteratively until all the taxa/sequences included in your alignment have been placed in a tree.\nHere are the details of the calculations for a small NJ tree example with 6 taxa:\n\nLuckily, you won’t have to do this by hand since MEGA allows you to build a NJ tree. For that go back to MEGA and click on the Phylogeny symbol (toolbar of the main menu window) and then select Construct Neighbour Joining Tree. Type “Yes” when you are asked if you want to use the currently active date. In the window that pop up, you will then chance the Model/Method to p-distance. Then press OK and a window with the calculated phylogenetic tree will pop up.\n\n\n\n\n\n\n\np-distances?\n\n\n\nA NJ tree can be built from any type of distances. This includes:\n\np-distances (also called raw distances): these are simply the proportion of differences between sequences\ncorrected distances: these are based on an underlying substitution model (JC69, K80, GTR…) and account for multiple substitutions at the same sites (which would result in only one visible difference)\np-distances and corrected distances should be similar when the number of substitutions is low compared to the genome length\n\nnote: a “substitution” is a type of mutation in which a nucleotide is replaced by another.\n\n\nSince the tree is not easily visualised in MEGA, we will export it in newick format (a standard text format for trees) and explore our tree in FigTree. This tool has a better interface for visually manipulating trees and allows us to interact with the phylogenetic tree.\nTo do that you will click on File, then Export current tree (Newick) and click on Branch Lengths to include those in the newick annotation. When you press OK, a new window with the tree in newick format will pop up and you will then press File -&gt; Save and saved it as NJ_tree.nwk. You can then close the text editor and tree explorer windows (no need to save the session).\n\nAs said above, we will explore own NJ tree in FigTree. Open the software by typing figtree & in the terminal (if you use the same terminal window as the one in which you ran mega, you might have to press enter first). Then, open the NJ tree by clicking on File -&gt; Open and selecting the file with the NJ tree NJ_tree.nwk\n\nNote that even though a root is displayed by default in FigTree, NJ trees are actually unrooted. We know that Yersinia pseudotuberculosis (labelled here as Y. pseudotuberculosis) is an outgroup to Yersinia pestis. You can reroot the tree by selecting Y.pseudotuberculosis and pressing Reroot.\n\nNow we have a rooted tree.\n\n\n\n\n\n\nQuestion\n\n\n\nHow much time did the NJ-tree calculation take?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~1 second\n\n\n\nHow many leaves/tips has our tree?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n34, i.e. the number of sequences in our SNP alignment.\n\n\n\nWhere are our taxa of interest? (KZL002, GZL002, CHC004 and VLI092)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey all fall ancestral to the rest of Yersinia pestis in this tree.\n\n\n\nDo they form a monophyletic group (a clade)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, they form a monophyletic group. We can also say that this group of prehistoric strains form their own lineage.\n\n\n\n\n15.4 Probabilistic methods: Maximum Likelihood and Bayesian inference\nThese are the most commonly used approach today. In general, probabilistic methods are statistical techniques that are based on models under which the observed data is generated through a stochastic process depending on a set of parameters which we want to estimate. The probability of the data given the model parameters is called the likelihood.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn a phylogenetic probabilistic model, what are the data and what are the parameters?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn a phylogenetic probabilistic model, the data is the sequence alignment and the parameters, are:\n\nthe parameters of the chosen substitution model (substitution rates and base frequencies)\nthe phylogenetic tree\n\n\n\n\n\n\n\n15.5 Maximum likelihood estimation and bootstrapping\nOne way we can make inferences from a probabilistic model is by finding the combination of parameters which maximises the likelihood. These parameter values are called maximum likelihood (ML) estimates. We are usually not able to compute the likelihood value for all possible combinations of parameters and have to rely on heuristic algorithms to find the maximum likelihood estimates.\n\nThe Maximum likelihood estimates are point estimates, i.e. single parameter values (for example, a tree), which does not allow to measure the associated uncertainty. A classic method to measure the uncertainty in ML trees is bootstrapping, which consists in repeatedly “disturbing” the alignment by masking sites randomly and estimating a tree from each of these bootstrap alignments.\n\nFor each clade in the ML tree, a bootstrap support value is computed which corresponds to the proportion of bootstrap trees containing the clade. This gives an indication of how robustly the clade is supported by the data (i.e. whether it holds even after disturbing the dataset). Bootstrapping can be used to measure the topology uncertainty of trees estimated with any inference method.\n\n\n\n\n\n\nNote\n\n\n\nBootstrapping can be used to measure uncertainty with any type of inference method, including distance methods\n\n\nLet’s make our own ML tree!\nHere is a command to estimate an ML phylogenetic tree together with bootstraps using RAxML (you may find the list of parameters in the RAxML manual):\nraxmlHPC-PTHREADS -m GTRGAMMA -T 3 -f a -x 12345 -p 12345 -N autoMRE -s snpAlignment_session5.fasta -n full_dataset.tre\nHere is the meaning of the chosen parameters:\n\nOnce the analysis has been completed, you can open the tree using Figtree (RAxML_bipartitions.full_dataset.tre file, change “label” to “bootstrap support” at the prompt).\n\nThe tree estimated using this model is a substitution tree (branch lengths represent genetic distances in substitutions/site). As for the NJ tree,it is not oriented in time: this is an unrooted tree (displayed with a random root in Figtree). You can reroot the tree in Figtree using Y. pseudotuberculosis as an outgroup, as previously.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you confirm the position of our genomes of interest (KZL002, GZL002, CHC004 and VLI092)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes. Just as in the NJ tree, they form a clade which is basal to the rest of the Y. pestis diversity.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs that placement well-supported? (look at the bootstrap support value: click on the “Node Labels” box and open the drop-down menu, change “Node ages” to “bootstrap support”)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe placement is strongly supported as indicated by a bootstrap support of 100% for this clade (it is not very easy to see, you probably need to zoom in a bit)\n\n\n\n\nYou can notice that the phylogeny is difficult to visualize due to the long branch leading to Y. pseudotuberculosis. Having a very distant outgroup can also have deleterious effects on the estimated phylogeny (due to the so-called “long branch attraction” effect). We can construct a new phylogeny after removing the outgroup:\n\ngo back to the original alignment in mega (that we used for the Neighbour Joining tree), untick Y.pseudotuberculosis, and export in fasta format (“Data” -&gt; “Export Data” -&gt; change “Format” to “Fasta” and click “Ok”; you can save it as: “snpAlignment_without_outgroup.fas”)\nrun raxml on this new alignment (change input to “snpAlignment_without_outgroup.fas” and output prefix (-n) to “without_outgroup” in the command line)\nopen the bipartition… file in figtree and reroot the tree based on the knowledge we have gained previously: place the root on the branch leading to the prehistoric Y. pestis strains (KZL002, GZL002, CHC004 and VLI092).\n\n\nLastly, we will export the rooted tree from figtree: File -&gt; Export trees -&gt; select the “save as currently displayed” box and save as “ML_tree_rooted.tre” (this will be useful for the section “Temporal signal assessment” at the end of this tutorial)\n\n\n\n15.6 Estimating a time-tree using Bayesian phylogenetics (BEAST2)\nNow, we will try to use reconstruct a phylogeny in which the branch lengths do not represent a number of substitutions but instead represent the time of evolution. To do so, we will use the dates of ancient genomes (C14 dates) to calibrate the tree in time. This assumes a molecular clock hypothesis in which substitutions occur at a rate that is relatively constant in time so that the time of evolution can be estimated based on the number of substitutions.\n\n\n\n\n\n\nNote\n\n\n\nA great advantage of ancient pathogen genomes is that they provide key calibration points to estimate molecular clocks and dated phylogenies. This is more difficult to do with modern data alone.\n\n\nWe will estimate a time-tree from our alignment using Bayesian inference as implemented in the BEAST2 software. Bayesian inference is based on a probability distribution that is different from the likelihood: the posterior probability. The posterior probability is the probability of the parameters given the data. It is easier to interpret than the likelihood because it directly contains all the information about the parameters: point estimates such as the median or the mean can be directly estimated from it, but also percentile intervals which can be used to measure uncertainty.\n\nThe Bayes theorem tells us that the posterior probability is proportional to the product of the likelihood and the “prior” probability of the parameters:\n\nTherefore, for Bayesian inference, we need to complement our probabilistic model with prior distributions for all the parameters. Because we want to estimate a time tree, we also add another parameter: the molecular clock (average substitution rate in time units).\n\nTo characterize the full posterior distribution of each parameter, we would need in theory to compute the posterior probability for each possible combination of parameters. This is impossible, and we will instead use an algorithm called Markov chain Monte Carlo (MCMC) to approximate the posterior distribution. The MCMC is an algorithm which iteratively samples values of the parameters from the posterior distribution. Therefore, if the MCMC has run long enough, the (marginal) posterior distribution of the parameters can be approximated by a histogram of the sampled values.\n\n\n\n\n\n\n\nTip\n\n\n\nThe “taming the beast” website has great tutorials to learn setting a BEAST2 analysis. In particular, the “Introduction to BEAST2”, “Prior selection” and “Time-stamped data” are good starts.\n\n\nThe different components of the BEAST2 analysis can be set up in the program BEAUti:\n\nOpen BEAUTi by typing beauti & in the terminal (if asked to update, press ‘not now’), and set up an analysis as followed:\n\nload the alignment without outgroup in the “Partitions” tab (“File” -&gt; “Import alignment”; select “nucleotide”)\n\n\n\nset the sampling dates in the “Tip dates” tab:\n\nselect “Use tip dates”\nclick on “Auto-configure” -&gt; “read from file” and select the sample_ages.txt file\nchange “Since some time in the past” to “Before present”\n\n\n\n\nselect the substitution model in the “Site model” tab:\n\nchose a GTR model\nuse 4 Gamma categories for the Gamma site model: this is to account for variations of the substitution rate across sites (site=nucleotide position)\n\n\n\n\nchoose the molecular clock model in the “Clock model” tab:\n\nuse a relaxed clock lognormal model (this is to allow for some variation of the clock rate across branches)\nchange the initial value of the clock rate to 10-4 substitution/site/year (10-4 can be written 1E-4)\n\n\n\n\nchoose the prior distribution of parameters in the “Priors” tab:\n\nuse a Coalescent Bayesian Skyline tree prior\nchange the mean clock prior to a uniform distribution between 1E-6 and 1E-3 subst/site/year\nleave everything else to default\n\n\n\n\nset up the MCMC in the “MCMC” tab:\n\nuse a chain length of 300M\nsample the mono-dimensional parameters and trees every 10,000 iterations (unfold “tracelog” and “treelog” menus and change “log every” to 10,000)\n\n\n\n\nsave the analysis setup as an xml file: “File” -&gt; “Save as”; you can name the file “beast_analysis_Y_pestis.xml”\n\nNow that the analysis is setup, we can run it using BEAST:\nbeast beast_analysis_Y_pestis.xml\nOnce the analysis is running two files should have been created and are continuously updated:\n\nthe “snpAlignment_without_outgroup.log” file which contains the values sampled by the MCMC for various mono-dimensional parameters such as the clock rate, as well as other values that are a logged along the MCMC such as the posterior probability and the likelihood.\nthe “snpAlignment_without_outgroup.trees” file which contains the MCMC trees sampled by the MCMC\n\nWhile the analysis is running, you can start reading the next section!\n\n15.6.1 Assessing BEAST2 results\n\n\n\n\n\n\nReminder\n\n\n\nWe are using an MCMC algorithm to sample the posterior distribution of parameters. If the MCMC has run long enough, we can use the sampled parameters to approximate the posterior distribution itself. Therefore, we have to check first that the MCMC chain has run long enough.\n\n\nWe can assess the MCMC sampling using the program Tracer. Tracer can read BEAST log files an generate statistics and plots for each of the sampled parameters. Most importantly, Tracer provides:\n\ntrace plots: show the sampled parameter values along the MCMC run. Trace plots are a very useful MCMC diagnostic tool.\n\n\nThe first thing that one needs to assess is whether the MCMC has passed the so called “burn-in” phase. The MCMC starts with a random set of parameters and will take some time to reach a zone of high posterior probability density. The parameter values that are sampled during this initial phase are usually considered as noise and discarded (by default, tracer discards the first 10% of samples). The burn-in phase can be visualize on trace plots as an initial phase during which the posterior probability of sampled parameters is constantly increasing before reaching a plateau:\n\nOnce the burn-in phase is passed, one can look at the trace plots to assess if the parameters have been sampled correctly and long enough. Usually, when this is the case, the trace should be quite dense and oscillating around a central value (nice trace plots should look like “hairy caterpillars”). In the figure below, the trace on the left doesn’t look good, the one on the right does:\n\n– ESS values: tracer also calculates effective sample sizes (ESS) for each of the sampled parameters. ESSs are estimates of the number of sampled parameter values after correcting for auto-correlation along the MCMC. As a rule of thumb, one usually considers that an MCMC as run long enough if all parameter’s ESS are &gt; 200. Note that if the trace looks like a hairy caterpillar, the corresponding ESS value should be high.\n\n– Parameter estimates: Tracer also provides statistics and plots to explore the posterior distribution of the parameters. These should be considered only if the trace plot and ESS values look fine. In the “Estimates” tab, after selecting the chosen parameter in the left panel, the upper-right panel shows point estimates (mean, median) and measures of uncertainty (95% HPD interval), and the bottom-right panel shows a histogram of the sampled value:\n\nLet’s now load the “snpAlignment_without_outgroup.log” file into Tracer.\nWhile the run is still going, open a new separate terminal, activate the conda environment with conda activate phylogenomics, open tracer with tracer &, and then “File” -&gt; “Import trace file” -&gt; select “snpAlignment_without_outgroup.log”. Note that one can load a BEAST2 log file into tracer even if the analysis is still running. This allows to assess if the MCMC is running correctly or has run long enough before it’s completed.\n\n\n\n\n\n\nQuestion\n\n\n\nHas the MCMC run long enough?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou have probably let you analysis run for 10-20 mins before looking at the log file, and this is definitely not sufficient: the burnin phase has recently been passed, the trace plots do not look very dense and ESS values are low. It would probably take a few hours for the analysis to complete. Luckily we have run the analysis in advance and saved the log files for you in the “intermediateFiles” folder: “snpAlignment_without_outgroup.log” and “snpAlignment_without_outgroup.trees”\n\n\n\n\nYou can now load the “intermediateFiles/snpAlignment_without_outgroup.log” file into Tracer.\n\n\n\n\n\n\nQuestion\n\n\n\nHas the MCMC run long enough?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The trace plots look good and all ESSs are &gt; 200 \n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is your mean estimate of the clock rate (ucld mean)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~7.10-6 substitution/site/year. Note, however, that this estimate is largely biased since we used a SNP alignment containing only variable positions. In order to get an unbiased estimate of the substitution rate, we should have used the full alignment or account for the number of constant sites by using a “filtered” alignment (see here). In general, this is good practice since not accounting for conserved positions in the alignment can sometimes affect the tree as well (although this should usually be minor, which is why we didn’t bother to do this here).\n\n\n\n\n\n\n15.6.2 MCC tree\nSince we are working in a Bayesian framework, we do not obtain a single phylogenetic tree as with Maximum likelihood, but a large set of trees which should be representative of the posterior distribution. In contrast with mono-dimensional parameters, a tree distribution cannot be easily summarized with mean or median estimates. Instead, we need to use specific tree-summarizing techniques. One of the most popular is the maximum clade credibility (MCC) tree, which works as follow:\n\n\nFor any node in any of the sampled trees, compute a posterior support: the proportion of trees in the sample which contain the node\n\n\nSelect the MCC tree: this is the tree in which the product of node posterior supports is the highest\n\n\nCalculate node/branch statistics on the MCC tree: typically, the mean/median estimates and HPD interval of node ages are calculated from the full tree sample and annotated on the MCC tree\n\n\n\nLet’s generate an MCC tree from our tree sample. We can do this using the TreeAnnotator software, which has both a command line and graphical interface. Let’s use the command line here and run the following (using a burn-in of 10%):\n treeannotator -burnin 10 intermediateFiles/snpAlignment_without_outgroup.trees snpAlignment_without_outgroup.MCC.tree\nOnce this is completed, we can open the MCC tree (snpAlignment_without_outgroup.MCC.tree) with figtree. Let’s then add a few elements to the plot:\n\n\nTick the “Scale Axis” box, unfold the corresponding menu, and select “Reverse axis” (now the timescale is in years BP)\n\n\nTick the “Node Labels” box, unfold the corresponding menu, and select “Display: posterior”. The posterior support of each node is now displayed. Note that the support value is a proportion (1=100%)\n\n\nTick the “Node Bars” box, unfold the corresponding menu, and select “Display: height_95%_HPD”. The 95% HPD intervals of node ages are now displayed.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the root of the tree consistent with what we found previously?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The root is placed between our prehistoric strains and the rest of Y. pestis strains. Note that this time we didn’t have to use an outgroup because we estimated a time-tree: the root is identified as the oldest node in the tree.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is your estimate for the age of the most recent common ancestor of all Y. pestis strains?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~5800 years BP (HPD 95%: ~8000-4500 years BP)\n\n\n\n\n\n15.6.3 Bonus: Temporal signal assessment\nIt is a good practice to assess if the genetic sequences that we analyse do indeed behave like molecular clocks before trying to estimate a time tree (i.e. we should have done this before the actual BEAST2 analysis). A classic way to assess the temporal signal of a dataset is the root-to-tip regression. The rationale of the root-to-tip regression is to verify that the oldest a sequence is, the closer it should be to the root in a (rooted) substitution tree because there was less time for substitution to accumulate. In other words, their should be a correlation between sample age and distance to the root, which we can assess using a linear regression (root-to-tip regression). This can be done using the program TempEst:\n\nopen TempEst by typing tempest & and load the rooted ML tree that we produced previously (you should have saved it as “ML_tree_rooted.tre”)\nclick on “Import Dates” in the “Sample Dates” tab, select the sample_age.txt file and click “OK”\nstill in the “Sample Dates” tab, change “Since some time in the past” to “Before present” (one might need to extend the TempEst window to see the pull down menu)\nlook at the “Root-to-tip tab”: is there a positive correlation between time and root-to-tip divergence as expected under the molecular clock hypothesis?\n\n\n\n\n\n15.7 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nYou can close all windows and any terminals. If you have a terminal with a command still running, just press ctrl + c a couple of times until it drops you to an empty prompt.\nOnce completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/phylogenomics directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/phylogenomics*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name phylogenomics --all -y",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#probabilistic-methods-maximum-likelihood-and-bayesian-inference",
    "href": "phylogenomics.html#probabilistic-methods-maximum-likelihood-and-bayesian-inference",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.4 Probabilistic methods: Maximum Likelihood and Bayesian inference",
    "text": "15.4 Probabilistic methods: Maximum Likelihood and Bayesian inference\nThese are the most commonly used approach today. In general, probabilistic methods are statistical techniques that are based on models under which the observed data is generated through a stochastic process depending on a set of parameters which we want to estimate. The probability of the data given the model parameters is called the likelihood.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn a phylogenetic probabilistic model, what are the data and what are the parameters?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn a phylogenetic probabilistic model, the data is the sequence alignment and the parameters, are:\n\nthe parameters of the chosen substitution model (substitution rates and base frequencies)\nthe phylogenetic tree",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#maximum-likelihood-estimation-and-bootstrapping",
    "href": "phylogenomics.html#maximum-likelihood-estimation-and-bootstrapping",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.5 Maximum likelihood estimation and bootstrapping",
    "text": "15.5 Maximum likelihood estimation and bootstrapping\nOne way we can make inferences from a probabilistic model is by finding the combination of parameters which maximises the likelihood. These parameter values are called maximum likelihood (ML) estimates. We are usually not able to compute the likelihood value for all possible combinations of parameters and have to rely on heuristic algorithms to find the maximum likelihood estimates.\n\nThe Maximum likelihood estimates are point estimates, i.e. single parameter values (for example, a tree), which does not allow to measure the associated uncertainty. A classic method to measure the uncertainty in ML trees is bootstrapping, which consists in repeatedly “disturbing” the alignment by masking sites randomly and estimating a tree from each of these bootstrap alignments.\n\nFor each clade in the ML tree, a bootstrap support value is computed which corresponds to the proportion of bootstrap trees containing the clade. This gives an indication of how robustly the clade is supported by the data (i.e. whether it holds even after disturbing the dataset). Bootstrapping can be used to measure the topology uncertainty of trees estimated with any inference method.\n\n\n\n\n\n\nNote\n\n\n\nBootstrapping can be used to measure uncertainty with any type of inference method, including distance methods\n\n\nLet’s make our own ML tree!\nHere is a command to estimate an ML phylogenetic tree together with bootstraps using RAxML (you may find the list of parameters in the RAxML manual):\nraxmlHPC-PTHREADS -m GTRGAMMA -T 3 -f a -x 12345 -p 12345 -N autoMRE -s snpAlignment_session5.fasta -n full_dataset.tre\nHere is the meaning of the chosen parameters:\n\nOnce the analysis has been completed, you can open the tree using Figtree (RAxML_bipartitions.full_dataset.tre file, change “label” to “bootstrap support” at the prompt).\n\nThe tree estimated using this model is a substitution tree (branch lengths represent genetic distances in substitutions/site). As for the NJ tree,it is not oriented in time: this is an unrooted tree (displayed with a random root in Figtree). You can reroot the tree in Figtree using Y. pseudotuberculosis as an outgroup, as previously.\n\n\n\n\n\n\nQuestion\n\n\n\nCan you confirm the position of our genomes of interest (KZL002, GZL002, CHC004 and VLI092)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes. Just as in the NJ tree, they form a clade which is basal to the rest of the Y. pestis diversity.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs that placement well-supported? (look at the bootstrap support value: click on the “Node Labels” box and open the drop-down menu, change “Node ages” to “bootstrap support”)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe placement is strongly supported as indicated by a bootstrap support of 100% for this clade (it is not very easy to see, you probably need to zoom in a bit)\n\n\n\n\nYou can notice that the phylogeny is difficult to visualize due to the long branch leading to Y. pseudotuberculosis. Having a very distant outgroup can also have deleterious effects on the estimated phylogeny (due to the so-called “long branch attraction” effect). We can construct a new phylogeny after removing the outgroup:\n\ngo back to the original alignment in mega (that we used for the Neighbour Joining tree), untick Y.pseudotuberculosis, and export in fasta format (“Data” -&gt; “Export Data” -&gt; change “Format” to “Fasta” and click “Ok”; you can save it as: “snpAlignment_without_outgroup.fas”)\nrun raxml on this new alignment (change input to “snpAlignment_without_outgroup.fas” and output prefix (-n) to “without_outgroup” in the command line)\nopen the bipartition… file in figtree and reroot the tree based on the knowledge we have gained previously: place the root on the branch leading to the prehistoric Y. pestis strains (KZL002, GZL002, CHC004 and VLI092).\n\n\nLastly, we will export the rooted tree from figtree: File -&gt; Export trees -&gt; select the “save as currently displayed” box and save as “ML_tree_rooted.tre” (this will be useful for the section “Temporal signal assessment” at the end of this tutorial)",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#estimating-a-time-tree-using-bayesian-phylogenetics-beast2",
    "href": "phylogenomics.html#estimating-a-time-tree-using-bayesian-phylogenetics-beast2",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.6 Estimating a time-tree using Bayesian phylogenetics (BEAST2)",
    "text": "15.6 Estimating a time-tree using Bayesian phylogenetics (BEAST2)\nNow, we will try to use reconstruct a phylogeny in which the branch lengths do not represent a number of substitutions but instead represent the time of evolution. To do so, we will use the dates of ancient genomes (C14 dates) to calibrate the tree in time. This assumes a molecular clock hypothesis in which substitutions occur at a rate that is relatively constant in time so that the time of evolution can be estimated based on the number of substitutions.\n\n\n\n\n\n\nNote\n\n\n\nA great advantage of ancient pathogen genomes is that they provide key calibration points to estimate molecular clocks and dated phylogenies. This is more difficult to do with modern data alone.\n\n\nWe will estimate a time-tree from our alignment using Bayesian inference as implemented in the BEAST2 software. Bayesian inference is based on a probability distribution that is different from the likelihood: the posterior probability. The posterior probability is the probability of the parameters given the data. It is easier to interpret than the likelihood because it directly contains all the information about the parameters: point estimates such as the median or the mean can be directly estimated from it, but also percentile intervals which can be used to measure uncertainty.\n\nThe Bayes theorem tells us that the posterior probability is proportional to the product of the likelihood and the “prior” probability of the parameters:\n\nTherefore, for Bayesian inference, we need to complement our probabilistic model with prior distributions for all the parameters. Because we want to estimate a time tree, we also add another parameter: the molecular clock (average substitution rate in time units).\n\nTo characterize the full posterior distribution of each parameter, we would need in theory to compute the posterior probability for each possible combination of parameters. This is impossible, and we will instead use an algorithm called Markov chain Monte Carlo (MCMC) to approximate the posterior distribution. The MCMC is an algorithm which iteratively samples values of the parameters from the posterior distribution. Therefore, if the MCMC has run long enough, the (marginal) posterior distribution of the parameters can be approximated by a histogram of the sampled values.\n\n\n\n\n\n\n\nTip\n\n\n\nThe “taming the beast” website has great tutorials to learn setting a BEAST2 analysis. In particular, the “Introduction to BEAST2”, “Prior selection” and “Time-stamped data” are good starts.\n\n\nThe different components of the BEAST2 analysis can be set up in the program BEAUti:\n\nOpen BEAUTi by typing beauti & in the terminal (if asked to update, press ‘not now’), and set up an analysis as followed:\n\nload the alignment without outgroup in the “Partitions” tab (“File” -&gt; “Import alignment”; select “nucleotide”)\n\n\n\nset the sampling dates in the “Tip dates” tab:\n\nselect “Use tip dates”\nclick on “Auto-configure” -&gt; “read from file” and select the sample_ages.txt file\nchange “Since some time in the past” to “Before present”\n\n\n\n\nselect the substitution model in the “Site model” tab:\n\nchose a GTR model\nuse 4 Gamma categories for the Gamma site model: this is to account for variations of the substitution rate across sites (site=nucleotide position)\n\n\n\n\nchoose the molecular clock model in the “Clock model” tab:\n\nuse a relaxed clock lognormal model (this is to allow for some variation of the clock rate across branches)\nchange the initial value of the clock rate to 10-4 substitution/site/year (10-4 can be written 1E-4)\n\n\n\n\nchoose the prior distribution of parameters in the “Priors” tab:\n\nuse a Coalescent Bayesian Skyline tree prior\nchange the mean clock prior to a uniform distribution between 1E-6 and 1E-3 subst/site/year\nleave everything else to default\n\n\n\n\nset up the MCMC in the “MCMC” tab:\n\nuse a chain length of 300M\nsample the mono-dimensional parameters and trees every 10,000 iterations (unfold “tracelog” and “treelog” menus and change “log every” to 10,000)\n\n\n\n\nsave the analysis setup as an xml file: “File” -&gt; “Save as”; you can name the file “beast_analysis_Y_pestis.xml”\n\nNow that the analysis is setup, we can run it using BEAST:\nbeast beast_analysis_Y_pestis.xml\nOnce the analysis is running two files should have been created and are continuously updated:\n\nthe “snpAlignment_without_outgroup.log” file which contains the values sampled by the MCMC for various mono-dimensional parameters such as the clock rate, as well as other values that are a logged along the MCMC such as the posterior probability and the likelihood.\nthe “snpAlignment_without_outgroup.trees” file which contains the MCMC trees sampled by the MCMC\n\nWhile the analysis is running, you can start reading the next section!\n\n15.6.1 Assessing BEAST2 results\n\n\n\n\n\n\nReminder\n\n\n\nWe are using an MCMC algorithm to sample the posterior distribution of parameters. If the MCMC has run long enough, we can use the sampled parameters to approximate the posterior distribution itself. Therefore, we have to check first that the MCMC chain has run long enough.\n\n\nWe can assess the MCMC sampling using the program Tracer. Tracer can read BEAST log files an generate statistics and plots for each of the sampled parameters. Most importantly, Tracer provides:\n\ntrace plots: show the sampled parameter values along the MCMC run. Trace plots are a very useful MCMC diagnostic tool.\n\n\nThe first thing that one needs to assess is whether the MCMC has passed the so called “burn-in” phase. The MCMC starts with a random set of parameters and will take some time to reach a zone of high posterior probability density. The parameter values that are sampled during this initial phase are usually considered as noise and discarded (by default, tracer discards the first 10% of samples). The burn-in phase can be visualize on trace plots as an initial phase during which the posterior probability of sampled parameters is constantly increasing before reaching a plateau:\n\nOnce the burn-in phase is passed, one can look at the trace plots to assess if the parameters have been sampled correctly and long enough. Usually, when this is the case, the trace should be quite dense and oscillating around a central value (nice trace plots should look like “hairy caterpillars”). In the figure below, the trace on the left doesn’t look good, the one on the right does:\n\n– ESS values: tracer also calculates effective sample sizes (ESS) for each of the sampled parameters. ESSs are estimates of the number of sampled parameter values after correcting for auto-correlation along the MCMC. As a rule of thumb, one usually considers that an MCMC as run long enough if all parameter’s ESS are &gt; 200. Note that if the trace looks like a hairy caterpillar, the corresponding ESS value should be high.\n\n– Parameter estimates: Tracer also provides statistics and plots to explore the posterior distribution of the parameters. These should be considered only if the trace plot and ESS values look fine. In the “Estimates” tab, after selecting the chosen parameter in the left panel, the upper-right panel shows point estimates (mean, median) and measures of uncertainty (95% HPD interval), and the bottom-right panel shows a histogram of the sampled value:\n\nLet’s now load the “snpAlignment_without_outgroup.log” file into Tracer.\nWhile the run is still going, open a new separate terminal, activate the conda environment with conda activate phylogenomics, open tracer with tracer &, and then “File” -&gt; “Import trace file” -&gt; select “snpAlignment_without_outgroup.log”. Note that one can load a BEAST2 log file into tracer even if the analysis is still running. This allows to assess if the MCMC is running correctly or has run long enough before it’s completed.\n\n\n\n\n\n\nQuestion\n\n\n\nHas the MCMC run long enough?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou have probably let you analysis run for 10-20 mins before looking at the log file, and this is definitely not sufficient: the burnin phase has recently been passed, the trace plots do not look very dense and ESS values are low. It would probably take a few hours for the analysis to complete. Luckily we have run the analysis in advance and saved the log files for you in the “intermediateFiles” folder: “snpAlignment_without_outgroup.log” and “snpAlignment_without_outgroup.trees”\n\n\n\n\nYou can now load the “intermediateFiles/snpAlignment_without_outgroup.log” file into Tracer.\n\n\n\n\n\n\nQuestion\n\n\n\nHas the MCMC run long enough?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The trace plots look good and all ESSs are &gt; 200 \n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is your mean estimate of the clock rate (ucld mean)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~7.10-6 substitution/site/year. Note, however, that this estimate is largely biased since we used a SNP alignment containing only variable positions. In order to get an unbiased estimate of the substitution rate, we should have used the full alignment or account for the number of constant sites by using a “filtered” alignment (see here). In general, this is good practice since not accounting for conserved positions in the alignment can sometimes affect the tree as well (although this should usually be minor, which is why we didn’t bother to do this here).\n\n\n\n\n\n\n15.6.2 MCC tree\nSince we are working in a Bayesian framework, we do not obtain a single phylogenetic tree as with Maximum likelihood, but a large set of trees which should be representative of the posterior distribution. In contrast with mono-dimensional parameters, a tree distribution cannot be easily summarized with mean or median estimates. Instead, we need to use specific tree-summarizing techniques. One of the most popular is the maximum clade credibility (MCC) tree, which works as follow:\n\n\nFor any node in any of the sampled trees, compute a posterior support: the proportion of trees in the sample which contain the node\n\n\nSelect the MCC tree: this is the tree in which the product of node posterior supports is the highest\n\n\nCalculate node/branch statistics on the MCC tree: typically, the mean/median estimates and HPD interval of node ages are calculated from the full tree sample and annotated on the MCC tree\n\n\n\nLet’s generate an MCC tree from our tree sample. We can do this using the TreeAnnotator software, which has both a command line and graphical interface. Let’s use the command line here and run the following (using a burn-in of 10%):\n treeannotator -burnin 10 intermediateFiles/snpAlignment_without_outgroup.trees snpAlignment_without_outgroup.MCC.tree\nOnce this is completed, we can open the MCC tree (snpAlignment_without_outgroup.MCC.tree) with figtree. Let’s then add a few elements to the plot:\n\n\nTick the “Scale Axis” box, unfold the corresponding menu, and select “Reverse axis” (now the timescale is in years BP)\n\n\nTick the “Node Labels” box, unfold the corresponding menu, and select “Display: posterior”. The posterior support of each node is now displayed. Note that the support value is a proportion (1=100%)\n\n\nTick the “Node Bars” box, unfold the corresponding menu, and select “Display: height_95%_HPD”. The 95% HPD intervals of node ages are now displayed.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the root of the tree consistent with what we found previously?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The root is placed between our prehistoric strains and the rest of Y. pestis strains. Note that this time we didn’t have to use an outgroup because we estimated a time-tree: the root is identified as the oldest node in the tree.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is your estimate for the age of the most recent common ancestor of all Y. pestis strains?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~5800 years BP (HPD 95%: ~8000-4500 years BP)\n\n\n\n\n\n15.6.3 Bonus: Temporal signal assessment\nIt is a good practice to assess if the genetic sequences that we analyse do indeed behave like molecular clocks before trying to estimate a time tree (i.e. we should have done this before the actual BEAST2 analysis). A classic way to assess the temporal signal of a dataset is the root-to-tip regression. The rationale of the root-to-tip regression is to verify that the oldest a sequence is, the closer it should be to the root in a (rooted) substitution tree because there was less time for substitution to accumulate. In other words, their should be a correlation between sample age and distance to the root, which we can assess using a linear regression (root-to-tip regression). This can be done using the program TempEst:\n\nopen TempEst by typing tempest & and load the rooted ML tree that we produced previously (you should have saved it as “ML_tree_rooted.tre”)\nclick on “Import Dates” in the “Sample Dates” tab, select the sample_age.txt file and click “OK”\nstill in the “Sample Dates” tab, change “Since some time in the past” to “Before present” (one might need to extend the TempEst window to see the pull down menu)\nlook at the “Root-to-tip tab”: is there a positive correlation between time and root-to-tip divergence as expected under the molecular clock hypothesis?",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "phylogenomics.html#optional-clean-up",
    "href": "phylogenomics.html#optional-clean-up",
    "title": "15  Introduction to Phylogenomics",
    "section": "15.7 (Optional) clean-up",
    "text": "15.7 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nYou can close all windows and any terminals. If you have a terminal with a command still running, just press ctrl + c a couple of times until it drops you to an empty prompt.\nOnce completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/phylogenomics directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/phylogenomics*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name phylogenomics --all -y",
    "crumbs": [
      "Ancient Genomics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Phylogenomics</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html",
    "href": "accessing-ancient-metagenomic-data.html",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "",
    "text": "16.1 Introduction\nIn most bioinformatic projects, we need to include publicly available comparative data to expand or compare our newly generated data with.\nIncluding public data can benefit ancient metagenomic studies in a variety of ways. It can help increase our sample sizes (a common problem when dealing with rare archaeological samples) - thus providing stronger statistical power. Comparison with a range of previously published data of different preservational levels can allow an estimate on the quality of the new samples. When considering solely (re)using public data, we can consider that this can also spawn new ideas, projects, and meta analyses to allow further deeper exploration of ancient metagenomic data (e.g., looking for correlations between various environmental factors and preservation).\nFortunately for us, geneticists and particularly palaeogenomicists have been very good at uploading raw sequencing data to well-established databases (Anagnostou et al. 2015).\nIn the vast majority of cases you will be able to find publically available sequencing data on the INSDC (https://www.insdc.org/) association of databases, namely the EBI’s European Nucleotide Archive (ENA; https://www.ebi.ac.uk/ena/), and NCBI (https://www.ncbi.nlm.nih.gov/sra) or DDBJ’s (https://www.ddbj.nig.ac.jp/dra/index-e.html) Sequence Read Archives (SRA). However, you may in some cases find ancient metagenomic data on institutional FTP servers, domain specific databases (e.g. OAGR (https://oagr.org), Zenodo (https://zenodo.org), Figshare (https://figshare.com), or GitHub (https://github.com)).\nBut while the data is publicly available, we need to ask whether it is ‘FAIR’.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#finding-ancient-metagenomic-data",
    "href": "accessing-ancient-metagenomic-data.html#finding-ancient-metagenomic-data",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.2 Finding Ancient Metagenomic Data",
    "text": "16.2 Finding Ancient Metagenomic Data\nFAIR principles (Wilkinson et al. 2016) were defined by researchers, librarians, and industry in 2016 to improve the quality of data uploads - primarily by making data uploads more ‘machine readable’. FAIR standards for:\n\nFindable\nAccessible\nInteroperable\nReproducible\n\nWhen we consider ancient (meta)genomic data, we are pretty close to this. Sequencing data is in most cases accessible (via the public databases like ENA, SRA), interoperable and reproducible because we use field standard formats such as FASTQ or BAM files. However findable remains an issue.\nThis is because the metadata about each data file is dispersed over many places, and very often not with the data files themselves.\nIn this case I am referring to metadata such as: What is the sample’s name? How old is it? Where is it from? Which enzymes were used for library construction? What sequencing machine was this library sequenced on?\nTo find this information about a given data file, you have to search many places (main text, supplementary information, the database itself), for different types of metadata (as authors report different things), and also in different formats (text, tables, figures).\nThis very heterogenous landscape makes it difficult for machines to index all this information (if at all), and thus means you cannot search for the data you want to use for your own research in online search engines.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#ancientmetagenomedir",
    "href": "accessing-ancient-metagenomic-data.html#ancientmetagenomedir",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.3 AncientMetagenomeDir",
    "text": "16.3 AncientMetagenomeDir\nThis is where the SPAAM community project ‘AncientMetagenomeDir’ (https://github.com/spaam-community/AncientMetagenomeDir) comes in (Fellows Yates, Andrades Valtueña, et al. 2021). AncientMetagenomeDir is a resource of lists of metadata of all publishing and publicly available ancient metagenomes and microbial genome-level enriched samples and their associated libraries.\nBy aggregating and standardising metadata and accession codes of ancient metagenomic samples and libraries, the project aims to make it easier for people to find comparative data for their own projects, appropriately re-analyse libraries, as well as help track the field over time and facilitate meta analyses.\nCurrently the project is split over three main tables: host-associated metagenomes (e.g. ancient microbiomes), host-associated single-genomes (e.g. ancient pathogens), and environmental metagenomes (e.g. lakebed cores or cave sediment sequences).\nThe repository already contains more than 2000 samples and 5000 libraries, spanning the entire globe and as far back as hundreds of thousands of years.\nTo make the lists of samples and their metadata as accessible and interoperable as possible, we utilise simple text (TSV - tab separate value) files - files that can be opened by pretty much all spreadsheet tools (e.g., Microsoft Office excel, LibreOffice Calc) and languages (R, Python etc.) (Figure 16.1).\n\n\n\n\n\n\nFigure 16.1: Example few columns and rows of an AncientMetagenomeDir table, including project name, publication year, site name, latitude, longitude, country and sample name.\n\n\n\nCritically, by standardising the recorded all metadata across all publications this makes it much easier for researchers to filter for particular time periods, geographical regions, or sample types of their interest - and then use the also recorded accession numbers to efficiently download the data.\nAt their core all different AncientMetagenomeDir tables must have at 6 minimum metadata sets at the sample level:\n\nPublication information (doi)\nSample name(s)\nGeographic location (e.g. country, coordinates)\nAge\nSample type (e.g. bone, sediment, etc.)\nData Archive and accessions\n\nEach table then has additional columns depending on the context (e.g. what time of microbiome is expected for host-associated metagenomes, or species name of the genome that was reconstructed).\nThe AncientMetagenomeDir project already has 12 major releases, and will continued to be regularly updated as the community continues to submit new metadata of samples of new publications as they come out.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the naming scheme of the AncientMetagenomeDir releases? Try to find the ‘release’ and ‘wiki’ sections of the GitHub repository interface and see if you can find the information…\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe ‘release’ listing of a GitHub repository can be found on the right hand metadata bar.\nIn many cases you can also find a ‘CHANGELOG’ file that will list all the changes that have been made on each release.\nThe release name scheme is of different places listed on the Unesco World Heritage list (https://whc.unesco.org/en/list/). More background you can find here on the Wiki page (https://github.com/SPAAM-community/AncientMetagenomeDir/wiki/Release-Name-List)",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#amdirt",
    "href": "accessing-ancient-metagenomic-data.html#amdirt",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.4 AMDirT",
    "text": "16.4 AMDirT\nBut how does one explore such a large dataset of tables with thousands of rows? You could upload this into a spreadsheet tool or in a programming language like R, but you would still have to do a lot of manual filtering and parsing of the dataset to make it useful for downstream analyses.\nIn response to this the SPAAM Community have also developed a companion tool ‘AMDirT’ to facilitate this (Borry et al. 2024). Amongst other functionality, AMDirT allows you to load different releases of AncientMetagenomeDir, filter and explore to specific samples or libraries of interest, and then generate download scripts, configuration files for pipelines, and reference BibTeX files for you, both via a command-line (CLI) or graphical user interface (GUI)!\n\n16.4.1 Running AMDirT viewer\nWe will now demonstrate how to use the AMDirT graphical user interface to load a dataset, filter to samples of interest, and download some configuration input files for downstream ancient DNA pipelines.\n\n\n\n\n\n\nFigure 16.2: AMDirT logo: a cog with the SPAAM icon in the middle with the word AMDirT to the side\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial will require a web-browser! Make sure to run on your local laptop/PC or, if on on a server, with X11 forwarding activated.\n\n\nFirst, we will need to activate a conda environment, and then install the latest development version of the tool for you.\nWhile in the accessing-ancient-metagenomic-data conda environment, run the following command to load the GUI into your web-browser. If the browser doesn’t automatically load, copy the IP address and paste it in your browser’s URL bar.\nAMDirT viewer\n\n\n\n\n\n\nNote\n\n\n\nThe first time opening AMDirT (a streamlit app), it may ask you to sign up for a newsletter using your email.\nDo not type anything when prompted (i.e., just press enter), this is entirely optional and will not affect the usage of the tool.\nYou will not be asked again when in the same conda environment.\n\n\nYour web browser should now load, and you should see a two panel page.\nUnder Select a table use the dropdown menu to select ‘ancientsinglegenome-hostassociated’.\nYou should then see a table (Figure 16.3), pretty similar what you are familiar with with spreadsheet tools such as Microsoft Excel or LibreOffice calc.\n\n\n\n\n\n\nFigure 16.3: Main AMDirT Viewer page on load. A toolbar on the left displays the AMDirT version, and dropdown menus for the AncientMetagenomeDir release, table, and downloading tool to use. The rest of the page shows a tabular window containing rows and columns corresponding to sample metadata and rows of samples with metadata such as project name, publication year and site name.\n\n\n\nTo navigate, you can scroll down to see more rows, and press shift and scroll to see more columns, or use click on a cell and use your arrow keys (⬆,⬇,⬅,➡) to move around the table.\nYou can reorder columns by clicking on the column name, and also filter by pressing the little ‘burger’ icon that appears on the column header when you hover over a given column.\nAs an exercise, we will try filtering to a particular set of samples, then generate some download scripts, and download the files.\nFirst, filter the project_name column to ‘Muhlemann2020’ from (Mühlemann et al. 2020, Figure 16.4).\n\n\n\n\n\n\nFigure 16.4: The AMDirT viewer with a filter menu coming out of the project name column, with an example search of the project ‘Muhlemann2020’ in the search bar.\n\n\n\nThen scroll to the right, and filter the geo_loc_name to ‘Norway’ (Figure 16.5).\n\n\n\n\n\n\nFigure 16.5: The AMDirT viewer with a filter menu coming out of the geo location column, with an example search of the country ‘Norway’ written in the search bar and the entry ticked in the results.\n\n\n\nYou should be left with 2 rows.\nFinally, scroll back to the first column and tick the boxes of these two samples (Figure 16.6).\n\n\n\n\n\n\nFigure 16.6: The AMDirT viewer with just two rows with samples from Muhlemann2020 that are located in the Norway being displayed, and the tickboxes next to each one selected\n\n\n\nOnce you’ve selected the samples you want, you can press Validate selection below the table box. You should then see a series loading-spinner, and then a new table will appear below (Figure 16.7).\n\n\n\n\n\n\nFigure 16.7: The AMDirT viewer with the library table loaded, with a range of libraries listed corresponding to the samples selected in the previous table with their AncientMetagenomeDir metadata. The tickboxes are unticked, and a has a red ‘validate library selection’ button below it.\n\n\n\nThis table contains all the AncientMetagenomeDir recorded library metadata for the samples you selected.\nYou can filter this table in exactly the same way as you did for the samples table, and reorder columns in the same way. For example, let’s select only libraries that underwent a genome capture (Figure 16.8).\n\n\n\n\n\n\nFigure 16.8: The AMDirT viewer with the library table loaded, and the library_strategy column filter open and only ‘Targeted-Capture’ selected in the filter.\n\n\n\nAgain, you can select the libraries of interested after the filtering (Figure 16.9).\n\n\n\n\n\n\nFigure 16.9: The AMDirT viewer with the library table loaded, and the four ‘Targeted-Capture’ libraries row’s tickboxes selected.\n\n\n\nOnce selected, you can press Validate library selection. A lot of buttons should then appear below the library table (Figure 16.10)!\n\n\n\n\n\n\nFigure 16.10: The AMDirT viewer after pressing the ‘validate selection’ button. A range of buttons are displayed at the bottom including a warning message, with the buttons offering download of a range of files such as download scripts, AncientMetagenomeDir library tables and input configuration sheets for a range of ancient DNA bioinformatics pipelines. Hovering over the ‘sample download script’ button produces a tooltip with an estimate of the expected download size of all selected FASTQ files\n\n\n\nYou should have four categories of buttons:\n\nDownload AncientMetagenomeDir Library Table\nDownload Curl sample download script\nDownload &lt;tool/pipeline name&gt; input TSV\nDownload Citations as BibText\n\nThe purposes of the buttons are as follows:\n\nThe first button is to download a table containing all the AncientMetagenomeDir metadata of the selected samples.\nThe second is for generating a download script that will allow you to immediately download all sequencing data of the samples you selected.\nThe third set of buttons generate (partially!) pre-configured input files for use in dedicated ancient DNA pipeline such as nf-core/eager (Fellows Yates, Lamnidis, et al. 2021), PALAEOMIX (Schubert et al. 2014), and/or and aMeta (Pochon et al. 2022).\nFinally, the fourth button generates a text file with (in most cases) all the citations of the data you downloaded, in a format accepted by most reference/citation managers.\n\nIt’s important to note you are not necessarily restricted to Curl (https://curl.se/) for downloading the data. AMDirT aims to add support for whatever tools or pipelines requested by the community. For example, an already supported downloading tool alternative is the nf-core/fetchNGS (https://nf-co.re/fetchngs) pipeline. You can select these using the drop-down menus on the left hand-side.\nFor the next step of this tutorial, we will press the following buttons:\n\nDownload AncientMetagenomeDir Library Table\nDownload Curl sample download script\nDownload nf-core/eager input TSV\nDownload Citations as BibText\n\nYour browser should then download two .tsv files, one .sh file and one .bib file into it’s default location.\nOnce everything is downloaded, you can close the tab of the web browser, and in the terminal you can press ctrl + c to shutdown the tool.\n\n\n\n\n\n\nNote\n\n\n\nDo not worry if you get a ‘Citation information could not be resolved’ warning! This is occasionally expected for some publications due to a CrossRef metadata information problem when requesting reference information about a DOI.\n\n\n\n\n16.4.2 Inspecting AMDirT viewer Output\nLets look at the files that AMDirT has generated for you.\nFirst we should mv our files from the directory that your web browser downloaded the files into into somewhere safer.\nTo start, we make sure we’re still in the chapter’s data directory (/&lt;path&gt;/&lt;to&gt;/accessing-ancient-metagenomic-data/), and move move the files into it.\nmv ~/Downloads/AncientMetagenomeDir_* .\n\n\n\n\n\n\nWarning\n\n\n\nThe default download directory on most Unix desktops is normally something like ~/Downloads/, however may vary on your machine!\n\n\nThen we can look inside the chapter’s directory. We should see at least the following four files.\nls\nAncientMetagenomeDir_bibliography.bib\nAncientMetagenomeDir_curl_download_script.sh\nAncientMetagenomeDir_filtered_samples.tsv\nAncientMetagenomeDir_nf_core_eager_input_table.tsv\nWe can simply run cat of the four files downloaded by AMDirT to look inside (the files starting with AncientMetagenomeDir_). If you run cat on the curl download script, you should see a series of curl commands with the correct ENA links for you for each of the samples you wish to download.\ncat AncientMetagenomeDir_curl_download_script.sh\n#!/usr/bin/env bash\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR409/000/ERR4093860/ERR4093860_1.fastq.gz -o ERR4093860_1.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR409/000/ERR4093860/ERR4093860_2.fastq.gz -o ERR4093860_2.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR409/006/ERR4093846/ERR4093846.fastq.gz -o ERR4093846.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR409/005/ERR4093845/ERR4093845.fastq.gz -o ERR4093845.fastq.gz\ncurl -L ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR409/008/ERR4093838/ERR4093838.fastq.gz -o ERR4093838.fastq.gz\nBy providing this script for you, AMDirT facilitates fast download of files of interest by replacing the one-by-one download commands for each sample with a single command!\nbash AncientMetagenomeDir_curl_download_script.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\ncurl: (28) Timeout was reached\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\ncurl: (28) Timeout was reached\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 7749k  100 7749k    0     0  12.5M      0 --:--:-- --:--:-- --:--:-- 12.6M\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  836k  100  836k    0     0  1634k      0 --:--:-- --:--:-- --:--:-- 1631k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 4042k  100 4042k    0     0  7117k      0 --:--:-- --:--:-- --:--:-- 7117k\n\nRunning this command should result in progress logs of the downloading of the data of the four selected samples!\nWe can check the output by running ls to verify we have seven FASTQ files (five single end, and one paired end libraries).\nOnce the four samples are downloaded, AMDirT then facilitates fast processing of the data, as the eager script can be given directly to nf-core/eager as input. Importantly by including the library metadata (mentioned above), researchers can leverage the complex automated processing that nf-core/eager can perform when given such relevant metadata.\ncat AncientMetagenomeDir_nf_core_eager_input_table.tsv\nSample_Name Library_ID  Lane    Colour_Chemistry    SeqType Organism    Strandedness    UDG_Treatment   R1  R2  BAM\nVK388   ERR4093838  0   4   SE  Homo sapiens    double  none    ERR4093838.fastq.gz NA  NA\nVK515   ERR4093845  0   4   SE  Homo sapiens    double  none    ERR4093845.fastq.gz NA  NA\nVK515   ERR4093846  0   2   SE  Homo sapiens    double  none    ERR4093846.fastq.gz NA  NA\nVK515   ERR4093860  0   2   PE  Homo sapiens    double  none    ERR4093860_1.fastq.gz   ERR4093860_2.fastq.gz   NA\nFinally, we can look into the BibTeX citations file (*bib) which will provide you with the citation information of all the downloaded data and AncientMetagenomeDir itself.\n\n\n\n\n\n\nWarning\n\n\n\nThe contents of this file is reliant on indexing of publications on CrossRef. In some cases not all citations will be present (as per the warning), so this should be double checked!\n\n\ncat AncientMetagenomeDir_bibliography.bib\n @article{M_hlemann_2020, title={Diverse variola virus (smallpox) strains were widespread in northern Europe in the Viking Age}, volume={369}, ISSN={1095-9203}, url={http://dx.doi.org/10.1126/science.aaw8977}, DOI={10.1126/science.aaw8977}, number={6502}, journal={Science}, publisher={American Association for the Advancement of Science (AAAS)}, author={Mühlemann, Barbara and Vinner, Lasse and Margaryan, Ashot and Wilhelmson, Helene and de la Fuente Castro, Constanza and Allentoft, Morten E. and de Barros Damgaard, Peter and Hansen, Anders Johannes and Holtsmark Nielsen, Sofie and Strand, Lisa Mariann and Bill, Jan and Buzhilova, Alexandra and Pushkina, Tamara and Falys, Ceri and Khartanovich, Valeri and Moiseyev, Vyacheslav and Jørkov, Marie Louise Schjellerup and Østergaard Sørensen, Palle and Magnusson, Yvonne and Gustin, Ingrid and Schroeder, Hannes and Sutter, Gerd and Smith, Geoffrey L. and Drosten, Christian and Fouchier, Ron A. M. and Smith, Derek J. and Willerslev, Eske and Jones, Terry C. and Sikora, Martin}, year={2020}, month=jul }\n\n @article{Fellows_Yates_2021, title={Community-curated and standardised metadata of published ancient metagenomic samples with AncientMetagenomeDir}, volume={8}, ISSN={2052-4463}, url={http://dx.doi.org/10.1038/s41597-021-00816-y}, DOI={10.1038/s41597-021-00816-y}, number={1}, journal={Scientific Data}, publisher={Springer Science and Business Media LLC}, author={Fellows Yates, James A. and Andrades Valtueña, Aida and Vågene, Åshild J. and Cribdon, Becky and Velsko, Irina M. and Borry, Maxime and Bravo-Lopez, Miriam J. and Fernandez-Guerra, Antonio and Green, Eleanor J. and Ramachandran, Shreya L. and Heintzman, Peter D. and Spyrou, Maria A. and Hübner, Alexander and Gancz, Abigail S. and Hider, Jessica and Allshouse, Aurora F. and Zaro, Valentina and Warinner, Christina}, year={2021}, month=jan }\n\nThis file can be easily loaded into most reference managers and then have all the citations quickly added to your manuscripts.\n\n\n\n\n\n\nWarning\n\n\n\nAs in note above sometimes not all citation information can be retrieved using web APIs, so you should always validate all citations of all data you have downloaded are represented.\n\n\n\n\n16.4.3 AMDirT download and convert\nIf you’re less of a GUI person and consider yourself a command-line wizard, you can also use the AMDirT download and AMDirT convert commands instead of the GUI version.\nMake a new directory called cli, and change into it.\nmkdir cli && cd cli/\nIn this case you must supply your own filtered AncientMetagenomeDir samples table, and use command line options to specify which files to generate.\nFor this, we will download the samples table, use a bit of bash filtering, and then use the AMDirT convert command to generate the same downstream-ready files as we did with the GUI.\nFirst we can download the ancientsinglegenome-hostassociated samples table with the following command.\nAMDirT download -t ancientsinglegenome-hostassociated -y samples\nThis will produce a file called ancientsinglegenome-hostassociated_samples_v24.03.0.tsv in your current directory (by default).\nls\nancientsinglegenome-hostassociated_samples_v24.03.0.tsv\nThen we can use a bit of bash to filter the table in the same way as we did in the GUI. In this command, we tell awk that the column separator is a tab, print the row if either it’s the first record (line of the file), or if column one matches Muhlemann2020 and column seven matches Norway.\nawk -F \"\\t\" 'NR==1 || $1 == \"Muhlemann2020\" && $7 == \"Norway\"' ancientsinglegenome-hostassociated_samples_v24.03.0.tsv &gt; ancientsinglegenome-hostassociated_samples_v24.03.0_filtered.tsv\nThen, we can pass this filtered table to the AMDirT convert command to firstly retrieve the library-level metadata.\nAMDirT convert --librarymetadata ancientsinglegenome-hostassociated_samples_v24.03.0_filtered.tsv ancientsinglegenome-hostassociated\nThis has downloaded a new file called AncientMetagenomeDir_filtered_libraries.tsv, which we can then further filter in the same away to match the desired libraries as we picked during the GUI section of the tutorial!\nawk -F \"\\t\" 'NR==1 || $14 == \"Targeted-Capture\"' AncientMetagenomeDir_filtered_libraries.tsv &gt; AncientMetagenomeDir_filtered_libraries_capturedonly.tsv\nThen with these two filtered files, one for samples,and one for libraries, we can supply them to the convert command to generate the same download scripts, eager input samplesheet, and citation .bib file as we did before!\nAMDirT convert -o . --bibliography --curl --eager --libraries AncientMetagenomeDir_filtered_libraries_capturedonly.tsv ancientsinglegenome-hostassociated_samples_v24.03.0_filtered.tsv ancientsinglegenome-hostassociated\nYou should see a few messages saying Writing &lt;XYZ&gt;, and then if we run ls, you should see the same resulting files starting with AncientMetagenomeDir_ as before with the GUI!\nls\nAncientMetagenomeDir_bibliography.bib\nAncientMetagenomeDir_curl_download_script.sh\nAncientMetagenomeDir_filtered_libraries.tsv\nAncientMetagenomeDir_filtered_libraries_capturedonly.tsv\nAncientMetagenomeDir_nf_core_eager_input_table.tsv\nancientsinglegenome-hostassociated_samples_v24.03.0.tsv\nancientsinglegenome-hostassociated_samples_v24.03.0_filtered.tsv\nTherefore the convert command route of AMDirT allows you to include AMDirT in more programmatic workflows to make downloading data more efficiently.\n\n\n16.4.4 AMDirT Practise\n\n\n\n\n\n\nQuestion\n\n\n\nTry to use your preferred AMDirT interface (GUI or CLI) to find the number of all single-stranded libraries of dental calculus metagenomes, published since 2021\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer is 16 libraries across 2 publications: Klapper et al. (2023) and Fotani et al. (2023)\nYou can calculate this with the CLI method as follows.\n## Download the ancient metagenome table\nAMDirT download -t ancientmetagenome-hostassociated -y samples\n\n## Filter to just dental calculus since 2021\nawk -F \"\\t\" 'NR==1 || $2 &gt;= 2021 && $13 == \"dental calculus\"' ancientmetagenome-hostassociated_samples_v24.03.0.tsv &gt; ancientmetagenome-hostassociated_samples_v24.03.0_filtered.tsv\n\n## Get the library metadata of the dental calculus (might take a little bit of time)\nAMDirT convert --librarymetadata ancientmetagenome-hostassociated_samples_v24.03.0_filtered.tsv ancientmetagenome-hostassociated\n\n## Filter to just single stranded libraries\nawk -F \"\\t\" 'NR==1 || $9 == \"single\"' AncientMetagenomeDir_filtered_libraries.tsv &gt; AncientMetagenomeDir_filtered_libraries_singlestranded.tsv\n\n## Count the number of libraries, starting from row 2 to skip the header\ntail -n +2 AncientMetagenomeDir_filtered_libraries_singlestranded.tsv | wc -l",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#git-practise",
    "href": "accessing-ancient-metagenomic-data.html#git-practise",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.5 Git Practise",
    "text": "16.5 Git Practise\nA critical factor of AncientMetagenomeDir is that it is community-based. The community curates all new submissions to the repository, and this all occurs with Git and GitHub.\nThe data is hosted and maintained on GitHub - new publications are evaluated on issues, submissions created on branches, made by pull requests, and PRs reviewed by other members of the community.\nYou can see the workflow in the image below (Figure 16.11) from the original AncientMetagenomeDir publication (Fellows Yates, Andrades Valtueña, et al. 2021), and read more about the workflow on the AncientMetagenomeDir wiki (https://github.com/SPAAM-community/AncientMetagenomeDir/wiki).\n\n\n\n\n\n\nFigure 16.11: Overview of the AncientMetagenomeDir contribution workflow. Potential publications are added as a GitHub issue where they undergo relevance evaluation. Once approved, a contributor makes a branch on the AncientMetagenomeDir GitHub repository, adds the new lines of metadata for the relevant samples and libraries, and once ready opens a pull request against the main repository. The pull request undergoes an automated consistent check against a schema and then undergoes a human-based peer review for accuracy against AncientMetagenomeDir guidelines. Once approved, the pull request is merged, and periodically the dataset is released on GitHub and automatically archived on Zenodo.\n\n\n\nAs AncientMetagenomeDir is on GitHub, the means we can also use this repository to try out our Git skills we learnt in the chapter Introduction to Git(Hub)!\n\n\n\n\n\n\nQuestion\n\n\n\nYour task is to complete the following steps. However, we’ve replaced the correct Git terminology with generic verbs, indicated with words in quotes. Recreate the following, but also note down what the correct Git terminology is.\n\nMake a ‘copy’ the jfy133/AncientMetagenomeDir (https://github.com/jfy133/AncientMetagenomeDir) GitHub repository to your account\n\n\n\n\n\n\n\nNote\n\n\n\nWe are forking a personal fork of the main repository to ensure we don’t accidentally edit the main AncientMetagenomeDir repository!\n\n\n\n‘Download’ the copied repo to your local machine\n‘Change’ into a new branch called dev\nModify the file ancientsinglegenome-hostassociated_samples.tsv. Add the following line to the end of the TSV\nMake sure your text editor doesn’t replace tabs with spaces!\nLong2022    2022    10.1038/s42003-022-03527-1  Basilica of St. Domenico Maggiore   40.848  14.254  Italy   NASD1   Homo sapiens    400 10.1038/s42003-022-03527-1  bacteria    Escherichia coli    calcified nodule    chromosome  SRA raw PRJNA810725 SRS12115743\n‘Save’, ‘Record’, and ‘Send’ back to Git(Hub)\nOpen a ‘merge suggestion’ proposing the changes to the original jfy133/AncientMetagenomeDir repo\n\nMake sure to put ‘Summer school’ in the title of the ‘Request’\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nFork the jfy133/AncientMetagenomeDir (https://github.com/jfy133/AncientMetagenomeDir) repository to your account (Figure 16.12)\n\n\n\n\n\n\n\nFigure 16.12: Screenshot of the top of the jfy133/AncientMetagenomeDir github repository, with the green Code button pressed and the SSH tab open\n\n\n\n\nClone the copied repo to your local machine\ngit clone git@github.com:&lt;YOUR_USERNAME&gt;/AncientMetagenomeDir.Git\ncd AncientMetagenomeDir\nSwitch to a new branch called dev\n\ngit switch -c dev\nModify ancientsinglegenome-hostassociated_samples.tsv\necho \"Long2022  2022    10.1038/s42003-022-03527-1  Basilica of St. Domenico Maggiore   40.848  14.254  Italy   NASD1   Homo sapiens    400 10.1038/s42003-022-03527-1  bacteria    Escherichia coli    calcified nodule    chromosome  SRA raw PRJNA810725 SRS12115743\" &gt;&gt; ancientsinglegenome-hostassociated/samples/ancientsinglegenome-hostassociated_samples.tsv\nAdd, Commit and Push back to your Fork on Git(Hub)\ngit add ancientsinglegenome-hostassociated/samples/ancientsinglegenome-hostassociated_samples.tsv\ngit commit -m 'Add Long2022'\ngit push\nOpen a Pull Request adding changes to the original jfy133/AncientMetagenomeDir repo (Figure 16.13)\n\nMake sure to make the pull request against jfy133/AncientMetagenomeDir and NOT SPAAM-community/AncientMetagenomeDir\nMake sure to put ‘Summer school’ in the title of the pull request!\n\n\n\n\n\n\n\n\nFigure 16.13: Screenshot of the top of the jfy133/AncientMetagenomeDir github repository, with the green pull request button displayed",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#summary",
    "href": "accessing-ancient-metagenomic-data.html#summary",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.6 Summary",
    "text": "16.6 Summary\n\nReporting of metadata messy! Consider when publishing your own work!\n\nUse AncientMetagenomeDir as a template for supplementary tables!\n\nUse AncientMetagenomeDir and AMDirT to rapidly find and download public ancient metagenomic data\n\nYou can use it to generate templates for dowsntream processing pipelines!\n\nContribute to AncientMetagenomeDir with git\n\nIt is community curated: it will be as good as you make it, the more people who contribute, the easier and better it is.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#optional-clean-up",
    "href": "accessing-ancient-metagenomic-data.html#optional-clean-up",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.7 (Optional) clean-up",
    "text": "16.7 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nWhen closing your jupyter notebook(s), say no to saving any additional files.\nPress ctrl + c on your terminal, and type y when requested. Once completed, the command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/accessing-metagenomic-data directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -r /&lt;PATH&gt;/&lt;TO&gt;/accessing-metagenomic-data*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name accessing-ancient-metagenomic-data --all -y",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#resources",
    "href": "accessing-ancient-metagenomic-data.html#resources",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.8 Resources",
    "text": "16.8 Resources\n\nAncientMetagenomeDir repository\nAMDirT web server\nAMDirT documentation\nAMDirT repository",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "accessing-ancient-metagenomic-data.html#references",
    "href": "accessing-ancient-metagenomic-data.html#references",
    "title": "16  Accessing Ancient Metagenomic Data",
    "section": "16.9 References",
    "text": "16.9 References\n\n\n\n\nAnagnostou, Paolo, Marco Capocasa, Nicola Milia, Emanuele Sanna, Cinzia Battaggia, Daniela Luzi, and Giovanni Destro Bisol. 2015. “When Data Sharing Gets Close to 100%: What Human Paleogenetics Can Teach the Open Science Movement.” PloS One 10 (3): e0121409. https://doi.org/10.1371/journal.pone.0121409.\n\n\nBorry, Maxime, Adrian Forsythe, Aida Andrades Valtueña, Alexander Hübner, Anan Ibrahim, Andrea Quagliariello, Anna E White, et al. 2024. “Facilitating Accessible, Rapid, and Appropriate Processing of Ancient Metagenomic Data with AMDirT.” F1000Research 12 (926): 926. https://doi.org/10.12688/f1000research.134798.2.\n\n\nFellows Yates, James A, Aida Andrades Valtueña, Åshild J Vågene, Becky Cribdon, Irina M Velsko, Maxime Borry, Miriam J Bravo-Lopez, et al. 2021. “Community-Curated and Standardised Metadata of Published Ancient Metagenomic Samples with AncientMetagenomeDir.” Scientific Data 8 (1): 31. https://doi.org/10.1038/s41597-021-00816-y.\n\n\nFellows Yates, James A, Thiseas C Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nFontani, Francesco, Rosa Boano, Alessandra Cinti, Beatrice Demarchi, Sarah Sandron, Simone Rampelli, Marco Candela, et al. 2023. “Bioarchaeological and Paleogenomic Profiling of the Unusual Neolithic Burial from Grotta Di Pietra Sant’angelo (Calabria, Italy).” Scientific Reports 13 (1): 11978. https://doi.org/10.1038/s41598-023-39250-y.\n\n\nKlapper, Martin, Alexander Hübner, Anan Ibrahim, Ina Wasmuth, Maxime Borry, Veit G Haensch, Shuaibing Zhang, et al. 2023. “Natural Products from Reconstructed Bacterial Genomes of the Middle and Upper Paleolithic.” Science (New York, N.Y.), May, eadf5300. https://doi.org/10.1126/science.adf5300.\n\n\nMühlemann, Barbara, Lasse Vinner, Ashot Margaryan, Helene Wilhelmson, Constanza de la Fuente Castro, Morten E. Allentoft, Peter de Barros Damgaard, et al. 2020. “Diverse Variola Virus (Smallpox) Strains Were Widespread in Northern Europe in the Viking Age.” Science 369 (6502). https://doi.org/10.1126/science.aaw8977.\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579.\n\n\nSchubert, Mikkel, Luca Ermini, Clio Der Sarkissian, Hákon Jónsson, Aurélien Ginolhac, Robert Schaefer, Michael D Martin, et al. 2014. “Characterization of Ancient and Modern Genomes by SNP Detection and Phylogenomic and Metagenomic Analysis Using PALEOMIX.” Nature Protocols 9 (5): 1056–82. https://doi.org/10.1038/nprot.2014.063.\n\n\nWilkinson, Mark D, Michel Dumontier, I Jsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (March): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing Ancient Metagenomic Data</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html",
    "href": "ancient-metagenomic-pipelines.html",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "",
    "text": "17.1 Introduction\nA pipeline is a series of linked computational steps, where the output of one process becomes the input of the next. Pipelines are critical for managing the huge quantities of data that are now being generated regularly as part of ancient DNA analyses. In this chapter we will go through three dedicated ancient DNA pipelines - all with some (or all!) functionality geared to ancient metagenomics - to show we how we can speed up the more routine aspects of the basic analyses we’ve learnt about earlier in this text book through workflow automation.\nWe will introduce:\nKeep in mind that that there are many other pipelines that exist, and picking which one often they come down to personal preference, such as which functionality they support, which language they are written in, and whether their computational requirements can fit in our available resources.\nOther examples of other ancient DNA genomic pipelines include Paleomix (https://paleomix.readthedocs.io/en/stable, Schubert et al. 2014), and Mapache (https://github.com/sneuensc/mapache, Neuenschwander et al. 2023), and for ancient metagenomics: metaBit (https://bitbucket.org/Glouvel/metabit/src/master/, Louvel et al. 2016) and HAYSTAC (https://github.com/antonisdim/haystac, Dimopoulos et al. 2022).",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#introduction",
    "href": "ancient-metagenomic-pipelines.html#introduction",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "",
    "text": "nf-core/eager (https://nf-co.re/eager) - a generalised aDNA ‘workhorse’ pipeline that can do both ancient genomics and (basic) metagenomics (Fellows Yates et al. 2021)\naMeta (https://github.com/NBISweden/aMeta) - a pipeline for resource efficient and accurate ancient microbial detection and authentication (Pochon et al. 2022)\nnf-core/mag (https://nf-co.re/mag) - a de novo metagenomics assembly pipeline (Krakau et al. 2022) that includes a dedicated ancient DNA mode for damage correction and validation.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#workflow-managers",
    "href": "ancient-metagenomic-pipelines.html#workflow-managers",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.2 Workflow managers",
    "text": "17.2 Workflow managers\nAll the pipelines introduced in this chapter utilise workflow managers. These are software that allows users to ‘chain’ together the inputs and outputs distinct ‘atomic’ steps of a bioinformatics analysis - e.g., separate terminal commands of different bioinformatic tools, so that ‘we don’t have to’. we have already seen very basic workflows or ‘pipelines’ when using the bash ‘pipe’ (|) in the Introduction to the Command Line chapter, where the each row of text the output of one command was ‘streamed’ into the next command.\nHowever in the case of bioinformatics, we are often dealing with non-row based text files, meaning that ‘classic’ command line pipelines don’t really work. Instead this is where bioinformatic workflow managers come in: they handle the passing of files from one tool to the next but in a reproducible manager.\nModern computational bioinformatic workflow managers focus on a few main concepts. To summarise Wratten, Wilm, and Göke (2021), these areas are: data provenance, portability, scalability, re-entrancy - all together which contribute to ensuring reproducibility of bioinformatic analyses.\nData provenance refers to the ability to track and visualise where each file goes and gets processed, as well as metadata about each file and process (e.g., What version of a tool was used? What parameters were used in that step? How much computing resources were used).\nPortability follows from data provenance where it’s not just can the entire execution of the pipeline be reconstructed - but can it also be run with the same results on a different machine? This is important to ensure that we can install and test the pipeline on our laptop, but when we then need to do heavy computation using real data, that it will still be able to execute on a high-performance computing cluster (HPC) or on the cloud - both that have very different configurations. This is normally achieved through the use of reusable software environments such as as conda (https://docs.conda.io/en/latest/) or container engines such as docker (https://www.docker.com/), and tight integration with HPC schedulers such as SLURM (https://slurm.schedmd.com/documentation.html).\nAs mentioned earlier, not having to run each command manually can be a great speed up to our analysis. However this needs to be able to be Scalable that it the workflow is still efficient regardless whether we’re running with one or ten thousands samples - modern workflow managers perform resource requirement optimisation and scheduling to ensure that all steps of the pipeline will be executed in the most resource efficient manner so it completes as fast as possible - but regardless of of the number of input data.\nFinally, as workflows get bigger and longer, re-entrancy has become more important, i.e., the ability to re-start a pipeline run that got stuck halfway through due to an error.\nAll workflow managers have different ways of implementing the concepts above, and these can be very simple (e.g., Makefiles, https://en.wikipedia.org/wiki/Make_(software))) to very powerful and abstract (e.g. Workflow Description Language, https://github.com/openwdl/wdl). In this chapter we will use pipelines that use two popular workflow managers in bioinformatics, Nextflow (https://nextflow.io, (Di Tommaso et al. 2017)) and Snakemake (https://snakemake.github.io, (Mölder et al. 2021)).\nThis chapter will not cover how to write our own workflow, as this would require a whole other textbook. However it is recommended to learn and use workflow managers when carrying out repetitive or routine bioinformatic analysis (Nextflow and Snakemake being two popular ones in bioinformatics). Use of workflow managers can help make our work more efficiently (as we only run one command, rather than each step separately), but also more reproducible by reducing the risk of user error when executing each step: the computer will do exactly what we tell it, and if we don’t change anything, will do the exact same thing every time. If you’re interested in writing our own workflows using workflow managers, many training and tutorials exist on the internet (e.g., for Nextflow there is the official training: https://training.nextflow.io/ or from software carpentries: https://carpentries-incubator.github.io/workflows-nextflow/, or the official training for snakemake: https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#tutorial).",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "href": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.3 What is nf-core/eager?",
    "text": "17.3 What is nf-core/eager?\nnf-core/eager is a computational pipeline specifically designed for preprocessing and analysis of ancient DNA data (Figure 17.1) (Fellows Yates et al. 2021). It is a reimplementation of the previously published EAGER (Efficient Ancient Genome Reconstruction) pipeline (Peltzer et al. 2016) written in the workflow manager Nextflow. In addition to reimplementing the original genome mapping and variant calling pipeline, in a more reproducible and portable manner the pipeline also included additional new functionality particularly for researchers interested in microbial sciences, namely a dedicated genotyper and consensus caller designed for low coverage genomes, the ability to get breadth and depth coverage statistics for particular genomic features (e.g. virulence genes), but also automated metagenomic screening and authentication of the off-target reads from mapping (e.g. against the host reference genome).\n\n\n\n\n\n\nFigure 17.1: nf-core/eager workflow summary in the form of a ‘metro map’ style diagram. FASTQ or BAM input for red and yellow lines (representing eukaryotic and prokaryotic workflows) goes through FastQC, AdapterRemoval, alignment to a reference FASTA input with a range of alignments, before going through BAM filtering, deduplication, in silico damage removal, and variant calling. Multiple statistics steps come out of the deduplication ‘station’. The blue line represents the metagenomic workflow, where off-target reads come out of the BAM filtering ‘station’, and goes through complexity filtering with BBDuk, MALT or Kraken2, and optionally into MaltExtract for MALT output.\n\n\n\nA detailed description of steps in the pipeline is available as part of nf-core/eager’s extensive documentation. For more information, check out the usage documentation (https://nf-co.re/eager/2.5.2/docs/usage/).\nBriefly, nf-core/eager takes at a minimum standard input file types that are shared across the genomics field, i.e., raw FASTQ files or aligned reads in bam format, and a reference fasta. nf-core/eager performs preprocessing of this raw data, including adapter clipping, read merging, and quality control of adapter-trimmed data. nf-core/eager then carries mapping using a variety of field-standard shot-read alignment tools with default parameters adapted for short and damaged aDNA sequences. The off-target reads from host DNA mapping can then go into metagenomic classification and authentication (in the case of MALT (Vågene et al. 2018; Herbig et al. 2016)). After genomic mapping, BAM files go through deduplication of PCR duplicates, damage profiling and removal, and finally variant calling. A myriad of additional statistics can be generated depending on the users preference. Finally, nf-core eager uses MultiQC (https://multiqc.info/, (Ewels et al. 2016)) to create an integrated html report that summarises the output/results from each of the pipeline steps.\n\n\n\n\n\n\nQuestion\n\n\n\nWhy cannot we use a standard genomics mapping and variant calling pipeline, such as nf-core/sarek Hanssen et al. (2024) for ancient DNA ?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMany tools used in standard genomics pipelines assume ‘modern’ data quality, i.e., high coverage, low error rates, and long read lengths.\nIn ancient DNA we need to use tools better suited for low coverage, and short read lengths. Furthermore, we would like additional tools for authenticating our ancient DNA - characteristics that you would not expect to find in modern data and thus not included in modern pipelines.\n\n\n\n\n17.3.1 Running nf-core/eager\nFor the practical portion of this chapter, we will utilise sequencing data from four aDNA libraries, which we should have already downloaded from NCBI. If not, please see the Preparation section above . We will use nf-core/eager to perform a typical microbial genomic analysis, i.e., reconstruction of an ancient genome to generate variant calls that can be used for generating phylogenomic trees and other evolutionary analysis, and gene feature coverage statistics to allow insight into the functional aspects of the genome.\nThese four libraries come from from two ancient individuals, GLZ002 and KZL002. GLZ002 comes from the Neolithic Siberian site of Glazkovskoe predmestie (Yu et al. 2020) and was radiocarbon dated to 3081-2913 calBCE. KZL002 is an Iron Age individual from Kazakhstan, radiocarbon dated to 2736-2457 calBCE (Andrades Valtueña et al. 2022). Both individuals were originally analysed for human population genetic analysis, but when undergoing metagenomic screening of the off-target reads, both set of authors identified reads from Yersinia pestis from these individuals - the bacterium that causes plague. Subsequently the libraries from these individuals were processed using hybridisation capture to increase the number of Y. pestis sequences available for analysis.\nOur aims in the following tutorial are to:\n\nPreprocess the FASTQ files by trimming adapters and merging paired-end reads\nAlign reads to the Y. pestis reference and compute the endogenous DNA percentage\nFilter the aligned reads to remove host DNA\nRemove duplicate reads for accurate coverage estimation and genotyping\nGenerate statistics on gene features (e.g. virulence factors)\nMerge data by sample and perform genotyping on the combined dataset\nReview quality control data to evaluate the success of the previous steps\n\nLet’s get started!\n\n\n\n\n\n\nWarning\n\n\n\nnf-core/eager v2 requires an older version of Nextflow - if installing manually, ensure we do not have Nextflow version any greater than v22.10.6!\n\n\nFirst, download the latest version of the nf-core/eager repo (or check for updates if we have a previously-installed version).\nnextflow pull nf-core/eager\nNext we can re-visit AMDirT (see Accessing Ancient Metagenomic Data, (Borry et al. 2024)) to download a pre-prepared configuration file for nf-core/eager\n\nLoad AMDirt (AMDirT viewer), and select the latest release and the ‘ancientsinglegenome-hostassociated’ table\nFilter the sample_name column to just show KZL002 and GLZ002, and select these two rows\n\nPress the burger icon on the column\nPress the filter tab and deselect everything\nSearch GLZ002 and select in filter menu\nSearch KZL002 and select in filter menu\nClose filter menu and select the two rows\n\nPress the Validate Selection button\nPress the ‘Download Curl sample download script’, ‘Download nf-core/eager input TSV’, and ‘Download Citations as BibTex’ buttons\nMove the downloaded files into eager/ of this tutorial’s directory\n\n\ni.e., if the files were downloaded to we Downloads/ folder rather than the chapters directory,\ne.g., assuming we’ve got no other previous downloads we can run mv ~/Downloads/AncientMetagenomeDir_* eager/\n\n\n\n\n\n\n\nTip\n\n\n\nWe won’t actually use the BibTex citations file for anything in this tutorial, but it is good habit to always to record and save all citations of any software or data we use!\n\n\nTo download the FASTQ files\n\nMove into the eager/ directory\nRun bash AncientMetagenomeDir_curl_download_script.sh to download the files (this may take ~3 minutes)\n\nNext we now inspect the AMDirT generated input TSV file for nf-core/eager!\ncat AncientMetagenomeDir_nf_core_eager_input_table.tsv\nSample_Name Library_ID  Lane    Colour_Chemistry    SeqType Organism    Strandedness    UDG_Treatment   R1  R2  BAM\nGLZ002  ERR4093961  0   4   PE  Homo sapiens    double  half    ERR4093961_1.fastq.gz   ERR4093961_2.fastq.gz   NA\nGLZ002  ERR4093962  0   4   PE  Homo sapiens    double  full    ERR4093962_1.fastq.gz   ERR4093962_2.fastq.gz   NA\nKZL002  ERR8958768  0   4   PE  Homo sapiens    double  half    ERR8958768_1.fastq.gz   ERR8958768_2.fastq.gz   NA\nKZL002  ERR8958769  0   4   PE  Homo sapiens    double  half    ERR8958769_1.fastq.gz   ERR8958769_2.fastq.gz   NA\nHere we see 10 columns, all pre-filled. The first two columns correspond to sample/library IDs that will be used for data provenance and grouping. When we have sequencing multiple lanes we can speed up preprocessing these independently before merging, so lane can specify this (although not used in this case as we have independent libraries per sample. we can indicate the colour chemistry to indicate whether our data requires additional pre-processing to remove poly-G tails, and then also strandedness and UDG damage treatment status of the libraries if we require further damage manipulation. Finally we provide paths to the FASTQ files or BAM files.\nOther than the raw FASTQ files, we will need a reference genome and annotation coordinates of genes present on the genome. In this case we will use a Yersinia pestis (accession: GCF_001293415.1) reference genome (.fna) and gene feature file (.gff) from NCBI Genome (https://www.ncbi.nlm.nih.gov/genome).\nThese have already been placed in the reference/ directory for us.\n\n\n\n\n\n\nSelf guided: reference download and preparation\n\n\n\n\n\nTo download the required reference genome and annotation file run the following command to download from the NCBI Genome database.\n## Download from NCBI\ncurl -OJX GET \"https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_001293415.1/download?include_annotation_type=GENOME_FASTA,GENOME_GFF,RNA_FASTA,CDS_FASTA,PROT_FASTA,SEQUENCE_REPORT&filename=GCF_001293415.1.zip\" -H \"Accept: application/zip\"\n\nunzip *.zip\nmv ncbi_dataset/data/GCF_001293415.1/* .\n\n## We have to sort the gff file to make it eager compatible\ngffread genomic.gff GCF_001293415.1_ASM129341v1_genomic.gff\n\n\n\nWith all of these, we can run the pipeline!\nFirst lets enter a screen session to make sure we can leave the pipeline running in the background and continue using our terminal.\nscreen -R eager\nconda activate ancient-metagenomic-pipelines\nNow we can construct an eager command from within the data/ directory so that it looks like this.\nnextflow run nf-core/eager -r 2.4.7 \\\n-profile docker \\\n--fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna \\\n--input AncientMetagenomeDir_nf_core_eager_input_table.tsv \\\n--anno_file ../reference/GCF_001293415.1_ASM129341v1_genomic.gff \\\n--run_bam_filtering --bam_unmapped_type discard \\\n--skip_preseq \\\n--run_genotyping --genotyping_tool ug --gatk_ug_out_mode EMIT_ALL_SITES \\\n--run_bcftools_stats --run_bedtools_coverage\nWhen the run starts, we see a long list of process lines with progress bars slowly appearing over time. If we press ctrl + a and then [ to access a ‘navigation’ (called ‘copy’) mode, then use our arrow keys on our keyboard to scroll up and down. To quit this mode just press q.\n\n\n\n\n\n\nTip\n\n\n\nWe don’t normally recommend running analyses in the directory our data is in! It is better to keep data and analysis results in separate directories. Here we are just running eager alongside the data for convenience (i.e., we don’t have to modify the downloaded input TSV)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis run will take between 20m-40m to run on a standard laptop! Time for a break, or we can continue reading this chapter as the critical output data has already been provided for we in the data directory.\n\n\nSo what is this command doing? The different parameters do the following:\n\nTell Nextflow to run the nf-core/eager pipeline with version 2.4.7\nSpecify which computing and software environment to use with -profile\n\nIn this case we are running locally so we don’t specify a computing environment (such as a configuration for an institutional HPC)\nWe use docker as our container engine, which downloads all the software and specific versions needed for nf-core/eager in immutable ‘containers’, to ensure nothing get broken and is as nf-core/eager expects\n\nProvide the various paths to the input files (TSV with paths to FASTQ files, a reference fasta, and the reference fasta’s annotations)\nActivate the various of the steps of the pipeline we’re interested in\n\nWe turn off preseq (e.g. when we know we can’t sequence more)\nWe want to turn on BAM filtering, and specify to generate unmapped reads in FASTQ file (so we could check off target reads e.g. for other pathogens)\nwe turn on genotyping using GATK UnifiedGenotyper (preferred over GATK HaplotypeCaller (Poplin et al. 2018) due to in compatibility with that method to low-coverage data)\nWe turn on variant statistics (from GATK) using bcftools (Danecek et al. 2021), and coverage statistics of gene features using bedtools (Quinlan and Hall 2010)\n\n\nFor full parameter documentation, see the website (https://nf-co.re/eager/2.5.2/parameters).\nAnd now we wait… \n\n\n\n\n\n\nTip\n\n\n\nAs a reminder, to detach from the screen session type ctrl + a then d. To reattach screen -r eager\n\n\nSpecifically for ancient (meta)genomic data, the following parameters and options may be useful to consider when running our own data:\n\n--mapper circularmapper\n\nThis aligner is a variant of bwa aln that allows even mapping across the end of linear references of circular genomes\nAllows reads spanning the start/end of the sequence to be correctly placed, and this provides better coverage estimates across the entire genome\n\n--hostremoval_input_fastq:\n\nAllows re-generation of input FASTQ files but with any reads aligned to the reference genome removed\nCan be useful when dealing with modern or ancient data where there are ethical restrictions on the publication of host DNA\nThe output can be used as ‘raw data’ upload to public data repositories\n\n--run_bam_filtering --bam_unmapped_type\n\nA pre-requisite for performing the metagenomic analysis steps of nf-core/eager\nGenerates FASTQ files off the unmapped reads present in the reference mapping BAM file\n\n--run_bedtools_coverage --anno_file /&lt;path&gt;/&lt;to&gt;/&lt;genefeature&gt;.bed\n\nTurns on calculating depth/breadth of annotations in the provided bed or GFF file, useful for generating e.g. virulence gene presence/absence plots\n\n--run_genotyping --genotyping_tool ug --gatk_ug_out_mode EMIT_ALL_SITES\n\nTurns on genotyping with GATK UnifiedGenotyper\nA pre-requisite for running MultiVCFAnalyzer (Bos et al. 2014) for consensus sequencing creation\nIt’s recommend to use either GATK UnifiedGenotyper or freeBayes (non-MultiVCFAnalyzer compatible!, Garrison and Marth 2012) for low-coverage data\nGATK HaplotypeCaller is not recommended for low coverage data as it performs local de novo assembly around possible variant sites, and this will fail with low-coverage short-read data\n\n--run_multivcfanalyzer --write_allele_frequencies\n\nTurns on SNP table and FASTA consensus sequence generation with MultiVCFAnalyzer (see pre-requisites above)\nBy providing --write_allele_frequencies, the SNP table will also provide the percentage of reads at that position supporting the call. This can help we evaluate the level of cross-mapping from related (e.g. contaminating environmental) species and may question the reliability of any resulting downstream analyses.\n\n--metagenomic_complexity_filtering\n\nAn additional preprocessing step of raw reads before going into metagenomic screening to remove low-complexity reads (e.g. mono- or di-nucleotide repeats)\nRemoving these will speed up and lower computational requirements during classification, and will not bias profiles as these sequences provide no taxon-specific information (i.e., can be aligned against thousands to millions of genomes)\n\n--run_metagenomic_screening\n\nTurns on metagenomic screening with either MALT or Kraken2\nIf running with MALT, can supply --run_maltextract to get authentication statistics and plots (damage, read lengths etc.) for evaluation\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is it critical to report versions of all pipelines and tools?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n(Bioinformatics) software is rarely stable.\nMany tools will update frequently either with bug fixes and new or optimised functionality.\nBoth can cause changes in the output of the software, and thus the results of our analyses. If we want our analyses to be completely reproducible, and increase trust of other scientists in our results, we need to make sure they can generate the same output as we did.\n\n\n\n\n\n17.3.2 Top Tips for nf-core/eager success\n\nScreen sessions\nDepending on the size of our input data, infrastructure, and analyses, running nf-core/eager can take hours or even days (e.g. the example command above will likely take around 20 minutes!).\nTo avoid crashes due to loss of power or network connectivity, try running nf-core/eager in a screen or tmux session.\nscreen -R eager\nMultiple ways to supply input data\nIn this tutorial, a tsv file to specify our input data files and formats. This is a powerful approach that allows nf-core eager to intelligently apply analyses to certain files only (e.g. merging for paired-end but not single-end libraries). However inputs can also be specified using wildcards, which can be useful for fast analyses with simple input data types (e.g. same sequencing configuration, file location, etc.).\nSee the online nf-core/eager documentation (https://nf-co.re/eager/usage) for more details.\n\n\n\n\n\n\n\nExample commands - do not run!\n\n\n\nnextflow run nf-core/eager -r 2.4.7 -profile docker --fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna \\\n--input \"data/*_{1,2}.fastq.gz\" &lt;...&gt; \\\n--udg_type half\n\n\n\nGet our MultiQC report via email\nIf we have GNU mail or sendmail set up on our system, we can add the following flag to send the MultiQC html to our email upon run completion:\n--email \"your_address@something.com\"\nCheck out the nf-core/eager launch GUI\nFor researchers who might be less comfortable with the command line, check out the nf-core/eager launch GUI (https://nf-co.re/launch?pipeline=eager&release=2.5.2)! The GUI also provides a full list of all pipeline options with short explanations for those interested in learning more about what the pipeline can do.\nWhen something fails, all is not lost!\nWhen individual jobs fail, nf-core/eager will try to automatically resubmit that job with increased memory and CPUs (up to two times per job).\nWhen the whole pipeline crashes, we can save time and computational resources by resubmitting with the -resume flag. nf-core/eager will retrieve cached results from previous steps as long as the input is the same.\nMonitor our pipeline in real time with the Seqera Platform\nRegular users may be interested in checking out the Seqera Platform (previously known as Nextflow Tower), a tool for monitoring the progress of Nextflow pipelines in real time. Check the website (https://tower.nf) for more information.\n\n\n\n17.3.3 nf-core/eager output: results files\nOnce completed, the results/ directory of our nf-core/eager run will contain a range of directories that will have output files and tool logs of all the steps of the pipeline. nf-core/eager tries to only save the ‘useful’ files.\nEveryday useful files for ancient (microbial) (meta)genomics typically are in folders such as:\n\ndeduplication/ or merged_bams\n\nThese contain the most basic BAM files we would want to use for downstream analyses (and used in the rest of the genomic workflow of the pipeline)\nThey contain deduplicated BAM files (i.e., with PCR artefacts removed)\nmerged_bams/\n\nThis directory will contain BAMs where multiple libraries of one sample have been merged into one final BAM file, when these have been supplied\n\n\ndamage_rescaling/ or trimmed bam/\n\nThese contain the output BAM files from in silico damage removal, if we have turned on e.g. BAM trimming or damage rescaling\nIf we have multiple libraries of one sample, the final BAMs we want will be in merged_bams/\n\ngenotyping/\n\nThis directory will contain our final VCF files from the genotyping step, e.g. from the GATK or freeBayes tools\n\nMultiVCFAnalyzer/\n\nThis will contain consensus sequences and SNP tables from MultiVCFAnalyzer, which also allows generation of ‘multi-allelic’ SNP position statistics (useful for the evaluation of cross-mapping from contaminants or relatives)\n\nbedtools/\n\nThis directory will contain the depth and breadth statistics of genomic features if a gff or bed file has been provided to the pipeline\nThe files can be used to generate gene heatmaps, that can be used to visualise a comparative presence/absence of virulence factor across genomes (e.g. for microbial pathogens)\n\nmetagenomic_classification or malt_extract\n\nThis directory contains the output RMA6 files from MALT, the profiles and taxon count tables from Kraken2, or the aDNA authentication output statistics from maltExtract.\n\n\nMost other folders contain either intermediate files that are only useful for technical evaluation in the case of problems, or statistics files that are otherwise summarised in the run report.\nSo, before we delve into these folders, it’s normally a good idea to do a ‘quality check’ of the pipeline run. we can do this using the interactive MultiQC pipeline run report.\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is it important to use deduplicated BAMs for downstream genomic analyses?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithout deduplication, you are artificially increasing the confidence in variant calls during genotyping. Many downstream analyses assume that each base call is made up of multiple indepndent ‘observations’ of that particular nucleotide. If you have not deduplicated your alignments, you may have the exact sample molecule represented twice (an artefact of amplification), thus violating the ‘independent observation’ assumption.\n\n\n\n\n\n17.3.4 nf-core/eager output: run report\ncd multiqc/\n\n\n\n\n\n\nNote\n\n\n\nIf we’re impatient, and our nf-core/eager run hasn’t finished yet, we can cancel the run with ctrl + c (possibly a couple of times), and we can open a pre-made file in the data directory under ancient-metagenomic-pipelines/multiqc_report.html\n\n\nIn here we should see a bunch of files, but we should open the multiqc_report.html file in our browser. we can either do this via the command-line (e.g. for firefox firefox multiqc_report.html) or navigate to the file using our file browser and double clicking on the HTML file.\nOnce opened we will see a table, and below it many figures and other tables (Figure 17.2). All of these statistics can help we evaluate the quality of our data, pipeline run, and also possibly some initial results!\n\n\n\n\n\n\nFigure 17.2: Screenshot of initial view of a MultiQC report. The left hand sidebar provides links to various sections of the report, most of them containing summary images of the outputs of all the different tools. On the main panel we see the MultiQC and nf-core/eager logos, some descriptive information about how the report was generated, and finally the top of the ‘General Statistics’ tables, that has columns such as Sample Name, Nr. Input reads, length of input reads, %trimmed, etc. Each numeric column contains a coloured bar chart to help readers to quickly evaluate the number of a given sample against all others in the table\n\n\n\nTypically we will look at the General Statistics table to get a rough overview of the pipeline run.\nIf we hover our cursor over the column headers, we can see which tool the column’s numbers came from, however generally the columns go in order of left to right, where the left most columns are from earlier in the pipeline run (e.g. removing adapters), to variant calling statistics (e.g. number of variants called). we can also configure which columns to display using the Configure columns button.\nIt’s important to note that in MultiQC tables, we may have duplicate rows from the same library or sample. This is due to MultiQC trying to squish in as many statistics from as many steps of the pipeline as possible (for example, statistics on each of a pair of FASTQ files, and then statistics on the single merged and mapped BAM file), so we should play around with the column configuration to help we visualise the best way to initially evaluate our data.\nThe bulk of the MultiQC report is made up of per-tool summary plots (e.g., barcharts, linecharts etc.). Most of these will be interactive, we can hover over lines and bars to get more specific numbers of each plot. However the visualisations are aimed at helping we identify possible outliers that may represent failed samples or libraries.\nEvaluating how good the data is and how well the run went will vary depending on the dataset and the options selected. However the nf-core/eager tutorials (https://nf-co.re/eager/usage#tutorial—how-to-set-up-nf-coreeager-for-pathogen-genomics) have a good overview of questions we can ask from our MultiQC report to see whether our data is good or not. We shamelessly copy these questions here (as the overlap authors of both the the nf-core/eager documentation and this text book is rather high).\nOnce completed, we can try going through the MultiQC report the command we executed above, and compare against the questions below. Keep in mind we have a sample N of two, so many of the questions in regards to identifying ‘outliers’ may be difficult.\nThe following questions are by no means comprehensive, but rather represent the ‘typical’ questions the nf-core/eager developers asked of their own data and reports. However they can act as a good framework for thinking critically about our own results.\nGeneral Stats Table\n\nDo we see the expected number of raw sequencing reads (summed across each set of FASTQ files per library) that was requested for sequencing?\nDoes the percentage of trimmed reads look normal for aDNA, and do lengths after trimming look short as expected of aDNA?\nDoes the Endogenous DNA (%) columns look reasonable (high enough to indicate we have received enough coverage for downstream, and/or do we lose an unusually high reads after filtering )\nDoes ClusterFactor or ‘% Dups’ look high (e.g. &gt;2 or &gt;10% respectively - high values suggesting over-amplified or badly preserved samples i.e. low complexity; note that genome-enrichment libraries may by their nature look higher)\nDo we see an increased frequency of C&gt;Ts on the 5’ end of molecules in the mapped reads?\nDo median read lengths look relatively low (normally &lt;= 100 bp) indicating typically fragmented aDNA?\nDoes the % coverage decrease relatively gradually at each depth coverage, and does not drop extremely drastically\nDoes the Median coverage and percent &gt;3x (or whatever we set) show sufficient coverage for reliable SNP calls and that a good proportion of the genome is covered indicating we have the right reference genome?\nDo we see a high proportion of % Hets, indicating many multi-allelic sites (and possibly presence of cross-mapping from other species, that may lead to false positive or less confident SNP calls)?\n\nFastQC (pre-AdapterRemoval)\n\nDo we see any very early drop off of sequence quality scores suggesting problematic sequencing run?\nDo we see outlier GC content distributions?\nDo we see high sequence duplication levels?\n\nAdapterRemoval\n\nDo we see high numbers of singletons or discarded read pairs?\n\nFastQC (post-AdapterRemoval)\n\nDo we see improved sequence quality scores along the length of reads?\nDo we see reduced adapter content levels?\n\nSamtools Flagstat (pre/post Filter)\n\nDo we see outliers, e.g. with unusually low levels of mapped reads, (indicative of badly preserved samples) that require downstream closer assessment?\n\nDeDup/Picard MarkDuplicates\n\nDo we see large numbers of duplicates being removed, possibly indicating over-amplified or badly preserved samples?\n\nMALT (metagenomics only)\n\nDo we have a reasonable level of mappability?\n\nSomewhere between 10-30% can be pretty normal for aDNA, whereas e.g. &lt;1% requires careful manual assessment\n\nDo we have a reasonable taxonomic assignment success?\n\nwe hope to have a large number of the mapped reads (from the mappability plot) that also have taxonomic assignment\n\n\nPreSeq (genomics only)\n\nDo we see a large drop off of a sample’s curve away from the theoretical complexity? If so, this may indicate it’s not worth performing deeper sequencing as we will get few unique reads (vs. duplicates that are not any more informative than the reads you’ve already sequenced)\n\nDamageProfiler (genomics only)\n\nDo we see evidence of damage on the microbial DNA (i.e. a % C&gt;T of more than ~5% in the first few nucleotide positions?) ? If not, possibly our mapped reads are deriving from modern contamination\n\nQualiMap (genomics only)\n\nDo we see a peak of coverage (X) at a good level, e.g. &gt;= 3x, indicating sufficient coverage for reliable SNP calls?\n\nMultiVCFAnalyzer (genomics only)\n\nDo we have a good number of called SNPs that suggest the samples have genomes with sufficient nucleotide diversity to inform phylogenetic analysis?\nDo we have a large number of discarded SNP calls?\nAre the % Hets very high indicating possible cross-mapping from off-target organisms that may confounding variant calling?\n\nAs above evaluating these outputs will vary depending on the data and or pipeline settings, and very much. However the extensive output documentation (https://nf-co.re/eager/2.4.7/output) of nf-core/eager can guide we through every single table and plot to assist we in continuing any type of ancient DNA project, assisted by fun little cartoony schematic diagrams (Figure 17.3)!\n\n\n\n\n\n\nFigure 17.3: Example of cartoon schematic diagram of the output from DamageProfiler (Neukamm, Peltzer, and Nieselt 2021) in different contexts. The six panels show different types of ancient DNA damage line-plots from having no damage (flat red/blue lines), however with a speech bubble noting that if the library was UDG treated, that the flat lines might be valid, all the way to the ‘classic’ ancient DNA damage plot with both red and blue lines showing an exponential decay from the ends of reads to the middle, with a motivational speech bubble. Source: Zandra Fagernäs, CC-BY 4.0 via nf-core/eager documentation (https://nf-co.re/eager/output).\n\n\n\n\n\n17.3.5 Clean up\nBefore continuing onto the next section of this chapter, we will need to deactivate from the conda environment.\nconda deactivate \nIf the nf-core/eager run is still not finished, we should enter the screen session with screen -r eager, press ctrl + c until we drop to the prompt, and type exit.\nThen may also need to delete the contents of the eager directory if we are low on hard-drive space.\ncd /&lt;path&gt;/&lt;to&gt;/ancient-metagenomic-pipelines/eager\nrm -r *",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-ameta",
    "href": "ancient-metagenomic-pipelines.html#what-is-ameta",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.4 What is aMeta?",
    "text": "17.4 What is aMeta?\n\n\n\n\n\n\nSelf guided: chapter environment setup\n\n\n\n\n\nFor this chapter’s exercises, if not already performed, we will need to create the special aMeta conda environment and activate the environment.\n\n\n\nWhile nf-core/eager is a solid pipeline for microbial genomics, and can also perform metagenomic screening via the integrated HOPS pipeline (Hübler et al. 2019) or Kraken2 (Wood, Lu, and Langmead 2019), in some cases we may wish to have a more accurate and resource efficient pipeline In this section, we will demonstrate an example of using aMeta, a Snakemake workflow proposed by Pochon et al. (2022) that aims to minimise resource usage by combining both low-resource requiring k-mer based taxonomic profiling as well as accurate read-alignment (Figure 17.4).\n\n\n\n\n\n\nFigure 17.4: Schematic diagram of the aMeta pipeline. Input samples initially undergo generalised screening using the K-mer based KrakenUniq. For every hit that the reads match inside this database, then sees the genome of that hit then constructed into a MALT database where the reads undergo a mapping step to generate alignments, a lowest-common-ancestor (LCA) algorithm step to refine taxonomic assignments, and ancient DNA authentication statistics generation\n\n\n\nRather than the very computationally heavy HOPS pipeline (Hübler et al. 2019), that requires extremely large computational nodes with large RAM (&gt;1 TB) to load MALT databases into memory, aMeta does this via a two step approach. Firstly it uses KrakenUniq (a k-mer based and thus memory efficient method, Breitwieser, Baker, and Salzberg 2018) to do a screening of sequencing reads against a broad generalised microbial database. Once all the possible taxa have been detected, aMeta will then make a new database of just the genomes of the taxa that were reported from KrakenUniq (i.e. a specific database) but using MALT. MALT on thus much reduced database is then used to perform computationally much heavier alignment against the reference genomes and LCA taxonomic reassignment. The output from MALT is then sent to the MaltExtract program of the HOPS pipeline for ancient DNA authentication statistics.\n\n17.4.1 Running aMeta\nIn this tutorial we will try running the small test data that comes with aMeta.\naMeta has been written in Snakemake, which means running the pipeline has to be installed in a slightly different manner to the nextflow pull command that can be used for nf-core/eager.\nMake sure you have followed the instructions in the Before You Start Chapter for cloning the aMeta GitHub repository to the ancient-metagenomic-pipelines/ directory. Once we have done this, we can make sure we are in the aMeta directory, if not already.\ncd /&lt;path&gt;/&lt;to&gt;/ancient-metagenomic-pipelines/aMeta\nAnd activate the dedicated aMeta conda environment.\nconda activate aMeta\nAs aMeta also includes tools that normally require very large computational resources that cannot fit on a standard laptop, we will instead try to re-use the internal very small ‘fake’ data the aMeta developers use to test the pipeline.\n\n\n\n\n\n\nWarning\n\n\n\nThe following steps are already performed up for Students in the summer schools as, particularly the set up conda envs will take a very long time!\nIf you are doing this chapter self-guided, it is critical to perform the following set up steps!\nWe don’t have to worry about trying to understand exactly what the following commands are doing, they will not be important for the rest of the chapter. However generally the commands try to pull all the relevant software (via conda), make a fake database and download other required files, and then reconstruct the basic directory and file structure required for running the pipeline.\n\n\n\n\n\n\nSelf-guided: aMeta set up and configuration\n\n\n\n\n\n## Change into ~/.test to set up all the required test resources (Databases etc.)\ncd .test/\n\n## Set up conda envs\n## If we can an error about a 'non-default solver backend' the run `conda config --set solver classic` and re-start the command\n## If we have installed the standalone mamba, change --conda-frontend to mamba\nsnakemake --use-conda --show-failed-logs -j 2 --conda-cleanup-pkgs cache --conda-create-envs-only -s ../workflow/Snakefile --conda-frontend conda\nsource $(dirname $(dirname $CONDA_EXE))/etc/profile.d/conda.sh\n\n## If we had to change the solver back to classic, we can switch to libmamba with `conda config --set solver libmamba`\n\n## Build dummy KrakenUniq database\nenv=$(grep krakenuniq .snakemake/conda/*yaml | awk '{print $1}' | sed -e \"s/.yaml://g\")\nconda activate $env\nkrakenuniq-build --db resources/KrakenUniq_DB --kmer-len 21 --minimizer-len 11 --jellyfish-bin $(pwd)/$env/bin/jellyfish\nconda deactivate\n\n## Get Krona taxonomy tax dump\nenv=$(grep krona .snakemake/conda/*yaml | awk '{print $1}' | sed -e \"s/.yaml://g\" | head -1)\nconda activate $env\ncd $env/opt/krona\n./updateTaxonomy.sh taxonomy\ncd -\nconda deactivate\n\n## Adjust malt max memory usage\nenv=$(grep hops .snakemake/conda/*yaml | awk '{print $1}' | sed -e \"s/.yaml://g\" | head -1)\nconda activate $env\nversion=$(conda list malt --json | grep version | sed -e \"s/\\\"//g\" | awk '{print $2}')\ncd $env/opt/malt-$version\nsed -i -e \"s/-Xmx64G/-Xmx3G/\" malt-build.vmoptions\nsed -i -e \"s/-Xmx64G/-Xmx3G/\" malt-run.vmoptions\ncd -\nconda deactivate\n\ntouch .initdb\n\n## Run a quick test and generate the report (you can open this to check it looks like everythin was generated)\nsnakemake --use-conda --show-failed-logs --conda-cleanup-pkgs cache -s ../workflow/Snakefile $@ --conda-frontend conda -j 4\nsnakemake -s ../workflow/Snakefile --report --report-stylesheet ../workflow/report/custom.css --conda-frontend conda\n\n## Now we move back into the main repository where we can symlink all the database files back to try running our 'own' test\ncd ../\ncd resources/\nln -s ../.test/resources/* .\ncd ../config\nmv config.yaml config.yaml.bkp\nmv samples.tsv samplest.tsv.bkp\ncd ../\nln -s .test/data/ .\nln -s .test/.snakemake/ . ## so we can re-use conda environments from the `.test` directory for the summer school run\n\n## Again get the taxonomy tax dump for Krona, but this time for a real run\n## Make sure you're now in the root directory of the repository!\nenv=$(grep krona .test/.snakemake/conda/*yaml | awk '{print $1}' | sed -e \"s/.yaml://g\" | head -1)\nconda activate $env\ncd $env/opt/krona\n./updateTaxonomy.sh taxonomy\ncd -\nconda deactivate\n\n## And back to the root of the repo for practising aMeta properly!\ncd ../\nNow hopefully we can forget all this, and imagine we are running data though aMeta as you would normally from scratch.\n\n\n\nOK now aMeta is all set up, we can now simulate running a ‘real’ pipeline job!\n\n\n\n\n17.4.2 aMeta configuration\nFirst, we will need to configure the workflow. First, we need to create a tab-delimited samples.tsv file inside aMeta/config/ and provide the names of the input fastq-files. In a text editor (e.g. nano), write the following names paths in TSV format.\nsample  fastq\nbar data/bar.fq.gz\nfoo data/foo.fq.gz\n\n\n\n\n\n\nWarning\n\n\n\nMake sure when copy pasting into our test editor, tabs are not replaced with spaces, otherwise the file might not be read!\n\n\nThen we need to write a config file. This tells aMeta where to find things such as database files and other settings.\nA minimal example config.yaml files can look like this. This includes specifying the location the main samplesheet, which points to a TSV file that contains all the FASTQs if the samples we want to analyse, and paths to all the required database files and reference genomes you may need. These paths and settings go inside the config.yaml file that must be placed inside inside aMeta/config/.\nMake a configuration file with your text editor of choice (e.g. nano).\nsamplesheet: \"config/samples.tsv\"\n\nkrakenuniq_db: resources/KrakenUniq_DB\n\nbowtie2_db: resources/ref.fa\nbowtie2_seqid2taxid_db: resources/seqid2taxid.pathogen.map\npathogenomesFound: resources/pathogenomesFound.tab\n\nmalt_nt_fasta: resources/ref.fa\nmalt_seqid2taxid_db: resources/KrakenUniq_DB/seqid2taxid.map\nmalt_accession2taxid: resources/accession2taxid.map\n\nncbi_db: resources/ncbi\n\nn_unique_kmers: 1000\nn_tax_reads: 200\nAnd make a two column samplesheet file with the following content in a file called samples.tsv, also under configs/.\nsample  fastq\nfoo data/foo.fq.gz\nbar data/bar.fq.gz\n\n\n\n\n\n\nWarning\n\n\n\naMeta (v1.0.0) currently only supports single-end or pre-merged- data only!\n\n\nOnce this config file is generated, we can start the run.\n\n\n\n\n\n\nNote\n\n\n\nAs this is only a dummy run (due to the large-ish computational resources required for KrakenUniq), we re-use some of the resource files here. While this will produce nonsense output, it is used here to demonstrate how we would execute the pipeline.\n\n\n\n\n17.4.3 Prepare and run aMeta\nMake sure we’re still in the aMeta conda environment, and that we are still in the main aMeta directory with the following.\nconda activate aMeta\ncd /&lt;path/&lt;to&gt;/ancient-metagenomic-pipelines/aMeta/\nFinally, we are ready to run aMeta, where it will automatically pick up our config and samplesheet file we placed in config/!\n## change conda-frontend to mamba if we set this up earlier!\nsnakemake --snakefile workflow/Snakefile --use-conda -j 10 --conda-frontend conda\n\n\n\n\n\n\nNote\n\n\n\nWe can modify -j to represent the number of available CPUs we have on our machine.\nIf we are using conda is the frontend this could be quite slow!\n\n\nFirstly we’ll have displayed on the console a bunch of messages about JSON schemas, building DAG of jobs and downloading installing of conda environments. We will know the pipeline has started when we get green messages saying things like Checkpoint and Finished job XX.\nWe’ll know the job is completed once we get the following messages\nFinished job 0.\n31 of 31 steps (100%) done\nComplete log: .snakemake/log/2023-10-05T155051.524987.snakemake.log\n\n\n17.4.4 aMeta output\nAll output files of the workflow are located in aMeta/results directory. To get a quick overview of ancient microbes present in our samples we should check a heatmap in results/overview_heatmap_scores.pdf.\n\n\n\n\n\n\nWarning\n\n\n\nIf running during the summer school, you can use the following command to open the PDF file from the command line.\nevince results/overview_heatmap_scores.pdf\n\n\n\n\n\n\n\n\nFigure 17.5: Example microbiome profiling summary heatmap from aMeta. The columns represent different samples, and the rows of different species. The cells of the heatmap are coloured from blue, to yellow, to red, representing aMeta authentication scores from 0 to 10, with the higher the number the more confident of the hit being both the correct taxonomic assignment and that it is ancient. Numbers in the coloured cells also provide the direct score number.\n\n\n\nThe heatmap demonstrates microbial species (in rows) authenticated for each sample (in columns) (Figure 17.5).\nThe colors and the numbers in the heatmap represent authentications scores, i.e. numeric quantification of seven quality metrics that provide information about microbial presence and ancient status.\nThe authentication scores can vary from 0 to 10, the higher is the score the more likely that a microbe is present in a sample and is ancient.\nTypically, scores from 8 to 10 (red color in the heatmap) provide good confidence of ancient microbial presence in a sample. Scores from 5 to 7 (yellow and orange colors in the heatmap) can imply that either: (a) a microbe is present but not ancient, i.e. modern contaminant, or (b) a microbe is ancient (the reads are damaged) but was perhaps aligned to a wrong reference, i.e. it is not the microbe we think about.\nThe former is a more common case scenario.\nThe latter often happens when an ancient microbe is correctly detected on a genus level but we are not confident about the exact species, and might be aligning the damaged reads to a non-optimal reference which leads to a lot of mismatches or poor evennes of coverage. Scores from 0 to 4 (blue color in the heatmap) typically mean that we have very little statistical evidence (very few reads) to claim presence of a microbe in a sample.\nTo visually examine the seven quality metrics\n\nDeamination profile\nEvenness of coverage\nEdit distance (amount of mismatches) for all reads\nEdit distance (amount of mismatches) for damaged reads\nRead length distribution\nPMD scores distribution\nNumber of assigned reads (depth of coverage)\n\nCorresponding to the numbers and colors of the heatmap, one can find them in results/AUTHENTICATION/sampleID/taxID/authentic_&lt;Sample&gt;_&lt;sampleID&gt;.trimmed.rma6_&lt;TaxID&gt;_&lt;taxID&gt;.pdf for each sample sampleID and each authenticated microbe taxID. An example of such quality metrics is shown below in Figure 17.6.\n\n\n\n\n\n\nFigure 17.6: Example of sample/hit specific PDF output from aMeta, 9 panels represent different figures that are useful for evaluating the authenticitiy of an ancient metagenomic hit. From Left to Right, Top from bottom, the panels consists of: 1. Edit distance (all reads) bar plot 2. Edit distance (ancient reads) bar plot 3. Breadth of coverage line plot 4. Deamination line plot 5. Read length distribution bar plot 6. PMD score histogram 7. Percent identity bar plot, 8. A table of similarity to hits from from closest hit to least closest 9. A general statistics table including the name of the taxonomic node, number of reads, duplicates, and mean read length etc.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn our test data, what score does the sample ‘foo’ for the hit against Yersinia pestis? Is this a good score?\nInspect the results AUTHENTICATION/xxx/authentic_Sample_foo_*.pdf file What could have contributed to this particular score?\nHint: Check Supplementary File 2, section S5 of (Pochon et al. 2022) for some hints.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe sample foo gets a score of 4. This is a low score, and indicates that aMeta is not very confident that this is a true hit. The metrics that contribute to this score are:\n\nEdit distance all reads (+1)\nDeamination plot (+2)\nReads mapped with identity (+1),\n\n\n\n\n\n\n17.4.5 Clean up\nBefore continuing onto the next section of this chapter, we will need to remove the output files, and deactivate from the conda environment.\nrm -r results/ log/\n## You can also optionall remove the conda environments if we are running out of space\n# rm -r .snakemake/ .test/.snakemake\nconda deactivate",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-nf-coremag",
    "href": "ancient-metagenomic-pipelines.html#what-is-nf-coremag",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.5 What is nf-core/mag?",
    "text": "17.5 What is nf-core/mag?\nnf-core/mag (Krakau et al. 2022) is another Nextflow best-practise pipeline for the de novo assembly, binning, and annotation of metagenomes (Figure 17.7). While it was originally designed for modern metagenomes (with a focus on optional support co-assembly with Nanopore long reads), it has since been updated to include specific functionality and parameters for ancient metagenomes too.\n\n\n\n\n\n\nFigure 17.7: Overview diagram of the main steps and tools of the nf-core/mag pipeline. Taking reads in FASTQ format or a samplesheet as input, reads can go through adapter and/or quality trimming for specific tools for both short and long reads. These reads can optionally go into taxonomic classification and visualisation, at the same time as the prepared reads go into sample-or group wise assemply, evaluation, as well as the ancient DNA valdiation subworkflow. Resulting contigs can be functionally annotated at the same time as optionally going through binning and binning refinement and evaluation (with statistics generation). Bins and refined bins can be taxonomically classified and annotated, with all final reports going into MultiQC.\n\n\n\nnf-core/mag covers the same major steps as we will have run if we followed the chapter on De Novo assembly.\nFirstly, as with nf-core/eager, nf-core/mag do some initial sequencing QC and cleanup with fastp (Chen et al. 2018) or AdapterRemoval (Schubert, Lindgreen, and Orlando 2016), running FastQC (Andrews 2010) before and after this step to make sure that adapters and overly short reads have been removed. It can then optionally do basic taxonomic classification of raw reads with Kraken2 (Wood, Lu, and Langmead 2019) or Centrifuge (Kim et al. 2016). The processed reads then ungo assembly with MEGAHIT (Li et al. 2015), metaSPAdes (Nurk et al. 2017), or SPAdeshybrid (Antipov et al. 2016) (the latter being for combining short and long reads), with contig annotation with Prodigal (Hyatt et al. 2010) to get gene prediction and assembly statistics with QUAST. A unique feature of nf-core/mag over other de novo assembly pipelines is an aDNA validation and correction step: contigs under go ‘reference free’ damage profiling using pyDamage (Borry et al. 2021), and then remaining damage bases that are mistakenly incorporated by assemblies are corrected using freebayes (Garrison and Marth 2012) and BCFTools. Contigs then optionally grouped into genomic ‘bins’ with a range of metagenomic binners (MetaBAT2 (Kang et al. 2019), MaxBin2 (Wu, Simmons, and Singer 2015), and CONCOCT (Alneberg et al. 2014)), as well as also optional binning refinement with DAS Tool (Sieber et al. 2018). Bins are then re-annotated with Prokka (Seemann 2014), taxonomically classified with either CAT (Meijenfeldt et al. 2019) or GTDB-Tk (Chaumeil et al. 2022) Evaluation of the resulting bins is carried out with BUSCO (Simão et al. 2015), CheckM (Parks et al. 2015), GUNC (Orakov et al. 2021) and/or QUAST (Gurevich et al. 2013) to assess the bin completeness. And, as with all nf-core pipelines, all the results are wrapped up into a MultiQC report. Detailed documentation can be see on the nf-core/mag parameters (https://nf-co.re/mag/2.3.2/parameters) and output (https://nf-co.re/mag/2.3.2/docs/output pages.\n\n\n\n\n\n\nQuestion\n\n\n\nWhy do we need ancient DNA specific steps in a de novo assembly pipeline?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFirstly we need a method of validating that contigs are from an ancient genome (in nf-core/mag, damage patterns are generated by pyDamage).\nSecondly, to correct for falsely included damage bases in contigs in some contexts. As previously stated a previous chapter by Alex Hübner:\n\nIn our group we realised that the gene sequences that were annotated on the contigs of MEGAHIT tended to have a higher number of nonsense mutations compared to the known variants of the genes. After manual inspection, we observed that many of these mutations appeared because MEGAHIT chose the damage-derived T allele over the C allele or the damage-derived A allele over a G allele (see Klapper et al. 2023, fig. S1).\n\n\nTo overcome this issue, my colleagues Maxime Borry, James Fellows Yates and I developed a strategy to replace these damage-derived alleles in the contig sequences. This approach consists of aligning the short-read data against the contigs, performing genotyping along them, and finally replacing all alleles for which we have strong support for an allele that is different from the one selected by MEGAHIT.\n\nSee the De novo assembly chapter for full context.\n\n\n\n\n17.5.1 Running nf-core/mag\nFirst re-activate the chapter’s conda environment.\nconda activate ancient-metagenomic-pipelines\nIn this tutorial, we will use the same data and aim to replicate the steps taken in the De Novo assembly chapter. It is important to note that generally, de novo assembly, is very computational intensive. In many cases it will not be able to run a de novo assembly on a standard laptop, therefore some of the parameters used here have been tweaked to get the pipeline runable on powerful laptop level machines.\n\n\n\n\n\n\nSelf guided: reference download and preparation\n\n\n\n\n\nThis tutorial will re-use the data from the de novo assembly chapter. If we have not-run that practical, or deleted the data, pleas follow the instructions at the top of the de novo assembly page to download the denovo-assembly.tar.gz file and unpack it into the denovo-assembly directory.\nMake sure once we’ve finished, change back into this chapter’s directory with the following.\ncd /&lt;path&gt;/&lt;to&gt;/ancient-metagenomic-pipelines\n\n\n\nThe manual steps of the previous chapter included:\n\nRead clean up with fastp,\nAssembly with MEGAHIT,\nAncient DNA assessment and correction with freebayes and pyDamage\nBinning using MetaBAT2 and MaxBin2 (originally using the MetaWRAP pipeline (Uritskiy, DiRuggiero, and Taylor 2018)),\nBin assessment with CheckM\nContig taxonomic classification with MMSeqs2 (Steinegger and Söding 2017) via the GTDB database\nGenome annotation with Prokka\n\nSo, what command would we use to recreate these?\n## 🛑 Don't run this yet! 🛑\nnextflow run nf-core/mag -r 2.3.2 \\\n-profile test,docker \\\n--input '../../denovo-assembly/seqdata_files/*{R1,R2}.fastq.gz' --outdir ./results \\\n--ancient_dna  --binning_map_mode own \\\n--binqc_tool checkm --checkm_db data/checkm_data_2015_01_16/ \\\n--centrifuge_db false --kraken2_db false --skip_krona --skip_spades --skip_spadeshybrid --skip_maxbin2 --skip_concoct --skip_prodigal --gtdb false  --busco_reference false \nIn fact, nf-core/mag will do most of the steps for us! In this case, we mostly just need to deactivate most of the additional steps (the last line). Otherwise, as with eager we have done the following:\n\nTell Nextflow to run the nf-core/mag pipeline with version 2.3.2\nSpecify which computing and software environment to use with -profile\n\nIn this case we are running locally so we don’t specify a computing environment (such as a configuration for an institutional HPC)\nWe use docker as our container engine, which downloads all the software and specific versions needed for nf-core/mag in immutable ‘containers’, to ensure nothing get broken and is as nf-core/mag expects\n\nProvide the various paths to the input files - the raw FASTQ files and the output directory\nSpecify we want to run the aDNA mode with mapping of reads back to the original contigs rather than for co-assembly\n\n\n\n\n\n\n\nBinning mapping mode with ancient DNA\n\n\n\n\n\nThe aDNA mode of nf-core/mag only supports mapping reads back to the original contigs. The other mapping modes are when doing co-assembly, where we assume there are similar organisms across all our metagenomes, and wish to map reads from all samples in a group to a single sample’s contig (for example). Doing this on aDNA reads would risk making false positive damage patterns, as the damaged reads may derive from reads from a different sample with damage, that are otherwise not present in the sample used for generating the given contig.\n\n\n\n5 . Specify we want to generate completeness statistics with CheckM (rather than BUSCO), with the associated pre-downloaded database\n\n\n\n\n\n\nnf-core pipelines and reference files\n\n\n\n\n\nMost nf-core pipelines will actually download reference databases, build reference indices for alignment, and in some cases reference genomes for we if we do not specify them as pipeline input. These download and reference building steps are often very time consuming, so it’s recommended that once we’ve let the database download it once, we should move the files somewhere safe and we can re-use in subsequent pipeline runs.\n\n\n\n6 . Skip a few steps that are either ‘nice to have’ (e.g. read taxonomic classification), or require large computational resources (e.g.metaSPAdes, CONCOCT or BUSCO)\n\n\n\n\n\n\nInformation on skipped tests\n\n\n\n\n\nThe steps we are skipping are: host/arefact removal (Bowtie2 removal of phiX), taxonomic classification of reads (centrifuge, kraken2, krona), the additional assemblers (metaSPAdes and SPAdeshybrid, as these require very large computational resources), additional binners (MaxBin2 and CONCOCT, while they are used in the de novo assembly chapter, they take a long time to run), ship raw contig annotation (with Prodigal, as we do this at the bin level), and contig taxonomic classification (with GTDB and BUSCO as they require very large databases).\n\n\n\nWith this we are almost ready for running the pipeline!\n\n\n17.5.2 Configuring Nextflow pipelines\nBefore we execute the command however, we need to again consider about the resource requirements. As described above, particularly de novo assembly (but also many other bioinformatic analyses) require a lot of computational resources, depending on the type of data and analyses we wish to run.\nFor most pipelines we must tweak the resource requirements to ensure a) they will fit on our cluster, and b) will run optimally so we don’t under- or over- use our computational resources, where latter will either make ourselves (takes too long), or our cluser/server system administrators (blocks other users) unhappy!\nWhile nf-core pipelines all have ‘reasonable default’ settings for memory, CPU, and time limits, they will not work in all contexts. Here we will give a brief example of how to tweak the parameters of the pipeline steps so that they can run on our compute node or laptop.\nFor Nextflow, we must make a special ‘config’ that defines the names of each step of the pipeline we wish to tweak and the corresponding resources.\nFor this our nf-core/mag run we will need to open our text editor an empty file called custom.config, and copy and paste the contents of the next code block.\nprocess {\n\n  withName: FASTP { \n    cpus = 8\n  }\n\n  withName: BOWTIE2_PHIX_REMOVAL_ALIGN { \n    cpus = 8\n  }\n\n  withName: BOWTIE2_ASSEMBLY_ALIGN {    \n    cpus = 8\n  }\n\n  withName: PYDAMAGE_ANALYZE {      \n    cpus = 8\n  }\n\n  withName: PROKKA {\n    cpus = 4\n  }\n\n  withName: MEGAHIT {\n    cpus = 8\n  }\n\n  withName: CHECKM_LINEAGEWF{\n    memory = 24.GB\n    cpus   = 8\n    ext.args = \"--reduced_tree\"\n  }\n\n}\nHere we set the number of CPUs for most tools to a maximum of 8, and then we limit the amount of memory for CheckM to 24 GB (down from the default (https://github.com/nf-core/mag/blob/66cf53aff834d2a254b78b94fc54cd656b8b7b57/conf/base.config#L37-L41) of 36 GB - which exceeds most laptop memory). We also give a specifc CheckM flag command to use a smaller database.\nWe can save this custom.config file, and supply this to the Nextflow command with the Nextflow parameter (-c). The good thing about the config file, is we can re-use across runs on the same machine! So set once, and never think about it again.\nAnd with that, we can run our nf-core/mag pipeline!\nAs recommended in the nf-core/eager section above, start a screen session.\nscreen -R mag\nThen execute the command.\nnextflow run nf-core/mag -r 2.3.2 \\\n-profile test,docker \\\n--input '../../denovo-assembly/seqdata_files/*{R1,R2}.fastq.gz' --outdir ./results \\\n--ancient_dna  --binning_map_mode own \\\n--binqc_tool checkm --checkm_db data/checkm_data_2015_01_16/ \\\n--centrifuge_db false --kraken2_db false --skip_krona --skip_spades --skip_spadeshybrid --skip_maxbin2 --skip_concoct --skip_prodigal --gtdb false  --busco_reference false \\\n-c custom.config\nAgain as with nf-core/eager when the run starts, we see a long list of process lines with progress bars slowly appearing over time. If in a screen session, we can press ctrl + a and then [ to access a ‘navigation’ (called ‘copy’) mode, then use our arrow keys on our keyboard to scroll up and down. To quit this mode just press q.\nThe pipeline run above should take around 20 minutes or so to complete.\n\n\n\n\n\n\nTip\n\n\n\nConfiguration files don’t need be personal nor custom!\nWe can also generate a HPC / server specific profile which can be used on all nf-core pipelines (and even our own!).\nSee nf-co.re/configs (https://nf-co.re/configs) for all existing institutional configs\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhy should you make sure to configure pipeline that use a workflow manager in the backend?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe primary reason for optimising the configuration of workflow managers is efficiency.\nBy correctly configuring your can ensure:\n\nA maximal number of jobs can run in parallel\nRuns are not killed (and repeatedly retried) if they request too many resources\nCan reuse files across runs, such as databases and software containers\n\namongst others.\n\n\n\n\n\n17.5.3 nf-core/mag output: results files\nAs with nf-core/eager, we will have a results directory containing many folders representing different stages of the pipeline, and a MultiQC report.\nRelevant directories for evaluting our assemblies are as follows:\n\nQC_shortreads/\n\nContains various folders from the sequencing QC and raw read processing (e.g., FastQC, fastp/AdapterRemoval, host-removal results etc.)\n\nTaxonomy/\n\nIf we have not already performed taxonomic classification of our reads, the output of the Kraken2 or Centrifuge integrated steps of nf-core/mag will be deposited here, including an optional Krona piechart\nIf activated, the contig-level taxonomic assignments will be deposited here (e.g., from CAT or GTDB)\n\nAssembly/\n\nThis directory will contain one directory per chosen assembler which will contain per-sample output files for each assembly.\nWithin each of the assembler sub directories, a QC folder will contain the QUAST output\n\nAncient_DNA\n\nThis folder will contain the output of pyDamage contig authentication to allow we to select or filter for just putatively ancient contigs\n\nvariant_calling\n\nThis directory will contain the ‘damage-corrected’ contigs that will be used in downstream steps of the pipeline, if the aDNA mode is turned on\n\nGenomeBinning/\n\nThis directory will contain all the output from each of any of the selected contig binning tools. As well as the binned FASTAs, we also have read-depth statistics from when input reads are re-mapped back to the contigs\nThe directory will also contain the output from the DASTool binning refinement if turned on\nIn the QC sub-directory of this folder, we will find the QUAST results on the genome bins (rather than raw contigs as above), as well as BUSCO/CheckM/GUNC MAG completeness estimates\n\nProdigal / Prokka\n\nThese directories will contain the genome annotation output of the raw contigs (Prodigal) or from bins (Prokka)\n\n\n\n\n17.5.4 nf-core/mag output: run report\nFor MultiQC, many of the questions asked in the preprocessing section of the nf-core/eager results interpretation will also apply here.\nOther than the QUAST results in the Assembly/*/QC/ or GenomeBinning/*/QC/ directories, the main table used for evaluating the output files is the bin_summary.tsv table in the GenomeBinning/ directory.\nIn this file we woud typically want to assess the following columns:\n\n% Complete * columns - with the higher the number of expected domain-specific genes, normally representing a better quality of the resulting bin.\n# contigs (&gt;=* bp) columns - which represnts the number of contigs at different lengths, the fewer the shorter reads and the greater the longer reads we have again suggests a better assembly.\nThis can also be evaluated by the N50 or L75 columns (as described in the De novo assembly chapter).\nfastani and closest_placement_taxonomy - these can tell we if our particular bin has a genome very similar to existing species in taxonomic databases\nwarnings - for specific comments on each assignment",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#optional-clean-up",
    "href": "ancient-metagenomic-pipelines.html#optional-clean-up",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.6 (Optional) clean-up",
    "text": "17.6 (Optional) clean-up\nLet’s clean up our working directory by removing all the data and output from this chapter.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/ancient-metagenomic-pipelines directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path we are specifying is exactly what we want to delete and nothing more before pressing ENTER!\n\n\nrm -rf /&lt;PATH&gt;/&lt;TO&gt;/ancient-metagenomic-pipelines*\nOnce deleted we can move elsewhere (e.g., cd ~).\nWe can also get out of the conda environment with the following.\nconda deactivate\nTo delete the conda environment we can use conda remove.\nconda remove --name ancient-metagenomic-data --all -y",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "href": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.7 Questions to think about",
    "text": "17.7 Questions to think about\n\nWhy is it important to use a pipeline for genomic analysis of ancient data?\nHow can the design of pipelines such as nf-core/eager pipeline help researchers comply with the FAIR principles for management of scientific data?\nWhat metrics do we use to evaluate the success/failure of ancient DNA sequencing experiments? How can these measures be evaluated when using nf-core/eager for data preprocessing and analysis?",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#references",
    "href": "ancient-metagenomic-pipelines.html#references",
    "title": "17  Ancient Metagenomic Pipelines",
    "section": "17.8 References",
    "text": "17.8 References\n\n\n\n\nAlneberg, Johannes, Brynjar Smári Bjarnason, Ino de Bruijn, Melanie Schirmer, Joshua Quick, Umer Z Ijaz, Leo Lahti, Nicholas J Loman, Anders F Andersson, and Christopher Quince. 2014. “Binning Metagenomic Contigs by Coverage and Composition.” Nature Methods 11 (11): 1144–46. https://doi.org/10.1038/nmeth.3103.\n\n\nAndrades Valtueña, Aida, Gunnar U Neumann, Maria A Spyrou, Lyazzat Musralina, Franziska Aron, Arman Beisenov, Andrey B Belinskiy, et al. 2022. “Stone Age Yersinia Pestis Genomes Shed Light on the Early Evolution, Diversity, and Ecology of Plague.” Proceedings of the National Academy of Sciences of the United States of America 119 (17): e2116722119. https://doi.org/10.1073/pnas.2116722119.\n\n\nAndrews, Simon. 2010. “FastQC: A Quality Control Tool for High Throughput Sequence Data.” http://www.bioinformatics.babraham.ac.uk/projects/fastqc/.\n\n\nAntipov, Dmitry, Anton Korobeynikov, Jeffrey S McLean, and Pavel A Pevzner. 2016. “hybridSPAdes: An Algorithm for Hybrid Assembly of Short and Long Reads.” Bioinformatics (Oxford, England) 32 (7): 1009–15. https://doi.org/10.1093/bioinformatics/btv688.\n\n\nBorry, Maxime, Adrian Forsythe, Aida Andrades Valtueña, Alexander Hübner, Anan Ibrahim, Andrea Quagliariello, Anna E White, et al. 2024. “Facilitating Accessible, Rapid, and Appropriate Processing of Ancient Metagenomic Data with AMDirT.” F1000Research 12 (926): 926. https://doi.org/10.12688/f1000research.134798.2.\n\n\nBorry, Maxime, Alexander Hübner, Adam B Rohrlach, and Christina Warinner. 2021. “PyDamage: Automated Ancient Damage Identification and Estimation for Contigs in Ancient DNA de Novo Assembly.” PeerJ 9 (July): e11845. https://doi.org/10.7717/peerj.11845.\n\n\nBos, Kirsten I, Kelly M Harkins, Alexander Herbig, Mireia Coscolla, Nico Weber, Iñaki Comas, Stephen A Forrest, et al. 2014. “Pre-Columbian Mycobacterial Genomes Reveal Seals as a Source of New World Human Tuberculosis.” Nature 514 (7523): 494–97. https://doi.org/10.1038/nature13591.\n\n\nBreitwieser, F P, D N Baker, and S L Salzberg. 2018. “KrakenUniq: Confident and Fast Metagenomics Classification Using Unique k-Mer Counts.” Genome Biology 19 (1): 198. https://doi.org/10.1186/s13059-018-1568-0.\n\n\nChaumeil, Pierre-Alain, Aaron J Mussig, Philip Hugenholtz, and Donovan H Parks. 2022. “GTDB-Tk v2: Memory Friendly Classification with the Genome Taxonomy Database.” Bioinformatics (Oxford, England) 38 (23): 5315–16. https://doi.org/10.1093/bioinformatics/btac672.\n\n\nChen, Shifu, Yanqing Zhou, Yaru Chen, and Jia Gu. 2018. “Fastp: An Ultra-Fast All-in-One FASTQ Preprocessor.” Bioinformatics 34 (17): i884–90. https://doi.org/10.1093/bioinformatics/bty560.\n\n\nDanecek, Petr, James K Bonfield, Jennifer Liddle, John Marshall, Valeriu Ohan, Martin O Pollard, Andrew Whitwham, et al. 2021. “Twelve Years of SAMtools and BCFtools.” GigaScience 10 (2). https://doi.org/10.1093/gigascience/giab008.\n\n\nDi Tommaso, Paolo, Maria Chatzou, Evan W Floden, Pablo Prieto Barja, Emilio Palumbo, and Cedric Notredame. 2017. “Nextflow Enables Reproducible Computational Workflows.” Nature Biotechnology 35 (4): 316–19. https://doi.org/10.1038/nbt.3820.\n\n\nDimopoulos, Evangelos A, Alberto Carmagnini, Irina M Velsko, Christina Warinner, Greger Larson, Laurent A F Frantz, and Evan K Irving-Pease. 2022. “HAYSTAC: A Bayesian Framework for Robust and Rapid Species Identification in High-Throughput Sequencing Data.” PLoS Computational Biology 18 (9): e1010493. https://doi.org/10.1371/journal.pcbi.1010493.\n\n\nEwels, Philip, Måns Magnusson, Sverker Lundin, and Max Käller. 2016. “MultiQC: Summarize Analysis Results for Multiple Tools and Samples in a Single Report.” Bioinformatics 32 (19): 3047–48. https://doi.org/10.1093/bioinformatics/btw354.\n\n\nFellows Yates, James A, Thiseas C Lamnidis, Maxime Borry, Aida Andrades Valtueña, Zandra Fagernäs, Stephen Clayton, Maxime U Garcia, Judith Neukamm, and Alexander Peltzer. 2021. “Reproducible, Portable, and Efficient Ancient Genome Reconstruction with Nf-Core/Eager.” PeerJ 9 (March): e10947. https://doi.org/10.7717/peerj.10947.\n\n\nGarcia, Maxime, Szilveszter Juhos, Malin Larsson, Pall I Olason, Marcel Martin, Jesper Eisfeldt, Sebastian DiLorenzo, et al. 2020. “Sarek: A Portable Workflow for Whole-Genome Sequencing Analysis of Germline and Somatic Variants.” F1000Research 9 (63): 63. https://doi.org/10.12688/f1000research.16665.2.\n\n\nGarrison, Erik, and Gabor Marth. 2012. “Haplotype-Based Variant Detection from Short-Read Sequencing.” arXiv [q-Bio.GN], July. http://arxiv.org/abs/1207.3907.\n\n\nGurevich, Alexey, Vladislav Saveliev, Nikolay Vyahhi, and Glenn Tesler. 2013. “QUAST: Quality Assessment Tool for Genome Assemblies.” Bioinformatics (Oxford, England) 29 (8): 1072–75. https://doi.org/10.1093/bioinformatics/btt086.\n\n\nHanssen, Friederike, Maxime U Garcia, Lasse Folkersen, Anders Sune Pedersen, Francesco Lescai, Susanne Jodoin, Edmund Miller, et al. 2024. “Scalable and Efficient DNA Sequencing Analysis on Different Compute Infrastructures Aiding Variant Discovery.” NAR Genomics and Bioinformatics 6 (2): lqae031. https://doi.org/10.1093/nargab/lqae031.\n\n\nHerbig, Alexander, Frank Maixner, Kirsten I Bos, Albert Zink, Johannes Krause, and Daniel H Huson. 2016. “MALT: Fast Alignment and Analysis of Metagenomic DNA Sequence Data Applied to the Tyrolean Iceman.” bioRxiv, April, 050559. https://doi.org/10.1101/050559.\n\n\nHübler, Ron, Felix M Key, Christina Warinner, Kirsten I Bos, Johannes Krause, and Alexander Herbig. 2019. “HOPS: Automated Detection and Authentication of Pathogen DNA in Archaeological Remains.” Genome Biology 20 (1): 280. https://doi.org/10.1186/s13059-019-1903-0.\n\n\nHyatt, Doug, Gwo-Liang Chen, Philip F Locascio, Miriam L Land, Frank W Larimer, and Loren J Hauser. 2010. “Prodigal: Prokaryotic Gene Recognition and Translation Initiation Site Identification.” BMC Bioinformatics 11 (March): 119. https://doi.org/10.1186/1471-2105-11-119.\n\n\nKang, Dongwan D, Feng Li, Edward Kirton, Ashleigh Thomas, Rob Egan, Hong An, and Zhong Wang. 2019. “MetaBAT 2: An Adaptive Binning Algorithm for Robust and Efficient Genome Reconstruction from Metagenome Assemblies.” PeerJ 7 (e7359): e7359. https://doi.org/10.7717/peerj.7359.\n\n\nKim, Daehwan, Li Song, Florian P Breitwieser, and Steven L Salzberg. 2016. “Centrifuge: Rapid and Sensitive Classification of Metagenomic Sequences.” Genome Research 26 (12): 1721–29. https://doi.org/10.1101/gr.210641.116.\n\n\nKlapper, Martin, Alexander Hübner, Anan Ibrahim, Ina Wasmuth, Maxime Borry, Veit G Haensch, Shuaibing Zhang, et al. 2023. “Natural Products from Reconstructed Bacterial Genomes of the Middle and Upper Paleolithic.” Science (New York, N.Y.), May, eadf5300. https://doi.org/10.1126/science.adf5300.\n\n\nKrakau, Sabrina, Daniel Straub, Hadrien Gourlé, Gisela Gabernet, and Sven Nahnsen. 2022. “Nf-Core/Mag: A Best-Practice Pipeline for Metagenome Hybrid Assembly and Binning.” NAR Genomics and Bioinformatics 4 (1). https://doi.org/10.1093/nargab/lqac007.\n\n\nLi, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76. https://doi.org/10.1093/bioinformatics/btv033.\n\n\nLouvel, Guillaume, Clio Der Sarkissian, Kristian Hanghøj, and Ludovic Orlando. 2016. “metaBIT, an Integrative and Automated Metagenomic Pipeline for Analysing Microbial Profiles from High-Throughput Sequencing Shotgun Data.” Molecular Ecology Resources 16 (6): 1415–27. https://doi.org/10.1111/1755-0998.12546.\n\n\nMeijenfeldt, F A Bastiaan von, Ksenia Arkhipova, Diego D Cambuy, Felipe H Coutinho, and Bas E Dutilh. 2019. “Robust Taxonomic Classification of Uncharted Microbial Sequences and Bins with CAT and BAT.” Genome Biology 20 (1): 217. https://doi.org/10.1186/s13059-019-1817-x.\n\n\nMölder, Felix, Kim Philipp Jablonski, Brice Letcher, Michael B Hall, Christopher H Tomkins-Tinch, Vanessa Sochat, Jan Forster, et al. 2021. “Sustainable Data Analysis with Snakemake.” F1000Research 10 (January): 33. https://doi.org/10.12688/f1000research.29032.2.\n\n\nNeuenschwander, Samuel, Diana I Cruz Dávalos, Lucas Anchieri, Bárbara Sousa da Mota, Davide Bozzi, Simone Rubinacci, Olivier Delaneau, Simon Rasmussen, and Anna-Sapfo Malaspinas. 2023. “Mapache: A Flexible Pipeline to Map Ancient DNA.” Bioinformatics (Oxford, England) 39 (2): btad028. https://doi.org/10.1093/bioinformatics/btad028.\n\n\nNeukamm, Judith, Alexander Peltzer, and Kay Nieselt. 2021. “DamageProfiler: Fast Damage Pattern Calculation for Ancient DNA.” Bioinformatics 37 (20): 3652–53. https://doi.org/10.1093/bioinformatics/btab190.\n\n\nNurk, Sergey, Dmitry Meleshko, Anton Korobeynikov, and Pavel A Pevzner. 2017. “metaSPAdes: A New Versatile Metagenomic Assembler.” Genome Research 27 (5): 824–34. https://doi.org/10.1101/gr.213959.116.\n\n\nOrakov, Askarbek, Anthony Fullam, Luis Pedro Coelho, Supriya Khedkar, Damian Szklarczyk, Daniel R Mende, Thomas S B Schmidt, and Peer Bork. 2021. “GUNC: Detection of Chimerism and Contamination in Prokaryotic Genomes.” Genome Biology 22 (1): 178. https://doi.org/10.1186/s13059-021-02393-0.\n\n\nParks, Donovan H, Michael Imelfort, Connor T Skennerton, Philip Hugenholtz, and Gene W Tyson. 2015. “CheckM: Assessing the Quality of Microbial Genomes Recovered from Isolates, Single Cells, and Metagenomes.” Genome Research 25 (7): 1043–55. https://doi.org/10.1101/gr.186072.114.\n\n\nPeltzer, Alexander, Günter Jäger, Alexander Herbig, Alexander Seitz, Christian Kniep, Johannes Krause, and Kay Nieselt. 2016. “EAGER: Efficient Ancient Genome Reconstruction.” Genome Biology 17 (1): 1–14. https://doi.org/10.1186/s13059-016-0918-z.\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579.\n\n\nPoplin, Ryan, Valentin Ruano-Rubio, Mark A DePristo, Tim J Fennell, Mauricio O Carneiro, Geraldine A Van der Auwera, David E Kling, et al. 2018. “Scaling Accurate Genetic Variant Discovery to Tens of Thousands of Samples.” bioRxiv, July, 201178. https://doi.org/10.1101/201178.\n\n\nQuinlan, Aaron R, and Ira M Hall. 2010. “BEDTools: A Flexible Suite of Utilities for Comparing Genomic Features.” Bioinformatics 26 (6): 841–42. https://doi.org/10.1093/bioinformatics/btq033.\n\n\nSchubert, Mikkel, Luca Ermini, Clio Der Sarkissian, Hákon Jónsson, Aurélien Ginolhac, Robert Schaefer, Michael D Martin, et al. 2014. “Characterization of Ancient and Modern Genomes by SNP Detection and Phylogenomic and Metagenomic Analysis Using PALEOMIX.” Nature Protocols 9 (5): 1056–82. https://doi.org/10.1038/nprot.2014.063.\n\n\nSchubert, Mikkel, Stinus Lindgreen, and Ludovic Orlando. 2016. “AdapterRemoval v2: Rapid Adapter Trimming, Identification, and Read Merging.” BMC Research Notes 9 (February): 88. https://doi.org/10.1186/s13104-016-1900-2.\n\n\nSeemann, Torsten. 2014. “Prokka: Rapid Prokaryotic Genome Annotation.” Bioinformatics 30 (14): 2068–69. https://doi.org/10.1093/bioinformatics/btu153.\n\n\nSieber, Christian M K, Alexander J Probst, Allison Sharrar, Brian C Thomas, Matthias Hess, Susannah G Tringe, and Jillian F Banfield. 2018. “Recovery of Genomes from Metagenomes via a Dereplication, Aggregation and Scoring Strategy.” Nature Microbiology 3 (7): 836–43. https://doi.org/10.1038/s41564-018-0171-1.\n\n\nSimão, Felipe A, Robert M Waterhouse, Panagiotis Ioannidis, Evgenia V Kriventseva, and Evgeny M Zdobnov. 2015. “BUSCO: Assessing Genome Assembly and Annotation Completeness with Single-Copy Orthologs.” Bioinformatics (Oxford, England) 31 (19): 3210–12. https://doi.org/10.1093/bioinformatics/btv351.\n\n\nSteinegger, Martin, and Johannes Söding. 2017. “MMseqs2 Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets.” Nature Biotechnology 35 (11): 1026–28. https://doi.org/10.1038/nbt.3988.\n\n\nUritskiy, Gherman V, Jocelyne DiRuggiero, and James Taylor. 2018. “MetaWRAP-a Flexible Pipeline for Genome-Resolved Metagenomic Data Analysis.” Microbiome 6 (1): 158. https://doi.org/10.1186/s40168-018-0541-1.\n\n\nVågene, Åshild J, Alexander Herbig, Michael G Campana, Nelly M Robles García, Christina Warinner, Susanna Sabin, Maria A Spyrou, et al. 2018. “Salmonella Enterica Genomes from Victims of a Major Sixteenth-Century Epidemic in Mexico.” Nature Ecology & Evolution 2 (3): 520–28. https://doi.org/10.1038/s41559-017-0446-6.\n\n\nWood, Derrick E, Jennifer Lu, and Ben Langmead. 2019. “Improved Metagenomic Analysis with Kraken 2.” Genome Biology 20 (1): 257. https://doi.org/10.1186/s13059-019-1891-0.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.\n\n\nWu, Yu-Wei, Blake A Simmons, and Steven W Singer. 2015. “MaxBin 2.0: An Automated Binning Algorithm to Recover Genomes from Multiple Metagenomic Datasets.” Bioinformatics 32 (4): 605–7. https://doi.org/10.1093/bioinformatics/btv638.\n\n\nYu, He, Maria A Spyrou, Marina Karapetian, Svetlana Shnaider, Rita Radzevičiūtė, Kathrin Nägele, Gunnar U Neumann, et al. 2020. “Paleolithic to Bronze Age Siberians Reveal Connections with First Americans and Across Eurasia.” Cell 181 (6): 1232–1245.e20. https://doi.org/10.1016/j.cell.2020.04.037.",
    "crumbs": [
      "Ancient Metagenomic Resources",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ancient Metagenomic Pipelines</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html",
    "href": "functional-profiling.html",
    "title": "18  Functional Profiling",
    "section": "",
    "text": "18.1 Preparation\nOpen R Studio from within the conda environment\nOpen a new script file we can load the required libraries for this walkthrough.",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html#preparation",
    "href": "functional-profiling.html#preparation",
    "title": "18  Functional Profiling",
    "section": "",
    "text": "rstudio\n\nlibrary(mixOmics) ## For PCA generation\n\n## Utility packages (pretty stuff)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(ggrepel)\nlibrary(viridis)\nlibrary(patchwork)",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html#humann3-pathways",
    "href": "functional-profiling.html#humann3-pathways",
    "title": "18  Functional Profiling",
    "section": "18.2 HUMAnN3 Pathways",
    "text": "18.2 HUMAnN3 Pathways\nFirst, we would run HUMAnN3 to align reads against gene databases and convert to gene family names counts.\n\n\n\n\n\n\nCaution\n\n\n\nRunning HUMAnN3 module requires about 72 GB of memory because it has to load a larger reference database containing the lineage-specific marker genes of checkM.\nIf you have sufficient computational memory resources, you can run the following steps to run the bin refinement yourself.\nWe will not run HUMANn3 here as it requires very large databases and takes a long time to run, we have already prepared output for you.\n\n\n\n\n\n\n\n\nExample commands - do not run!\n\n\n\n\n\n## DO NOT RUN!\n\n# run humann3\nhumann3 --input file.fastq --output output --threads &lt;threads&gt;\n\n# join all output tables (can do for both gene and pathways)\nhumann_join_tables -i output/ -o genefamilies_joined.tsv --file_name unmapped_genefamilies\n\n# normalize the output (here by tss - total sum scaling, can do for both gene and pathways)\nhumann_renorm_table --input genefamilies_joined.tsv --output genefamilies_joined_cpm.tsv --units tss\n\n# regroup the table to combine gene families (standardise gene family IDs across taxa)\nhumann_regroup_table --input genefamilies_joined_cpm.tsv --output genefamilies_joined_cpm_ur90rxn.tsv --groups uniref90_rxn\n\n# give the gene families names\nhumann_rename_table --input genefamilies_joined_cpm_ur90rxn.tsv --output genefamilies_joined_cpm_ur90rxn_names.tsv -n metacyc-rxn",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html#humann3-tables",
    "href": "functional-profiling.html#humann3-tables",
    "title": "18  Functional Profiling",
    "section": "18.3 humann3 tables",
    "text": "18.3 humann3 tables\nFirst lets load a pre-made pathway abundance file\n## load the species and genus tables generated with humann3\nhumann3_path_full &lt;- fread(\"./pathabundance_joined_cpm.tsv\")\nhumann3_path_full &lt;- as_tibble(humann3_path_full)\n\n# clean the file names\nhumann3_path_full &lt;- rename(humann3_path_full, Pathway = `# Pathway`)\ncolnames(humann3_path_full) &lt;- gsub(\".unmapped_Abundance\",\"\", colnames(humann3_path_full))\ncolnames(humann3_path_full) &lt;- gsub(\".SG1\",\"\", colnames(humann3_path_full))\n\n# remove unmapped and ungrouped reads\nhumann3_path &lt;- humann3_path_full %&gt;% filter(!str_detect(Pathway, \"UNMAPPED|UNINTEGRATED\"))\nThen lets load associated sample metadata to help make it easier for comparative analysis and make actual informative inferences.\nThe data being used in this session, is from Velsko et al. 2022 (PNAS Nexus), where we tried to find associations between dental pathologies and taxonomic and genome content. We had a large skeletal collection from a single site in the Netherlands, with a lot of osteological metadata. The study aimed to see if there were any links between the oral microbiome and groups of dental pathologies.\n# load the metadata file\nfull_metadata &lt;- fread(\"full_combined_metadata.tsv\")\n\n\n## Example of metadata\ntibble(full_metadata %&gt;%\n    filter(Site_code == \"MID\") %&gt;%\n    select(Site, Time_period, Library_ID, Sequencing_instrument, Pipenotch, Max_Perio_Score, `%teeth_with_caries`))\nFirst step: we can pre-define various functions for generate PCAs we will use downstream - you don’t have to worry about these too much they are just custom functions to quickly plot PCAs from a mixOmics PCA output object with ggplot, but we leave the code here for if you’re curious.\n# plot PCA with colored dots and the title including the # of species or genera\nplot_pca &lt;- function(df, pc1, pc2, color_group, shape_group, ncomps) {\n    metadata_group_colors &lt;- get(paste(color_group, \"_colors\", sep = \"\"))\n    metadata_group_shapes &lt;- get(paste(shape_group, \"_shapes\", sep = \"\"))\n\n    pca.list &lt;- mixOmics::pca(df, ncomp = ncomps, logratio = 'CLR')\n\n    ## Pull out loadings\n    exp_var &lt;- paste0(round(pca.list$explained_variance * 100, 2), \"%\")\n    df_X &lt;- pca.list$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    color_group = df_X[[color_group]]\n    shape_group = df_X[[shape_group]]\n\n    ## Selecting which PCs to plot\n    if (pc1 == 'PC1') {\n        pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    }  else if (pc1 == 'PC2') {\n        pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n    }\n\n    if (pc2 == 'PC1') {\n        pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n    }  else if (pc2 == 'PC2') {\n        pc2 &lt;- df_X$PC2\n        exp_var_pc2 &lt;- exp_var[2]\n        yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n        pc2 &lt;- df_X$PC3\n        exp_var_pc2 &lt;- exp_var[3]\n        yaxis &lt;- c(\"PC3\")\n    }\n\n    ## Generate figure\n    pca_plot &lt;- ggplot(df_X, aes(pc1, pc2)) +\n     geom_point(aes(fill = color_group, shape = shape_group), size = 4.5, stroke = 0.3) +\n     scale_fill_manual(values = metadata_group_colors) +\n     scale_shape_manual(values = metadata_group_shapes) +\n     # stat_ellipse() +\n     xlab(paste(xaxis, \" - \", exp_var_pc1)) +\n     ylab(paste(yaxis, \" - \", exp_var_pc2)) +\n     theme_minimal(base_size = 16) +\n     theme(text = element_text(size=16)) +\n     theme(legend.title = element_blank(),\n           legend.key.size = unit(2,\"mm\"),\n           legend.text = element_text(size = 6)) +\n     theme(legend.position = \"top\")\n\n    return(pca_plot)\n}\n\n# for continuous data\nplot_pca_cont &lt;- function(df, pc1, pc2, color_group, shape_group, ncomps, title_text) {\n\n    pca.list &lt;- mixOmics::pca(df, ncomp = ncomps, logratio = 'CLR')\n\n    exp_var &lt;- paste0(round(pca.list$explained_variance * 100, 2), \"%\")\n    df_X &lt;- pca.list$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    color_group = df_X[[color_group]]\n    shape_group = df_X[[shape_group]]\n\n    if (pc1 == 'PC1') {\n        pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    }  else if (pc1 == 'PC2') {\n        pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n    }\n\n    if (pc2 == 'PC1') {\n        pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n    }  else if (pc2 == 'PC2') {\n        pc2 &lt;- df_X$PC2\n        exp_var_pc2 &lt;- exp_var[2]\n        yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n        pc2 &lt;- df_X$PC3\n        exp_var_pc2 &lt;- exp_var[3]\n        yaxis &lt;- c(\"PC3\")\n    }\n\n    pca_plot &lt;- ggplot(df_X, aes(pc1, pc2, fill = color_group, shape = shape_group)) +\n     geom_point(size = 5, color = \"black\") +\n     scale_fill_viridis_c(option = \"C\") +\n     scale_shape_manual(values = c(24,21)) +\n     # stat_ellipse() +\n     xlab(paste(xaxis, \" - \", exp_var_pc1)) +\n     ylab(paste(yaxis, \" - \", exp_var_pc2)) +\n     theme_minimal(base_size = 16) +\n     theme(text = element_text(size=16)) +\n     theme(legend.title = element_blank(),\n           legend.key.size = unit(2,\"mm\"),\n           legend.text = element_text(size = 6)) +\n     theme(legend.position = \"top\") +\n     ggtitle(title_text) + theme(plot.title = element_text(size = 10))\n\n    return(pca_plot)\n}\n\nplot_pca_bi &lt;- function(df, pc1, pc2, metadata_group, columntitle) {\n    metadata_group_colors &lt;- get(paste(metadata_group, \"_colors\", sep = \"\"))\n    metadata_group_shapes &lt;- get(paste(metadata_group, \"_shapes\", sep = \"\"))\n\n    arrow_pc &lt;- enquo(columntitle)\n\n    exp_var &lt;- paste0(round(df$explained_variance * 100, 2), \"%\") # explained variance for x- and y-labels\n\n    # select only the PCs from the PCA and add metadata\n    df_X &lt;- df$variates$X %&gt;%\n              as.data.frame() %&gt;%\n              rownames_to_column(\"Library_ID\") %&gt;%\n              inner_join(full_metadata, by = \"Library_ID\")\n\n    metadata_group = df_X[[metadata_group]]\n\n    corr_lam &lt;- df$sdev[c(\"PC1\", \"PC2\", \"PC3\")] * sqrt(nrow(df_X))\n\n    df_X &lt;- df_X %&gt;%\n      mutate(PC1 = PC1 / corr_lam[1],\n             PC2 = PC2 / corr_lam[2],\n             PC3 = PC3 / corr_lam[3])\n\n    # select the correct PC column and explained variance for PC1\n    if (pc1 == 'PC1') {\n        Pc1 &lt;- df_X$PC1\n        exp_var_pc1 &lt;- exp_var[1]\n        xaxis &lt;- c(\"PC1\")\n    } else if (pc1 == 'PC2') {\n        Pc1 &lt;- df_X$PC2\n        exp_var_pc1 &lt;- exp_var[2]\n        xaxis &lt;- c(\"PC2\")\n    } else if (pc1 == 'PC3') {\n        Pc1 &lt;- df_X$PC3\n        exp_var_pc1 &lt;- exp_var[3]\n        xaxis &lt;- c(\"PC3\")\n   }\n\n    # select the correct PC column and explained variance for PC2\n    if (pc2 == 'PC1') {\n        Pc2 &lt;- df_X$PC1\n        exp_var_pc2 &lt;- exp_var[1]\n        yaxis &lt;- c(\"PC1\")\n } else if (pc2 == 'PC2') {\n       Pc2 &lt;- df_X$PC2\n       exp_var_pc2 &lt;- exp_var[2]\n       yaxis &lt;- c(\"PC2\")\n    } else if (pc2 == 'PC3') {\n       Pc2 &lt;- df_X$PC3\n       exp_var_pc2 &lt;- exp_var[3]\n       yaxis &lt;- c(\"PC3\")\n   }\n\n    # Identify the 10 pathways that have highest positive and negative loadings in the selected PC\n    pws_10 &lt;- df$loadings$X %&gt;%\n      as.data.frame(.) %&gt;%\n      rownames_to_column(var = \"Pathway\") %&gt;%\n      separate(Pathway, into = \"Pathway\", sep = \":\", extra = \"drop\") %&gt;%\n      top_n(10, !!arrow_pc)\n\n    neg_10 &lt;- df$loadings$X %&gt;%\n      as.data.frame(.) %&gt;%\n      rownames_to_column(var = \"Pathway\") %&gt;%\n      separate(Pathway, into = \"Pathway\", sep = \":\", extra = \"drop\") %&gt;%\n      top_n(-10, !!arrow_pc)\n\n\n    pca_plot_bi &lt;- ggplot(df_X, aes(x = Pc1, y = Pc2)) +\n      geom_point(size = 3.5, aes(shape = metadata_group, fill = metadata_group))+\n      geom_segment(data = pws_10,\n                   aes(xend = get(paste(pc1)), yend = get(paste(pc2))),\n                   x = 0, y = 0, colour = \"black\",\n                   size = 0.5,\n                   arrow = arrow(length = unit(0.03, \"npc\"))) +\n      geom_label_repel(data = pws_10,\n                   aes(x = get(paste(pc1)), y = get(paste(pc2)), label = Pathway),\n                   size = 2.5, colour = \"grey20\", label.padding = 0.2, force = 5, max.overlaps = 20) +\n      geom_segment(data = neg_10,\n                   aes(xend = get(paste(pc1)), yend = get(paste(pc2))),\n                   x = 0, y = 0, colour = \"grey50\",\n                   size = 0.5,\n                   arrow = arrow(length = unit(0.03, \"npc\"))) +\n      geom_label_repel(data = neg_10,\n                   aes(x = get(paste(pc1)), y = get(paste(pc2)), label = Pathway),\n                   size = 2.5, colour = \"grey20\", label.padding = 0.2, max.overlaps = 12) +\n      labs(x = paste(xaxis, \" - \", exp_var_pc1),\n           y = paste(yaxis, \" - \", exp_var_pc2)) +\n      scale_fill_manual(values = metadata_group_colors) +\n      scale_shape_manual(values = metadata_group_shapes) +\n      theme_minimal() + theme(text = element_text(size = 16)) +\n      theme(text = element_text(size=16)) +\n      theme(legend.position = \"top\")\n\n    return(pca_plot_bi)\n}\nAs we are dealing with aDNA, and we often have bad samples, its sometimes interesting to see differences between well/badly preserved samples at all stages of analysis.\nTherefore we may generate results for all samples. However for actual analysis where we want to interpret biological differences, should exclude outliers (in this case highly contaminated samples - as identified by the decontam package - see Velsko et al. 2022 _PNAS Nexus for more details).\nWe can make a list the outliers from the previous authentication analyses.\noutliers_mpa3 &lt;- c(\"EXB059.A2101\",\"EXB059.A2501\",\"EXB015.A3301\",\"EXB034.A2701\",\n                   \"EXB059.A2201\",\"EXB059.A2301\",\"EXB059.A2401\",\"LIB058.A0103\",\"LIB058.A0106\",\"LIB058.A0104\")\npoor_samples_mpa3 &lt;- c(\"CS28\",\"CS38\",\"CSN\",\"ELR003.A0101\",\"ELR010.A0101\",\n                       \"KT09calc\",\"MID024.A0101\",\"MID063.A0101\",\"MID092.A0101\")\n\noutliersF &lt;- str_c(outliers_mpa3, collapse = \"|\")",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html#sample-clustering-with-pca",
    "href": "functional-profiling.html#sample-clustering-with-pca",
    "title": "18  Functional Profiling",
    "section": "18.4 Sample Clustering with PCA",
    "text": "18.4 Sample Clustering with PCA\n\n18.4.1 Pathway abundance analyses\nOnce we’ve removed outlier samples, our first simple question is - what is the functional relationships of the groups?\nCan we already see distinctive patterns between the different groups in our dataset?\nTo do this lets clean up the data a bit (cleaning names, removing samples with no metadata etc.), normalise (via a ‘centered-log-ratio’ transform ), and run a PCA.\nOnce we’ve done this we should always check our PCA’s Scree plot first.\n\nhumann3_path_l1 &lt;- humann3_path %&gt;%\n  filter(!str_detect(Pathway, \"\\\\|\")) %&gt;%\n  # no full_metadata, remove these\n  select(-c(\"MID025.A0101\",\"MID033.A0101\",\"MID052.A0101\",\"MID056.A0101\",\n            \"MID065.A0101\",\"MID068.A0101\",\"MID076.A0101\",\"MID078.A0101\")) %&gt;%\n  # remove poorly preserved saples\n  select(-c(\"MID024.A0101\",\"MID063.A0101\",\"MID092.A0101\")) %&gt;%\n  select(-matches(\"EXB|LIB\")) %&gt;%\n  # inner_join(., humann3_path.decontam_noblanks_presence_more_30, by = \"Pathway\") %&gt;%\n  gather(\"Library_ID\",\"Counts\",2:ncol(.)) %&gt;%\n  mutate(Counts = Counts + 1) %&gt;%\n  spread(Pathway,Counts) %&gt;%\n  column_to_rownames(\"Library_ID\")\n\n# prepare to run a PCA\n# check the number of components to retain by tuning the PCA\nmixOmics::tune.pca(humann3_path_l1, logratio = 'CLR')\n\n\nhumann3_all_otu.pca &lt;- mixOmics::pca(humann3_path_l1, ncomp = 3, logratio = 'CLR')\nhumann3_all_pca_values &lt;- humann3_all_otu.pca$variates$X %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Library_ID\") %&gt;%\n  inner_join(., full_metadata, by = \"Library_ID\")\nWe can see the first couple of PCs in the scree plot account for a good chunk of the variation of our dataset, so lets visualise the PCA itself.\nWe visualise the PCA with one of our custom functions defined above, and colour by the Pipe notch metadata column.\n# pipenotch colors/shapes\nPipenotch_colors = c(\"#311068\",\"#C83E73\")\nPipenotch_shapes = c(16,17)\n\n# by minimum number of pipenotches\npipenotch &lt;- plot_pca_cont(humann3_path_l1, \"PC1\", \"PC2\",\"Min_no_Pipe_notches\",\"Pipenotch\", 3,\"Min. No. Pipe Notches\")\npipenotch\nWe can see there is a slight separation between the groups, but how do we find out which pathways are maybe driving this pattern?\nFor this we can generate a PCA bi-plot which show what loadings are driving the spread of the samples.\nPipenotch_colors = c(\"#311068\",\"#C83E73\")\nPipenotch_shapes = c(24,21)\n\nbiplot &lt;- plot_pca_bi(humann3_all_otu.pca, \"PC1\", \"PC2\", \"Pipenotch\", PC1)\nbiplot\nFrom the biplot we can see which pathways are differentiating along PC1.\nWe can pull these IDs out to find out what pathways there are from the biplot object itself.\n# make a table of the pathways to save, to use again later in another R notebook\nhumann3_pathway_biplot_list &lt;- biplot$plot_env$pws_10 %&gt;% arrange(desc(PC1)) %&gt;% select(Pathway, PC1, PC2) %&gt;% mutate(Direction = \"PC1+\")\nhumann3_pathway_biplot_list &lt;- humann3_pathway_biplot_list %&gt;%\n  bind_rows(biplot$plot_env$neg_10 %&gt;% arrange(desc(PC1)) %&gt;% select(Pathway, PC1, PC2)%&gt;% mutate(Direction = \"PC1-\"))\n\n\n18.4.2 Species contributions to pathways\nHowever, this ID numbers aren’t very informative to us. At this point we have to do a bit of literature review/database scraping to pull the human-readable names/descriptions of the IDs - which we have already done for you.\nWe can load these back into our environment\n# PC biplot loading top 10s\nhumann3_pathway_biplot_list &lt;- fread(\"./humann3_pathway_biplot_list.tsv\")\nhumann3_pathway_biplot_list &lt;- humann3_pathway_biplot_list %&gt;%\n  rename(Pathway = pathway) %&gt;%\n  mutate(Path = sapply(Pathway, function(f) {\n                                  unlist(str_split(f, \":\"))[1]\n                                  })) %&gt;%\n  select(Pathway, Path, everything()) %&gt;%\n  # remove 3 of the 4 ubiquinol pathways w/identical loadings\n  filter(!str_detect(Pathway, \"5856|5857|6708\"))\n\ntibble(humann3_pathway_biplot_list)\nWe now have the pathway ID, and a pathway description for each of the loadings of the PCA.\n\n18.4.2.0.1 PC1\nWhile we have the pathways, we don’t who contributed these.\nFor this, we can join our pathway table back onto the original output from HUMANn3 we loaded at the beginning, which includes the taxa information.\n# list the 10 orthologs with strongest loading in PC1 + values\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1+\") %&gt;%\n  pull(Path) %&gt;%\n  str_c(., collapse = \"|\") # need this format for filtering in the next step\n\n\n# select only those 10 pathways from the list, and split the column with names into 3 (Pathway, Genus, Species)\nhumann3_path_pc1pws &lt;- humann3_path %&gt;%\n  filter(str_detect(Pathway, humann3_path_biplot_pc)) %&gt;%\n  filter(str_detect(Pathway, \"\\\\|\")) %&gt;%\n  gather(\"SampleID\", \"CPM\", 2:ncol(.)) %&gt;%\n  mutate(Pathway = str_replace_all(Pathway, \"\\\\.s__\", \"|s__\")) %&gt;%\n  separate(., Pathway, into = c(\"Pathway\", \"Genus\", \"Species\"), sep = \"\\\\|\") %&gt;%\n  mutate(Species = replace_na(Species, \"unclassified\"),\n         Genus = str_replace_all(Genus, \"g__\", \"\"),\n         Species = str_replace_all(Species, \"s__\", \"\")) %&gt;%\n  inner_join(., humann3_pathway_biplot_list %&gt;%\n              select(Pathway, Path) %&gt;%\n               distinct(.), by = \"Pathway\") %&gt;%\n  inner_join(.,  humann3_pathway_biplot_list %&gt;%\n               filter(Direction == \"PC1+\") %&gt;%\n               select(Path), by = \"Path\") %&gt;%\n  # select(-Pathway) %&gt;%\n  select(Path, everything()) %&gt;%\n  arrange(Path)\n\ntibble(humann3_path_pc1pws)\nWe can now see who contributed which pathway, and also the abundance information (CPM)!\nGiven many taxa may contribute to the same pathway, we may want to see which taxa are more ‘dominantly’ contributing to this.\nFor this we can calculate of all copies of a given pathway what fraction comes from which taxa (you can imagine this like ‘depth’ coverage in genomic analysis), based on the percentage of the total copies per million for that pathway.\n# calculate the % for each pathway contributed by each genus\nhumann3_path_pc1pws_stats &lt;- humann3_path_pc1pws %&gt;%\n  group_by(Path, Genus) %&gt;%\n  summarize(Sum = sum(CPM)) %&gt;%\n  mutate(Percent = Sum/sum(Sum)*100) %&gt;%\n  ungroup(.)\n\n# create the list of 10 orthologs again, but don't collapse the list as above\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1+\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path)\n\n# calculate the total % of all genera that contribute &lt; X% to each ortholog\nhumann3_path_pc1pws_stats_extra &lt;- lapply(humann3_path_biplot_pc, function(eclass) {\n high_percent &lt;- humann3_path_pc1pws_stats %&gt;%\n   filter(Path == eclass) %&gt;%\n   filter(Percent &lt; 5) %&gt;%\n   summarise(Remaining = sum(Percent)) %&gt;%\n   mutate(Path = eclass,\n          Genus = \"Other\")\n}) %&gt;%\n bind_rows(.)\n\n# add this additional % to the main table\nhumann3_path_pcbi_bar_df &lt;- humann3_path_pc1pws_stats_extra %&gt;%\n  rename(Percent = Remaining) %&gt;%\n  bind_rows(., humann3_path_pc1pws_stats %&gt;%\n              select(-Sum)) %&gt;%\n  select(Path, Genus, Percent) %&gt;%\n  mutate(Direction = \"PC1+\") %&gt;%\n  distinct()\nAnd we can visualize the contributors to the top 10 pathways) driving the main variation along PC1 (with the assumption these maybe the most biological significant, and to reduce the numbers of pathways we have to research.\nFor the loadings falling in the positive direction of the PC1:\n# plot the values in a bar chart\npaths_sp_pc1 &lt;- humann3_path_pcbi_bar_df %&gt;%\n  # filter(Direction == \"PC1+\", Genus != \"Other\") %&gt;% # removing Other plots all species/unassigned - no need to filter the pathways\n  filter(Percent &gt;= 5 | (Percent &lt;= 5 & Genus == \"Other\")) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  # filter(Percent &gt;= 5) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  mutate(\n    Genus = fct_relevel(Genus, \"Other\",\"unclassified\",\"Aggregatibacter\",\"Capnocytophaga\",\"Cardiobacterium\",\n                        \"Eikenella\",\"Haemophilus\",\"Kingella\",\"Lautropia\",\"Neisseria\",\"Ottowia\",\"Streptococcus\"),\n         Path = fct_relevel(Path, humann3_pathway_biplot_list %&gt;%\n                              filter(Direction == \"PC1+\") %&gt;%\n                              pull(Path))) %&gt;%\n  ggplot(., aes(x=Path, y=Percent, fill = Genus)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0D0887FF\",\"#969696\",\"#5D01A6FF\",\"#7E03A8FF\",\n                                 \"#9C179EFF\",\"#B52F8CFF\",\"#CC4678FF\",\"#DE5F65FF\",\n                                 \"#ED7953FF\",\"#F89441FF\",\"#FDB32FFF\",\"#FBD424FF\",\"#F0F921FF\")) +\n    theme(text = element_text(size=18),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    ylab(\"Percent\") +\n    ggtitle(\"Metacyc pathways - PC1 positive\") +\n    theme(title = element_text(size=10))\n\n# viridis_pal(option = \"B\")(13)\npaths_sp_pc1\n\n\n\n\n\n\nWarning\n\n\n\nPWY-5345 has no species assignment to that pathway.\n\n\nAnd the negative loadings:\n# list the 10 orthologs with strongest loading in PC1 + values\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1-\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path) %&gt;%\n  str_c(., collapse = \"|\") # need this format for filtering in the next step\n\n# select only those 10 orthologs from the list, and split the column with names into 3 (Ortholog, Genus, Species)\nhumann3_path_pc1neg &lt;- humann3_path %&gt;%\n  filter(str_detect(Pathway, humann3_path_biplot_pc)) %&gt;%\n  filter(str_detect(Pathway, \"\\\\|\")) %&gt;%\n  gather(\"SampleID\", \"CPM\", 2:ncol(.)) %&gt;%\n  mutate(Pathway = str_replace_all(Pathway, \"\\\\.s__\", \"|s__\")) %&gt;%\n  separate(., Pathway, into = c(\"Pathway\", \"Genus\", \"Species\"), sep = \"\\\\|\") %&gt;%\n  mutate(Species = replace_na(Species, \"unclassified\"),\n         Genus = str_replace_all(Genus, \"g__\", \"\"),\n         Species = str_replace_all(Species, \"s__\", \"\")) %&gt;%\n  inner_join(., humann3_pathway_biplot_list %&gt;%\n              select(Pathway, Path) %&gt;%\n               distinct(.), by = \"Pathway\") %&gt;%\n  inner_join(.,  humann3_pathway_biplot_list %&gt;%\n               filter(Direction == \"PC1-\") %&gt;%\n               select(Path), by = \"Path\") %&gt;%\n  select(-Pathway) %&gt;%\n  select(Path, everything()) %&gt;%\n  arrange(Path)\n\n# calculate the % for each ortholog contributed by each genus\nhumann3_path_pc1neg_stats &lt;- humann3_path_pc1neg %&gt;%\n  group_by(Path, Genus) %&gt;%\n  summarize(Sum = sum(CPM)) %&gt;%\n  mutate(Percent = Sum/sum(Sum)*100) %&gt;%\n  ungroup(.)\n\n# create the list of 10 orthologs again, but don't collapse the list as above\nhumann3_path_biplot_pc &lt;- humann3_pathway_biplot_list %&gt;%\n  filter(Direction == \"PC1-\") %&gt;%\n  arrange(Path) %&gt;%\n  pull(Path)\n\n# calculate the total % of all genera that contribute &lt; X% to each ortholog\nhumann3_path_pc1neg_stats_extra &lt;- lapply(humann3_path_biplot_pc, function(eclass) {\n high_percent &lt;- humann3_path_pc1neg_stats %&gt;%\n   filter(Path == eclass) %&gt;%\n   filter(Percent &lt; 5) %&gt;%\n   summarise(Remaining = sum(Percent)) %&gt;%\n   mutate(Path = eclass,\n          Genus = \"Other\")\n}) %&gt;%\n bind_rows(.)\n\n# add this additional % to the main table\nhumann3_path_pcbi_bar_df &lt;- humann3_path_pcbi_bar_df %&gt;%\n  bind_rows(humann3_path_pc1neg_stats_extra %&gt;%\n            rename(Percent = Remaining) %&gt;%\n            bind_rows(., humann3_path_pc1neg_stats %&gt;%\n                      select(-Sum)) %&gt;%\n            select(Path, Genus, Percent) %&gt;%\n            mutate(Direction = \"PC1-\")) %&gt;%\n  distinct()\n\n# plot the values in a bar chart\npaths_sp_pc2 &lt;- humann3_path_pcbi_bar_df %&gt;%\n  filter(Direction == \"PC1-\") %&gt;% # removing Other plots all species/unassigned - no need to filter the pathways\n  filter(Percent &gt;= 5 | (Percent &lt;= 5 & Genus == \"Other\")) %&gt;% # filter out the genera with % &lt; 5, but keep Other &lt; 5\n  mutate(Genus = fct_relevel(Genus, \"Other\",\"unclassified\",\"Desulfobulbus\",\"Desulfomicrobium\",\"Methanobrevibacter\"),\n         Path = fct_relevel(Path, humann3_pathway_biplot_list %&gt;%\n                              filter(Direction == \"PC1-\") %&gt;%\n                              pull(Path))) %&gt;%\n  ggplot(., aes(x=Path, y=Percent, fill = Genus)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0D0887FF\",\"#969696\",\"#B52F8CFF\",\"#ED7953FF\",\"#FCFFA4FF\")) +\n    theme(text = element_text(size=18),\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    # facet_wrap(~pathrtholog, nrow=2) +\n    ylab(\"Percent\") +\n    ggtitle(\"Metacyc pathways - PC1 negative\") +\n    theme(title = element_text(size=12))\n\npaths_sp_pc2\n\n\n\n18.4.3 Final Visualisation\nFinally, we can stick the biplot and the taxon contribution plots together!\nh3biplots &lt;- biplot + paths_sp_pc2 + paths_sp_pc1 +\n  plot_layout(widths = c(2, 1,1))\n\nggsave(\"./h3_paths_biplots.pdf\", plot = h3biplots,\n        device = \"pdf\", scale = 1, width = 20, height = 9.25, units = c(\"in\"), dpi = 300)\n\nsystem(paste0('firefox \"h3_paths_biplots.pdf\"'))\nThis allows us to evaluate all the information together.\nFrom this point onwards, we would have to do manual research/literature reviews into each of the pathways, see if they make ‘sense’ to the sample type and associated groups of samples, and evaluate whether they are interesting or not.",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "functional-profiling.html#optional-clean-up",
    "href": "functional-profiling.html#optional-clean-up",
    "title": "18  Functional Profiling",
    "section": "18.5 (Optional) clean-up",
    "text": "18.5 (Optional) clean-up\nLet’s clean up your working directory by removing all the data and output from this chapter.\nThe command below will remove the /&lt;PATH&gt;/&lt;TO&gt;/functional-profiling directory as well as all of its contents.\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways be VERY careful when using rm -r. Check 3x that the path you are specifying is exactly what you want to delete and nothing more before pressing ENTER!\n\n\nrm -rf /&lt;PATH&gt;/&lt;TO&gt;/functional-profiling*\nOnce deleted you can move elsewhere (e.g. cd ~).\nWe can also get out of the conda environment with\nconda deactivate\nTo delete the conda environment\nconda remove --name functional-profiling --all -y",
    "crumbs": [
      "Deprecated Chapters",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "19  Resources",
    "section": "",
    "text": "19.1 Introduction to NGS Sequencing",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#introduction-to-ngs-sequencing",
    "href": "resources.html#introduction-to-ngs-sequencing",
    "title": "19  Resources",
    "section": "",
    "text": "https://www.youtube.com/watch?v=fCd6B5HRaZ8\nhttps://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "20  Tools",
    "section": "",
    "text": "20.1 Introduction to R and the Tidyverse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#introduction-to-r-and-the-tidyverse",
    "href": "tools.html#introduction-to-r-and-the-tidyverse",
    "title": "20  Tools",
    "section": "",
    "text": "r\nr studio (desktop)\ntidyverse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#introduction-to-python-and-pandas",
    "href": "tools.html#introduction-to-python-and-pandas",
    "title": "20  Tools",
    "section": "20.2 Introduction to Python and Pandas",
    "text": "20.2 Introduction to Python and Pandas\n\npython\njupyter",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#introduction-to-github",
    "href": "tools.html#introduction-to-github",
    "title": "20  Tools",
    "section": "20.3 Introduction to Git(Hub)",
    "text": "20.3 Introduction to Git(Hub)\n\ngit (normally installed by default on all UNIX based operating systems e.g. Linux, OSX)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#functional-profiling",
    "href": "tools.html#functional-profiling",
    "title": "20  Tools",
    "section": "20.4 Functional Profiling",
    "text": "20.4 Functional Profiling\n\nr\nr studio (desktop)\ntidyverse\nhumann3",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#de-novo-assembly",
    "href": "tools.html#de-novo-assembly",
    "title": "20  Tools",
    "section": "20.5 De novo assembly",
    "text": "20.5 De novo assembly\n\nfastp\nmegahit\nbowtie2\nsamtools\nbioawk\ndiamond\nmetabat2\nmaxbin2\nconcoct\nmetawrap\ncheckm-genome\ngunc\npydamage\nprokka",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#genome-mapping",
    "href": "tools.html#genome-mapping",
    "title": "20  Tools",
    "section": "20.6 Genome Mapping",
    "text": "20.6 Genome Mapping\n\nbwa\nigv\ngatk",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#phylogenomics",
    "href": "tools.html#phylogenomics",
    "title": "20  Tools",
    "section": "20.7 Phylogenomics",
    "text": "20.7 Phylogenomics\n\nbeast2_\ntracer\ntempest\nmega",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "tools.html#ancient-metagenomic-pipelines",
    "href": "tools.html#ancient-metagenomic-pipelines",
    "title": "20  Tools",
    "section": "20.8 Ancient Metagenomic Pipelines",
    "text": "20.8 Ancient Metagenomic Pipelines\n\nnextflow\nnf-core tools\nnf-core/eager",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html",
    "href": "chapter-writing-guidelines.html",
    "title": "21  Chapter Writing Guidelines",
    "section": "",
    "text": "21.1 Language\nThe following language guidance is being displayed in a bullet point list using the hyphen character to indicate each item:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#language",
    "href": "chapter-writing-guidelines.html#language",
    "title": "21  Chapter Writing Guidelines",
    "section": "",
    "text": "All spelling should be in British English.\nFull stops should come after closing parentheses.\n\nFor example (the full stop comes after the closing parenthesis).\n\nVoice should be in a ‘collective’ first person, as in ‘we’ or ‘our’.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#section-headings",
    "href": "chapter-writing-guidelines.html#section-headings",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.2 Section Headings",
    "text": "21.2 Section Headings\nAll chapters should use section headings, up to a maximum of two sub-levels. The heading above corresponds to the second level.\nMandatory headings are\n\nThe title (specified in the markdown header block, by default a highest level)\nSummary (second-level header, indicated by two ##)\nReferences (second-level header, indicated by two ##)\n\n\n21.2.1 Sub-section heading\nThis header here corresponds to the maximum heading depth you can get to. The purpose of this is to ensure that the document is easy to navigate and read. You will see therefore that we never go further than three numbers in header references, e.g. 20.03.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#formatting",
    "href": "chapter-writing-guidelines.html#formatting",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.3 Formatting",
    "text": "21.3 Formatting\nGenerally, Quarto follows standard Markdown with some extra ‘syntax’ sugar. A Markdown cheatsheet can be found here.\n\n21.3.1 General Markdown\n\nOne sentence per line.\ni.e., this is bad:\nFor example this is the first sentence. Here is the second sentence. This makes reviewing on GitHub harder.\nThis is good:\nFor example this is the first sentence.\nHere is the second sentence.\nThis makes reviewing on GitHub easier.\nNew paragraphs should be separated by a blank line.\nThis would be a first sentence.\nThen a second sentence.\n\nThen a new paragraph.\nAnd so on.\n\nTo continue\n\nItalic formatting should be used with a single underscore _ and should be used for emphasis\nIf required, Bold formatting should be used with double asterisks **. This should not be used for emphasis\n\nBold formatting should be used to indicate words for inclusion in a glossary, or to indicate the first word of bullet lists\n\nURLs should be both hyperlinked around a relevant bit of text\n\nFor example, for visit the SPAAM Website this should be rendered visit the[SPAAM Website](https://www.spaam-community.org/)\nThis is to make sure URLs in printed copies of the textbook are still visible\n\nBullet point lists must start with a capital letter and not end with a full stop\n\n\n\n21.3.2 Notes and tips\nFor side nodes, tips, warnings, etc. you should Quarto callout blocks.\nFor example.\n::: {.callout-warning}\nWe reserve some callout formatting for specific objects!\n:::\nGets rendered like this.\n\n\n\n\n\n\nWarning\n\n\n\nWe reserve some callout formatting for specific objects!\n\n\nSee the Chapter Content for the reserved formatting. Note: I rendered that within-page link with [Chapter Content](#chapter-content) syntax, where #chapter-content is the name of the header in all lowercase and all punctiation replaced with -.\n\n\n21.3.3 Code\nSingle backticks for inline code should only be used for single words such as tool names, e.g. samtools, file extensions such as .txt, or single paths &lt;path&gt;/&lt;to&gt;/&lt;here&gt;.\nAny form of commands or code should be formatted as code in a syntax-named triple backtick block. Execution should be turned off by default for those languages that may auto-execute (i..e, R, with eval=FALSE in a code block header block), unless has a very low computational overhead and can be rendered on the fly. Language should be specified.\nFor example, this is a bash code block that has a whole command.\n    ```bash\n    echo \"SPAAM rules!\"\n    ```\nAnd gets rendered as like this.\necho \"SPAAM rules!\"\nIf you wish to display large output from a code block that does get executed, you should turn off the command execution printing in the code block itself, and use a collapsible callout block with a ‘Expand to see output’ title immediately after the code block itself that contains. The content of the output should also be in a triple backtick block, with the the verbatim syntax specified.\necho \"SPAAM rules!\"\n\n\n\n\n\n\nExpand see output\n\n\n\n\n\nSPAAM rules!\n\n\n\nThis was generated with\nAnother example, with setting eval=FALSE for R.\n```{.r eval=FALSE}\nprint('SPAAM Rules!')\n```\n\n\n\n\n\n\nExpand see output\n\n\n\n\n\n```{.r eval=FALSE}\n[1] \"SPAAM Rules!\"\n```\n\n\n\nAdditional guidance is as follows:\n\nSentences prior a code block must end in a full stop and not a colon\nAll code blocks must have a language specified\nDo not include leading $ or other prompt indicators at the beginning of code blocks\n\nThis makes copy pasting harder\n\nAny paths to should ideally be relative to the ‘working’ directory of that chapter\n\nThis ensures portability of code between different systems and readers\n\nIf paths require placeholders they should be in &lt;angle brackets&gt;\nAny code that should not actually be run by the user should be wrapped in a ‘warning’ callout block\nOptional ‘self-guided’ sections (i.e. sections that require user setup and would normally be already prepared for users by summer school organisers prior the summer school), should have be in a collapsed self-guided .callout-note with a title prefixed with Self guided:\n\n\n\n21.3.4 Figures\nFigures should be included in the assets/images/chapters/&lt;your-chapter&gt;/ directory.\nThey can be inserted into a chapter either with typical markdown syntax or with the HTML (e.g. for centering purposes).\nRegardless of the method used, the image should be given a caption, and a quarto label with the fig prefix ({#fig-&lt;chapternamewithnopunctiation&gt;-&lt;imageidwithnopunctuation&gt;}) for referencing the figure in the text.\nFor example, the following.\n![A mock up of a possible full-book jacket for this textbook. The left hand side includes a blurb and a list of contributors, and hte SPAAM logo. The right hand side includes the title of the book, a sankey diagram, and an 'edited by' section.](assets/images/jacket.png){#fig-chapterwritingguidelines-jacket}\nWould be rendered as as follows.\n\n\n\n\n\n\nFigure 21.1: A mock up of a possible full-book jacket for this textbook. The left hand side includes a blurb and a list of contributors, and hte SPAAM logo. The right hand side includes the title of the book, a sankey diagram, and an ‘edited by’ section.\n\n\n\nIt is important for the caption to be as descriptive as possible to account for accessibility reasons.\nAll figures should be referenced in the text using the quarto label, and without a leading ‘Figure’ (this is rendered for you) e.g,.\n \"@fig-chapterwritingguidelines-jacket shows an example figure\"\nGets rendered as:\n“Figure Figure 21.1 shows an example figure”.\n\n\n21.3.5 Tables\nMuch like figures, all markdown tables must have a caption and referenced in text. To copy the quarto documentation example.\n| Default | Left | Right | Center |\n|---------|:-----|------:|:------:|\n| 12      | 12   |    12 |   12   |\n| 123     | 123  |   123 |  123   |\n| 1       | 1    |     1 |   1    |\n\n: Demonstration of pipe table syntax {#tbl-chapterwritingguidelines-exampletable1}\nGets rendered as in Table 21.1.\n\n\n\nTable 21.1: Demonstration of pipe table syntax\n\n\n\n\n\nDefault\nLeft\nRight\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nThis also applies to code-block generated tables e.g. as in Table 21.2. For example the next code block (if wrapped in the triple-backticks with r syntax, not shown here).\n    \n    #| label: tbl-chapterwritingguidelines-examplecodetable2\n    #| echo: false\n    #| results: 'asis'\n    #| tbl-cap: \"Your caption goes here.\"\n    #| tbl-cap-location: top\n    library(tidyverse)\n    library(pander)\n    tibble(a = 1, b = 2) %&gt;%\n        pandoc.table(., split.tables = Inf)\n    \nGets rendered as follows.\n\n\n\nTable 21.2: Your caption goes here.\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n1\n2\n\n\n\n\n\n\nIf your table is too wide, you may load the table and use some R code render the table in a more readable format.\nTo do so, place the CSV file in the assets/images/chapters/&lt;your-chapter&gt;/ directory. Then use the following template to render the table in your chapter. .qmd file.\nlibrary(tidyverse)\nlibrary(gt)\n# Load the data from CSV\ndata &lt;- read_csv(\"assets/images/chapters/&lt;chapter-name&gt;/&lt;filename&gt;.csv\")\n\n# Create the table with gt\ndata %&gt;%\n    gt() %&gt;%\n    tab_options(\n        table.width = pct(100),\n        table.layout = \"fixed\",\n        container.overflow.x = \"scroll\"\n    )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#chapter-content",
    "href": "chapter-writing-guidelines.html#chapter-content",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.4 Chapter Content",
    "text": "21.4 Chapter Content\n\n21.4.1 Questions and Exercises\nThe end of every main section of the chapter (e.g. every second-level header) should have a little ‘task’ or question to test the reader’s understanding of the content.\nThese should be formatted as a Quarto callout block, with a renamed callout-tip class. This should be rendered in this ‘reserved’ call out block format.\n::: {.callout-tip title=\"Question\" appearance=\"simple\"}\nYour question(s) goes here.\n:::\n\n::: {.callout-note collapse=\"true\" title=\"Answer\"}\nThe answer(s) goes here, in a by-default 'hidden' block the reader can open to view.\n:::\nGets rendered as\n\n\n\n\n\n\nQuestion\n\n\n\nYour question(s) goes here.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer(s) goes here, in a by-default ‘hidden’ block the reader can open to view.\n\n\n\n\n\n21.4.2 Data\nThere should be no data for practical tasks used on the chapter within the website repository.\nAll data should be be on Zenodo, or DOI-citable repository, and linked to from the chapter. Institutional Nextcloud or other cloud storage solutions are not acceptable for data storage. In most cases this will be stored for you by the course coorindator.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#bibliography",
    "href": "chapter-writing-guidelines.html#bibliography",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.5 Bibliography",
    "text": "21.5 Bibliography\nEach chapters that use citations must have a ‘References’ header as the last line of the document.\nCitations can be specified in the text using the [@citation_key] syntax. This will automatically format the in text citation with parentheses. You should not use ‘free text’ or write manual citations. The references and keys themselves should be stored in the .bib (bibtex) file that you have created and specified in the markdown header of the page.\nFor example this textbook is Fellows Yates and Warinner (2024), or we really like AncientMetagenomeDir (Fellows Yates et al. 2021). For more citation styles please see the Quarto documentation. All references made in text will be by default rendered at the end of the document under the References header.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#summary",
    "href": "chapter-writing-guidelines.html#summary",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.6 Summary",
    "text": "21.6 Summary\nAll chapters should have a summary section at the end. This should:\n\nSummarise the key points of the chaper\nSummarise the main takeaway points the chapter\nShould ideally be summarised in a few bullet points",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-writing-guidelines.html#references",
    "href": "chapter-writing-guidelines.html#references",
    "title": "21  Chapter Writing Guidelines",
    "section": "21.7 References",
    "text": "21.7 References\n\n\n\n\nFellows Yates, James A, Aida Andrades Valtueña, Åshild J Vågene, Becky Cribdon, Irina M Velsko, Maxime Borry, Miriam J Bravo-Lopez, et al. 2021. “Community-Curated and Standardised Metadata of Published Ancient Metagenomic Samples with AncientMetagenomeDir.” Scientific Data 8 (1): 31. https://doi.org/10.1038/s41597-021-00816-y.\n\n\nFellows Yates, James A, and Christina Warinner. 2024. “Introduction to Ancient Metagenomics.” https://www.spaam-community.org/intro-to-ancient-metagenomics-book/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Chapter Writing Guidelines</span>"
    ]
  },
  {
    "objectID": "chapter-template.html",
    "href": "chapter-template.html",
    "title": "22  Chapter Page Template",
    "section": "",
    "text": "22.1 Section\nFirst section. One sentence per line. Emphasis should be with italics via underscores\nNew paragraph through a line break above.\nLists can be rendered with hyphens as bullets, and 4 spaces for sub-bullets:\nBold can be used to indicate first words of a list as above, or as a word that could be included in a glossary.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-template.html#section",
    "href": "chapter-template.html#section",
    "title": "22  Chapter Page Template",
    "section": "",
    "text": "Bullet 1\nBullet 2\n\nSub-bullet 1\nSub-bullet 2\n\nBullet 3\n\n\n\n22.1.1 Sub section\nHere is an example of a code block. The line before must end in a full stop.\necho \"Hello world!\"\nThe code block above has bash as the code identifier. Inline text should only be used for single commands, file extensions, or paths, i.e., not whole commands.\nFor code blocks that execute - by default only R - should be set to eval=FALSE. Pre-made output should be copy pasted and placed in a collapsible block. For example the following.\nprint('SPAAM Rules!')\n\n\n\n\n\n\nExpand see output\n\n\n\n\n\n[1] \"SPAAM Rules!\"\n\n\n\nCan be written as.\n```{.r eval=FALSE}\nprint('SPAAM Rules!')\n```\n\n\n\n\n\n\nExpand see output\n\n\n\n\n\n```{.r eval=FALSE}\n[1] \"SPAAM Rules!\"\n```\n\n\n\nOther code blocks will not execute by default, and can be simply set without setting no execution.\n```bash\necho \"SPAAM Rules!\"\n```",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-template.html#second-section",
    "href": "chapter-template.html#second-section",
    "title": "22  Chapter Page Template",
    "section": "22.2 Second section",
    "text": "22.2 Second section\nHere is an example of a figure. It must have a caption, and a label for referencing in text.\n\n\n\n\n\n\nFigure 22.1: The figure caption goes here\n\n\n\nThe label must be prefixed with fig-&lt;chaptername&gt;-&lt;figureidentifier&gt; and should be in lowercase.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-template.html#third-section",
    "href": "chapter-template.html#third-section",
    "title": "22  Chapter Page Template",
    "section": "22.3 Third section",
    "text": "22.3 Third section\nTables should also have a caption and a label for referencing in text.\n\n\n\nTable 22.1: Demonstration of pipe table syntax\n\n\n\n\n\nDefault\nLeft\nRight\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n22.3.1 Third sub section\n\n\n\n\n\n\nWarning\n\n\n\nThis is a call out block with a warning style.\nNote that we reserve some callout formatting styles for specific objects!\n\n\nFor example the end of every section should have a task or question with a hidden answer.\n\n\n\n\n\n\nQuestion\n\n\n\nYour question(s) goes here.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answer(s) goes here, in a by-default ‘hidden’ block the reader can open to view.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-template.html#summary",
    "href": "chapter-template.html#summary",
    "title": "22  Chapter Page Template",
    "section": "22.4 Summary",
    "text": "22.4 Summary\nPlease refer to Chapter Writing Guidelines for the full list of guidelines.\nCitations come from the associated .bib file, and are referenced in the text like this (Fellows Yates and Warinner 2024). References are automatically rendered for you at the end of the chapter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-template.html#references",
    "href": "chapter-template.html#references",
    "title": "22  Chapter Page Template",
    "section": "22.5 References",
    "text": "22.5 References\n\n\n\n\nFellows Yates, James A, and Christina Warinner. 2024. “Introduction to Ancient Metagenomics.” https://www.spaam-community.org/intro-to-ancient-metagenomics-book/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Chapter Page Template</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html",
    "href": "chapter-checklist.html",
    "title": "23  Chapter Checklist",
    "section": "",
    "text": "23.1 Structure",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#structure",
    "href": "chapter-checklist.html#structure",
    "title": "23  Chapter Checklist",
    "section": "",
    "text": "Each chapter should start with a brief introduction (without header!) and bullet point list of learning objectives\nEach chapter’s penultimate header should be # Summary and summarise list of key takeaway messages\nEach chapter that has used citations should have the last section as # References with nothing under underneath\nHeaders should either be two- or three-levels deep (e.g. ## or ###), and no deeper or fewer.\n\nThe single level header is reserved for the chapter title\n\nData should not be stored in the textbook, but rather linked to Zenodo archives (generally set up by the summer school organising team)\nEach major section should end in a mini-test question/answer block in a specially formatted call-out",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#language",
    "href": "chapter-checklist.html#language",
    "title": "23  Chapter Checklist",
    "section": "23.2 Language",
    "text": "23.2 Language\n\nAll text should be in British english\nPlease always use a spelling check tool\nCollective first person ‘we’/‘us’/‘our’ should be used (i.e., generally remove ‘I’/‘my’ or ‘you’/‘your’ etc. where possible)\n\nExceptions can be the learning objectivesm question/answer blocks, and summary sections",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#text-formatting",
    "href": "chapter-checklist.html#text-formatting",
    "title": "23  Chapter Checklist",
    "section": "23.3 Text Formatting",
    "text": "23.3 Text Formatting\n\nEach sentence should be on a new line\nEach paragraph should be separated from the previous paragraph with an empty line\nBullet points lists\n\nShould be indicated with a - and sub-points with a 4-space indent\nBullet points should start with a capital letter and end with no full stop\n\nEmphasis\n\nUse italics (_italics_) for emphasis with single underscores (_italics_)\nIf bold can be used for potential ‘glossary words’, or as to highlight first word of bullet point lists, and can be formatted with double asterisks (**double asterisks**)\n\nExternal URLs should be in the format [SPAAM Website](https://spaam-community.org)\nInternal links to other pages or sections are recommended where possible, and should be in the format [Chapter Checklist](chapter-checklist.qmd) for other pages or [Text Formatting](#text-formatting) for internal-sections\nDo not use quote blocks for text that is not a direct quote (i.e., don’t use the Markdown &gt; syntax)\n\nInstead use callouts\n\nPaths should generally be relative to the working directory of the chapter’s practical session\nPlace holder text (e.g. in paths etc.) should be lower case and in angled brackets, for example &lt;path&gt;/&lt;to&gt;/&lt;something&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#callouts",
    "href": "chapter-checklist.html#callouts",
    "title": "23  Chapter Checklist",
    "section": "23.4 Callouts",
    "text": "23.4 Callouts\n\nQuarto callouts should be used for any text that requires special attention from the reader, for example warnings, tips, etc.\nA special format of call-out is used for the end-of-section mini-test questions and answers (see), see the template or writing guidelines for examples",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#references",
    "href": "chapter-checklist.html#references",
    "title": "23  Chapter Checklist",
    "section": "23.5 References",
    "text": "23.5 References\n\nAll chapters that use citations must have a dedicated .bib file under assets/references/ and should be defined in the markdown header\nIn text citations are made like this @Fellows-Yates2024-tg or [@Fellows-Yates2024-tg]\n\nYou don’t need to write brackets yourself, the square brackets will be rendered accordingly\n\nBibliographies don’t need to be written manually, they will automatically be rendered for you at the end of the chapter",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#tables",
    "href": "chapter-checklist.html#tables",
    "title": "23  Chapter Checklist",
    "section": "23.6 Tables",
    "text": "23.6 Tables\n\nTables should be ideally formatted using markdown syntax\nAll tables should be referenced in the text\nAll tables should have a caption and label (writing guidelines for examples)\nCode generated tables should also have a label and caption defined in the code block using Quarto syntax (see writing guidelines for examples)\nLabels should be in form of: tbl-&lt;chapternamenopunctiation&gt;-&lt;descriptiveid&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#figures",
    "href": "chapter-checklist.html#figures",
    "title": "23  Chapter Checklist",
    "section": "23.7 Figures",
    "text": "23.7 Figures\n\nAll figure should have a caption and a label (writing guidelines for examples)\nLabels should be in form of: fig-&lt;chapternamenopunctiation&gt;-&lt;descriptiveid&gt;\nAll figures should be referenced in text\nCaptions should be descriptive to aid accessibility",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "chapter-checklist.html#code-blocks",
    "href": "chapter-checklist.html#code-blocks",
    "title": "23  Chapter Checklist",
    "section": "23.8 Code Blocks",
    "text": "23.8 Code Blocks\n\nText prior code blocks should end in a full stop\nInline code defined with single backticks should be used for a single word or object (e.g., variable, tool, .suffix)\nAll commands or longer code related text must go in a dedicated markdown triple backticks with the language specified (e.g., ```bash)\nDo not prefix commands with a $ or &gt; to indicate a command prompt\nCode blocks that automatically execute in Quarto (e.g. R), output should be set to eval=FALSE in the code block options, unless the code/plot is expected to executed on the fly\nOutput from large/long-running output should be placed in a collapsible call-out block (writing guidelines for examples)\nCode for self-guided chapter set-up should be wrapped in a collapsible note call-out block with a ‘Self guided: &lt;title&gt;’ format\nExample code that should not be run by the user should wrapped in a ‘warning’ call-out block with the title ‘Example command - do not run!’",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Chapter Checklist</span>"
    ]
  },
  {
    "objectID": "section-theory.html",
    "href": "section-theory.html",
    "title": "Theory",
    "section": "",
    "text": "Lectures",
    "crumbs": [
      "Theory"
    ]
  },
  {
    "objectID": "section-theory.html#lectures",
    "href": "section-theory.html#lectures",
    "title": "Theory",
    "section": "",
    "text": "Introduction to NGS Sequencing\nIn this chapter, we will introduce how we are able to convert DNA molecules to human readable sequences of A, C, T, and Gs, which we can subsequently can computationally analyse.\nThe field of Ancient DNA was revolutionised by the development of ‘Next Generation Sequencing’ (NGS), which relies on sequencing of millions of short fragments of DNA in parallel. The global leading DNA sequencing company is Illumina, and the technology used by Illumina is also most popular by palaeogeneticists. Therefore we will go through the various technologies behind Illumina next-generation sequencing machines.\nWe will also look at some important differences in the way different models of Illumina sequences work, and how this can influence ancient DNA research. Finally we will cover the structure of ‘FASTQ’ files, the most popular file format for representing the DNA sequence output of NGS sequencing machines.\n\n\nIntroduction to Ancient DNA\nThis chapter introduces you to ancient DNA and the enormous technological changes that have taken place since the field’s origins in 1984. Starting with the quagga and proceeding to microbes, we discuss where ancient microbial DNA can be found in the archaeological record and examine how ancient DNA is defined by its condition, not by a fixed age.\nWe next cover genome basics and take an in-depth look at the way DNA degrades over time. We detail the fundamentals of DNA damage, including the specific chemical processes that lead to DNA fragmentation and C-&gt;T miscoding lesions. We then demystify the DNA damage “smiley plot” and explain the how the plot’s symmetry or asymmetry is related to the specific enzymes used to repair DNA during library construction. We discuss how DNA damage is and is not clock-like, how to interpret and troubleshoot DNA damage plots, and how DNA damage patters can be used to authenticate ancient samples, specific taxa, and even sequences. We cover laboratory strategies for removing or reducing damage for greater accuracy for genotype calling, and we discuss the pros and cons of single-stranded library protocols. We then take a closer look at proofreading and non-proofreading polymerases and note key steps in NGS library preparation during which enzyme selection is critical in ancient DNA studies.\nFinally, we examine the big picture of why DNA damage matters in ancient microbial studies, and its effects on taxonomic identification of sequences, accurate genome mapping, and metagenomic assembly.\n\n\nIntroduction to Metagenomics\nThis chapter introduces you to the basics of metagenomics, with an emphasis on tools and approaches that are used to study ancient metagenomes. We begin by covering the basic terminology used in metagenomics and microbiome research and discuss how the field has changed over time. We examine the species concept for microbes and challenges that arise in classifying microbial species with respect to taxonomy and phylogeny. We then proceed to taxonomic profiling and discuss the pros and cons of different taxonomic profilers.\nAfterwards, we explain how to estimate preservation in ancient metagenomic samples and how to clean up your datasets and remove contaminants. Finally, we discuss strategies for exploring and comparing the ecological diversity in your samples, including different strategies for data normalization, distance calculation, and ordination.\n\n\nIntroduction to Microbial Genomics\nThe field of microbial genomics aims at the reconstruction and comparative analyses of genomes for gaining insights into the genetic foundation and evolution of various functional aspects such as virulence mechanisms in pathogens.\nIncluding data from ancient samples into this comparative assessment allows for studying these evolutionary changes through time. This, for example, provides insights into the emergence of human pathogens and their development in conjunction with human cultural transitions.\nIn this chapter we will look examples for how to utilise data from ancient genomes in comparative studies of human pathogens and today’s practical sessions will highlight methodologies for the reconstruction of microbial genomes.\n\n\nIntroduction to Evolutionary Biology\nPathogen genome data are an invaluable source of information about the evolution and spread of these organisms. This chapter will focus on molecular phylogenetic methods and the insight that they can reveal from improving our understanding of ancient evolution to the epidemiological dynamics of current outbreaks.\nThe first section will introduce phylognenetic trees and a set of core terms and concepts for their interpretation. Next, it will focus on some of the most popular approaches to inferring phylogenetic trees; those based on genetic distance, maximum likelihood, and Bayesian inference. These methods carry important considerations regarding the process that generated the data, computational capability, and data quality, all of which will be discussed here. Finally, we will direct our attention to examples of analyses of ancient and modern pathogens (e.g. Yersinia pestis, Hepatitis B virus, SARS-CoV-2) and critically assess appropriate choice of models and methods.",
    "crumbs": [
      "Theory"
    ]
  },
  {
    "objectID": "section-useful-skills.html",
    "href": "section-useful-skills.html",
    "title": "Useful Skills",
    "section": "",
    "text": "Introduction to the Command Line (Bare Bones Bash)\nComputational work in metagenomics often involves connecting to remote servers to run analyses via the use of command line tools. Bash is a programming language that is used as the main command line interface of most UNIX systems, and hence most remote servers a user will encounter. By learning bash, users can work more efficiently and reproducibly on these remote servers.\nIn this chapter we will introduce the basic concepts of bash and the command line. Students will learn how to move around the filesystem and interact with files, how to chain multiple commands together using “pipes”, and how to use loops and regular expressions to simplify the running of repetitive tasks.\nFinally, readers will learn how to create a bash script of their own, that can run a set of commands in sequence. This session requires no prior knowledge of bash or the command line and is meant to serve as an entry-level introduction to basic programming concepts that can be applicable in other programming languages too.",
    "crumbs": [
      "Useful Skills"
    ]
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-r",
    "href": "section-useful-skills.html#introduction-to-r",
    "title": "Useful Skills",
    "section": "Introduction to R",
    "text": "Introduction to R\nR is an interpreted programming language with a particular focus on data manipulation and analysis. It is very well established for scientific computing and supported by an active community developing and maintaining a huge ecosystem of software packages for both general and highly derived applications.\nIn this chapter we will explore how to use R for a simple, standard data science workflow. We will import, clean, and visualise context and summary data for and from our ancient metagenomics analysis workflow. On the way we will learn about the RStudio integrated development environment, dip into the basic logic and syntax of R and finally write some first useful code within the tidyverse framework for tidy, readable and reproducible data analysis.\nThis chapter will be targeted at beginners without much previous experience with R or programming and will kickstart your journey to master this powerful tool.",
    "crumbs": [
      "Useful Skills"
    ]
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-python",
    "href": "section-useful-skills.html#introduction-to-python",
    "title": "Useful Skills",
    "section": "Introduction to Python",
    "text": "Introduction to Python\nWhile R has traditionally been the language of choice for statistical programming for many years, Python has taken away some of the hegemony thanks to its numerous available libraries for machine and deep learning. With its ever increasing collection of libraries for statistics and bioinformatics, Python has now become one the most used language in the bioinformatics community.\nIn this tutorial, mirroring to the R session, we will learn how to use the Python libraries Pandas for importing, cleaning, and manipulating data tables, and producing simple plots with the Python sister library of ggplot2, plotnine.\nWe will also get ourselves familiar with the Jupyter notebook environment, often used by many high performance computing clusters as an interactive scripting interface.\nThis chapter is meant for participants with a basic experience in R/tidyverse, but assumes no prior knowledge of Python/Jupyter.",
    "crumbs": [
      "Useful Skills"
    ]
  },
  {
    "objectID": "section-useful-skills.html#introduction-to-git-and-github",
    "href": "section-useful-skills.html#introduction-to-git-and-github",
    "title": "Useful Skills",
    "section": "Introduction to Git and GitHub",
    "text": "Introduction to Git and GitHub\nAs the size and complexity of metagenomic analyses continues to expand, effectively organizing and tracking changes to scripts, code, and even data, continues to be a critical part of ancient metagenomic analyses. Furthermore, this complexity is leading to ever more collaborative projects, with input from multiple researchers.\nIn this chapter, we will introduce ‘Git’, an extremely popular version control system used in bioinformatics and software development to store, track changes, and collaborate on scripts and code. We will also introduce, GitHub, a cloud-based service for Git repositories for sharing data and code, and where many bioinformatic tools are stored. We will learn how to access and navigate course materials stored on GitHub through the web interface as well as the command line, and we will create our own repositories to store and share the output of upcoming sessions.",
    "crumbs": [
      "Useful Skills"
    ]
  },
  {
    "objectID": "section-ancient-metagenomics.html",
    "href": "section-ancient-metagenomics.html",
    "title": "Ancient Metagenomics",
    "section": "",
    "text": "Taxonomic Profiling\nTBC",
    "crumbs": [
      "Ancient Metagenomics"
    ]
  },
  {
    "objectID": "section-ancient-metagenomics.html#functional-profiling",
    "href": "section-ancient-metagenomics.html#functional-profiling",
    "title": "Ancient Metagenomics",
    "section": "Functional Profiling",
    "text": "Functional Profiling\nThe value of microbial taxonomy lies in the implied biochemical properties of a given taxon. Historically taxonomy was determined by growth characteristics and cell properties, and more recently through genomic and genetic similarity.\nThe genomic content of microbial taxa, specifically the presence or absence of genes, determine how those taxa interact with their environment, including all the biochemical processes they participate in, both internally and externally. Strains within any microbial species may have different genetic content and therefore may behave strikingly differently in the same environment, which cannot be determined through taxonomic profiling. Functionally profiling a microbial community, or determining all of the genes present independent of the species they are derived from, reveals the biochemical reactions and metabolic products the community may perform and produce, respectively.\nThis approach may provide insights to community activity and environmental interactions that are hidden when using taxonomic approaches alone. In this chapter we will perform functional profiling of metagenomic communities to assess their genetic content and inferred metabolic pathways.",
    "crumbs": [
      "Ancient Metagenomics"
    ]
  },
  {
    "objectID": "section-ancient-metagenomics.html#de-novo-assembly",
    "href": "section-ancient-metagenomics.html#de-novo-assembly",
    "title": "Ancient Metagenomics",
    "section": "De novo Assembly",
    "text": "De novo Assembly\nDe novo assembly of ancient metagenomic samples enables the recovery of the genetic information of organisms without requiring any prior knowledge about their genomes. Therefore, this approach is very well suited to study the biological diversity of species that have not been studied well or are simply not known yet.\nIn this chapter, we will show you how to prepare your sequencing data and subsequently de novo assemble them. Furthermore, we will then learn how we can actually evaluate what organisms we might have assembled and whether we obtained enough data to reconstruct a whole metagenome-assembled genome. We will particularly focus on the quality assessment of these reconstructed genomes and how we can ensure that we obtained high-quality genomes.",
    "crumbs": [
      "Ancient Metagenomics"
    ]
  },
  {
    "objectID": "section-ancient-genomics.html",
    "href": "section-ancient-genomics.html",
    "title": "Ancient Genomics",
    "section": "",
    "text": "Genome Mapping\nAn important step in the reconstruction of full genomic sequences is mapping. Even relatively short genomes usually cannot be sequenced as a single consecutive piece. Instead, millions of short sequence reads are generated from genomic fragments. These reads can be several hundred nucleotides in length but are considerably shorter for ancient DNA (aDNA).\nFor many applications involving comparative genomics these ‘reads’ have to be aligned to one or multiple already-reconstructed reference genomes in order to identify differences between the sequenced genome and any given contextual dataset. Aligning millions of short reads to much longer genome sequences in a time-efficient and accurate manner is a bioinformatics challenge for which numerous algorithms and tools have been developed. Each of these programs comes with a variety of parameters that can significantly alter the results and default settings are often not optimal when working with aDNA. Furthermore, read mapping procedures are often part of complex computational genomics pipelines and are therefore not directly applied by many users.\nIn this chapter we will take a look at specific challenges during read mapping when dealing with aDNA. We will get an overview of common input and output formats and manually apply a read mapper to aDNA data studying the direct effects of variation in mapping parameters. We will conclude the session with an outlook on genotyping, which is an important follow-up analysis step, that in turn is very relevant for down-stream analyses such as phylogenetics.",
    "crumbs": [
      "Ancient Genomics"
    ]
  },
  {
    "objectID": "section-ancient-genomics.html#phylogenomics",
    "href": "section-ancient-genomics.html#phylogenomics",
    "title": "Ancient Genomics",
    "section": "Phylogenomics",
    "text": "Phylogenomics\nPhylogenetic trees are central tools for studying the evolution of microorganisms, as they provide essential information about their relationships and timing of divergence between microbial strains.\nIn this chapter, we will introduce basic phylogenetic concepts and definitions, and provide guidance on how to interpret phylogenetic trees. We will then learn how to reconstruct phylogenetic trees from DNA sequences using various methods ranging from distance-based methods to probabilistic approaches, including maximum likelihood and Bayesian phylogenetics. In particular, we will learn how to use ancient genomic data to reconstruct time-calibrated trees with BEAST2.",
    "crumbs": [
      "Ancient Genomics"
    ]
  },
  {
    "objectID": "section-ancient-metagenomic-resources.html",
    "href": "section-ancient-metagenomic-resources.html",
    "title": "Ancient Metagenomic Resources",
    "section": "",
    "text": "Accessing Ancient Metagenome Data\nFinding relevant comparative data for your ancient metagenomic analysis is not trivial. While palaeogenomicists are very good at uploading their raw sequencing data to large sequencing data repositories such as the EBI’s ENA or NCBI’s SRA archives in standardised file formats, these files often have limited metadata. This often makes it difficult for researchers to search for and download relevent published data they wish to use use to augment their own analysis.\nAncientMetagenomeDir is a community project from the SPAAM community to make ancient metagenomic data more accessible. We curate a list of standardised metadata of all published ancient metagenomic samples and libraries, hosted on GitHub. In this chapter we will go through how to use the AncientMetagenomeDir repository and associated tools to find and download data for your own analyses. We will also discuss important things to consider when publishing your own data to make it more accessible for other researchers.",
    "crumbs": [
      "Ancient Metagenomic Resources"
    ]
  },
  {
    "objectID": "section-ancient-metagenomic-resources.html#ancient-metagenomic-pipelines",
    "href": "section-ancient-metagenomic-resources.html#ancient-metagenomic-pipelines",
    "title": "Ancient Metagenomic Resources",
    "section": "Ancient Metagenomic Pipelines",
    "text": "Ancient Metagenomic Pipelines\nAnalyses in the field of ancient DNA are growing, both in terms of the number of samples processed and in the diversity of our research questions and analytical methods. Computational pipelines are a solution to the challenges of big data, helping researchers to perform analyses efficiently and in a reproducible fashion. Today we will introduce nf-core/eager, one of several pipelines designed specifically for the preprocessing, analysis, and authentication of ancient next-generation sequencing data.\nIn this chapter we will learn how to practically perform basic analyses with nf-core/eager, starting from raw data and performing preprocessing, alignment, and genotyping of several Yersinia pestis-positive samples. We will gain an appreciation of the diversity of analyses that can be performed within nf-core eager, as well as where to find additional information for customizing your own nf-core/eager runs. Finally, we will learn how to use nf-core/eager to evaluate the quality and authenticity of our ancient samples. After this session, you will be ready to strike out into the world of nf-core/eager and build your own analyses from scratch!",
    "crumbs": [
      "Ancient Metagenomic Resources"
    ]
  },
  {
    "objectID": "deprecated-chapters.html",
    "href": "deprecated-chapters.html",
    "title": "Deprecated Chapters",
    "section": "",
    "text": "The following chapters are based on chapters that were taught in earlier edition of the summer schools but are no longer maintained and are kept here for historical purposes, as they may contain useful information. They may be outdated and may not reflect the current state of the field, and may not reach the same standards as currently maintained chapters.",
    "crumbs": [
      "Deprecated Chapters"
    ]
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "Appendices",
    "section": "",
    "text": "Important\n\n\n\n🚧 This page is still under construction 🚧",
    "crumbs": [
      "Appendices"
    ]
  }
]